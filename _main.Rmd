---
knit: "bookdown::render_book"
title: "The Epidemiologist R Handbook"  
description: "The Epi R Handbook is a R reference manual for applied epidemiology and public health."
author: "the handbook team"
date: "`r Sys.Date()`"
#url: 'https://github.com/nsbatra/Epi_R_handbook'
#twitter-handle: 
#cover-image: images/R_Handbook_Logo.png
site: bookdown::bookdown_site
# output: bookdown::gitbook:
#      config:
#           sharing:
#                twitter: yes
#                facebook: yes
#                whatsapp: yes
#                github: yes
documentclass: book
---





#  {-}

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Epi R Handbook banner beige 1500x500.png"))
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<meta name="description" content="The Epi R Handbook is an R reference manual for applied epidemiology and public health.">

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<!-- <span style="color: red;">**THIS IS A DRAFT.  REVIEWERS GIVE FEEDBACK AT THIS [LINK](https://forms.gle/4RNdRRLGx67xW9yq9)**.</span> -->

<!-- <span style="color: darkgreen;">**DO YOU LIKE THIS HANDBOOK? SHOULD SOMETHING BE CHANGED? PLEASE TELL US!**</span> -->

<!-- <form target="_blank" action="https://forms.gle/A5SnRVws7tPD15Js9"> -->
<!--     <input type="submit" value="FEEDBACK" /> -->
<!-- </form> -->



<!-- ======================================================= -->
<!-- ## An R reference manual for applied epidemiology and public health {.unnumbered} -->


<!-- <span style="color: brown;">**The Epi R Handbook is an R reference manual for applied epidemiology and public health.**</span> -->

<!-- ## About this handbook   -->

## R for applied epidemiology and public health {-}  

**Usage**: This handbook has been used over **1 million times by 300,000 people** around the world.

**Objective:** Serve as a quick R code reference manual (online and **[offline][Download handbook and data]**) with task-centered examples that address common epidemiological problems.  

**Are you just starting with R?** Try our **[free interactive tutorials](https://www.appliedepi.org/tutorial/)** or synchronous, virtual **[intro course](https://www.appliedepi.org/live/)** used by US CDC, WHO, and 75+ other health agencies and Field Epi Training Programs worldwide.  

**Languages:** [Vietnamese (Tiếng Việt)](https://epirhandbook.com/vn/), [Japanese (日本)](https://epirhandbook.com/jp/)  


<!-- * Use practical epi examples - cleaning case linelists, making transmission chains and epidemic curves, automated reports and dashboards, modeling incidence and making projections, demographic pyramids and rate standardization, record matching, outbreak detection, survey analysis, survival analysis, GIS basics, contact tracing, phylogenetic trees...   -->



<!-- **How is this different than other R books?**   -->

<!-- * It is community-driven - *written for epidemiologists by epidemiologists* in their spare time and leveraging experience in local, national, academic, and emergency settings   -->

<!-- Dual-column created based on the rmarkdown cookbook here: https://bookdown.org/yihui/rmarkdown-cookbook/multi-column.html -->

<!-- <form target="_blank" action="https://www.paypal.com/donate?hosted_button_id=YTEZELC8VBXV6"> -->
<!--     <input type="submit" value="Donate to maintain this resource" /> -->
<!-- </form> -->

<br>
<span style="color: black;">**Written by epidemiologists, for epidemiologists**</span>

:::: {style="display: flex;"}

::: {}
```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Applied_Epi_logo.png"))
```
:::


::: {.col data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::

::: {}

**[Applied Epi](http://www.appliedepi.org)** is a nonprofit organisation and grassroots movement of frontline epis from around the world. We write in our spare time to offer this resource to the community. Your encouragement and feedback is most welcome:  

* Visit our **[website](http://www.appliedepi.org)** and **[join our contact list](https://forms.gle/9awNd8syypTSYUsn7)**  
* **contact@appliedepi.org**, tweet **[\@appliedepi](https://twitter.com/appliedepi)**, or **[LinkedIn](www.linkedin.com/company/appliedepi)**  
* Submit issues to our **[Github repository](https://github.com/appliedepi/epiRhandbook_eng)**  

**We offer live R training** from instructors with decades of applied epidemiology experience - email us to discuss.
:::

::::


<form target="_blank" action="https://www.paypal.com/donate" method="post" target="_top">
<input type="hidden" name="hosted_button_id" value="YTEZELC8VBXV6" />
<input type="image" src="https://github.com/appliedepi/epiRhandbook_eng/raw/master/images/donate_button_long.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
<img alt="" border="0" src="https://www.paypal.com/en_US/i/scr/pixel.gif" />
</form>



<!-- ======================================================= -->
## How to use this handbook {-} 

* Browse the pages in the Table of Contents, or use the search box
* Click the "copy" icons to copy code  
* You can follow-along with [the example data][Download handbook and data]  

**Offline version**  

See instructions in the [Download handbook and data] page.  



<!-- ======================================================= -->
## Acknowledgements {-}  

This handbook is produced by a collaboration of epidemiologists from around the world drawing upon experience with organizations including local, state, provincial, and national health agencies, the World Health Organization (WHO), Médecins Sans Frontières / Doctors without Borders (MSF), hospital systems, and academic institutions.

This handbook is **not** an approved product of any specific organization. Although we strive for accuracy, we provide no guarantee of the content in this book.  


### Contributors {-}  

**Editor:** [Neale Batra](https://www.linkedin.com/in/neale-batra/) 

**Authors**: [Neale Batra](https://www.linkedin.com/in/neale-batra/), [Alex Spina](https://github.com/aspina7), [Paula Blomquist](https://www.linkedin.com/in/paula-bianca-blomquist-53188186/), [Finlay Campbell](https://github.com/finlaycampbell), [Henry Laurenson-Schafer](https://github.com/henryls1), [Isaac Florence](www.Twitter.com/isaacatflorence), [Natalie Fischer](https://www.linkedin.com/in/nataliefischer211/), [Aminata Ndiaye](https://twitter.com/aminata_fadl), [Liza Coyer]( https://www.linkedin.com/in/liza-coyer-86022040/), [Jonathan Polonsky](https://twitter.com/jonny_polonsky), [Yurie Izawa](https://ch.linkedin.com/in/yurie-izawa-a1590319), [Chris Bailey](https://twitter.com/cbailey_58?lang=en), [Daniel Molling](https://www.linkedin.com/in/daniel-molling-4005716a/), [Isha Berry](https://twitter.com/ishaberry2), [Emma Buajitti](https://twitter.com/buajitti), [Mathilde Mousset](https://mathildemousset.wordpress.com/research/), [Sara Hollis](https://www.linkedin.com/in/saramhollis/), Wen Lin  

**Reviewers and supporters**: Pat Keating,  [Amrish Baidjoe](https://twitter.com/Ammer_B), Annick Lenglet, Margot Charette, Danielly Xavier, Marie-Amélie Degail Chabrat, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Kate Kelsey, [Berhe Etsay](https://www.linkedin.com/in/berhe-etsay-5752b1154/), John Rossow, Mackenzie Zendt, James Wright, Laura Haskins, [Flavio Finger](ffinger.github.io), Tim Taylor, [Jae Hyoung Tim Lee](https://www.linkedin.com/in/jaehyoungtlee/), [Brianna Bradley](https://www.linkedin.com/in/brianna-bradley-bb8658155), [Wayne Enanoria](https://www.linkedin.com/in/wenanoria), Manual Albela Miranda, [Molly Mantus](https://www.linkedin.com/in/molly-mantus-174550150/), Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga  

**Illustrations**: Calder Fong  


<!-- **Editor-in-Chief:** Neale Batra  -->

<!-- **Project core team:** Neale Batra, Alex Spina, Amrish Baidjoe, Pat Keating, Henry Laurenson-Schafer, Finlay Campbell   -->

<!-- **Authors**: Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, [Isaac Florence](www.Twitter.com/isaacatflorence), Natalie Fischer, Aminata Ndiaye, Liza Coyer, Jonathan Polonsky, Yurie Izawa, Chris Bailey, Daniel Molling, Isha Berry, Emma Buajitti, Mathilde Mousset, Sara Hollis, Wen Lin   -->

<!-- **Reviewers**: Pat Keating, Mathilde Mousset, Annick Lenglet, Margot Charette, Isha Berry, Paula Blomquist, Natalie Fischer, Daniely Xavier, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Daniel Molling, Kate Kelsey, Berhe Etsay, John Rossow, Mackenzie Zendt, James Wright, Wayne Enanoria, Laura Haskins, Flavio Finger, Tim Taylor, Jae Hyoung Tim Lee, Brianna Bradley, Manual Albela Miranda, Molly Mantus, Priscilla Spencer, Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga   -->


### Funding and support {-}  

This book was primarily a volunteer effort that took thousands of hours to create.  

The handbook received some supportive funding via a COVID-19 emergency capacity-building grant from [TEPHINET](https://www.tephinet.org/), the global network of Field Epidemiology Training Programs (FETPs).  

Administrative support was provided by the EPIET Alumni Network ([EAN](https://epietalumni.net/)), with special thanks to Annika Wendland. EPIET is the European Programme for Intervention Epidemiology Training.  

Special thanks to Médecins Sans Frontières (MSF) Operational Centre Amsterdam (OCA) for their support during the development of this handbook.  


*This publication was supported by Cooperative Agreement number NU2GGH001873, funded by the Centers for Disease Control and Prevention through TEPHINET, a program of The Task Force for Global Health. Its contents are solely the responsibility of the authors and do not necessarily represent the official views of the Centers for Disease Control and Prevention, the Department of Health and Human Services, The Task Force for Global Health, Inc. or TEPHINET.*



### Inspiration {-}  

The multitude of tutorials and vignettes that provided knowledge for development of handbook content are credited within their respective pages.  

More generally, the following sources provided inspiration for this handbook:  
[The "R4Epis" project](https://r4epis.netlify.app/) (a collaboration between MSF and RECON)  
[R Epidemics Consortium (RECON)](https://www.repidemicsconsortium.org/)  
[R for Data Science book (R4DS)](https://r4ds.had.co.nz/)  
[bookdown: Authoring Books and Technical Documents with R Markdown](https://bookdown.org/yihui/bookdown/)  
[Netlify](https://www.netlify.com) hosts this website  


<!-- ### Image credits {-}   -->

<!-- Images in logo from US CDC Public Health Image Library) include [2013 Yemen looking for mosquito breeding sites](https://phil.cdc.gov/Details.aspx?pid=19623), [Ebola virus](https://phil.cdc.gov/Details.aspx?pid=23186), and [Survey in Rajasthan](https://phil.cdc.gov/Details.aspx?pid=19838).   -->


## Terms of Use and Contribution {-}  

### License {.unnumbered} 

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a> Applied Epi Incorporated, 2021 <br />This work is licensed by Applied Epi Incorporated under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.


Academic courses and epidemiologist training programs are welcome to use this handbook with their students, but please send us an email to let us know. If you have questions about your intended use, email **epiRhandbook@gmail.com**.  


### Citation {.unnumbered}

Batra, Neale, et al. The Epidemiologist R Handbook. 2021.  <a rel="license" href="https://zenodo.org/badge/231610102.svg"><img alt="DOI" style="border-width:0" src="https://zenodo.org/badge/231610102.svg" /></a><br />

### Contribution {.unnumbered}  

If you would like to make a content contribution, please contact with us first via Github issues or by email. We are implementing a schedule for updates and are creating a contributor guide.  

Please note that the epiRhandbook project is released with a [Contributor Code of Conduct](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:index.Rmd-->

# (PART) About this book {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_about_book.Rmd-->

# Editorial and technical notes { }

In this page we describe the philosophical approach, style, and specific editorial decisions made during the creation of this handbook.  


## Approach and style

The potential audience for this book is large. It will surely be used by people very new to R, and also by experienced R users looking for best practices and tips. So it must be both accessible and succinct. Therefore, our approach was to provide *just enough* text explanation that someone very new to R can apply the code and follow what the code is doing.  

A few other points:  

* This is a code reference book accompanied by relatively brief examples - *not* a thorough textbook on R or data science  
* This is a *R handbook* for use within applied epidemiology - not a manual on the methods or science of applied epidemiology  
* This is intended to be a living document - optimal R packages for a given task change often and we welcome discussion about which to emphasize in this handbook  




### R packages {.unnumbered}

**So many choices**  

One of the most challenging aspects of learning R is knowing which R package to use for a given task. It is a common occurrence to struggle through a task only later to realize - hey, there's an R package that does all that in one command line!  

In this handbook, we try to offer you at least two ways to complete each task: one tried-and-true method (probably in **base** R or **tidyverse**) and one special R package that is custom-built for that purpose. We want you to have a couple options in case you can't download a given package or it otherwise does not work for you.  

In choosing which packages to use, we prioritized R packages and approaches that have been tested and vetted by the community, minimize the number of packages used in a typical work session, that are stable (not changing very often), and that accomplish the task simply and cleanly  

This handbook generally prioritizes R packages and functions from the **tidyverse**. Tidyverse is a collection of R packages designed for data science that share underlying grammar and data structures. All tidyverse packages can be installed or loaded via the **tidyverse** package. Read more at the [tidyverse website](https://www.tidyverse.org/).  

When applicable, we also offer code options using **base** R - the packages and functions that come with R at installation. This is because we recognize that some of this book's audience may not have reliable internet to download extra packages.  

**Linking functions to packages explicitly**

It is often frustrating in R tutorials when a function is shown in code, but you don't know which package it is from! We try to avoid this situation.  

In the narrative text, package names are written in bold (e.g. **dplyr**) and functions are written like this: `mutate()`. We strive to be explicit about which package a function comes from, either by referencing the package in nearby text or by specifying the package explicitly in the code like this: `dplyr::mutate()`. It may look redundant, but we are doing it on purpose.  

See the page on [R basics] to learn more about packages and functions.  


### Code style {.unnumbered}

In the handbook, we frequently utilize "new lines", making our code appear "long". We do this for a few reasons:  

* We can write explanatory comments with `#` that are adjacent to each little part of the code  
* Generally, longer (vertical) code is easier to read  
* It is easier to read on a narrow screen (no sideways scrolling needed)  
* From the indentations, it can be easier to know which arguments belong to which function  

As a result, code that *could* be written like this:  

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>%  # group rows by hospital
  slice_max(date, n = 1, with_ties = F) # if there's a tie (of date), take the first row
```

...is written like this:  

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>% # group rows by hospital
  slice_max(
    date,                # keep row per group with maximum date value 
    n = 1,               # keep only the single highest row 
    with_ties = F)       # if there's a tie (of date), take the first row
```

R code is generally not affected by new lines or indentations. When writing code, if you initiate a new line after a comma it will apply automatic indentation patterns. 

We also use lots of spaces (e.g. `n = 1` instead of `n=1`) because it is easier to read. Be kind to the people reading your code!  



### Nomenclature {.unnumbered}  

In this handbook, we generally reference "columns" and "rows" instead of "variables" and "observations". As explained in this primer on ["tidy data"](https://tidyr.tidyverse.org/articles/tidy-data.html), most epidemiological statistical datasets consist structurally of rows, columns, and values.  

*Variables* contain the values that measure the same underlying attribute (like age group, outcome, or date of onset). *Observations* contain all values measured on the same unit (e.g. a person, site, or lab sample). So these aspects can be more difficult to tangibly define.  

In "tidy" datasets, each column is a variable, each row is an observation, and each cell is a single value. However some datasets you encounter will not fit this mold - a "wide" format dataset may have a variable split across several columns (see an example in the [Pivoting data] page). Likewise, observations could be split across several rows.  

Most of this handbook is about managing and transforming data, so referring to the concrete data structures of rows and columns is more relevant than the more abstract observations and variables. Exceptions occur primarily in pages on data analysis, where you will see more references to variables and observations.  



### Notes {.unnumbered} 

Here are the types of notes you may encounter in the handbook:  

<span style="color: black;">**_NOTE:_** This is a note</span>  
<span style="color: darkgreen;">**_TIP:_** This is a tip.</span>  
<span style="color: orange;">**_CAUTION:_** This is a cautionary note.</span>  
<span style="color: red;">**_DANGER:_** This is a warning.</span>  



## Editorial decisions  

Below, we track significant editorial decisions around package and function choice. If you disagree or want to offer a new tool for consideration, please join/start a conversation on our [Github page](https://github.com/appliedepi/epirhandbook_eng).    


**Table of package, function, and other editorial decisions**  


Subject           |     Considered      |   Outcome              |    Brief rationale   
----------------- | --------------------|------------------------|-----------------------------------------------
General coding approach|**tidyverse**, **data.table**, **base**|**tidyverse**, with a page on **data.table**, and mentions of **base** alternatives for readers with no internet|**tidyverse** readability, universality, most-taught  
Package loading|`library()`,`install.packages()`, `require()`, **pacman**|**pacman**|Shortens and simplifies code for most multi-package install/load use-cases
Import and export|**rio**, many other packages|**rio**|Ease for many file types
Grouping for summary statistics|**dplyr** `group_by()`, **stats** `aggregate()`|**dplyr** `group_by()`|Consistent with **tidyverse** emphasis
Pivoting|**tidyr** (pivot functions), **reshape2** (melt/cast), **tidyr** (spread/gather)|**tidyr** (pivot functions)|**reshape2** is retired, **tidyr** uses pivot functions as of v1.0.0
Clean column names|**linelist**, **janitor**|**janitor**|Consolidation of packages emphasized
Epiweeks |**lubridate**, **aweek**, **tsibble**, **zoo**|**lubridate** generally, the others for specific cases| **lubridate's** flexibility, consistency, package maintenance prospects  
ggplot labels |`labs()`, `ggtitle()`/`ylab()`/`xlab()` |`labs()` |all labels in one place, simplicity  
Convert to factor |`factor()`, **forcats**|**forcats**|its various functions also convert to factor in same command
Epidemic curves|**incidence**, **ggplot2**, **EpiCurve**|**incidence2** as quick, **ggplot2** as detailed|dependability
Concatenation|`paste()`, `paste0()`, `str_glue()`, `glue()`|`str_glue()`|More simple syntax than paste functions; within **stringr**


## Major revisions  


Date           |Major changes        
---------------| ------------------------------------------    
10 May 2021    |Release of version 1.0.0    
20 Nov 2022    |Release of version 1.0.1

**NEWS**
With version 1.0.1 the following changes have been implemented:  

* Update to R version 4.2  
* Data cleaning: switched {linelist} to {matchmaker}, removed unnecessary line from `case_when()` example  
* Dates: switched {linelist} `guess_date()` to {parsedate} `parse_date()`
* Pivoting: slight update to `pivot_wider()` `id_cols=`  
* Survey analysis: switched `plot_age_pyramid()` to `age_pyramid()`, slight change to alluvial plot code  
* Heat plots: added `ungroup()` to `agg_weeks` chunk  
* Interactive plots: added `ungroup()` to chunk that makes `agg_weeks` so that `expand()` works as intended  
* Time series: added `data.frame()` around objects within all `trending::fit()` and `predict()` commands  
* Combinations analysis: Switch `case_when()` to `ifelse()` and added optional `across()` code for preparing the data  
* Transmission chains: Update to more recent version of {epicontacts}



## Session info (R, RStudio, packages)  

Below is the information on the versions of R, RStudio, and R packages used during this rendering of the Handbook.  


```{r}
sessioninfo::session_info()
```




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/editorial_style.Rmd-->

# Download handbook and data  


<!-- Note to self: If you want to create a download link to Github, right-click the "View Raw" button on Github, copy the address, and use that in the HTML below. -->




## Download offline handbook  

You can download the offline version of this handbook as an HTML file so that you can view the file in your web browser even if you no longer have internet access. If you are considering offline use of the Epi R Handbook here are a few things to consider:  

* When you open the file it may take a minute or two for the images and the Table of Contents to load  
* The offline handbook has a slightly different layout - one very long page with Table of Contents on the left. To search for specific terms use Ctrl+f (Cmd-f)  
* See the [Suggested packages] page to assist you with installing appropriate R packages before you lose internet connectivity  
* Install our R package **epirhandbook** that contains all the example data (install process described below)  

**There are two ways you can download the handbook:**  



### Use download link {.unnumbered}  

For quick access, **right-click** [this link](https://github.com/appliedepi/epirhandbook_eng/raw/master/offline_long/Epi_R_Handbook_offline.html) **and select "Save link as"**.  

If on a Mac, use Cmd+click. If on a mobile, press and hold the link and select "Save link". The handbook will download to your device. If a screen with raw HTML code appears, ensure you have followed the above instructions or try Option 2.  




### Use our R package {.unnumbered}  

We offer an R package called **epirhandbook**. It includes a function `download_book()` that downloads the handbook file from our Github repository to your computer.  


This package also contains a function `get_data()` that downloads all the example data to your computer.  

Run the following code to install our R package **epirhandbook** from the [Github repository *appliedepi*](https://github.com/appliedepi/epirhandbook). This package is not on CRAN, so use the special function `p_install_gh()` to install it from Github.  


```{r, eval=F}
# install the latest version of the Epi R Handbook package
pacman::p_install_gh("appliedepi/epirhandbook")
```

Now, load the package for use in your current R session:  

```{r, eval=F}
# load the package for use
pacman::p_load(epirhandbook)
```

Next, run the package's function `download_book()` (with empty parentheses) to download the handbook to your computer. Assuming you are in RStudio, a window will appear allowing you to select a save location.  

```{r, eval=F}
# download the offline handbook to your computer
download_book()
```





## Download data to follow along  

To "follow along" with the handbook pages, you can download the example data and outputs.  

### Use our R package {.unnumbered}  

The easiest approach to download all the data is to install our R package **epirhandbook**. It contains a function `get_data()` that saves all the example data to a folder of your choice on your computer.  

To install our R package **epirhandbook**, run the following code. This package is not on CRAN, so use the function `p_install_gh()` to install it. The input is referencing our Github organisation ("*appliedepi*") and the **epirhandbook** package.  

```{r, eval=F}
# install the latest version of the Epi R Handbook package
pacman::p_install_gh("appliedepi/epirhandbook")
```


Now, load the package for use in your current R session:  

```{r, eval=F}
# load the package for use
pacman::p_load(epirhandbook)
```

Next, use the package's function `get_data()` to download the example data to your computer. Run `get_data("all")` to get *all* the example data, or provide a specific file name and extension within the quotes to retrieve only one file.  

The data have already been downloaded with the package, and simply need to be transferred out to a folder on your computer. A pop-up window will appear, allowing you to select a save folder location. We suggest you create a new "data" folder as there are about 30 files (including example data and example outputs).  

```{r, eval=F}
# download all the example data into a folder on your computer
get_data("all")

# download only the linelist example data into a folder on your computer
get_data(file = "linelist_cleaned.rds")

```


```{r, eval=F}
# download a specific file into a folder on your computer
get_data("linelist_cleaned.rds")
```

Once you have used `get_data()` to save a file to your computer, you will still need to import it into R. See the [Import and export] page for details.  

If you wish, you can review all the data used in this handbook in the **["data" folder](https://github.com/appliedepi/epirhandbook_eng/tree/master/data)** of our Github repository.  



### Download one-by-one {.unnumbered}  

This option involves downloading the data file-by-file from our Github repository via either a link or an R command specific to the file. Some file types allow a download button, while others can be downloaded via an R command.  


#### Case linelist {.unnumbered}

This is a fictional Ebola outbreak, expanded by the handbook team from the `ebola_sim` practice dataset in the **outbreaks** package.  

* <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_raw.xlsx' class='download-button'>Click to download the "raw" linelist (.xlsx)</span></a>. The "raw" case linelist is an Excel spreadsheet with messy data. Use this to follow-along with the [Cleaning data and core functions] page.  

* <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>Click to download the "clean" linelist (.rds)</a>. Use this file for all other pages of this handbook that use the linelist. A .rds file is an R-specific file type that preserves column classes. This ensures you will have only minimal cleaning to do after importing the data into R.  

*Other related files:*  

* <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.xlsx' class='download-button'>Click to download the "clean" linelist as an Excel file</a>

* Part of the cleaning page uses a "cleaning dictionary" (.csv file). You can load it directly into R by running the following commands:   

```{r, eval=F}
pacman::p_load(rio) # install/load the rio package

# import the file directly from Github
cleaning_dict <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/cleaning_dict.csv")
```


#### Malaria count data {#data_malaria .unnumbered}  

These data are fictional counts of malaria cases by age group, facility, and day. A .rds file is an R-specific file type that preserves column classes. This ensures you will have only minimal cleaning to do after importing the data into R.  

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/malaria_facility_count_data.rds' class='download-button'>
	Click to download
	<span>the malaria count data (.rds file)</span>
</a>


#### Likert-scale data {.unnumbered}  

These are fictional data from a Likert-style survey, used in the page on [Demographic pyramids and Likert-scales]. You can load these data directly into R by running the following commands:    

```{r, eval=F}
pacman::p_load(rio) # install/load the rio package

# import the file directly from Github
likert_data <- import("https://raw.githubusercontent.com/appliedepi/epirhandbook_eng/master/data/likert_data.csv")
```


#### Flexdashboard {.unnumbered}  

Below are links to the file associated with the page on [Dashboards with R Markdown]:  

* To download the R Markdown for the outbreak dashboard, right-click this [link](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/flexdashboard/outbreak_dashboard.Rmd) (Cmd+click for Mac) and select "Save link as".  
* To download the HTML dashboard, right-click this [link](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/flexdashboard/outbreak_dashboard_test.html) (Cmd+click for Mac) and select "Save link as".  

#### Contact Tracing {.unnumbered} 

The [Contact Tracing] page demonstrated analysis of contact tracing data, using example data from [Go.Data](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting). The data used in the page can be downloaded as .rds files by clicking the following links:  

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the case investigation data (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the contact registration data (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the contact follow-up data (.rds file)</span>
</a>



<span style="color: black;">**_NOTE:_** Structured contact tracing data from other software (e.g. KoBo, DHIS2 Tracker, CommCare) may look different. If you would like to contribute alternative sample data or content for this page, please [contact us](#contact_us).</span> 

<span style="color: darkgreen;">**_TIP:_** If you are deploying Go.Data and want to connect to your instance's API, see the Import and export page [(API section)](#import_api) and the [Go.Data Community of Practice](https://community-godata.who.int/).</span>


#### GIS {.unnumbered}  

Shapefiles have many sub-component files, each with a different file extention. One file will have the ".shp" extension, but others may have ".dbf", ".prj", etc.  

The [GIS basics] page provides links to the *Humanitarian Data Exchange* website where you can download the shapefiles directly as zipped files.  

For example, the health facility points data can be downloaded [here](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities). Download "hotosm_sierra_leone_health_facilities_points_shp.zip". Once saved to your computer, "unzip" the folder. You will see several files with different extensions (e.g. ".shp", ".prj", ".shx") - all these must be saved to the same folder on your computer. Then to import into R, provide the file path and name of the ".shp" file to `st_read()` from the **sf** package (as described in the [GIS basics] page).  

If you follow Option 1 to download all the example data (via our R package **epirhandbook**), all the shapefiles are included.  


Alternatively, you can download the shapefiles from the R Handbook Github "data" folder (see the "gis" sub-folder). However, be aware that you will need to download *each* sub-file individually to your computer. In Github, click on each file individually and download them by clicking on the "Download" button. Below, you can see how the shapefile "sle_adm3" consists of many files - each of which would need to be downloaded from Github.  

```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```


#### Phylogenetic trees {.unnumbered}  

See the page on [Phylogenetic trees]. Newick file of phylogenetic tree constructed from whole genome sequencing of 299 Shigella sonnei samples and corresponding sample data (converted to a text file). The Belgian samples and resulting data are kindly provided by the Belgian NRC for Salmonella and Shigella in the scope of a project conducted by an ECDC EUPHEM Fellow, and will also be published in a manuscript. The international data are openly available on public databases (ncbi) and have been previously published.  

* To download the "Shigella_tree.txt" phylogenetic tree file, right-click this [link](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/phylo/Shigella_tree.txt) (Cmd+click for Mac) and select "Save link as".  
* To download the "sample_data_Shigella_tree.csv" with additional information on each sample, right-click this [link](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/phylo/sample_data_Shigella_tree.csv) (Cmd+click for Mac) and select "Save link as".  
* To see the new, created subset-tree, right-click this [link](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/phylo/Shigella_subtree_2.txt) (Cmd+click for Mac) and select "Save link as". The .txt file will download to your computer.  


You can then import the .txt files with `read.tree()` from the **ape** package, as explained in the page.

```{r, eval=F}
ape::read.tree("Shigella_tree.txt")
```


#### Standardization {.unnumbered}  

See the page on [Standardised rates]. You can load the data directly from our Github repository on the internet into your R session with the following commands:  


```{r, eval=F}
# install/load the rio package
pacman::p_load(rio) 

##############
# Country A
##############
# import demographics for country A directly from Github
A_demo <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics.csv")

# import deaths for country A directly from Github
A_deaths <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryA.csv")

##############
# Country B
##############
# import demographics for country B directly from Github
B_demo <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics_2.csv")

# import deaths for country B directly from Github
B_deaths <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryB.csv")


###############
# Reference Pop
###############
# import demographics for country B directly from Github
standard_pop_data <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/world_standard_population_by_sex.csv")
```



#### Time series and outbreak detection {#data_outbreak .unnumbered}  

See the page on [Time series and outbreak detection]. We use campylobacter cases reported in Germany 2002-2011, as available from the **surveillance** R package. (*nb.* this dataset has been adapted from the original, in that 3 months of data have been deleted from the end of 2011 for demonstration purposes)

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
	Click to download
	<span> Campylobacter in Germany (.xlsx)</span>
</a>

We also use climate data from Germany 2002-2011 (temperature in degrees celsius and rain fail in millimetres) . These were downloaded from the EU Copernicus satellite reanalysis dataset using the **ecmwfr** package. You will need to download all of these and import them with `stars::read_stars()` as explained in the time series page.  

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2002.nc' class='download-button'>
	Click to download
	<span> Germany weather 2002 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2003.nc' class='download-button'>
	Click to download
	<span> Germany weather 2003 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2004.nc' class='download-button'>
	Click to download
	<span> Germany weather 2004 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2005.nc' class='download-button'>
	Click to download
	<span> Germany weather 2005 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2006.nc' class='download-button'>
	Click to download
	<span> Germany weather 2006 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2007.nc' class='download-button'>
	Click to download
	<span> Germany weather 2007 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2008.nc' class='download-button'>
	Click to download
	<span> Germany weather 2008 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2009.nc' class='download-button'>
	Click to download
	<span> Germany weather 2009 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2010.nc' class='download-button'>
	Click to download
	<span> Germany weather 2010 (.nc file)</span>
</a> 

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2011.nc' class='download-button'>
	Click to download
	<span> Germany weather 2011 (.nc file)</span>
</a>



#### Survey analysis {#data_survey .unnumbered}  

For the [survey analysis](https://epirhandbook.com/survey-analysis.html) page we use fictional mortality survey data based off MSF OCA survey templates. This fictional data was generated as part of the ["R4Epis" project](https://r4epis.netlify.app/).

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/surveys/survey_data.xlsx' class='download-button'>
	Click to download
	<span> Fictional survey data (.xlsx)</span>
</a>

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/surveys/survey_dict.xlsx' class='download-button'>
	Click to download
	<span> Fictional survey data dictionary (.xlsx)</span>
</a>

<a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/surveys/population.xlsx' class='download-button'>
	Click to download
	<span> Fictional survey population data (.xlsx)</span>
</a>




#### Shiny {#data_shiny .unnumbered}  

The page on [Dashboards with Shiny] demonstrates the construction of a simple app to display malaria data.  

To download the R files that produce the Shiny app:  

You can <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/malaria_app/app.R' class='download-button'>
	click here to download the app.R file<span> that contains both the UI and Server code for the Shiny app.</span></a>

You can <a href='https://github.com/appliedepi/epirhandbook_eng/blob/master/data/malaria_app/data/facility_count_data.rds' class='download-button'>
	click here to download the facility_count_data.rds file<span></a> that contains malaria data for the Shiny app. Note that you may need to store it within a "data" folder for the here() file paths to work correctly.  

You can <a href='https://github.com/appliedepi/epirhandbook_eng/blob/master/data/malaria_app/global.R' class='download-button'>
	click here to download the global.R file<span></a> that should run prior to the app opening, as explained in the page.
	
You can <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/malaria_app/funcs/plot_epicurve.R' class='download-button'>
	click here to download the plot_epicurve.R file<span></a> that is sourced by global.R. Note that you may need to store it within a "funcs" folder for the here() file paths to work correctly.


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_used.Rmd-->

# (PART) Basics {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_basics.Rmd-->

# R Basics

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "basics_header_close.png"))
```

Welcome!

This page reviews the essentials of R. It is not intended to be a comprehensive tutorial, but it provides the basics and can be useful for refreshing your memory. The section on [Resources for learning](#learning) links to more comprehensive tutorials.

Parts of this page have been adapted with permission from the [R4Epis project](https://r4epis.netlify.app/).

See the page on [Transition to R] for tips on switching to R from STATA, SAS, or Excel.

```{r, echo=F}
# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
pacman::p_load(apyramid)
```

<!-- ======================================================= -->

## Why use R?

As stated on the [R project website](https://www.r-project.org/about.html), R is a programming language and environment for statistical computing and graphics. It is highly versatile, extendable, and community-driven.

**Cost**

R is free to use! There is a strong ethic in the community of free and open-source material.

**Reproducibility**

Conducting your data management and analysis through a programming language (compared to Excel or another primarily point-click/manual tool) enhances **reproducibility**, makes **error-detection** easier, and eases your workload.

**Community**

The R community of users is enormous and collaborative. New packages and tools to address real-life problems are developed daily, and vetted by the community of users. As one example, [R-Ladies](https://rladies.org/) is a worldwide organization whose mission is to promote gender diversity in the R community, and is one of the largest organizations of R users. It likely has a chapter near you!

## Key terms

**RStudio** - RStudio is a Graphical User Interface (GUI) for easier use of **R**. Read more [in the RStudio section](#rstudio).

**Objects** - Everything you store in R - datasets, variables, a list of village names, a total population number, even outputs such as graphs - are *objects* which are *assigned a name* and *can be referenced* in later commands. Read more [in the Objects section](#objects).

**Functions** - A function is a code operation that accept inputs and returns a transformed output. Read more [in the Functions section](#functions).

**Packages** - An R package is a shareable bundle of functions. Read more [in the Packages section](#packages).

**Scripts** - A script is the document file that hold your commands. Read more [in the Scripts section](#scripts)

## Resources for learning {#learning}

### Resources within RStudio {.unnumbered}

**Help documentation**

Search the RStudio "Help" tab for documentation on R packages and specific functions. This is within the pane that also contains Files, Plots, and Packages (typically in the lower-right pane). As a shortcut, you can also type the name of a package or function into the R console after a question-mark to open the relevant Help page. Do not include parentheses.

For example: `?filter` or `?diagrammeR`.

**Interactive tutorials**

There are several ways to learn R interactively *within* RStudio.

RStudio itself offers a Tutorial pane that is powered by the [**learnr**](https://blog.rstudio.com/2020/02/25/rstudio-1-3-integrated-tutorials/) R package. Simply install this package and open a tutorial via the new "Tutorial" tab in the upper-right RStudio pane (which also contains Environment and History tabs).

The R package [**swirl**](https://swirlstats.com/) offers interactive courses in the R Console. Install and load this package, then run the command `swirl()` (empty parentheses) in the R console. You will see prompts appear in the Console. Respond by typing in the Console. It will guide you through a course of your choice.

### Cheatsheets {.unnumbered}

There are many PDF "cheatsheets" available on the [RStudio website](https://rstudio.com/resources/cheatsheets/), for example:

-   Factors with **forcats** package\
-   Dates and times with **lubridate** package\
-   Strings with **stringr** package\
-   iterative opertaions with **purrr** package\
-   Data import\
-   Data transformation cheatsheet with **dplyr** package\
-   R Markdown (to create documents like PDF, Word, Powerpoint...)\
-   Shiny (to build interactive web apps)\
-   Data visualization with **ggplot2** package\
-   Cartography (GIS)\
-   **leaflet** package (interactive maps)\
-   Python with R (**reticulate** package)

This is an online R resource specifically for [Excel users](https://jules32.github.io/r-for-excel-users/)

### Twitter {.unnumbered}

R has a vibrant twitter community where you can learn tips, shortcuts, and news - follow these accounts:

-   Follow us! [\@epiRhandbook](https://twitter.com/epirhandbook)\
-   R Function A Day [\@rfuntionaday](https://twitter.com/rfunctionaday) is an *incredible* resource\
-   R for Data Science [\@rstats4ds](https://twitter.com/rstats4ds?lang=en)\
-   RStudio [\@RStudio](https://twitter.com/rstudio?lang=en)\
-   RStudio Tips [\@rstudiotips](https://twitter.com/rstudiotips)\
-   R-Bloggers [\@Rbloggers](https://twitter.com/Rbloggers)\
-   R-ladies [\@RLadiesGlobal](https://twitter.com/RLadiesGlobal)\
-   Hadley Wickham [\@hadleywickham](https://twitter.com/hadleywickham?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)

Also:

**\#epitwitter** and **\#rstats**

### Free online resources {.unnumbered}

A definitive text is the [R for Data Science](https://r4ds.had.co.nz/) book by Garrett Grolemund and Hadley Wickham

The [R4Epis](https://r4epis.netlify.app/) project website aims to "develop standardised data cleaning, analysis and reporting tools to cover common types of outbreaks and population-based surveys that would be conducted in an MSF emergency response setting." You can find R basics training materials, templates for RMarkdown reports on outbreaks and surveys, and tutorials to help you set them up.

### Languages other than English {.unnumbered}

[Materiales de RStudio en Español](https://www.rstudio.com/collections/espanol/)

[Introduction à R et au tidyverse (Francais)](https://juba.github.io/tidyverse/index.html)

<!-- ======================================================= -->

## Installation

### R and RStudio {.unnumbered}

**How to install R**

Visit this website <https://www.r-project.org/> and download the latest version of R suitable for your computer.

**How to install RStudio**

Visit this website <https://rstudio.com/products/rstudio/download/> and download the latest free Desktop version of RStudio suitable for your computer.

**Permissions**\
Note that you should install R and RStudio to a drive where you have read and write permissions. Otherwise, your ability to install R packages (a frequent occurrence) will be impacted. If you encounter problems, try opening RStudio by right-clicking the icon and selecting "Run as administrator". Other tips can be found in the page [R on network drives].

**How to update R and RStudio**

Your version of R is printed to the R Console at start-up. You can also run `sessionInfo()`.

To update R, go to the website mentioned above and re-install R. Alternatively, you can use the **installr** package (on Windows) by running `installr::updateR()`. This will open dialog boxes to help you download the latest R version and update your packages to the new R version. More details can be found in the **installr** [documentation](https://www.r-project.org/nosvn/pandoc/installr.html).

Be aware that the old R version will still exist in your computer. You can temporarily run an older version (older "installation") of R by clicking "Tools" -\> "Global Options" in RStudio and choosing an R version. This can be useful if you want to use a package that has not been updated to work on the newest version of R.

To update RStudio, you can go to the website above and re-download RStudio. Another option is to click "Help" -\> "Check for Updates" within RStudio, but this may not show the very latest updates.

To see which versions of R, RStudio, or packages were used when this Handbook as made, see the page on [Editorial and technical notes].

### Other software you *may* need to install {.unnumbered}

-   TinyTeX (*for compiling an RMarkdown document to PDF*)\
-   Pandoc (*for compiling RMarkdown documents*)\
-   RTools (*for building packages for R*)\
-   phantomjs (*for saving still images of animated networks, such as transmission chains*)

#### TinyTex {.unnumbered}

TinyTex is a custom LaTeX distribution, useful when trying to produce PDFs from R.\
See <https://yihui.org/tinytex/> for more informaton.

To install TinyTex from R:

```{r, eval=F}
install.packages('tinytex')
tinytex::install_tinytex()
# to uninstall TinyTeX, run tinytex::uninstall_tinytex()
```

#### Pandoc {.unnumbered}

Pandoc is a document converter, a separate software from R. **It comes bundled with RStudio and should not need to be downloaded.** It helps the process of converting Rmarkdown documents to formats like .pdf and adding complex functionality.

#### RTools {.unnumbered}

RTools is a collection of software for building packages for R

Install from this website: <https://cran.r-project.org/bin/windows/Rtools/>

#### phantomjs {.unnumbered}

This is often used to take "screenshots" of webpages. For example when you make a transmission chain with **epicontacts** package, an HTML file is produced that is interactive and dynamic. If you want a static image, it can be useful to use the [**webshot**](https://wch.github.io/webshot/articles/intro.html) package to automate this process. This will require the external program "phantomjs". You can install phantomjs via the **webshot** package with the command `webshot::install_phantomjs()`.

<!-- ======================================================= -->

## RStudio {#rstudio}

### RStudio orientation {.unnumbered}

**First, open RStudio.** As their icons can look very similar, be sure you are opening *RStudio* and not R.

For RStudio to work you must also have R installed on the computer (see above for installation instructions).

**RStudio** is an interface (GUI) for easier use of **R**. You can think of R as being the engine of a vehicle, doing the crucial work, and RStudio as the body of the vehicle (with seats, accessories, etc.) that helps you actually use the engine to move forward! You can see the complete RStudio user-interface cheatsheet (PDF) [here](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)

By default RStudio displays four rectangle panes.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "RStudio_overview.png"))
```

[***TIP:*** If your RStudio displays only one left pane it is because you have no scripts open yet.]{style="color: black;"}

**The Source Pane**\
This pane, by default in the upper-left, is a space to edit, run, and save your [scripts](#scripts). Scripts contain the commands you want to run. This pane can also display datasets (data frames) for viewing.

For Stata users, this pane is similar to your Do-file and Data Editor windows.

**The R Console Pane**

The R Console, by default the left or lower-left pane in R Studio, is the home of the R "engine". This is where the commands are actually run and non-graphic outputs and error/warning messages appear. You can directly enter and run commands in the R Console, but realize that these commands are not saved as they are when running commands from a script.

If you are familiar with Stata, the R Console is like the Command Window and also the Results Window.

**The Environment Pane**\
This pane, by default in the upper-right, is most often used to see brief summaries of [objects](#objects) in the R Environment in the current session. These objects could include imported, modified, or created datasets, parameters you have defined (e.g. a specific epi week for the analysis), or vectors or lists you have defined during analysis (e.g. names of regions). You can click on the arrow next to a data frame name to see its variables.

In Stata, this is most similar to the Variables Manager window.

This pane also contains *History* where you can see commands that you can previously. It also has a "Tutorial" tab where you can complete interactive R tutorials if you have the **learnr** package installed. It also has a "Connections" pane for external connections, and can have a "Git" pane if you choose to interface with Github.

**Plots, Viewer, Packages, and Help Pane**\
The lower-right pane includes several important tabs. Typical plot graphics including maps will display in the Plot pane. Interactive or HTML outputs will display in the Viewer pane. The Help pane can display documentation and help files. The Files pane is a browser which can be used to open or delete files. The Packages pane allows you to see, install, update, delete, load/unload R packages, and see which version of the package you have. To learn more about packages see the [packages section](#packages) below.

This pane contains the Stata equivalents of the Plots Manager and Project Manager windows.

### RStudio settings {.unnumbered}

Change RStudio settings and appearance in the *Tools* drop-down menu, by selecting *Global Options*. There you can change the default settings, including appearance/background color.

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "RStudio_tools_options_1.png"))

knitr::include_graphics(here::here("images", "RStudio_tools_options.png"))
```

**Restart**

If your R freezes, you can re-start R by going to the Session menu and clicking "Restart R". This avoids the hassle of closing and opening RStudio. Everything in your R environment will be removed when you do this.

### Keyboard shortcuts {.unnumbered}

Some very useful keyboard shortcuts are below. See all the keyboard shortcuts for Windows, Max, and Linux in the second page of this RStudio [user interface cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf).

+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Windows/Linux                    | Mac                    | Action                                                                                                                         |
+==================================+========================+================================================================================================================================+
| Esc                              | Esc                    | Interrupt current command (useful if you accidentally ran an incomplete command and cannot escape seeing "+" in the R console) |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl+s                           | Cmd+s                  | Save (script)                                                                                                                  |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Tab                              | Tab                    | Auto-complete                                                                                                                  |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Enter                     | Cmd + Enter            | Run current line(s)/selection of code                                                                                          |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + C                 | Cmd + Shift + c        | comment/uncomment the highlighted lines                                                                                        |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Alt + -                          | Option + -             | Insert `<-`                                                                                                                    |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + m                 | Cmd + Shift + m        | Insert `%>%`                                                                                                                   |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + l                         | Cmd + l                | Clear the R console                                                                                                            |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + b                   | Cmd + Option + b       | Run from start to current line                                                                                                 |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + t                   | Cmd + Option + t       | Run the current code section (R Markdown)                                                                                      |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + i                   | Cmd + Shift + r        | Insert code chunk (into R Markdown)                                                                                            |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Alt + c                   | Cmd + Option + c       | Run current code chunk (R Markdown)                                                                                            |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| up/down arrows in R console      | Same                   | Toggle through recently run commands                                                                                           |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Shift + up/down arrows in script | Same                   | Select multiple code lines                                                                                                     |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + f                         | Cmd + f                | Find and replace in current script                                                                                             |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Ctrl + Shift + f                 | Cmd + Shift + f        | Find in files (search/replace across many scripts)                                                                             |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Alt + l                          | Cmd + Option + l       | Fold selected code                                                                                                             |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+
| Shift + Alt + l                  | Cmd + Shift + Option+l | Unfold selected code                                                                                                           |
+----------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------+

[***TIP:*** Use your Tab key when typing to engage RStudio's auto-complete functionality. This can prevent spelling errors. Press Tab while typing to produce a drop-down menu of likely functions and objects, based on what you have typed so far.]{style="color: darkgreen;"}

<!-- ======================================================= -->

## Functions {#functions}

Functions are at the core of using R. Functions are how you perform tasks and operations. Many functions come installed with R, many more are available for download in *packages* (explained in the [packages](#packages) section), and you can even write your own custom functions!

This basics section on functions explains:

-   What a function is and how they work\
-   What function *arguments* are\
-   How to get help understanding a function

*A quick note on syntax:* In this handbook, functions are written in code-text with open parentheses, like this: `filter()`. As explained in the [packages](#packages) section, functions are downloaded within *packages*. In this handbook, package names are written in **bold**, like **dplyr**. Sometimes in example code you may see the function name linked explicitly to the name of its package with two colons (`::`) like this: `dplyr::filter()`. The purpose of this linkage is explained in the packages section.

<!-- ======================================================= -->

### Simple functions {.unnumbered}

**A function is like a machine that receives inputs, does some action with those inputs, and produces an output.** What the output is depends on the function.

**Functions typically operate upon some object placed within the function's parentheses**. For example, the function `sqrt()` calculates the square root of a number:

```{r basics_function_sqrt}
sqrt(49)
```

The object provided to a function also can be a column in a dataset (see the [Objects](#objects) section for detail on all the kinds of objects). Because R can store multiple datasets, you will need to specify both the dataset and the column. One way to do this is using the `$` notation to link the name of the dataset and the name of the column (`dataset$column`). In the example below, the function `summary()` is applied to the numeric column `age` in the dataset `linelist`, and the output is a summary of the column's numeric and missing values.

```{r basics_functions_summary}
# Print summary statistics of column 'age' in the dataset 'linelist'
summary(linelist$age)
```

[***NOTE:*** Behind the scenes, a function represents complex additional code that has been wrapped up for the user into one easy command.]{style="color: black;"}

<!-- ======================================================= -->

### Functions with multiple arguments {.unnumbered}

Functions often ask for several inputs, called ***arguments***, located within the parentheses of the function, usually separated by commas.

-   Some arguments are required for the function to work correctly, others are optional\
-   Optional arguments have default settings\
-   Arguments can take character, numeric, logical (TRUE/FALSE), and other inputs

Here is a fun fictional function, called `oven_bake()`, as an example of a typical function. It takes an input object (e.g. a dataset, or in this example "dough") and performs operations on it as specified by additional arguments (`minutes =` and `temperature =`). The output can be printed to the console, or saved as an object using the assignment operator `<-`.

```{r basics_functions_image, echo=F, out.width = "75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Function_Bread_Example.png"))
```

**In a more realistic example**, the `age_pyramid()` command below produces an age pyramid plot based on defined age groups and a binary split column, such as `gender`. The function is given three arguments within the parentheses, separated by commas. The values supplied to the arguments establish `linelist` as the dataframe to use, `age_cat5` as the column to count, and `gender` as the binary column to use for splitting the pyramid by color.

```{r basics_functions_arguments, include=FALSE, results='hide', message=FALSE, warning=FALSE,}
## create an age group variable by specifying categorical breaks
linelist$age_group <- cut(linelist$age, breaks = c(0, 5, 10, 15, 20, 30, 45, 60))
```

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Create an age pyramid
age_pyramid(data = linelist, age_group = "age_cat5", split_by = "gender")
```

The above command can be equivalently written as below, in a longer style with a new line for each argument. This style can be easier to read, and easier to write "comments" with `#` to explain each part (commenting extensively is good practice!). To run this longer command you can highlight the entire command and click "Run", or just place your cursor in the first line and then press the Ctrl and Enter keys simultaneously.

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Create an age pyramid
age_pyramid(
  data = linelist,        # use case linelist
  age_group = "age_cat5", # provide age group column
  split_by = "gender"     # use gender column for two sides of pyramid
  )
```

The first half of an argument assignment (e.g. `data =`) does not need to be specified if the arguments are written in a specific order (specified in the function's documentation). The below code produces the exact same pyramid as above, because the function expects the argument order: data frame, `age_group` variable, `split_by` variable.

```{r, basics_functions_pyramid2, eval = FALSE, warning=FALSE, message=FALSE, , out.width = "75%", out.height="75%", eval=F}
# This command will produce the exact same graphic as above
age_pyramid(linelist, "age_cat5", "gender")
```

**A more complex `age_pyramid()` command might include the *optional* arguments to:**

-   Show proportions instead of counts (set `proportional = TRUE` when the default is `FALSE`)\
-   Specify the two colors to use (`pal =` is short for "palette" and is supplied with a vector of two color names. See the [objects](#objectstructure) page for how the function `c()` makes a vector)

[***NOTE:*** For arguments that you specify with both parts of the argument (e.g. `proportional = TRUE`), their order among all the arguments does not matter.]{style="color: black;"}

```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
age_pyramid(
  linelist,                    # use case linelist
  "age_cat5",                  # age group column
  "gender",                    # split by gender
  proportional = TRUE,         # percents instead of counts
  pal = c("orange", "purple")  # colors
  )
```

<!-- ======================================================= -->

### Writing Functions {.unnumbered}

R is a language that is oriented around functions, so you should feel empowered to write your own functions. Creating functions brings several advantages:

-   To facilitate modular programming - the separation of code in to independent and manageable pieces\
-   Replace repetitive copy-and-paste, which can be error prone\
-   Give pieces of code memorable names

How to write a function is covered in-depth in the [Writing functions] page.

<!-- A function is given a name and defined with the assignment operator `<-` to a special **base** R function called `function()`. Within the parentheses, the arguments that the function will accept are defined. This is followed by curly brackets `{ }`, within which the actual code of the function is written.     -->

```{r, eval=F, echo=F}
my_function <- function( ARGUMENTS HERE ){ CODE HERE }
```

<!-- The arguments should be provided in the syntax `argument = default`, separated by commas.   -->

<!-- Here is an example where we create a function `staff_calc()` to serve as a staffing calculator for COVID-19 case investigation and contact tracing calls.   -->

<!-- The arguments (inputs) and their default values will be:   -->

<!-- * `daily_cases = NULL` The number of new COVID-19 cases per day   -->

<!-- * `contacts_each = 5` The number contacts enumerated for each case   -->

<!-- * `time_case = 0.5`  Number of hours to complete a case investigaton by phone   -->

<!-- * `time_contact = 0.25`  Number of hours to complete a contact follow-up by phone   -->

<!-- * `time_day = 8` The number of hours one staff works per day   -->

<!-- Below, the function is created. The code ends with the special function `return()`, which is what the function produces.    -->

<!-- ```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"} -->

<!-- staff_calc <- function(daily_cases = NULL, contacts_each = 5, -->

<!--                        time_case = 0.5, time_contact = 0.25, time_day = 8){ -->

<!--   # Define total daily hours for calling cases -->

<!--   case_hours <- daily_cases * time_case  -->

<!--   # Define total daily hours for calling contacts -->

<!--   contact_hours <- daily_cases * contacts_each * time_contact -->

<!--   # Calculate number of staff required -->

<!--   staff_required <- (case_hours + contact_hours)/time_day -->

<!--   return(staff_required) -->

<!-- } -->

<!-- ``` -->

<!-- Once this code is run, the function will be defined and will appear in the R Environment. We can run the function. Below all the default values are used and the `daily_cases = ` is set to 150.   -->

```{r eval=F, echo=F, message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
staff_calc(daily_cases = 150)
```

```{r, eval=F, echo=F}
case_incidence <- tibble(
  dates = seq.Date(from = as.Date("2020-05-01"), to = as.Date("2020-05-21"), by = 1),
  projected_incidence = c(102,110,50,37,106,190,146,138,135,111,60,43,189,184,185,80,44,97,254,291,288),
  staff_needed = staff_calc(projected_incidence)
)

ggplot(case_incidence, aes(x = dates))+
  geom_line(aes(y = projected_incidence))+
  geom_line(aes(y = staff_needed))
```

<!-- There are many other nuances to understand when writing functions, as discussed in the page [Writing functions].   -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Packages {#packages}

**Packages contain functions.**

An R package is a shareable bundle of code and documentation that contains pre-defined functions. Users in the R community develop packages all the time catered to specific problems, it is likely that one can help with your work! You will install and use hundreds of packages in your use of R.

On installation, R contains **"base"** packages and functions that perform common elementary tasks. But many R users create specialized functions, which are verified by the R community and which you can download as a **package** for your own use. In this handbook, package names are written in **bold**. One of the more challenging aspects of R is that there are often many functions or packages to choose from to complete a given task.

### Install and load {.unnumbered}

*Functions* are contained within **packages** which can be downloaded ("installed") to your computer from the internet. Once a package is downloaded, it is stored in your "library". You can then access the functions it contains during your current R session by "loading" the package.

*Think of R as your personal library*: When you download a package, your library gains a new book of functions, but each time you want to use a function in that book, you must borrow ("load") that book from your library.

In summary: to use the functions available in an R package, 2 steps must be implemented:

1)  The package must be **installed** (once), *and*\
2)  The package must be **loaded** (each R session)

#### Your library {.unnumbered}

Your "library" is actually a folder on your computer, containing a folder for each package that has been installed. Find out where R is installed in your computer, and look for a folder called "win-library". For example: `R\win-library\4.0` (the 4.0 is the R version - you'll have a different library for each R version you've downloaded).

You can print the file path to your library by entering `.libPaths()` (empty parentheses). This becomes especially important if working with [R on network drives].

#### Install from CRAN {.unnumbered}

Most often, R users download packages from CRAN. CRAN (Comprehensive R Archive Network) is an online public warehouse of R packages that have been published by R community members.

Are you worried about viruses and security when downloading a package from CRAN? Read [this article](https://support.rstudio.com/hc/en-us/articles/360042593974-R-and-R-Package-Security) on the topic.

#### How to install and load {.unnumbered}

In this handbook, we suggest using the **pacman** package (short for "package manager"). It offers a convenient function `p_load()` which will install a package if necessary *and* load it for use in the current R session.

The syntax quite simple. Just list the names of the packages within the `p_load()` parentheses, separated by commas. This command will install the **rio**, **tidyverse**, and **here** packages if they are not yet installed, and will load them for use. This makes the `p_load()` approach convenient and concise if sharing scripts with others. Note that package names are case-sensitive.

```{r}
# Install (if necessary) and load packages for use
pacman::p_load(rio, tidyverse, here)
```

Note that we have used the syntax `pacman::p_load()` which explicitly writes the package name (**pacman**) prior to the function name (`p_load()`), connected by two colons `::`. This syntax is useful because it also loads the **pacman** package (assuming it is already installed).

There are alternative **base** R functions that you will see often. The **base** R function for installing a package is `install.packages()`. The name of the package to install must be provided in the parentheses *in quotes*. If you want to install multiple packages in one command, they must be listed within a character vector `c()`.

Note: this command *installs* a package, but does *not* load it for use in the current session.

```{r, eval=F}
# install a single package with base R
install.packages("tidyverse")

# install multiple packages with base R
install.packages(c("tidyverse", "rio", "here"))
```

Installation can also be accomplished point-and-click by going to the RStudio "Packages" pane and clicking "Install" and searching for the desired package name.

The **base** R function to **load** a package for use (after it has been installed) is `library()`. It can load only one package at a time (another reason to use `p_load()`). You can provide the package name with or without quotes.

```{r, eval=F}
# load packages for use, with base R
library(tidyverse)
library(rio)
library(here)
```

To check whether a package in installed and/or loaded, you can view the Packages pane in RStudio. If the package is installed, it is shown there with version number. If its box is checked, it is loaded for the current session.

**Install from Github**

Sometimes, you need to install a package that is not yet available from CRAN. Or perhaps the package is available on CRAN but you want the *development version* with new features not yet offered in the more stable published CRAN version. These are often hosted on the website [github.com](https://github.com/) in a free, public-facing code "repository". Read more about Github in the handbook page on [Version control and collaboration with Git and Github].

To download R packages from Github, you can use the function `p_load_gh()` from **pacman**, which will install the package if necessary, and load it for use in your current R session. Alternatives to install include using the **remotes** or **devtools** packages. Read more about all the **pacman** functions in the [package documentation](https://cran.r-project.org/web/packages/pacman/pacman.pdf).

To install from Github, you have to provide more information. You must provide:

1)  The Github ID of the repository owner
2)  The name of the repository that contains the package\
3)  *(optional) The name of the "branch" (specific development version) you want to download*

In the examples below, the first word in the quotation marks is the Github ID of the repository owner, after the slash is the name of the repository (the name of the package).

```{r, eval=F}
# install/load the epicontacts package from its Github repository
p_load_gh("reconhub/epicontacts")
```

If you want to install from a "branch" (version) other than the main branch, add the branch name after an "\@", after the repository name.

```{r, eval=F}
# install the "timeline" branch of the epicontacts package from Github
p_load_gh("reconhub/epicontacts@timeline")
```

If there is no difference between the Github version and the version on your computer, no action will be taken. You can "force" a re-install by instead using `p_load_current_gh()` with the argument `update = TRUE`. Read more about **pacman** in this [online vignette](http://trinker.github.io/pacman/vignettes/Introduction_to_pacman.html)

**Install from ZIP or TAR**

You could install the package from a URL:

```{r, eval=F}
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

Or, download it to your computer in a zipped file:

Option 1: using `install_local()` from the **remotes** package

```{r, eval=F}
remotes::install_local("~/Downloads/dplyr-master.zip")
```

Option 2: using `install.packages()` from **base** R, providing the file path to the ZIP file and setting `type = "source` and `repos = NULL`.

```{r, eval=F}
install.packages("~/Downloads/dplyr-master.zip", repos=NULL, type="source")
```

### Code syntax {.unnumbered}

For clarity in this handbook, functions are sometimes preceded by the name of their package using the `::` symbol in the following way: `package_name::function_name()`

Once a package is loaded for a session, this explicit style is not necessary. One can just use `function_name()`. However writing the package name is useful when a function name is common and may exist in multiple packages (e.g. `plot()`). Writing the package name will also load the package if it is not already loaded.

```{r eval=FALSE}
# This command uses the package "rio" and its function "import()" to import a dataset
linelist <- rio::import("linelist.xlsx", which = "Sheet1")
```

### Function help {.unnumbered}

To read more about a function, you can search for it in the Help tab of the lower-right RStudio. You can also run a command like `?thefunctionname` (put the name of the function after a question mark) and the Help page will appear in the Help pane. Finally, try searching online for resources.

### Update packages {.unnumbered}

You can update packages by re-installing them. You can also click the green "Update" button in your RStudio Packages pane to see which packages have new versions to install. Be aware that your old code may need to be updated if there is a major revision to how a function works!

### Delete packages {.unnumbered}

Use `p_delete()` from **pacman**, or `remove.packages()` from **base** R. Alternatively, go find the folder which contains your library and manually delete the folder.

### Dependencies {.unnumbered}

Packages often depend on other packages to work. These are called dependencies. If a dependency fails to install, then the package depending on it may also fail to install.

See the dependencies of a package with `p_depends()`, and see which packages depend on it with `p_depends_reverse()`

### Masked functions {.unnumbered}

It is not uncommon that two or more packages contain the same function name. For example, the package **dplyr** has a `filter()` function, but so does the package **stats**. The default `filter()` function depends on the order these packages are first loaded in the R session - the later one will be the default for the command `filter()`.

You can check the order in your Environment pane of R Studio - click the drop-down for "Global Environment" and see the order of the packages. Functions from packages *lower* on that drop-down list will mask functions of the same name in packages that appear higher in the drop-down list. When first loading a package, R will warn you in the console if masking is occurring, but this can be easy to miss.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "masking_functions.png"))
```

Here are ways you can fix masking:

1)  Specify the package name in the command. For example, use `dplyr::filter()`\
2)  Re-arrange the order in which the packages are loaded (e.g. within `p_load()`), and **start a new R session**

### Detach / unload {.unnumbered}

To detach (unload) a package, use this command, with the correct package name and only one colon. Note that this may not resolve masking.

```{r, eval=F}
detach(package:PACKAGE_NAME_HERE, unload=TRUE)
```

### Install older version {.unnumbered}

See this [guide](https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages) to install an older version of a particular package.

### Suggested packages {.unnumbered}

See the page on [Suggested packages] for a listing of packages we recommend for everyday epidemiology.

<!-- ======================================================= -->

## Scripts {#scripts}

Scripts are a fundamental part of programming. They are documents that hold your commands (e.g. functions to create and modify datasets, print visualizations, etc). You can save a script and run it again later. There are many advantages to storing and running your commands from a script (vs. typing commands one-by-one into the R console "command line"):

-   Portability - you can share your work with others by sending them your scripts\
-   Reproducibility - so that you and others know exactly what you did\
-   Version control - so you can track changes made by yourself or colleagues\
-   Commenting/annotation - to explain to your colleagues what you have done

### Commenting {.unnumbered}

In a script you can also annotate ("comment") around your R code. Commenting is helpful to explain to yourself and other readers what you are doing. You can add a comment by typing the hash symbol (\#) and writing your comment after it. The commented text will appear in a different color than the R code.

Any code written after the \# will not be run. Therefore, placing a \# before code is also a useful way to temporarily block a line of code ("comment out") if you do not want to delete it). You can comment out/in multiple lines at once by highlighting them and pressing Ctrl+Shift+c (Cmd+Shift+c in Mac).

```{r, eval = F}
# A comment can be on a line by itself
# import data
linelist <- import("linelist_raw.xlsx") %>%   # a comment can also come after code
# filter(age > 50)                          # It can also be used to deactivate / remove a line of code
  count()

```

-   Comment on *what* you are doing *and on **why** you are doing it*.\
-   Break your code into logical sections\
-   Accompany your code with a text step-by-step description of what you are doing (e.g. numbered steps)

### Style {.unnumbered}

It is important to be conscious of your coding style - especially if working on a team. We advocate for the **tidyverse** [style guide](https://style.tidyverse.org/). There are also packages such as **styler** and **lintr** which help you conform to this style.

A few very basic points to make your code readable to others:\
\* When naming objects, use only lowercase letters, numbers, and underscores `_`, e.g. `my_data`\
\* Use frequent spaces, including around operators, e.g. `n = 1` and `age_new <- age_old + 3`

### Example Script {.unnumbered}

Below is an example of a short R script. Remember, the better you succinctly explain your code in comments, the more your colleagues will like you!

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "example_script.png"))
```

<!-- ======================================================= -->

### R markdown {.unnumbered}

An R markdown script is a type of R script in which the script itself *becomes* an output document (PDF, Word, HTML, Powerpoint, etc.). These are incredibly useful and versatile tools often used to create dynamic and automated reports. Even this website and handbook is produced with R markdown scripts!

It is worth noting that beginner R users can also use R Markdown - do not be intimidated! To learn more, see the handbook page on [Reports with R Markdown] documents.

<!-- ======================================================= -->

### R notebooks {.unnumbered}

There is no difference between writing in a Rmarkdown vs an R notebook. However the execution of the document differs slightly. See this [site](http://uc-r.github.io/r_notebook) for more details.

<!-- ======================================================= -->

### Shiny {.unnumbered}

Shiny apps/websites are contained within one script, which must be named `app.R`. This file has three components:

1)  A user interface (ui)\
2)  A server function\
3)  A call to the `shinyApp` function

See the handbook page on [Dashboards with Shiny], or this online tutorial: [Shiny tutorial](https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/)

*In older times, the above file was split into two files (`ui.R` and `server.R`)*

### Code folding {.unnumbered}

You can collapse portions of code to make your script easier to read.

To do this, create a text header with \#, write your header, and follow it with at least 4 of either dashes (-), hashes (\#) or equals (=). When you have done this, a small arrow will appear in the "gutter" to the left (by the row number). You can click this arrow and the code below until the next header will collapse and a dual-arrow icon will appear in its place.

To expand the code, either click the arrow in the gutter again, or the dual-arrow icon. There are also keyboard shortcuts as explained in the [RStudio section](#rstudio) of this page.

By creating headers with \#, you will also activate the Table of Contents at the bottom of your script (see below) that you can use to navigate your script. You can create sub-headers by adding more \# symbols, for example \# for primary, \#\# for seconary, and \#\#\# for tertiary headers.

Below are two versions of an example script. On the left is the original with commented headers. On the right, four dashes have been written after each header, making them collapsible. Two of them have been collapsed, and you can see that the Table of Contents at the bottom now shows each section.

```{r, out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "code_folding1.png"))
knitr::include_graphics(here::here("images", "code_folding2.png"))
```

Other areas of code that are automatically eligible for folding include "braced" regions with brackets `{ }` such as function definitions or conditional blocks (if else statements). You can read more about code folding at the RStudio [site](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections).

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Working directory

The working directory is the root folder location used by R for your work - where R looks for and saves files by default. By default, it will save new files and outputs to this location, and will look for files to import (e.g. datasets) here as well.

The working directory appears in grey text at the top of the RStudio Console pane. You can also print the current working directory by running `getwd()` (leave the parentheses empty).

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "working_directory_1.png"))
```

### Recommended approach {.unnumbered}

**See the page on [R projects] for details on our recommended approach to managing your working directory.**\
A common, efficient, and trouble-free way to manage your working directory and file paths is to combine these 3 elements in an [R project][R projects]-oriented workflow:

1)  An R Project to store all your files (see page on [R projects])\
2)  The **here** package to locate files (see page on [Import and export])\
3)  The **rio** package to import/export files (see page on [Import and export])

<!-- ======================================================= -->

### Set by command {.unnumbered}

Until recently, many people learning R were taught to begin their scripts with a `setwd()` command. Please instead consider using an [R project][R projects]-oriented workflow and read the [reasons for not using `setwd()`](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/). In brief, your work becomes specific to your computer, file paths used to import and export files become "brittle", and this severely hinders collaboration and use of your code on any other computer. There are easy alternatives!

As noted above, although we do not recommend this approach in most circumstances, you can use the command `setwd()` with the desired folder file path in quotations, for example:

```{r, eval=F}
setwd("C:/Documents/R Files/My analysis")
```

[***DANGER:*** Setting a working directory with `setwd()` *can* be "brittle" if the file path is specific to one computer. Instead, use file paths relative to an R Project root directory (with the **here** package). ]{style="color: red;"}

<!-- ======================================================= -->

### Set manually {.unnumbered}

To set the working directory manually (the point-and-click equivalent of `setwd()`), click the Session drop-down menu and go to "Set Working Directory" and then "Choose Directory". This will set the working directory for that specific R session. Note: if using this approach, you will have to do this manually each time you open RStudio.

<!-- ======================================================= -->

### Within an R project {.unnumbered}

If using an R project, the working directory will default to the R project root folder that contains the ".rproj" file. This will apply if you open RStudio by clicking open the R Project (the file with ".rproj" extension).

<!-- ======================================================= -->

### Working directory in an R markdown {.unnumbered}

In an R markdown script, the default working directory is the folder the Rmarkdown file (`.Rmd`) is saved within. If using an R project and **here** package, this does not apply and the working directory will be `here()` as explained in the [R projects] page.

If you want to change the working directory of a stand-alone R markdown (not in an R project), if you use `setwd()` this will only apply to that specific code chunk. To make the change for all code chunks in an R markdown, edit the setup chunk to add the `root.dir =` parameter, such as below:

```{r, eval=F}
knitr::opts_knit$set(root.dir = 'desired/directorypath')
```

It is much easier to just use the R markdown within an R project and use the **here** package.

<!-- ======================================================= -->

### Providing file paths {.unnumbered}

Perhaps the most common source of frustration for an R beginner (at least on a Windows machine) is typing in a file path to import or export data. There is a thorough explanation of how to best input file paths in the [Import and export] page, but here are a few key points:

**Broken paths**

Below is an example of an "absolute" or "full address" file path. These will likely break if used by another computer. One exception is if you are using a shared/network drive.

    C:/Users/Name/Document/Analytic Software/R/Projects/Analysis2019/data/March2019.csv  

**Slash direction**

*If typing in a file path, be aware the direction of the slashes.* Use *forward slashes* (`/`) to separate the components ("data/provincial.csv"). For Windows users, the default way that file paths are displayed is with *back slashes* (\\) - so you will need to change the direction of each slash. If you use the **here** package as described in the [R projects] page the slash direction is not an issue.

**Relative file paths**

We generally recommend providing "relative" filepaths instead - that is, the path *relative to* the root of your R Project. You can do this using the **here** package as explained in the [R projects] page. A relativel filepath might look like this:

```{r, eval=F}
# Import csv linelist from the data/linelist/clean/ sub-folders of an R project
linelist <- import(here("data", "clean", "linelists", "marin_country.csv"))
```

Even if using relative file paths within an R project, you can still use absolute paths to import/export data outside your R project.

<!-- ======================================================= -->

## Objects {#objects}

Everything in R is an object, and R is an "object-oriented" language. These sections will explain:

-   How to create objects (`<-`)
-   Types of objects (e.g. data frames, vectors..)\
-   How to access subparts of objects (e.g. variables in a dataset)\
-   Classes of objects (e.g. numeric, logical, integer, double, character, factor)

<!-- ======================================================= -->

### Everything is an object {.unnumbered}

*This section is adapted from the [R4Epis project](https://r4epis.netlify.app/training/r_basics/objects/).*\
Everything you store in R - datasets, variables, a list of village names, a total population number, even outputs such as graphs - are **objects** which are **assigned a name** and **can be referenced** in later commands.

An object exists when you have assigned it a value (see the assignment section below). When it is assigned a value, the object appears in the Environment (see the upper right pane of RStudio). It can then be operated upon, manipulated, changed, and re-defined.

<!-- ======================================================= -->

### Defining objects (`<-`) {.unnumbered}

**Create objects *by assigning them a value* with the \<- operator.**\
You can think of the assignment operator `<-` as the words "is defined as". Assignment commands generally follow a standard order:

**object_name** \<- **value** (or process/calculation that produce a value)

For example, you may want to record the current epidemiological reporting week as an object for reference in later code. In this example, the object `current_week` is created when it is assigned the value `"2018-W10"` (the quote marks make this a character value). The object `current_week` will then appear in the RStudio Environment pane (upper-right) and can be referenced in later commands.

See the R commands and their output in the boxes below.

```{r basics_objects_assignment}
current_week <- "2018-W10"   # this command creates the object current_week by assigning it a value
current_week                 # this command prints the current value of current_week object in the console
```

[***NOTE:*** Note the `[1]` in the R console output is simply indicating that you are viewing the first item of the output]{style="color: black;"}

[***CAUTION:*** **An object's value can be over-written** at any time by running an assignment command to re-define its value. Thus, the **order of the commands run is very important**.]{style="color: orange;"}

The following command will re-define the value of `current_week`:

```{r basics_objects_reassignment}
current_week <- "2018-W51"   # assigns a NEW value to the object current_week
current_week                 # prints the current value of current_week in the console
```

**Equals signs `=`**

You will also see equals signs in R code:

-   A double equals sign `==` between two objects or values asks a logical *question*: "is this equal to that?".\
-   You will also see equals signs within functions used to specify values of function arguments (read about these in sections below), for example `max(age, na.rm = TRUE)`.\
-   You *can* use a single equals sign `=` in place of `<-` to create and define objects, but this is discouraged. You can read about why this is discouraged [here](https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/).

**Datasets**

Datasets are also objects (typically "dataframes") and must be assigned names when they are imported. In the code below, the object `linelist` is created and assigned the value of a CSV file imported with the **rio** package and its `import()` function.

```{r basics_objects_dataframes, eval=FALSE}
# linelist is created and assigned the value of the imported CSV file
linelist <- import("my_linelist.csv")
```

You can read more about importing and exporting datasets with the section on [Import and export].

[***CAUTION:*** A quick note on naming of objects:]{style="color: orange;"}

-   Object names must not contain spaces, but you should use underscore (\_) or a period (.) instead of a space.\
-   Object names are case-sensitive (meaning that Dataset_A is different from dataset_A).
-   Object names must begin with a letter (cannot begin with a number like 1, 2 or 3).

**Outputs**

Outputs like tables and plots provide an example of how outputs can be saved as objects, or just be printed without being saved. A cross-tabulation of gender and outcome using the **base** R function `table()` can be printed directly to the R console (*without* being saved).

```{r}
# printed to R console only
table(linelist$gender, linelist$outcome)
```

But the same table can be saved as a named object. Then, optionally, it can be printed.

```{r}
# save
gen_out_table <- table(linelist$gender, linelist$outcome)

# print
gen_out_table
```

**Columns**

Columns in a dataset are also objects and can be defined, over-written, and created as described below in the section on Columns.

You can use the assignment operator from **base** R to create a new column. Below, the new column `bmi` (Body Mass Index) is created, and for each row the new value is result of a mathematical operation on the row's value in the `wt_kg` and `ht_cm` columns.

```{r, eval=F}
# create new "bmi" column using base R syntax
linelist$bmi <- linelist$wt_kg / (linelist$ht_cm/100)^2
```

However, in this handbook, we emphasize a different approach to defining columns, which uses the function `mutate()` from the **dplyr** package and *piping* with the pipe operator (`%>%`). The syntax is easier to read and there are other advantages explained in the page on [Cleaning data and core functions]. You can read more about *piping* in the Piping section below.

```{r, eval=F}
# create new "bmi" column using dplyr syntax
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

<!-- ======================================================= -->

### Object structure {.unnumbered}

**Objects can be a single piece of data (e.g. `my_number <- 24`), or they can consist of structured data.**

The graphic below is borrowed from [this online R tutorial](http://venus.ifca.unican.es/Rintro/dataStruct.html). It shows some common data structures and their names. Not included in this image is spatial data, which is discussed in the [GIS basics] page.

```{r basics_objects_structures, echo=F, out.width = "75%", out.height="50%", fig.align = "center"}
knitr::include_graphics(here::here("images", "R_data_structures.png"))
```

In epidemiology (and particularly field epidemiology), you will *most commonly* encounter data frames and vectors:

+------------------+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| Common structure | Explanation                                                                                      | Example                                                                             |
+==================+==================================================================================================+=====================================================================================+
| Vectors          | A container for a sequence of singular objects, all of the same class (e.g. numeric, character). | **"Variables" (columns) in data frames are vectors** (e.g. the column `age_years`). |
+------------------+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+
| Data Frames      | Vectors (e.g. columns) that are bound together that all have the same number of rows.            | `linelist` is a data frame.                                                         |
+------------------+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+

Note that to create a vector that "stands alone" (is not part of a data frame) the function `c()` is used to combine the different elements. For example, if creating a vector of colors plot's color scale: `vector_of_colors <- c("blue", "red2", "orange", "grey")`

<!-- ======================================================= -->

### Object classes {.unnumbered}

All the objects stored in R have a *class* which tells R how to handle the object. There are many possible classes, but common ones include:

+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Class      | Explanation                                                                                                                                                                             | Examples                                                                                              |
+============+=========================================================================================================================================================================================+=======================================================================================================+
| Character  | These are text/words/sentences **"within quotation marks"**. Math cannot be done on these objects.                                                                                      | "Character objects are in quotation marks"                                                            |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Integer    | Numbers that are **whole only** (no decimals)                                                                                                                                           | -5, 14, or 2000                                                                                       |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Numeric    | These are numbers and **can include decimals**. If within quotation marks they will be considered character class.                                                                      | 23.1 or 14                                                                                            |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Factor     | These are vectors that have a **specified order** or hierarchy of values                                                                                                                | An variable of economic status with ordered values                                                    |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Date       | **Once R is told that certain data are Dates**, these data can be manipulated and displayed in special ways. See the page on [Working with dates] for more information.                 | 2018-04-12 or 15/3/1954 or Wed 4 Jan 1980                                                             |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| Logical    | Values must be one of the two special values TRUE or FALSE (note these are **not** "TRUE" and "FALSE" in quotation marks)                                                               | TRUE or FALSE                                                                                         |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| data.frame | A data frame is how R stores a **typical dataset**. It consists of vectors (columns) of data bound together, that all have the same number of observations (rows).                      | The example AJS dataset named `linelist_raw` contains 68 variables with 300 observations (rows) each. |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| tibble     | tibbles are a variation on data frame, the main operational difference being that they print more nicely to the console (display first 10 rows and only columns that fit on the screen) | Any data frame, list, or matrix can be converted to a tibble with `as_tibble()`                       |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+
| list       | A list is like vector, but holds other objects that can be other different classes                                                                                                      | A list could hold a single number, and a dataframe, and a vector, and even another list within it!    |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------+

**You can test the class of an object by providing its name to the function `class()`**. Note: you can reference a specific column within a dataset using the `$` notation to separate the name of the dataset and the name of the column.

```{r, echo=TRUE,}
class(linelist)         # class should be a data frame or tibble

class(linelist$age)     # class should be numeric

class(linelist$gender)  # class should be character
```

Sometimes, a column will be converted to a different class automatically by R. Watch out for this! For example, if you have a vector or column of numbers, but a character value is inserted... the entire column will change to class character.

```{r}
num_vector <- c(1,2,3,4,5) # define vector as all numbers
class(num_vector)          # vector is numeric class
num_vector[3] <- "three"   # convert the third element to a character
class(num_vector)          # vector is now character class
```

One common example of this is when manipulating a data frame in order to print a table - if you make a total row and try to paste/glue together percents in the same cell as numbers (e.g. `23 (40%)`), the entire numeric column above will convert to character and can no longer be used for mathematical calculations.**Sometimes, you will need to convert objects or columns to another class.**

+------------------+---------------------------------------------------------------------------------------+
| Function         | Action                                                                                |
+==================+=======================================================================================+
| `as.character()` | Converts to character class                                                           |
+------------------+---------------------------------------------------------------------------------------+
| `as.numeric()`   | Converts to numeric class                                                             |
+------------------+---------------------------------------------------------------------------------------+
| `as.integer()`   | Converts to integer class                                                             |
+------------------+---------------------------------------------------------------------------------------+
| `as.Date()`      | Converts to Date class - Note: see section on [dates](#dates) for details             |
+------------------+---------------------------------------------------------------------------------------+
| `factor()`       | Converts to factor - Note: re-defining order of value levels requires extra arguments |
+------------------+---------------------------------------------------------------------------------------+

Likewise, there are **base** R functions to check whether an object IS of a specific class, such as `is.numeric()`, `is.character()`, `is.double()`, `is.factor()`, `is.integer()`

Here is [more online material on classes and data structures in R](https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/).

<!-- ======================================================= -->

### Columns/Variables (`$`) {.unnumbered}

**A column in a data frame is technically a "vector" (see table above)** - a series of values that must all be the same class (either character, numeric, logical, etc).

A vector can exist independent of a data frame, for example a vector of column names that you want to include as explanatory variables in a model. To create a "stand alone" vector, use the `c()` function as below:

```{r, warning=F, message=F}
# define the stand-alone vector of character values
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")

# print the values in this named vector
explanatory_vars
```

**Columns in a data frame are also vectors and can be called, referenced, extracted, or created using the `$` symbol.** The `$` symbol connects the name of the column to the name of its data frame. In this handbook, we try to use the word "column" instead of "variable".

```{r basics_objects_call, eval=F}
# Retrieve the length of the vector age_years
length(linelist$age) # (age is a column in the linelist data frame)

```

By typing the name of the dataframe followed by `$` you will also see a drop-down menu of all columns in the data frame. You can scroll through them using your arrow key, select one with your Enter key, and avoid spelling mistakes!

```{r echo=F, out.width = "100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Calling_Names.gif"))
```

[***ADVANCED TIP:*** Some more complex objects (e.g. a list, or an `epicontacts` object) may have multiple levels which can be accessed through multiple dollar signs. For example `epicontacts$linelist$date_onset`]{style="color: darkgreen;"}

<!-- ======================================================= -->

### Access/index with brackets (`[ ]`) {.unnumbered}

You may need to view parts of objects, also called "indexing", which is often done using the square brackets `[ ]`. Using `$` on a dataframe to access a column is also a type of indexing.

```{r}
my_vector <- c("a", "b", "c", "d", "e", "f")  # define the vector
my_vector[5]                                  # print the 5th element
```

Square brackets also work to return specific parts of an returned output, such as the output of a `summary()` function:

```{r}
# All of the summary
summary(linelist$age)

# Just the second element of the summary, with name (using only single brackets)
summary(linelist$age)[2]

# Just the second element, without name (using double brackets)
summary(linelist$age)[[2]]

# Extract an element by name, without showing the name
summary(linelist$age)[["Median"]]

```

Brackets also work on data frames to view specific rows and columns. You can do this using the syntax `dataframe[rows, columns]`:

```{r basics_objects_access, eval=F}
# View a specific row (2) from dataset, with all columns (don't forget the comma!)
linelist[2,]

# View all rows, but just one column
linelist[, "date_onset"]

# View values from row 2 and columns 5 through 10
linelist[2, 5:10] 

# View values from row 2 and columns 5 through 10 and 18
linelist[2, c(5:10, 18)] 

# View rows 2 through 20, and specific columns
linelist[2:20, c("date_onset", "outcome", "age")]

# View rows and columns based on criteria
# *** Note the dataframe must still be named in the criteria!
linelist[linelist$age > 25 , c("date_onset", "outcome", "age")]

# Use View() to see the outputs in the RStudio Viewer pane (easier to read) 
# *** Note the capital "V" in View() function
View(linelist[2:20, "date_onset"])

# Save as a new object
new_table <- linelist[2:20, c("date_onset")] 
```

Note that you can also achieve the above row/column indexing on data frames and tibbles using **dplyr** syntax (functions `filter()` for rows, and `select()` for columns). Read more about these core functions in the [Cleaning data and core functions] page.

To filter based on "row number", you can use the **dplyr** function `row_number()` with open parentheses as part of a logical filtering statement. Often you will use the `%in%` operator and a range of numbers as part of that logical statement, as shown below. To see the *first* N rows, you can also use the special **dplyr** function `head()`.

```{r, eval=F}
# View first 100 rows
linelist %>% head(100)

# Show row 5 only
linelist %>% filter(row_number() == 5)

# View rows 2 through 20, and three specific columns (note no quotes necessary on column names)
linelist %>% filter(row_number() %in% 2:20) %>% select(date_onset, outcome, age)
```

When indexing an object of class **list**, single brackets always return with class list, even if only a single object is returned. Double brackets, however, can be used to access a single element and return a different class than list.\
Brackets can also be written after one another, as demonstrated below.

This [visual explanation of lists indexing, with pepper shakers](https://r4ds.had.co.nz/vectors.html#lists-of-condiments) is humorous and helpful.

```{r}
# define demo list
my_list <- list(
  # First element in the list is a character vector
  hospitals = c("Central", "Empire", "Santa Anna"),
  
  # second element in the list is a data frame of addresses
  addresses   = data.frame(
    street = c("145 Medical Way", "1048 Brown Ave", "999 El Camino"),
    city   = c("Andover", "Hamilton", "El Paso")
    )
  )
```

Here is how the list looks when printed to the console. See how there are two named elements:

-   `hospitals`, a character vector\
-   `addresses`, a data frame of addresses

```{r}
my_list
```

Now we extract, using various methods:

```{r}
my_list[1] # this returns the element in class "list" - the element name is still displayed

my_list[[1]] # this returns only the (unnamed) character vector

my_list[["hospitals"]] # you can also index by name of the list element

my_list[[1]][3] # this returns the third element of the "hospitals" character vector

my_list[[2]][1] # This returns the first column ("street") of the address data frame

```

<!-- ======================================================= -->

### Remove objects {.unnumbered}

You can remove individual objects from your R environment by putting the name in the `rm()` function (no quote marks):

```{r, eval=F}
rm(object_name)
```

You can remove all objects (clear your workspace) by running:

```{r, eval=F}
rm(list = ls(all = TRUE))
```

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Piping (`%>%`)

**Two general approaches to working with objects are:**

1)  **Pipes/tidyverse** - pipes send an object from function to function - emphasis is on the *action*, not the object\
2)  **Define intermediate objects** - an object is re-defined again and again - emphasis is on the object

<!-- ======================================================= -->

### **Pipes** {.unnumbered}

**Simply explained, the pipe operator (`%>%`) passes an intermediate output from one function to the next.**\
You can think of it as saying "then". Many functions can be linked together with `%>%`.

-   **Piping emphasizes a sequence of actions, not the object the actions are being performed on**\
-   Pipes are best when a sequence of actions must be performed on one object\
-   Pipes come from the package **magrittr**, which is automatically included in packages **dplyr** and **tidyverse**
-   Pipes can make code more clean and easier to read, more intuitive

Read more on this approach in the tidyverse [style guide](https://style.tidyverse.org/pipes.html)

Here is a fake example for comparison, using fictional functions to "bake a cake". First, the pipe method:

```{r piping_example_pipe, eval=F}
# A fake example of how to bake a cake using piping syntax

cake <- flour %>%       # to define cake, start with flour, and then...
  add(eggs) %>%   # add eggs
  add(oil) %>%    # add oil
  add(water) %>%  # add water
  mix_together(         # mix together
    utensil = spoon,
    minutes = 2) %>%    
  bake(degrees = 350,   # bake
       system = "fahrenheit",
       minutes = 35) %>%  
  let_cool()            # let it cool down
```

Here is another [link](https://cfss.uchicago.edu/notes/pipes/#:~:text=Pipes%20are%20an%20extremely%20useful,code%20and%20combine%20multiple%20operations) describing the utility of pipes.

Piping is not a **base** function. To use piping, the **magrittr** package must be installed and loaded (this is typically done by loading **tidyverse** or **dplyr** package which include it). You can [read more about piping in the magrittr documentation](https://magrittr.tidyverse.org/).

Note that just like other R commands, pipes can be used to just display the result, or to save/re-save an object, depending on whether the assignment operator `<-` is involved. See both below:

```{r, eval=F}
# Create or overwrite object, defining as aggregate counts by age category (not printed)
linelist_summary <- linelist %>% 
  count(age_cat)
```

```{r}
# Print the table of counts in the console, but don't save it
linelist %>% 
  count(age_cat)
```

**`%<>%`**\
This is an "assignment pipe" from the **magrittr** package, which *pipes an object forward and also re-defines the object*. It must be the first pipe operator in the chain. It is shorthand. The below two commands are equivalent:

```{r, eval=F}
linelist <- linelist %>%
  filter(age > 50)

linelist %<>% filter(age > 50)
```

<!-- ======================================================= -->

### Define intermediate objects {.unnumbered}

This approach to changing objects/dataframes may be better if:

-   You need to manipulate multiple objects\
-   There are intermediate steps that are meaningful and deserve separate object names

**Risks:**

-   Creating new objects for each step means creating lots of objects. If you use the wrong one you might not realize it!\
-   Naming all the objects can be confusing\
-   Errors may not be easily detectable

Either name each intermediate object, or overwrite the original, or combine all the functions together. All come with their own risks.

Below is the same fake "cake" example as above, but using this style:

```{r piping_example_redefine, eval=F}
# a fake example of how to bake a cake using this method (defining intermediate objects)
batter_1 <- left_join(flour, eggs)
batter_2 <- left_join(batter_1, oil)
batter_3 <- left_join(batter_2, water)

batter_4 <- mix_together(object = batter_3, utensil = spoon, minutes = 2)

cake <- bake(batter_4, degrees = 350, system = "fahrenheit", minutes = 35)

cake <- let_cool(cake)
```

Combine all functions together - this is difficult to read:

```{r eval=F}
# an example of combining/nesting mutliple functions together - difficult to read
cake <- let_cool(bake(mix_together(batter_3, utensil = spoon, minutes = 2), degrees = 350, system = "fahrenheit", minutes = 35))
```

<!-- ======================================================= -->

## Key operators and functions {#operators}

This section details operators in R, such as:

-   Definitional operators\
-   Relational operators (less than, equal too..)\
-   Logical operators (and, or...)\
-   Handling missing values\
-   Mathematical operators and functions (+/-, \>, sum(), median(), ...)\
-   The `%in%` operator

<!-- ======================================================= -->

### Assignment operators {.unnumbered}

**`<-`**

The basic assignment operator in R is `<-`. Such that `object_name <- value`.\
This assignment operator can also be written as `=`. We advise use of `<-` for general R use.\
We also advise surrounding such operators with spaces, for readability.

**`<<-`**

If [Writing functions], or using R in an interactive way with sourced scripts, then you may need to use this assignment operator `<<-` (from **base** R). This operator is used to define an object in a higher 'parent' R Environment. See this [online reference](https://stat.ethz.ch/R-manual/R-devel/library/base/html/assignOps.html).

**`%<>%`**

This is an "assignment pipe" from the **magrittr** package, which pipes an object forward and *also re-defines the object*. It must be the first pipe operator in the chain. It is shorthand, as shown below in two equivalent examples:

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age_months = age_years * 12)
```

The above is equivalent to the below:

```{r, eval=F}
linelist %<>% mutate(age_months = age_years * 12)
```

**`%<+%`**

This is used to add data to phylogenetic trees with the **ggtree** package. See the page on [Phylogenetic trees] or this online [resource book](https://yulab-smu.top/treedata-book/).

<!-- ======================================================= -->

### Relational and logical operators {.unnumbered}

Relational operators compare values and are often used when defining new variables and subsets of datasets. Here are the common relational operators in R:

+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Meaning                  | Operator   | Example      | Example Result                                                                                                                                         |
+==========================+============+==============+========================================================================================================================================================+
| Equal to                 | `==`       | `"A" == "a"` | `FALSE` (because R is case sensitive) *Note that == (double equals) is different from = (single equals), which acts like the assignment operator `<-`* |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Not equal to             | `!=`       | `2 != 0`     | `TRUE`                                                                                                                                                 |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Greater than             | `>`        | `4 > 2`      | `TRUE`                                                                                                                                                 |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Less than                | `<`        | `4 < 2`      | `FALSE`                                                                                                                                                |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Greater than or equal to | `>=`       | `6 >= 4`     | `TRUE`                                                                                                                                                 |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Less than or equal to    | `<=`       | `6 <= 4`     | `FALSE`                                                                                                                                                |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Value is missing         | `is.na()`  | `is.na(7)`   | `FALSE` (see page on [Missing data])                                                                                                                   |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+
| Value is not missing     | `!is.na()` | `!is.na(7)`  | `TRUE`                                                                                                                                                 |
+--------------------------+------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+

Logical operators, such as AND and OR, are often used to connect relational operators and create more complicated criteria. Complex statements might require parentheses ( ) for grouping and order of application.

+---------------------+-----------------------------------------------------------------------+
| Meaning             | Operator                                                              |
+=====================+=======================================================================+
| AND                 | `&`                                                                   |
+---------------------+-----------------------------------------------------------------------+
| OR                  | `|` (vertical bar)                                                    |
+---------------------+-----------------------------------------------------------------------+
| Parentheses         | `( )` Used to group criteria together and clarify order of operations |
+---------------------+-----------------------------------------------------------------------+

For example, below, we have a linelist with two variables we want to use to create our case definition, `hep_e_rdt`, a test result and `other_cases_in_hh`, which will tell us if there are other cases in the household. The command below uses the function `case_when()` to create the new variable `case_def` such that:

```{r eval=FALSE}
linelist_cleaned <- linelist %>%
  mutate(case_def = case_when(
    is.na(rdt_result) & is.na(other_case_in_home)            ~ NA_character_,
    rdt_result == "Positive"                                 ~ "Confirmed",
    rdt_result != "Positive" & other_cases_in_home == "Yes"  ~ "Probable",
    TRUE                                                     ~ "Suspected"
  ))
```

+------------------------------------------------------------------------------------------------+--------------------------------------------+
| Criteria in example above                                                                      | Resulting value in new variable "case_def" |
+================================================================================================+============================================+
| If the value for variables `rdt_result` and `other_cases_in_home` are missing                  | `NA` (missing)                             |
+------------------------------------------------------------------------------------------------+--------------------------------------------+
| If the value in `rdt_result` is "Positive"                                                     | "Confirmed"                                |
+------------------------------------------------------------------------------------------------+--------------------------------------------+
| If the value in `rdt_result` is NOT "Positive" AND the value in `other_cases_in_home` is "Yes" | "Probable"                                 |
+------------------------------------------------------------------------------------------------+--------------------------------------------+
| If one of the above criteria are not met                                                       | "Suspected"                                |
+------------------------------------------------------------------------------------------------+--------------------------------------------+

*Note that R is case-sensitive, so "Positive" is different than "positive"...*

<!-- ======================================================= -->

### Missing values {.unnumbered}

In R, missing values are represented by the special value `NA` (a "reserved" value) (capital letters N and A - not in quotation marks). If you import data that records missing data in another way (e.g. 99, "Missing", or .), you may want to re-code those values to `NA`. How to do this is addressed in the [Import and export] page.

**To test whether a value is `NA`, use the special function `is.na()`**, which returns `TRUE` or `FALSE`.

```{r basics_operators_missing}
rdt_result <- c("Positive", "Suspected", "Positive", NA)   # two positive cases, one suspected, and one unknown
is.na(rdt_result)  # Tests whether the value of rdt_result is NA
```

Read more about missing, infinite, `NULL`, and impossible values in the page on [Missing data]. Learn how to convert missing values when importing data in the page on [Import and export].

<!-- ======================================================= -->

### Mathematics and statistics {.unnumbered}

All the operators and functions in this page are automatically available using **base** R.

#### Mathematical operators {.unnumbered}

These are often used to perform addition, division, to create new columns, etc. Below are common mathematical operators in R. Whether you put spaces around the operators is not important.

| Purpose             | Example in R |
|---------------------|--------------|
| addition            | 2 + 3        |
| subtraction         | 2 - 3        |
| multiplication      | 2 \* 3       |
| division            | 30 / 5       |
| exponent            | 2\^3         |
| order of operations | ( )          |

#### Mathematical functions {.unnumbered}

| Purpose            | Function                              |
|--------------------|---------------------------------------|
| rounding           | round(x, digits = n)                  |
| rounding           | janitor::round_half_up(x, digits = n) |
| ceiling (round up) | ceiling(x)                            |
| floor (round down) | floor(x)                              |
| absolute value     | abs(x)                                |
| square root        | sqrt(x)                               |
| exponent           | exponent(x)                           |
| natural logarithm  | log(x)                                |
| log base 10        | log10(x)                              |
| log base 2         | log2(x)                               |

Note: for `round()` the `digits =` specifies the number of decimal placed. Use `signif()` to round to a number of significant figures.

#### Scientific notation {.unnumbered}

The likelihood of scientific notation being used depends on the value of the `scipen` option.

From the documentation of `?options`: scipen is a penalty to be applied when deciding to print numeric values in fixed or exponential notation. Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than 'scipen' digits wider.

If it is set to a low number (e.g. 0) it will be "turned on" always. To "turn off" scientific notation in your R session, set it to a very high number, for example:

```{r, eval=F}
# turn off scientific notation
options(scipen=999)
```

#### Rounding {.unnumbered}

[***DANGER:*** `round()` uses "banker's rounding" which rounds up from a .5 only if the upper number is even. Use `round_half_up()` from **janitor** to consistently round halves up to the nearest whole number. See [this explanation](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes) ]{style="color: red;"}

```{r}
# use the appropriate rounding function for your work
round(c(2.5, 3.5))

janitor::round_half_up(c(2.5, 3.5))
```

#### Statistical functions {.unnumbered}

[***CAUTION:*** The functions below will by default include missing values in calculations. Missing values will result in an output of `NA`, unless the argument `na.rm = TRUE` is specified. This can be written shorthand as `na.rm = T`.]{style="color: orange;"}

| Objective               | Function           |
|-------------------------|--------------------|
| mean (average)          | mean(x, na.rm=T)   |
| median                  | median(x, na.rm=T) |
| standard deviation      | sd(x, na.rm=T)     |
| quantiles\*             | quantile(x, probs) |
| sum                     | sum(x, na.rm=T)    |
| minimum value           | min(x, na.rm=T)    |
| maximum value           | max(x, na.rm=T)    |
| range of numeric values | range(x, na.rm=T)  |
| summary\*\*             | summary(x)         |

Notes:

-   `*quantile()`: `x` is the numeric vector to examine, and `probs =` is a numeric vector with probabilities within 0 and 1.0, e.g `c(0.5, 0.8, 0.85)`
-   `**summary()`: gives a summary on a numeric vector including mean, median, and common percentiles

[***DANGER:*** If providing a vector of numbers to one of the above functions, be sure to wrap the numbers within `c()` .]{style="color: red;"}

```{r}
# If supplying raw numbers to a function, wrap them in c()
mean(1, 6, 12, 10, 5, 0)    # !!! INCORRECT !!!  

mean(c(1, 6, 12, 10, 5, 0)) # CORRECT
```

#### Other useful functions {.unnumbered}

+----------------------------+-------------------+-------------------------------------------------+
| Objective                  | Function          | Example                                         |
+============================+===================+=================================================+
| create a sequence          | seq(from, to, by) | `seq(1, 10, 2)`                                 |
+----------------------------+-------------------+-------------------------------------------------+
| repeat x, n times          | rep(x, ntimes)    | `rep(1:3, 2)` or `rep(c("a", "b", "c"), 3)`     |
+----------------------------+-------------------+-------------------------------------------------+
| subdivide a numeric vector | cut(x, n)         | `cut(linelist$age, 5)`                          |
+----------------------------+-------------------+-------------------------------------------------+
| take a random sample       | sample(x, size)   | `sample(linelist$id, size = 5, replace = TRUE)` |
+----------------------------+-------------------+-------------------------------------------------+

<!-- ======================================================= -->

### `%in%` {.unnumbered}

A very useful operator for matching values, and for quickly assessing if a value is within a vector or dataframe.

```{r}
my_vector <- c("a", "b", "c", "d")
```

```{r}
"a" %in% my_vector
"h" %in% my_vector
```

To ask if a value is **not** `%in%` a vector, put an exclamation mark (!) **in front** of the logic statement:

```{r}
# to negate, put an exclamation in front
!"a" %in% my_vector
!"h" %in% my_vector
```

`%in%` is very useful when using the **dplyr** function `case_when()`. You can define a vector previously, and then reference it later. For example:

```{r eval=F}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")

linelist <- linelist %>% 
  mutate(child_hospitaled = case_when(
    hospitalized %in% affirmative & age < 18 ~ "Hospitalized Child",
    TRUE                                      ~ "Not"))
```

Note: If you want to detect a partial string, perhaps using `str_detect()` from **stringr**, it will not accept a character vector like `c("1", "Yes", "yes", "y")`. Instead, it must be given a *regular expression* - one condensed string with OR bars, such as "1\|Yes\|yes\|y". For example, `str_detect(hospitalized, "1|Yes|yes|y")`. See the page on [Characters and strings] for more information.

You can convert a character vector to a named regular expression with this command:

```{r}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")
affirmative

# condense to 
affirmative_str_search <- paste0(affirmative, collapse = "|")  # option with base R
affirmative_str_search <- str_c(affirmative, collapse = "|")   # option with stringr package

affirmative_str_search
```

<!-- ======================================================= -->

<!-- ======================================================= -->

<!-- ======================================================= -->

## Errors & warnings

This section explains:

-   The difference between errors and warnings\
-   General syntax tips for writing R code\
-   Code assists

Common errors and warnings and troubleshooting tips can be found in the page on [Errors and help].

<!-- ======================================================= -->

### Error versus Warning {.unnumbered}

When a command is run, the R Console may show you warning or error messages in red text.

-   A **warning** means that R has completed your command, but had to take additional steps or produced unusual output that you should be aware of.

-   An **error** means that R was not able to complete your command.

Look for clues:

-   The error/warning message will often include a line number for the problem.

-   If an object "is unknown" or "not found", perhaps you spelled it incorrectly, forgot to call a package with library(), or forgot to re-run your script after making changes.

If all else fails, copy the error message into Google along with some key terms - chances are that someone else has worked through this already!

<!-- ======================================================= -->

### General syntax tips {.unnumbered}

A few things to remember when writing commands in R, to avoid errors and warnings:

-   Always close parentheses - tip: count the number of opening "(" and closing parentheses ")" for each code chunk
-   Avoid spaces in column and object names. Use underscore ( \_ ) or periods ( . ) instead
-   Keep track of and remember to separate a function's arguments with commas
-   R is case-sensitive, meaning `Variable_A` is *different* from `variable_A`

<!-- ======================================================= -->

### Code assists {.unnumbered}

Any script (RMarkdown or otherwise) will give clues when you have made a mistake. For example, if you forgot to write a comma where it is needed, or to close a parentheses, RStudio will raise a flag on that line, on the right side of the script, to warn you.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/basics.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Transition to R { }  

Below, we provide some advice and resources if you are transitioning to R.  

R was introduced in the late 1990s and has since grown dramatically in scope. Its capabilities are so extensive that commercial alternatives have reacted to R developments in order to stay competitive! ([read this article comparing R, SPSS, SAS, STATA, and Python](https://www.inwt-statistics.com/read-blog/comparison-of-r-python-sas-spss-and-stata.html)).  

Moreover, R is much easier to learn than it was 10 years ago. Previously, R had a reputation of being difficult for beginners. It is now much easier with friendly user-interfaces like RStudio, intuitive code like the **tidyverse**, and many tutorial resources.  

<span style="color: darkgreen;">**Do not be intimidated - come discover the world of R!**</span>  

  

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "transition_door.png"))
```




## From Excel  

Transitioning from Excel directly to R is a very achievable goal. It may seem daunting, but you can do it!  

It is true that someone with strong Excel skills can do very advanced activities in Excel alone - even using scripting tools like VBA. Excel is used across the world and is an essential tool for an epidemiologist. However, complementing it with R can dramatically improve and expand your work flows.  

### Benefits {.unnumbered}  

You will find that using R offers immense benefits in time saved, more consistent and accurate analysis, reproducibility, shareability, and faster error-correction. Like any new software there is a learning "curve" of time you must invest to become familiar. The dividends will be significant and immense scope of new possibilities will open to you with R.  

Excel is a well-known software that can be easy for a beginner to use to produce simple analysis and visualizations with "point-and-click". In comparison, it can take a couple weeks to become comfortable with R functions and interface. However, R has evolved in recent years to become much more friendly to beginners.  

Many Excel workflows rely on memory and on repetition - thus, there is much opportunity for error. Furthermore, generally the data cleaning, analysis methodology, and equations used are hidden from view. It can require substantial time for a new colleague to learn what an Excel workbook is doing and how to troubleshoot it. With R, all the steps are explicitly written in the script and can be easily viewed, edited, corrected, and applied to other datasets.   


**To begin your transition from Excel to R you must adjust your mindset in a few important ways:**  


### Tidy data {.unnumbered}  

Use machine-readable "tidy" data instead of messy "human-readable" data. These are the three main requirements for "tidy" data, as explained in this tutorial on ["tidy" data in R](https://r4ds.had.co.nz/tidy-data.html):  

* Each variable must have its own column  
* Each observation must have its own row  
* Each value must have its own cell  

To Excel users - think of the role that [Excel "tables"](https://exceljet.net/excel-tables) play in standardizing data and making the format more predictable.  

An example of "tidy" data would be the case linelist used throughout this handbook - each variable is contained within one column, each observation (one case) has it's own row, and every value is in just one cell. Below you can view the first 50 rows of the linelist:  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

*The main reason one encounters non-tidy data is because many Excel spreadsheets are designed to prioritize easy reading by humans, not easy reading by machines/software.*  

To help you see the difference, below are some fictional examples of **non-tidy data** that prioritize *human*-readability over *machine*-readability:  

```{r, echo=F, out.width = "100%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_1.png"))
```


*Problems:* In the spreadsheet above, there are *merged cells* which are not easily digested by R. Which row should be considered the "header" is not clear. A color-based dictionary is to the right side and cell values are represented by colors - which is also not easily interpreted by R (nor by humans with color-blindness!). Furthermore, different pieces of information are combined into one cell (multiple partner organizations working in one area, or the status "TBC" in the same cell as "Partner D").  


```{r, echo=F, out.width = "100%", out.height="100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_2.png"))
```


*Problems:* In the spreadsheet above, there are numerous extra empty rows and columns within the dataset - this will cause cleaning headaches in R. Furthermore, the GPS coordinates are spread across two rows for a given treatment center. As a side note - the GPS coordinates are in two different formats!  

"Tidy" datasets may not be as readable to a human eye, but they make data cleaning and analysis much easier! Tidy data can be stored in various formats, for example "long" or "wide""(see page on [Pivoting data]), but the principles above are still observed.


### Functions {.unnumbered}  

The R word "function" might be new, but the concept exists in Excel too as *formulas*. Formulas in Excel also require precise syntax (e.g. placement of semicolons and parentheses). All you need to do is learn a few new functions and how they work together in R.  



### Scripts {.unnumbered}  

Instead of clicking buttons and dragging cells you will be writing *every* step and procedure into a "script". 
Excel users may be familiar with "VBA macros" which also employ a scripting approach.  

*The R script consists of step-by-step instructions.* This allows any colleague to read the script and easily see the steps you took. This also helps de-bug errors or inaccurate calculations. See the [R basics] section on scripts for examples.  

Here is an example of an R script:  

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "example_script.png"))
```







### Excel-to-R resources {.unnumbered}

Here are some links to tutorials to help you transition to R from Excel:  

* [R vs. Excel](https://www.northeastern.edu/graduate/blog/r-vs-excel/)  
* [RStudio course in R for Excel users](https://rstudio-conf-2020.github.io/r-for-excel/)  


### R-Excel interaction {.unnumbered}  

R has robust ways to import Excel workbooks, work with the data, export/save Excel files, and work with the nuances of Excel sheets.  

It is true that some of the more aesthetic Excel formatting can get lost in translation (e.g. italics, sideways text, etc.). If your work flow requires passing documents back-and-forth between R and Excel while retaining the original Excel formatting, try packages such as **openxlsx**.  







## From Stata  
<!-- ======================================================= -->

**Coming to R from Stata**  

Many epidemiologists are first taught how to use Stata, and it can seem daunting to move into R. However, if you are a comfortable Stata user then the jump into R is certainly more manageable than you might think. While there are some key differences between Stata and R in how data can be created and modified, as well as how analysis functions are implemented – after learning these key differences you will be able to translate your skills.

Below are some key translations between Stata and R, which may be handy as your review this guide.


**General notes**

**STATA**                    | **R**  
---------------------------- | ---------------------------------------------    
You can only view and manipulate one dataset at a time | You can view and manipulate multiple datasets at the same time, therefore you will frequently have to specify your dataset within the code
Online community available through [https://www.statalist.org/](https://www.statalist.org/) | Online community available through [RStudio](https://community.rstudio.com/), [StackOverFlow](https://stackoverflow.com/questions/tagged/r), and [R-bloggers](https://www.r-bloggers.com/)
Point and click functionality as an option | Minimal point and click functionality
Help for commands available by `help [command]` | Help available by `[function]?` or search in the Help pane
Comment code using * or /// or  /* TEXT */ | Comment code using #
Almost all commands are built-in to Stata. New/user-written functions can be installed as **ado** files using **ssc install** [package] | R installs with **base** functions, but typical use involves installing other packages from CRAN (see page on [R basics])
Analysis is usually written in a **do** file | Analysis written in an R script in the RStudio source pane. R markdown scripts are an alternative.


**Working directory**  

**STATA**                        | **R**  
-------------------------------- | ---------------------------------------------
Working directories involve absolute filepaths (e.g. "C:/usename/documents/projects/data/")| Working directories can be either absolute, or relative to a project root folder by using the **here** package (see [Import and export]) 
See current working directory with **pwd** | Use `getwd()` or `here()` (if using the **here** package), with empty parentheses 
Set working directory with **cd** “folder location” | Use `setwd(“folder location”)`, or `set_here("folder location)` (if using **here** package)

**Importing and viewing data**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Specific commands per file type | Use `import()` from **rio** package for almost all filetypes. Specific functions exist as alternatives (see [Import and export])
Reading in csv files is done by **import delimited** “filename.csv” | Use `import("filename.csv")`
Reading in xslx files is done by **import excel** “filename.xlsx” | Use `import("filename.xlsx")`
Browse your data in a new window using the command **browse** | View a dataset in the RStudio source pane using `View(dataset)`. *You need to specify your dataset name to the function in R because multiple datasets can be held at the same time. Note capital "V" in this function*
Get a high-level overview of your dataset using **summarize**, which provides the variable names and basic information | Get a high-level overview of your dataset using `summary(dataset)`

**Basic data manipulation**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Dataset columns are often referred to as "variables" | More often referred to as "columns" or sometimes as "vectors" or "variables"
No need to specify the dataset | In each of the below commands, you need to specify the dataset - see the page on [Cleaning data and core functions] for examples
New variables are created using the command **generate** *varname* =  | Generate new variables using the function `mutate(varname = )`. See page on [Cleaning data and core functions] for details on all the below **dplyr** functions.
Variables are renamed using **rename** *old_name new_name* | Columns can be renamed using the function `rename(new_name = old_name)`
Variables are dropped using **drop** *varname* | Columns can be removed using the function `select()` with the column name in the parentheses following a minus sign
Factor variables can be labeled using a series of commands such as **label define** | Labeling values can done by converting the column to Factor class and specifying levels. See page on [Factors]. Column names are not typically labeled as they are in Stata.

**Descriptive analysis**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Tabulate counts of a variable using **tab** *varname* | Provide the dataset and column name to `table()` such as `table(dataset$colname)`. Alternatively, use `count(varname)` from the **dplyr** package, as explained in [Grouping data]
Cross-tabulaton of two variables in a 2x2 table is done with **tab** *varname1 varname2* | Use `table(dataset$varname1, dataset$varname2` or `count(varname1, varname2)`


While this list gives an overview of the basics in translating Stata commands into R, it is not exhaustive. There are many other great resources for Stata users transitioning to R that could be of interest:  

* https://dss.princeton.edu/training/RStata.pdf  
* https://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html  
* http://r4stats.com/books/r4stata/  




## From SAS  
<!-- ======================================================= -->

**Coming from SAS to R**  

SAS is commonly used at public health agencies and academic research fields. Although transitioning to a new language is rarely a simple process, understanding key differences between SAS and R may help you start to navigate the new language using your native language. 
Below outlines the key translations in data management and descriptive analysis between SAS and R.   

**General notes**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Online community available through [SAS Customer Support](https://support.sas.com/en/support-home.html)|Online community available through RStudio, StackOverFlow, and R-bloggers
Help for commands available by `help [command]`|Help available by [function]? or search in the Help pane
Comment code using `* TEXT ;` or `/* TEXT */`|Comment code using #
Almost all commands are built-in.  Users can write new functions using SAS macro, SAS/IML, SAS Component Language (SCL), and most recently, procedures `Proc Fcmp` and `Proc Proto`|R installs with **base** functions, but typical use involves installing other packages from CRAN (see page on [R basics])
Analysis is usually conducted by writing a SAS program in the Editor window.|Analysis written in an R script in the RStudio source pane. R markdown scripts are an alternative.

**Working directory**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Working directories can be either absolute, or relative to a project root folder by defining the root folder using `%let rootdir=/root path; %include “&rootdir/subfoldername/filename”`|Working directories can be either absolute, or relative to a project root folder by using the **here** package (see [Import and export])
See current working directory with `%put %sysfunc(getoption(work));`|Use `getwd()` or `here()` (if using the **here** package), with empty parentheses
Set working directory with `libname “folder location”`|Use `setwd(“folder location”)`, or `set_here("folder location)` if using **here** package


**Importing and viewing data**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Use `Proc Import` procedure or using `Data Step Infile` statement.|Use `import()` from **rio** package for almost all filetypes. Specific functions exist as alternatives (see [Import and export])
Reading in csv files is done by using `Proc Import datafile=”filename.csv” out=work.filename dbms=CSV; run;` OR using [Data Step Infile statement](http://support.sas.com/techsup/technote/ts673.pdf)|Use `import("filename.csv")`
Reading in xslx files is done by using `Proc Import datafile=”filename.xlsx” out=work.filename dbms=xlsx; run;` OR using [Data Step Infile statement](http://support.sas.com/techsup/technote/ts673.pdf)|Use import("filename.xlsx")
Browse your data in a new window by opening the Explorer window and select desired library and the dataset|View a dataset in the RStudio source pane using View(dataset). You need to specify your dataset name to the function in R because multiple datasets can be held at the same time. Note capital “V” in this function

**Basic data manipulation**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Dataset columns are often referred to as “variables”|More often referred to as “columns” or sometimes as “vectors” or “variables”
No special procedures are needed to create a variable. New variables are created simply by typing the new variable name, followed by an equal sign, and then an expression for the value|Generate new variables using the function `mutate()`. See page on [Cleaning data and core functions] for details on all the below **dplyr** functions.
Variables are renamed using `rename *old_name=new_name*`|Columns can be renamed using the function `rename(new_name = old_name)`
Variables are kept using `**keep**=varname`|Columns can be selected using the function `select()` with the column name in the parentheses
Variables are dropped using `**drop**=varname`|Columns can be removed using the function `select()` with the column name in the parentheses following a minus sign
Factor variables can be labeled in the Data Step using `Label` statement|Labeling values can done by converting the column to Factor class and specifying levels. See page on [Factors]. Column names are not typically labeled.
Records are selected using `Where` or `If` statement in the Data Step. Multiple selection conditions are separated using “and” command.|Records are selected using the function `filter()` with multiple selection conditions separated either by an AND operator (&) or a comma  
Datasets are combined using `Merge` statement in the Data Step. The datasets to be merged need to be sorted first using `Proc Sort` procedure.|**dplyr** package offers a few functions for merging datasets. See page [Joining Data] for details.

**Descriptive analysis**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Get a high-level overview of your dataset using `Proc Summary` procedure, which provides the variable names and descriptive statistics|Get a high-level overview of your dataset using `summary(dataset)` or `skim(dataset)` from the **skimr** package
Tabulate counts of a variable using `proc freq data=Dataset; Tables varname; Run;`|See the page on [Descriptive tables]. Options include `table()` from **base** R, and `tabyl()` from **janitor** package, among others. Note you will need to specify the dataset and column name as R holds multiple datasets.
Cross-tabulation of two variables in a 2x2 table is done with `proc freq data=Dataset; Tables rowvar*colvar; Run;`|Again, you can use `table()`, `tabyl()` or other options as described in the [Descriptive tables] page.  

**Some useful resources:**  

[R for SAS and SPSS Users (2011)](https://www.amazon.com/SAS-SPSS-Users-Statistics-Computing/dp/1461406846/ref=sr_1_1?dchild=1&gclid=EAIaIQobChMIoqLOvf6u7wIVAhLnCh1c9w_DEAMYASAAEgJLIfD_BwE&hvadid=241675955927&hvdev=c&hvlocphy=9032185&hvnetw=g&hvqmt=e&hvrand=16854847287059617468&hvtargid=kwd-44746119007&hydadcr=16374_10302157&keywords=r+for+sas+users&qid=1615698213&sr=8-1)

[SAS and R, Second Edition (2014)](https://www.amazon.com/SAS-Management-Statistical-Analysis-Graphics-dp-1466584491/dp/1466584491/ref=dp_ob_title_bk)



## Data interoperability  
<!-- ======================================================= -->

See the [Import and export] page for details on how the R package **rio** can import and export files such as STATA .dta files, SAS .xpt and.sas7bdat files, SPSS .por and.sav files, and many others.  



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/transition_to_R.Rmd-->

# Suggested packages

Below is a long list of suggested packages for common epidemiological work in R. You can copy this code, run it, and all of these packages will install from CRAN and load for use in the current R session. If a package is already installed, it will be loaded for use only.  

You can modify the code with `#` symbols to exclude any packages you do not want.  

Of note:  

* Install the **pacman** package first before running the below code. You can do this with `install.packages("pacman")`. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use in the current R session. You can also load packages that are already installed with `library()` from **base** R.  
* In the code below, packages that are included when installing/loading another package are indicated by an indent and hash. For example how **ggplot2** is listed under **tidyverse**.  
* If multiple packages have functions with the same name, *masking* can occur when the function from the more recently-loaded package takes precedent. Read more in the [R basics] page. Consider using the package **conflicted** to manage such conflicts.  
* See the [R basics] section on packages for more information on **pacman** and masking.  

To see the versions of R, RStudio, and R packages used during the production of this handbook, see the page on [Editorial and technical notes].  

## Packages from CRAN  

```{r, eval=F}

##########################################
# List of useful epidemiology R packages #
##########################################

# This script uses the p_load() function from pacman R package, 
# which installs if package is absent, and loads for use if already installed


# Ensures the package "pacman" is installed
if (!require("pacman")) install.packages("pacman")


# Packages available from CRAN
##############################
pacman::p_load(
     
     # learning R
     ############
     learnr,   # interactive tutorials in RStudio Tutorial pane
     swirl,    # interactive tutorials in R console
        
     # project and file management
     #############################
     here,     # file paths relative to R project root folder
     rio,      # import/export of many types of data
     openxlsx, # import/export of multi-sheet Excel workbooks 
     
     # package install and management
     ################################
     pacman,   # package install/load
     renv,     # managing versions of packages when working in collaborative groups
     remotes,  # install from github
     
     # General data management
     #########################
     tidyverse,    # includes many packages for tidy data wrangling and presentation
          #dplyr,      # data management
          #tidyr,      # data management
          #ggplot2,    # data visualization
          #stringr,    # work with strings and characters
          #forcats,    # work with factors 
          #lubridate,  # work with dates
          #purrr       # iteration and working with lists
     linelist,     # cleaning linelists
     naniar,       # assessing missing data
     
     # statistics  
     ############
     janitor,      # tables and data cleaning
     gtsummary,    # making descriptive and statistical tables
     rstatix,      # quickly run statistical tests and summaries
     broom,        # tidy up results from regressions
     lmtest,       # likelihood-ratio tests
     easystats,
          # parameters, # alternative to tidy up results from regressions
          # see,        # alternative to visualise forest plots 
     
     # epidemic modeling
     ###################
     epicontacts,  # Analysing transmission networks
     EpiNow2,      # Rt estimation
     EpiEstim,     # Rt estimation
     projections,  # Incidence projections
     incidence2,   # Make epicurves and handle incidence data
     i2extras,     # Extra functions for the incidence2 package
     epitrix,      # Useful epi functions
     distcrete,    # Discrete delay distributions
     
     
     # plots - general
     #################
     #ggplot2,         # included in tidyverse
     cowplot,          # combining plots  
     # patchwork,      # combining plots (alternative)     
     RColorBrewer,     # color scales
     ggnewscale,       # to add additional layers of color schemes

     
     # plots - specific types
     ########################
     DiagrammeR,       # diagrams using DOT language
     incidence2,       # epidemic curves
     gghighlight,      # highlight a subset
     ggrepel,          # smart labels
     plotly,           # interactive graphics
     gganimate,        # animated graphics 

     
     # gis
     ######
     sf,               # to manage spatial data using a Simple Feature format
     tmap,             # to produce simple maps, works for both interactive and static maps
     OpenStreetMap,    # to add OSM basemap in ggplot map
     spdep,            # spatial statistics 
     
     # routine reports
     #################
     rmarkdown,        # produce PDFs, Word Documents, Powerpoints, and HTML files
     reportfactory,    # auto-organization of R Markdown outputs
     officer,          # powerpoints
     
     # dashboards
     ############
     flexdashboard,    # convert an R Markdown script into a dashboard
     shiny,            # interactive web apps
     
     # tables for presentation
     #########################
     knitr,            # R Markdown report generation and html tables
     flextable,        # HTML tables
     #DT,              # HTML tables (alternative)
     #gt,              # HTML tables (alternative)
     #huxtable,        # HTML tables (alternative) 
     
     # phylogenetics
     ###############
     ggtree,           # visualization and annotation of trees
     ape,              # analysis of phylogenetics and evolution
     treeio            # to visualize phylogenetic files
 
)

```

## Packages from Github  


Below are commmands to install two packages directly from Github repositories.  

* The development version of **epicontacts** contains the ability to make transmission trees with an temporal x-axis  
* The **epirhandbook** package contains all the example data for this handbook and can be used to download the offline version of the handbook.  


```{r, eval=F}
# Packages to download from Github (not available on CRAN)
##########################################################

# Development version of epicontacts (for transmission chains with a time x-axis)
pacman::p_install_gh("reconhub/epicontacts@timeline")

# The package for this handbook, which includes all the example data  
pacman::p_install_gh("appliedepi/epirhandbook")



```

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/packages_suggested.Rmd-->


# R projects {}  


An R project enables your work to be bundled in a portable, self-contained folder. Within the project, all the relevant scripts, data files, figures/outputs, and history are stored in sub-folders and importantly - the *working directory* is the project's root folder.  


## Suggested use  

A common, efficient, and trouble-free way to use R is to combine these 3 elements. One discrete work project is hosted within one R project. Each element is described in the sections below.  

1) An **R project**  
     - A self-contained working environment with folders for data, scripts, outputs, etc.  
2) The **here** package for relative filepaths  
     - Filepaths are written relative to the root folder of the R project - see [Import and export] for more information  
3) The **rio** package for importing/exporting  
     - `import()` and `export()` handle any file type by by its extension (e.g. .csv, .xlsx, .png)  
     
     


<!-- ======================================================= -->
## Creating an R project {}

To create an R project, select "New Project" from the File menu.

* If you want to create a new folder for the project, select "New directory" and indicate where you want it to be created.  
* If you want to create the project within an existing folder, click "Existing directory" and indicate the folder.  
* If you want to clone a Github repository, select the third option "Version Control" and then "Git". See the page on [Version control and collaboration with Git and Github] for further details.  


```{r out.width = "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "create_project.png"))
```


The R project you create will come in the form of a folder containing a *.Rproj* file. This file is a shortcut and likely the primary way you will open your project. You can also open a project by selecting "Open Project" from the File menu. Alternatively on the far upper right side of RStudio you will see an R project icon and a drop-down menu of available R projects. 

To exit from an R project, either open a new project, or close the project (File - Close Project).  


### Switch projects {.unnumbered}

To switch between projects, click the R project icon and drop-down menu at the very top-right of RStudio. You will see options to Close Project, Open Project, and a list of recent projects.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Rproject_dropdown.png"))
```


### Settings {.unnumbered}  

It is generally advised that you start RStudio each time with a "clean slate" - that is, with your workspace **not** preserved from your previous session. This will mean that your objects and results will not persist session-to-session (you must re-create them by running your scripts). This is good, because it will force you to write better scripts and avoid errors in the long run.  

To set RStudio to have a "clean slate" each time at start-up:  

* Select "Project Options" from the Tools menu.  
* In the "General" tab, set RStudio to **not** restore .RData into workspace at startup, and to **not** save workspace to .RData on exit.  



### Organization {.unnumbered}  

It is common to have subfolders in your project. Consider having folders such as "data", "scripts", "figures", "presentations". You can add folders in the typical way you would add a new folder for your computer. Alternatively, see the page on [Directory interactions] to learn how to create new folders with R commands.  


### Version control {.unnumbered}  

Consider a version control system. It could be something as simple as having dates on the names of scripts (e.g. "transmission_analysis_2020-10-03.R") and an "archive" folder. Consider also having commented header text at the top of each script with a description, tags, authors, and change log.  

A more complicated method would involve using Github or a similar platform for version control. See the page on [Version control and collaboration with Git and Github].  

One tip is that you can search across an entire project or folder using the "Find in Files" tool (Edit menu). It can search and even replace strings across multiple files.  






## Examples  

Below are some examples of import/export/saving using `here()` from within an R projct. Read more about using the **here** package in the [Import and export] page.  


*Importing `linelist_raw.xlsx` from the "data" folder in your R project*  

```{r eval=F}
linelist <- import(here("data", "linelist_raw.xlsx"))
```

*Exporting the R object `linelist` as "my_linelist.rds" to the "clean" folder within the "data" folder in your R project.*   

```{r, eval=F}
export(linelist, here("data","clean", "my_linelist.rds"))
```

*Saving the most recently printed plot as "epicurve_2021-02-15.png" within the "epicurves" folder in "outputs" folder in your R project.*  

```{r, eval=F}
ggsave(here("outputs", "epicurves", "epicurve_2021-02-15.png"))
```




<!-- ======================================================= -->
## Resources {}

RStudio webpage on [using R projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/r_projects.Rmd-->

# Import and export {}


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Import_Export_1500x500.png"))
```



In this page we describe ways to locate, import, and export files:  

* Use of the **rio** package to flexibly `import()` and `export()` many types of files  
* Use of the **here** package to locate files relative to an R project root - to prevent complications from file paths that are specific to one computer  
* Specific import scenarios, such as:  
  * Specific Excel sheets  
  * Messy headers and skipping rows  
  * From Google sheets  
  * From data posted to websites  
  * With APIs  
  * Importing the *most recent* file  
* Manual data entry  
* R-specific file types such as RDS and RData  
* Exporting/saving files and plots  


<!-- ======================================================= -->
## Overview

When you import a "dataset" into R, you are generally creating a new *data frame* object in your R environment and defining it as an imported file (e.g. Excel, CSV, TSV, RDS) that is located in your folder directories at a certain file path/address.  

You can import/export many types of files, including those created by other statistical programs (SAS, STATA, SPSS). You can also connect to relational databases.  

R even has its own data formats:  

* An RDS file (.rds) stores a single R object such as a data frame. These are useful to store cleaned data, as they maintain R column classes. Read more in [this section](#import_rds).    
* An RData file (.Rdata) can be used to store multiple objects, or even a complete R workspace. Read more in [this section](#import_rdata).  


<!-- ======================================================= -->
## The **rio** package {}  

The R package we recommend is: **rio**. The name "rio" is an abbreviation of "R I/O" (input/output).  

Its functions `import()` and `export()` can handle many different file types (e.g. .xlsx, .csv, .rds, .tsv). When you provide a file path to either of these functions (including the file extension like ".csv"), **rio** will read the extension and use the correct tool to import or export the file.  

The alternative to using **rio** is to use functions from many other packages, each of which is specific to a type of file. For example, `read.csv()` (**base** R), `read.xlsx()` (**openxlsx** package), and `write_csv()` (**readr** pacakge), etc. These alternatives can be difficult to remember, whereas using `import()` and `export()` from **rio** is easy.  

**rio**'s functions `import()` and `export()` use the appropriate package and function for a given file, based on its file extension. See the end of this page for a complete table of which packages/functions **rio** uses in the background. It can also be used to import STATA, SAS, and SPSS files, among dozens of other file types.  

Import/export of shapefiles requires other packages, as detailed in the page on [GIS basics].    





## The **here** package {#here}

The package **here** and its function `here()` make it easy to tell R where to find and to save your files - in essence, it builds file paths.  

Used in conjunction with an R project, **here** allows you to describe the location of files in your R project in relation to the R project's *root directory* (the top-level folder). This is useful when the R project may be shared or accessed by multiple people/computers. It prevents complications due to the unique file paths on different computers (e.g. `"C:/Users/Laura/Documents..."` by "starting" the file path in a place common to all users (the R project root).  

This is how `here()` works within an R project:  

* When the **here** package is first loaded within the R project, it places a small file called ".here" in the root folder of your R project as a "benchmark" or "anchor"  
* In your scripts, to reference a file in the R project's sub-folders, you use the function `here()` to build the file path *in relation to that anchor*
* To build the file path, write the names of folders beyond the root, within quotes, separated by commas, finally ending with the file name and file extension as shown below  
* `here()` file paths can be used for both importing and exporting  

For example, below, the function `import()` is being provided a file path constructed with `here()`.  

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```

The command `here("data", "linelists", "ebola_linelist.xlsx")` is actually providing the full file path that is *unique to the user's computer*:  

```
"C:/Users/Laura/Documents/my_R_project/data/linelists/ebola_linelist.xlsx"
```

The beauty is that the R command using `here()` can be successfully run on any computer accessing the R project.   


<span style="color: darkgreen;">**_TIP:_** If you are unsure where the “.here” root is set to, run the function `here()` with empty parentheses.</span>  

Read more about the **here** package [at this link](https://here.r-lib.org/).  



<!-- ======================================================= -->
## File paths  

When importing or exporting data, you must provide a file path. You can do this one of three ways:  

1) *Recommended:* provide a "relative" file path with the **here** package  
2) Provide the "full" / "absolute" file path  
3) Manual file selection  



### "Relative" file paths {.unnumbered}

In R, "relative" file paths consist of the file path *relative to* the root of an R project. They allow for more simple file paths that can work on different computers (e.g. if the R project is on a shared drive or is sent by email). As described [above](#here), relative file paths are facilitated by use of the **here** package.  

An example of a relative file path constructed with `here()` is below. We assume the work is in an R project that contains a sub-folder "data" and within that a subfolder "linelists", in which there is the .xlsx file of interest.  

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```



### "Absolute" file paths {.unnumbered}  

Absolute or "full" file paths can be provided to functions like `import()` but they are "fragile" as they are unique to the user's specific computer and therefore *not recommended*. 

Below is an example of an absolute file path, where in Laura's computer there is a folder "analysis", a sub-folder "data" and within that a sub-folder "linelists", in which there is the .xlsx file of interest.  

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/analysis/data/linelists/ebola_linelist.xlsx")
```

A few things to note about absolute file paths:  

* **Avoid using absolute file paths** as they will break if the script is run on a different computer
* Use *forward* slashes (`/`), as in the example above (note: this is *NOT* the default for Windows file paths)  
* File paths that begin with double slashes (e.g. "//...") will likely **not be recognized by R** and will produce an error. Consider moving your work to a "named" or "lettered" drive that begins with a letter (e.g. "J:" or "C:"). See the page on [Directory interactions] for more details on this issue.  

One scenario where absolute file paths may be appropriate is when you want to import a file from a shared drive that has the same full file path for all users.  

<span style="color: darkgreen;">**_TIP:_** To quickly convert all `\` to `/`, highlight the code of interest, use Ctrl+f (in Windows), check the option box for "In selection", and then use the replace functionality to convert them.</span>  



<!-- ======================================================= -->
### Select file manually {.unnumbered}

You can import data manually via one of these methods:  

1) Environment RStudio Pane, click "Import Dataset", and select the type of data 
2) Click File / Import Dataset / (select the type of data)  
3) To hard-code manual selection, use the *base R* command `file.choose()` (leaving the parentheses empty) to trigger appearance of a **pop-up window** that allows the user to manually select the file from their computer. For example:  

```{r import_choose, eval=F}
# Manual selection of a file. When this command is run, a POP-UP window will appear. 
# The file path selected will be supplied to the import() command.

my_data <- import(file.choose())
```

<span style="color: darkgreen;">**_TIP:_** The **pop-up window** may appear BEHIND your RStudio window.</span>



## Import data  

To use `import()` to import a dataset is quite simple. Simply provide the path to the file (including the file name and file extension) in quotes. If using `here()` to build the file path, follow the instructions above. Below are a few examples:  

Importing a csv file that is located in your "working directory" or in the R project root folder:  

```{r, eval=F}
linelist <- import("linelist_cleaned.csv")
```


Importing the first sheet of an Excel workbook that is located in "data" and "linelists" sub-folders of the R project (the file path built using `here()`):  

```{r, eval=F}
linelist <- import(here("data", "linelists", "linelist_cleaned.xlsx"))
```


Importing a data frame (a .rds file) using an absolute file path:  

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/tuberculosis/data/linelists/linelist_cleaned.rds")
```





### Specific Excel sheets {.unnumbered}

By default, if you provide an Excel workbook (.xlsx) to `import()`, the workbook's first sheet will be imported. If you want to import a specific **sheet**, include the sheet name to the `which = ` argument. For example:  

```{r eval=F}
my_data <- import("my_excel_file.xlsx", which = "Sheetname")
```

If using the `here()` method to provide a relative pathway to `import()`, you can still indicate a specific sheet by adding the `which = ` argument after the closing parentheses of the `here()` function.  

```{r import_sheet_here, eval=F}
# Demonstration: importing a specific Excel sheet when using relative pathways with the 'here' package
linelist_raw <- import(here("data", "linelist.xlsx"), which = "Sheet1")`  
```

To *export* a data frame from R to a specific Excel sheet and have the rest of the Excel workbook remain unchanged, you will have to import, edit, and export with an alternative package catered to this purpose such as **openxlsx**. See more information in the page on [Directory interactions] or [at this github page](https://ycphs.github.io/openxlsx/).

If your Excel workbook is .xlsb (binary format Excel workbook) you may not be able to import it using **rio**. Consider re-saving it as .xlsx, or using a package like **readxlsb** which is built for [this purpose](https://cran.r-project.org/web/packages/readxlsb/vignettes/read-xlsb-workbook.html).  






<!-- ======================================================= -->
### Missing values {#import_missing .unnumbered} 

You may want to designate which value(s) in your dataset should be considered as missing. As explained in the page on [Missing data], the value in R for missing data is `NA`, but perhaps the dataset you want to import uses 99, "Missing", or just empty character space "" instead.  

Use the `na = ` argument for `import()` and provide the value(s) within quotes (even if they are numbers). You can specify multiple values by including them within a vector, using `c()` as shown below.  

Here, the value "99" in the imported dataset is considered missing and converted to `NA` in R.  

```{r, eval=F}
linelist <- import(here("data", "my_linelist.xlsx"), na = "99")
```

Here, any of the values "Missing", "" (empty cell), or " " (single space) in the imported dataset are converted to `NA` in R.  

```{r, eval=F}
linelist <- import(here("data", "my_linelist.csv"), na = c("Missing", "", " "))
```


<!-- ======================================================= -->
### Skip rows {.unnumbered} 

Sometimes, you may want to avoid importing a row of data. You can do this with the argument `skip = ` if using `import()` from **rio** on a .xlsx or .csv file. Provide the number of rows you want to skip. 


```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx", skip = 1)  # does not import header row
```

Unfortunately `skip = ` only accepts one integer value, *not* a range (e.g. "2:10" does not work). To skip import of specific rows that are not consecutive from the top, consider importing multiple times and using `bind_rows()` from **dplyr**. See the example below of skipping only row 2.  



### Manage a second header row {.unnumbered}  

Sometimes, your data may have a *second* row, for example if it is a "data dictionary" row as shown below. This situation can be problematic because it can result in all columns being imported as class "character".  

```{r, echo=F}
# HIDDEN FROM READER
####################
# Create second header row of "data dictionary" and insert into row 2. Save as new dataframe.
linelist_2headers <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>%         
        mutate(across(everything(), as.character)) %>% 
        add_row(.before = 1,
                #row_num = "000",
                case_id = "case identification number assigned by MOH",
                generation = "transmission chain generation number",
                date_infection = "estimated date of infection, mm/dd/yyyy",
                date_onset = "date of symptom onset, YYYY-MM-DD",
                date_hospitalisation = "date of initial hospitalization, mm/dd/yyyy",
                date_outcome = "date of outcome status determination",
                outcome = "either 'Death' or 'Recovered' or 'Unknown'",
                gender = "either 'm' or 'f' or 'unknown'",
                hospital = "Name of hospital of first admission",
                lon = "longitude of residence, approx",
                lat = "latitude of residence, approx",
                infector = "case_id of infector",
                source = "context of known transmission event",
                age = "age number",
                age_unit = "age unit, either 'years' or 'months' or 'days'",
                fever = "presence of fever on admission, either 'yes' or 'no'",
                chills = "presence of chills on admission, either 'yes' or 'no'",
                cough = "presence of cough on admission, either 'yes' or 'no'",
                aches = "presence of aches on admission, either 'yes' or 'no'",
                vomit = "presence of vomiting on admission, either 'yes' or 'no'",
                time_admission = "time of hospital admission HH:MM")
```

Below is an example of this kind of dataset (with the first row being the data dictionary).  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_2headers, 5), rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```

#### Remove the second header row {.unnumbered}  

To drop the second header row, you will likely need to import the data twice.  

1) Import the data in order to store the correct column names  
2) Import the data again, skipping the first *two* rows (header and second rows)  
3) Bind the correct names onto the reduced dataframe

The exact argument used to bind the correct column names depends on the type of data file (.csv, .tsv, .xlsx, etc.). This is because **rio** is using a different function for the different file types (see table above).  

**For Excel files:** (`col_names = `)  

```{r, eval=F}
# import first time; store the column names
linelist_raw_names <- import("linelist_raw.xlsx") %>% names()  # save true column names

# import second time; skip row 2, and assign column names to argument col_names =
linelist_raw <- import("linelist_raw.xlsx",
                       skip = 2,
                       col_names = linelist_raw_names
                       ) 
```

**For CSV files:** (`col.names = `)  

```{r, eval=F}
# import first time; sotre column names
linelist_raw_names <- import("linelist_raw.csv") %>% names() # save true column names

# note argument for csv files is 'col.names = '
linelist_raw <- import("linelist_raw.csv",
                       skip = 2,
                       col.names = linelist_raw_names
                       ) 
```

**Backup option** - changing column names as a separate command

```{r, eval=F}
# assign/overwrite headers using the base 'colnames()' function
colnames(linelist_raw) <- linelist_raw_names
```


#### Make a data dictionary {.unnumbered}  

Bonus! If you do have a second row that is a data dictionary, you can easily create a proper data dictionary from it. This tip is adapted from this [post](https://alison.rbind.io/post/2018-02-23-read-multiple-header-rows/).  


```{r}
dict <- linelist_2headers %>%             # begin: linelist with dictionary as first row
  head(1) %>%                             # keep only column names and first dictionary row                
  pivot_longer(cols = everything(),       # pivot all columns to long format
               names_to = "Column",       # assign new column names
               values_to = "Description")
```


```{r message=FALSE, echo=F}
DT::datatable(dict, rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```



#### Combine the two header rows {.unnumbered}  

In some cases when your raw dataset has *two* header rows (or more specifically, the 2nd row of data is a secondary header), you may want to "combine" them or add the values in the second header row into the first header row.  

The command below will define the data frame's column names as the combination (pasting together) of the first (true) headers with the value immediately underneath (in the first row).  

```{r, eval=F}
names(my_data) <- paste(names(my_data), my_data[1, ], sep = "_")
```



<!-- ======================================================= -->
### Google sheets {.unnumbered}

You can import data from an online Google spreadsheet with the **googlesheet4** package and by authenticating your access to the spreadsheet.  


```{r, eval=F}
pacman::p_load("googlesheets4")
```

Below, a demo Google sheet is imported and saved. This command may prompt confirmation of authentification of your Google account. Follow prompts and pop-ups in your internet browser to grant Tidyverse API packages permissions to edit, create, and delete your spreadsheets in Google Drive.  


The sheet below is "viewable for anyone with the link" and you can try to import it.  

```{r, eval=F}
Gsheets_demo <- read_sheet("https://docs.google.com/spreadsheets/d/1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY/edit#gid=0")
```

The sheet can also be imported using only the sheet ID, a shorter part of the URL:  

```{r, eval=F}
Gsheets_demo <- read_sheet("1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY")
```


Another package, **googledrive** offers useful functions for writing, editing, and deleting Google sheets. For example, using the  `gs4_create()` and `sheet_write()` functions found in this package. 

Here are some other helpful online tutorials:  
[basic Google sheets importing tutorial](https://arbor-analytics.com/post/getting-your-data-into-r-from-google-sheets/)  
[more detailed tutorial](https://googlesheets4.tidyverse.org/articles/googlesheets4.html)  
[interaction between the googlesheets4 and tidyverse](https://googlesheets4.tidyverse.org/articles/articles/drive-and-sheets.html)  




## Multiple files - import, export, split, combine  

See the page on [Iteration, loops, and lists] for examples of how to import and combine multiple files, or multiple Excel workbook files. That page also has examples on how to split a data frame into parts and export each one separately, or as named sheets in an Excel workbook.  




<!-- ======================================================= -->
## Import from Github {#import_github}

Importing data directly from Github into R can be very easy or can require a few steps - depending on the file type. Below are some approaches:  

### CSV files {.unnumbered}  

It can be easy to import a .csv file directly from Github into R with an R command.  

1) Go to the Github repo, locate the file of interest, and click on it  
3) Click on the "Raw" button (you will then see the "raw" csv data, as shown below)  
4) Copy the URL (web address)  
5) Place the URL in quotes within the `import()` R command  

```{r, out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_csv_raw.png"))
```

### XLSX files {.unnumbered}  

You may not be able to view the "Raw" data for some files (e.g. .xlsx, .rds, .nwk, .shp)  

1) Go to the Github repo, locate the file of interest, and click on it  
2) Click the "Download" button, as shown below  
3) Save the file on your computer, and import it into R  


```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_xlsx.png"))
```

### Shapefiles {.unnumbered} 

Shapefiles have many sub-component files, each with a different file extention. One file will have the ".shp" extension, but others may have ".dbf", ".prj", etc.  To download a shapefile from Github, you will need to download each of the sub-component files individually, and save them in the *same* folder on your computer. In Github, click on each file individually and download them by clicking on the "Download" button.  

Once saved to your computer you can import the shapefile as shown in the [GIS basics] page using `st_read()` from the **sf** package. You only need to provide the filepath and name of the ".shp" file - as long as the other related files are within the same folder on your computer.  

Below, you can see how the shapefile "sle_adm3" consists of many files - each of which must be downloaded from Github.  

```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```





<!-- ======================================================= -->
## Manual data entry {}

### Entry by rows {.unnumbered}  

Use the `tribble` function from the **tibble** package from the tidyverse ([online tibble reference](https://tibble.tidyverse.org/reference/tribble.html)).  
  
Note how column headers start with a *tilde* (`~`).  Also note that each column must contain only one class of data (character, numeric, etc.). You can use tabs, spacing, and new rows to make the data entry more intuitive and readable. Spaces do not matter between values, but each row is represented by a new line of code. For example:  

```{r import_manual_row}
# create the dataset manually by row
manual_entry_rows <- tibble::tribble(
  ~colA, ~colB,
  "a",   1,
  "b",   2,
  "c",   3
  )
```

And now we display the new dataset:  

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_rows)
```


### Entry by columns {.unnumbered}  

Since a data frame consists of vectors (vertical columns), the **base** approach to manual dataframe creation in R expects you to define each column and then bind them together. This can be counter-intuitive in epidemiology, as we usually think about our data in rows (as above). 

```{r import_manual_col}
# define each vector (vertical column) separately, each with its own name
PatientID <- c(235, 452, 778, 111)
Treatment <- c("Yes", "No", "Yes", "Yes")
Death     <- c(1, 0, 1, 0)
```

<span style="color: orange;">**_CAUTION:_** All vectors must be the same length (same number of values).</span>

The vectors can then be bound together using the function `data.frame()`:  

```{r}
# combine the columns into a data frame, by referencing the vector names
manual_entry_cols <- data.frame(PatientID, Treatment, Death)
```

And now we display the new dataset:  

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_cols)
```




### Pasting from clipboard {.unnumbered}  

If you copy data from elsewhere and have it on your clipboard, you can try one of the two ways below:  

From the **clipr** package, you can use `read_clip_tbl()` to import as a data frame, or just just `read_clip()` to import as a character vector. In both cases, leave the parentheses empty.    

```{r, eval=F}
linelist <- clipr::read_clip_tbl()  # imports current clipboard as data frame
linelist <- clipr::read_clip()      # imports as character vector
```
You can also easily export to your system's clipboard with **clipr**. See the section below on Export.  


Alternatively, you can use the the `read.table()` function from **base** R with `file = "clipboard")` to import as a data frame:  

```{r, eval=F}
df_from_clipboard <- read.table(
  file = "clipboard",  # specify this as "clipboard"
  sep = "t",           # separator could be tab, or commas, etc.
  header=TRUE)         # if there is a header row
```






## Import most recent file  

Often you may receive daily updates to your datasets. In this case you will want to write code that imports the most recent file. Below we present two ways to approach this:  

* Selecting the file based on the date in the file name  
* Selecting the file based on file metadata (last modification)  


### Dates in file name {.unnumbered}  

This approach depends on three premises:  

1) You trust the dates in the file names  
2) The dates are numeric and appear in *generally* the same format (e.g. year then month then day)  
3) There are no other numbers in the file name  

We will explain each step, and then show you them combined at the end.  

First, use `dir()` from **base** R to extract just the file names for each file in the folder of interest. See the page on [Directory interactions] for more details about `dir()`. In this example, the folder of interest is the folder "linelists" within the folder "example" within "data" within the R project. 

```{r}
linelist_filenames <- dir(here("data", "example", "linelists")) # get file names from folder
linelist_filenames                                              # print
```

Once you have this vector of names, you can extract the dates from them by applying `str_extract()` from **stringr** using this regular expression. It extracts any numbers in the file name (including any other characters in the middle such as dashes or slashes). You can read more about **stringr** in the [Strings and characters] page.  

```{r}
linelist_dates_raw <- stringr::str_extract(linelist_filenames, "[0-9].*[0-9]") # extract numbers and any characters in between
linelist_dates_raw  # print
```

Assuming the dates are written in generally the same date format (e.g. Year then Month then Day) and the years are 4-digits, you can use **lubridate**'s flexible conversion functions (`ymd()`, `dmy()`, or `mdy()`) to convert them to dates. For these functions, the dashes, spaces, or slashes do not matter, only the order of the numbers. Read more in the [Working with dates] page.  

```{r}
linelist_dates_clean <- lubridate::ymd(linelist_dates_raw)
linelist_dates_clean
```


The **base** R function `which.max()` can then be used to return the index position (e.g. 1st, 2nd, 3rd, ...) of the maximum date value. The latest file is correctly identified as the 6th file - "case_linelist_2020-10-08.xlsx".  

```{r}
index_latest_file <- which.max(linelist_dates_clean)
index_latest_file
```

If we condense all these commands, the complete code could look like below. Note that the `.` in the last line is a placeholder for the piped object at that point in the pipe sequence. At that point the value is simply the number 6. This is placed in double brackets to extract the 6th element of the vector of file names produced by `dir()`.    

```{r}
# load packages
pacman::p_load(
  tidyverse,         # data management
  stringr,           # work with strings/characters
  lubridate,         # work with dates
  rio,               # import / export
  here,              # relative file paths
  fs)                # directory interactions

# extract the file name of latest file
latest_file <- dir(here("data", "example", "linelists")) %>%  # file names from "linelists" sub-folder          
  str_extract("[0-9].*[0-9]") %>%                  # pull out dates (numbers)
  ymd() %>%                                        # convert numbers to dates (assuming year-month-day format)
  which.max() %>%                                  # get index of max date (latest file)
  dir(here("data", "example", "linelists"))[[.]]              # return the filename of latest linelist

latest_file  # print name of latest file
```

You can now use this name to finish the relative file path, with `here()`:  

```{r, eval=F}
here("data", "example", "linelists", latest_file) 
```

And you can now import the latest file:  

```{r, eval=F}
# import
import(here("data", "example", "linelists", latest_file)) # import 
```

 



### Use the file info {.unnumbered}  

If your files do not have dates in their names (or you do not trust those dates), you can try to extract the last modification date from the file metadata. Use functions from the package **fs** to examine the metadata information for each file, which includes the last modification time and the file path.  

Below, we provide the folder of interest to **fs**'s `dir_info()`. In this case, the folder of interest is in the R project in the folder "data", the sub-folder "example", and its sub-folder "linelists".  The result is a data frame with one line per file and columns for `modification_time`, `path`, etc. You can see a visual example of this in the page on [Directory interactions].    

We can sort this data frame of files by the column `modification_time`, and then keep only the top/latest row (file) with **base** R's `head()`. Then we can extract the file path of this latest file only with the **dplyr** function `pull()` on the column `path`. Finally we can pass this file path to `import()`. The imported file is saved as `latest_file`.  

```{r, eval=F}
latest_file <- dir_info(here("data", "example", "linelists")) %>%  # collect file info on all files in directory
  arrange(desc(modification_time)) %>%      # sort by modification time
  head(1) %>%                               # keep only the top (latest) file
  pull(path) %>%                            # extract only the file path
  import()                                  # import the file

```



<!-- ======================================================= -->
## APIs {#import_api}

An "Automated Programming Interface" (API) can be used to directly request data from a website. APIs are a set of rules that allow one software application to interact with another. The client (you) sends a "request" and receives a "response" containing content. The R packages **httr** and **jsonlite** can facilitate this process. 

Each API-enabled website will have its own documentation and specifics to become familiar with. Some sites are publicly available and can be accessed by anyone. Others, such as platforms with user IDs and credentials, require authentication to access their data. 

Needless to say, it is necessary to have an internet connection to import data via API. We will briefly give examples of use of APIs to import data, and link you to further resources.  

*Note: recall that data may be *posted* on a website without an API, which may be easier to retrieve. For example a posted CSV file may be accessible simply by providing the site URL to `import()` as described in the section on [importing from Github](#import_github).*  


### HTTP request {.unnumbered}  

The API exchange is most commonly done through an HTTP request. HTTP is Hypertext Transfer Protocol, and is the underlying format of a request/response between a client and a server. The exact input and output may vary depending on the type of API but the process is the same - a "Request" (often HTTP Request) from the user, often containing a query, followed by a "Response", containing status information about the request and possibly the requested content.  

Here are a few components of an *HTTP request*:  

* The URL of the API endpoint  
* The "Method" (or "Verb")  
* Headers  
* Body  

The HTTP request "method" is the action your want to perform. The two most common HTTP methods are `GET` and `POST` but others could include `PUT`, `DELETE`, `PATCH`, etc. When importing data into R it is most likely that you will use `GET`.  

After your request, your computer will receive a "response" in a format similar to what you sent, including URL, HTTP status (Status 200 is what you want!), file type, size, and the desired content. You will then need to parse this response and turn it into a workable data frame within your R environment.


### Packages {.unnumbered}  

The **httr** package works well for handling HTTP requests in R. It requires little prior knowledge of Web APIs and can be used by people less familiar with software development terminology. In addition, if the HTTP response is .json, you can use **jsonlite** to parse the response.  

```{r, eval=F}
# load packages
pacman::p_load(httr, jsonlite, tidyverse)
```


### Publicly-available data {.unnumbered}  

Below is an example of an HTTP request, borrowed from a tutorial from [the Trafford Data Lab](https://www.trafforddatalab.io/open_data_companion/#A_quick_introduction_to_APIs). This site has several other resources to learn and API exercises.

Scenario: We want to import a list of fast food outlets in the city of Trafford, UK. The data can be accessed from the API of the Food Standards Agency, which provides food hygiene rating data for the United Kingdom.  

Here are the parameters for our request:  

* HTTP verb: GET  
* API endpoint URL: http://api.ratings.food.gov.uk/Establishments  
* Selected parameters: name, address, longitude, latitude, businessTypeId, ratingKey, localAuthorityId  
* Headers: “x-api-version”, 2  
* Data format(s): JSON, XML  
* Documentation: http://api.ratings.food.gov.uk/help  

The R code would be as follows:  

```{r, eval=F, warning=F, message=F}
# prepare the request
path <- "http://api.ratings.food.gov.uk/Establishments"
request <- GET(url = path,
             query = list(
               localAuthorityId = 188,
               BusinessTypeId = 7844,
               pageNumber = 1,
               pageSize = 5000),
             add_headers("x-api-version" = "2"))

# check for any server error ("200" is good!)
request$status_code

# submit the request, parse the response, and convert to a data frame
response <- content(request, as = "text", encoding = "UTF-8") %>%
  fromJSON(flatten = TRUE) %>%
  pluck("establishments") %>%
  as_tibble()
```

You can now clean and use the `response` data frame, which contains one row per fast food facility.  


### Authentication required {.unnumbered}  

Some APIs require authentication - for you to prove who you are, so you can access restricted data. To import these data, you may need to first use a POST method to provide a username, password, or code. This will return an access token, that can be used for subsequent GET method requests to retrieve the desired data.  

Below is an example of querying data from *Go.Data*, which is an outbreak investigation tool. *Go.Data* uses an API for all interactions between the web front-end and smartphone applications used for data collection. *Go.Data* is used throughout the world. Because outbreak data are sensitive and you should only be able to access data for *your* outbreak, authentication is required.  

Below is some sample R code using **httr** and **jsonlite** for connecting to the *Go.Data* API to import data on contact follow-up from your outbreak.  


```{r, eval=F}
# set credentials for authorization
url <- "https://godatasampleURL.int/"           # valid Go.Data instance url
username <- "username"                          # valid Go.Data username 
password <- "password"                          # valid Go,Data password 
outbreak_id <- "xxxxxx-xxxx-xxxx-xxxx-xxxxxxx"  # valid Go.Data outbreak ID

# get access token
url_request <- paste0(url,"api/oauth/token?access_token=123") # define base URL request

# prepare request
response <- POST(
  url = url_request,  
  body = list(
    username = username,    # use saved username/password from above to authorize                               
    password = password),                                       
    encode = "json")

# execute request and parse response
content <-
  content(response, as = "text") %>%
  fromJSON(flatten = TRUE) %>%          # flatten nested JSON
  glimpse()

# Save access token from response
access_token <- content$access_token    # save access token to allow subsequent API calls below

# import outbreak contacts
# Use the access token 
response_contacts <- GET(
  paste0(url,"api/outbreaks/",outbreak_id,"/contacts"),          # GET request
  add_headers(
    Authorization = paste("Bearer", access_token, sep = " ")))

json_contacts <- content(response_contacts, as = "text")         # convert to text JSON

contacts <- as_tibble(fromJSON(json_contacts, flatten = TRUE))   # flatten JSON to tibble
```

<span style="color: orange;">**_CAUTION:_** If you are importing large amounts of data from an API requiring authentication, it may time-out. To avoid this, retrieve access_token again before each API GET request and try using filters or limits in the query. </span> 

<span style="color: darkgreen;">**_TIP:_** The `fromJSON()` function in the **jsonlite** package does not fully un-nest the first time it's executed, so you will likely still have list items in your resulting tibble. You will need to further un-nest for certain variables; depending on how nested your .json is. To view more info on this, view the documentation for the **jsonlite** package, such as the [`flatten()` function](https://rdrr.io/cran/jsonlite/man/flatten.html). </span>


For more details, View documentation on [LoopBack Explorer](https://loopback.io/doc/en/lb4/index.html), the [Contact Tracing] page or API tips on [Go.Data Github repository](https://worldhealthorganization.github.io/godata/api-docs)

You can read more about the *httr* package [here](https://httr.r-lib.org/articles/quickstart.html)  

This section was also informed by [this tutorial](https://www.dataquest.io/blog/r-api-tutorial/) and [this tutorial](https://medium.com/@traffordDataLab/querying-apis-in-r-39029b73d5f1). 




<!-- ======================================================= -->
## Export {}  

### With **rio** package {.unnumbered}
With **rio**, you can use the `export()` function in a very similar way to `import()`. First give the name of the R object you want to save (e.g. `linelist`) and then in quotes put the file path where you want to save the file, including the desired file name and file extension. For example:  

This saves the data frame `linelist` as an Excel workbook to the working directory/R project root folder:  

```{r, eval=F}
export(linelist, "my_linelist.xlsx") # will save to working directory
```

You could save the same data frame as a csv file by changing the extension. For example, we also save it to a file path constructed with `here()`:  

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.csv"))
```


### To clipboard {.unnumbered}

To export a data frame to your computer's "clipboard" (to then paste into another software like Excel, Google Spreadsheets, etc.) you can use `write_clip()` from the **clipr** package. 

```{r, eval=F}
# export the linelist data frame to your system's clipboard
clipr::write_clip(linelist)
```




## RDS files {#import_rds}

Along with .csv, .xlsx, etc, you can also export/save R data frames as .rds files. This is a file format specific to R, and is very useful if you know you will work with the exported data again in R. 

The classes of columns are stored, so you don't have do to cleaning again when it is imported (with an Excel or even a CSV file this can be a headache!). It is also a smaller file, which is useful for export and import if your dataset is large.  

For example, if you work in an Epidemiology team and need to send files to a GIS team for mapping, and they use R as well, just send them the .rds file! Then all the column classes are retained and they have less work to do.  

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.rds"))
```



<!-- ======================================================= -->
## Rdata files and lists {#import_rdata}

`.Rdata` files can store multiple R objects - for example multiple data frames, model results, lists, etc. This can be very useful to consolidate or share a lot of your data for a given project.  

In the below example, multiple R objects are stored within the exported file "my_objects.Rdata":  

```{r, eval=F}
rio::export(my_list, my_dataframe, my_vector, "my_objects.Rdata")
```

Note: if you are trying to *import* a list, use `import_list()` from **rio** to import it with the complete original structure and contents.  

```{r, eval=F}
rio::import_list("my_list.Rdata")
```







<!-- ======================================================= -->
## Saving plots {} 

Instructions on how to save plots, such as those created by `ggplot()`, are discussed in depth in the [ggplot basics] page.  

In brief, run `ggsave("my_plot_filepath_and_name.png")` after printing your plot. You can either provide a saved plot object to the `plot = ` argument, or only specify the destination file path (with file extension) to save the most recently-displayed plot. You can also control the `width = `, `height = `, `units = `, and `dpi = `.  

How to save a network graph, such as a transmission tree, is addressed in the page on [Transmission chains]. 


<!-- ======================================================= -->
## Resources {} 

The [R Data Import/Export Manual](https://cran.r-project.org/doc/manuals/r-release/R-data.html)  
[R 4 Data Science chapter on data import](https://r4ds.had.co.nz/data-import.html#data-import)  
[ggsave() documentation](https://ggplot2.tidyverse.org/reference/ggsave.html)  


Below is a table, taken from the **rio** online [vignette](https://cran.r-project.org/web/packages/rio/vignettes/rio.html). For each type of data it shows: the expected file extension, the package **rio** uses to import or export the data, and whether this functionality is included in the default installed version of **rio**.  



Format                     | Typical Extension | Import Package    | Export Package     | Installed by Default
---------------------------|-------------------|-------------------|--------------------|---------------------
Comma-separated data | .csv | data.table `fread()` | data.table |	Yes
Pipe-separated data |	.psv | data.table `fread()` | data.table | Yes
Tab-separated data| .tsv | data.table `fread()` | data.table | Yes
SAS | .sas7bdat | haven | haven | Yes
SPSS | .sav | haven | haven | Yes
Stata | .dta | haven | haven | Yes
SAS | XPORT | .xpt | haven | haven | Yes
SPSS Portable | .por | haven | | Yes
Excel | .xls | readxl | | Yes
Excel | .xlsx | readxl | openxlsx | Yes
R syntax | .R	| base | base | Yes
Saved R objects | .RData, .rda | base | base | Yes
Serialized R objects | .rds | base | base | Yes
Epiinfo | .rec | foreign | | Yes
Minitab | .mtp | foreign | | Yes
Systat | .syd |	foreign | | Yes
“XBASE” | database files | .dbf | foreign | foreign | Yes
Weka Attribute-Relation File Format | .arff | foreign | foreign | Yes
Data Interchange Format | .dif | utils | | Yes
Fortran data | no recognized extension | utils | | Yes
Fixed-width format data | .fwf | utils | utils | Yes
gzip comma-separated data | .csv.gz | utils | utils | Yes
CSVY (CSV + YAML metadata header) | .csvy | csvy | csvy | No
EViews | .wf1 |hexView | | No
Feather R/Python interchange format | .feather | feather | feather | No
Fast Storage | .fst | fst |	fst | No
JSON | .json | jsonlite | jsonlite | No
Matlab | .mat | rmatio | rmatio | No
OpenDocument Spreadsheet | .ods | readODS | readODS | No
HTML Tables | .html | xml2 | xml2 | No
Shallow XML documents | .xml | xml2 | xml2 | No
YAML | .yml | yaml | yaml	| No
Clipboard	default is tsv | |  clipr | clipr | No



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/importing.Rmd-->

# (PART) Data Management {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_management.Rmd-->

# Cleaning data and core functions {}


```{r, out.height = "10%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "data_cleaning.png"))
```


This page demonstrates common steps used in the process of "cleaning" a dataset, and also explains the use of many essential R data management functions.  

To demonstrate data cleaning, this page begins by importing a raw case linelist dataset, and proceeds step-by-step through the cleaning process. In the R code, this manifests as a "pipe" chain, which references the "pipe" operator ` %>%` that passes a dataset from one operation to the next.  


### Core functions {.unnumbered}  

This handbook emphasizes use of the functions from the [**tidyverse**](https://www.tidyverse.org/) family of R packages. The essential R functions demonstrated in this page are listed below.  

Many of these functions belong to the [**dplyr**](https://dplyr.tidyverse.org/) R package, which provides "verb" functions to solve data manipulation challenges (the name is a reference to a "data frame-[plier](https://www.thefreedictionary.com/plier#:~:text=also%20ply%C2%B7er%20(pl%C4%AB%E2%80%B2,holding%2C%20bending%2C%20or%20cutting.)"). **dplyr** is part of the **tidyverse** family of R packages (which also includes **ggplot2**, **tidyr**, **stringr**, **tibble**, **purrr**, **magrittr**, and **forcats** among others).  


Function       | Utility                               | Package
---------------|---------------------------------------|------------------------------
` %>% `|"pipe" (pass) data from one function to the next|**magrittr** 
`mutate()`|create, transform, and re-define columns|**dplyr**  
`select()`|keep, remove, select, or re-name columns|**dplyr**
`rename()`|rename columns|**dplyr** 
`clean_names()`|standardize the syntax of column names|**janitor**
`as.character()`, `as.numeric()`, `as.Date()`, etc.|convert the class of a column|**base** R
`across()`|transform multiple columns at one time|**dplyr** 
**tidyselect** functions|use logic to select columns|**tidyselect**   
`filter()`|keep certain rows|**dplyr** 
`distinct()`|de-duplicate rows|**dplyr** 
`rowwise()`|operations by/within each row|**dplyr**  
`add_row()`|add rows manually|**tibble** 
`arrange()`|sort rows|**dplyr**
`recode()`|re-code values in a column|**dplyr** 
`case_when()`|re-code values in a column using more complex logical criteria|**dplyr** 
`replace_na()`, `na_if()`, `coalesce()`|special functions for re-coding|**tidyr**  
`age_categories()` and `cut()`|create categorical groups from a numeric column|**epikit** and **base** R
`clean_variable_spelling()`|re-code/clean values using a data dictionary|**linelist**
`which()`|apply logical criteria; return indices|**base** R

If you want to see how these functions compare to Stata or SAS commands, see the page on [Transition to R].  

You may encounter an alternative data management framework from the **data.table** R package with operators like `:=` and frequent use of brackets `[ ]`. This approach and syntax is briefly explained in the [Data Table] page.  

### Nomenclature {.unnumbered}  

In this handbook, we generally reference "columns" and "rows" instead of "variables" and "observations". As explained in this primer on ["tidy data"](https://tidyr.tidyverse.org/articles/tidy-data.html), most epidemiological statistical datasets consist structurally of rows, columns, and values.  

*Variables* contain the values that measure the same underlying attribute (like age group, outcome, or date of onset). *Observations* contain all values measured on the same unit (e.g. a person, site, or lab sample). So these aspects can be more difficult to tangibly define.  

In "tidy" datasets, each column is a variable, each row is an observation, and each cell is a single value. However some datasets you encounter will not fit this mold - a "wide" format dataset may have a variable split across several columns (see an example in the [Pivoting data] page). Likewise, observations could be split across several rows.  

Most of this handbook is about managing and transforming data, so referring to the concrete data structures of rows and columns is more relevant than the more abstract observations and variables. Exceptions occur primarily in pages on data analysis, where you will see more references to variables and observations.  



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Cleaning pipeline

**This page proceeds through typical cleaning steps, adding them sequentially to a cleaning pipe chain.**

In epidemiological analysis and data processing, cleaning steps are often performed sequentially, linked together. In R, this often manifests as a cleaning "pipeline", where *the raw dataset is passed or "piped" from one cleaning step to another*.  

Such chains utilize **dplyr** "verb" functions and the **magrittr** pipe operator `%>%`. This pipe begins with the "raw" data ("linelist_raw.xlsx") and ends with a "clean" R data frame (`linelist`) that can be used, saved, exported, etc.  

In a cleaning pipeline the order of the steps is important. Cleaning steps might include:  

* Importing of data  
* Column names cleaned or changed  
* De-duplication  
* Column creation and transformation (e.g. re-coding or standardising values)  
* Rows filtered or added  



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Load packages  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, message = F}
pacman::p_load(
  rio,        # importing data  
  here,       # relative file pathways  
  janitor,    # data cleaning and tables
  lubridate,  # working with dates
  matchmaker, # dictionary-based cleaning
  epikit,     # age_categories() function
  tidyverse   # data management and visualization
)
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Import data  

### Import {.unnumbered}  

Here we import the "raw" case linelist Excel file using the `import()` function from the package **rio**. The **rio** package flexibly handles many types of files (e.g. .xlsx, .csv, .tsv, .rds. See the page on [Import and export] for more information and tips on unusual situations (e.g. skipping rows, setting missing values, importing Google sheets, etc).  

If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_raw.xlsx' class='download-button'>click to download the "raw" linelist</a> (as .xlsx file).  

If your dataset is large and takes a long time to import, it can be useful to have the import command be separate from the pipe chain and the "raw" saved as a distinct file. This also allows easy comparison between the original and cleaned versions.  

Below we import the raw Excel file and save it as the data frame `linelist_raw`. We assume the file is located in your working directory or R project root, and so no sub-folders are specified in the file path.  

```{r, echo=F, message=F}
# HIDDEN FROM READER
# actually load the data using here()
linelist_raw <- rio::import(here::here("data", "case_linelists", "linelist_raw.xlsx"))
```

```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx")
```

You can view the first 50 rows of the the data frame below. Note: the **base** R function `head(n)` allow you to view just the first `n` rows in the R console.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_raw,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
### Review {.unnumbered}  

You can use the function `skim()` from the package **skimr** to get an overview of the entire dataframe (see page on [Descriptive tables] for more info). Columns are summarised by class/type such as character, numeric. Note: "POSIXct" is a type of raw date class (see [Working with dates].  


```{r, eval=F}
skimr::skim(linelist_raw)
```

```{r, echo=F}
skimr::skim_without_charts(linelist_raw)
```




 





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Column names {} 

In R, column *names* are the "header" or "top" value of a column. They are used to refer to columns in the code, and serve as a default label in figures.  


Other statistical software such as SAS and STATA use *"labels"* that co-exist as longer printed versions of the shorter column names. While R does offer the possibility of adding column labels to the data, this is not emphasized in most practice. To make column names "printer-friendly" for figures, one typically adjusts their display within the plotting commands that create the outputs (e.g. axis or legend titles of a plot, or column headers in a printed table - see the [scales section of the ggplot tips page](#ggplot_tips_scales) and [Tables for presentation] pages). If you want to assign column labels in the data, read more online [here](https://cran.r-project.org/web/packages/expss/vignettes/labels-support.html) and [here](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html).  


As R column names are used very often, so they must have "clean" syntax. We suggest the following:  

* Short names
* No spaces (replace with underscores _ ) 
* No unusual characters (&, #, <, >, ...)  
* Similar style nomenclature (e.g. all date columns named like **date_**onset, **date_**report, **date_**death...)  

The columns names of `linelist_raw` are printed below using `names()` from **base** R. We can see that initially:  

* Some names contain spaces (e.g. `infection date`)  
* Different naming patterns are used for dates (`date onset` vs. `infection date`)  
* There must have been a *merged header* across the two last columns in the .xlsx. We know this because the name of two merged columns ("merged_header") was assigned by R to the first column, and the second column was assigned a placeholder  name "...28" (as it was then empty and is the 28th column).  

```{r}
names(linelist_raw)
```

<span style="color: black;">**_NOTE:_** To reference a column name that includes spaces, surround the name with back-ticks, for example: linelist$`` ` '\x60infection date\x60'` ``. note that on your keyboard, the back-tick (`) is different from the single quotation mark (').</span>


### Automatic cleaning {.unnumbered}  

The function `clean_names()` from the package **janitor** standardizes column names and makes them unique by doing the following:  

* Converts all names to consist of only underscores, numbers, and letters  
* Accented characters are transliterated to ASCII (e.g. german o with umlaut becomes "o", spanish "enye" becomes "n")  
* Capitalization preference for the new column names can be specified using the `case = ` argument ("snake" is default, alternatives include "sentence", "title", "small_camel"...)  
* You can specify specific name replacements by providing a vector to the `replace = ` argument (e.g. `replace = c(onset = "date_of_onset")`)  
* Here is an online [vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#cleaning)  

Below, the cleaning pipeline begins by using `clean_names()` on the raw linelist.  

```{r clean_names}
# pipe the raw dataset through the function clean_names(), assign result as "linelist"  
linelist <- linelist_raw %>% 
  janitor::clean_names()

# see the new column names
names(linelist)
```

<span style="color: black;">**_NOTE:_** The last column name "...28" was changed to "x28".</span>


### Manual name cleaning {.unnumbered}  

Re-naming columns manually is often necessary, even after the standardization step above. Below, re-naming is performed using the `rename()` function from the **dplyr** package, as part of a pipe chain. `rename()` uses the style `NEW = OLD` - the new column name is given before the old column name.  

Below, a re-naming command is added to the cleaning pipeline. Spaces have been added strategically to align code for easier reading.  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome)
```


Now you can see that the columns names have been changed:  

```{r message=FALSE, echo=F}
names(linelist)
```


#### Rename by column position {.unnumbered} 

You can also rename by column position, instead of column name, for example:  

```{r, eval=F}
rename(newNameForFirstColumn  = 1,
       newNameForSecondColumn = 2)
```



#### Rename via `select()` and `summarise()` {.unnumbered}  

As a shortcut, you can also rename columns within the **dplyr** `select()` and `summarise()` functions. `select()` is used to keep only certain columns (and is covered later in this page). `summarise()` is covered in the [Grouping data] and [Descriptive tables] pages. These functions also uses the format `new_name = old_name`. Here is an example:  

```{r, eval=F}
linelist_raw %>% 
  select(# NEW name             # OLD name
         date_infection       = `infection date`,    # rename and KEEP ONLY these columns
         date_hospitalisation = `hosp date`)
```





### Other challenges {.unnumbered}  


#### Empty Excel column names {.unnumbered} 

R cannot have dataset columns that do not have column names (headers). So, if you import an Excel dataset with data but no column headers, R will fill-in the headers with names like "...1" or "...2". The number represents the column number (e.g. if the 4th column in the dataset has no header, then R will name it "...4").  

You can clean these names manually by referencing their position number (see example above), or their assigned name (`linelist_raw$...1`).  



#### Merged Excel column names and cells {.unnumbered}  

Merged cells in an Excel file are a common occurrence when receiving data. As explained in [Transition to R], merged cells can be nice for human reading of data, but are not "tidy data" and cause many problems for machine reading of data. R cannot accommodate merged cells.  

Remind people doing data entry that **human-readable data is not the same as machine-readable data**. Strive to train users about the principles of [**tidy data**](https://r4ds.had.co.nz/tidy-data.html). If at all possible, try to change procedures so that data arrive in a tidy format without merged cells.  

* Each variable must have its own column.  
* Each observation must have its own row.  
* Each value must have its own cell.  

When using **rio**'s `import()` function, the value in a merged cell will be assigned to the first cell and subsequent cells will be empty.  

One solution to deal with merged cells is to import the data with the function `readWorkbook()` from the package **openxlsx**. Set the argument `fillMergedCells = TRUE`. This gives the value in a merged cell to all cells within the merge range.

```{r, eval=F}
linelist_raw <- openxlsx::readWorkbook("linelist_raw.xlsx", fillMergedCells = TRUE)
```

<span style="color: red;">**_DANGER:_** If column names are merged with `readWorkbook()`, you will end up with duplicate column names, which you will need to fix manually - R does not work well with duplicate column names! You can re-name them by referencing their position (e.g. column 5), as explained in the section on manual column name cleaning.</span>






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Select or re-order columns {} 

Use `select()` from **dplyr** to select the columns you want to retain, and to specify their order in the data frame. 

<span style="color: orange;">**_CAUTION:_** In the examples below, the `linelist` data frame is modified with `select()` and displayed, but not saved. This is for demonstration purposes. The modified column names are printed by piping the data frame to `names()`.</span>

**Here are ALL the column names in the linelist at this point in the cleaning pipe chain:**

```{r}
names(linelist)
```

### Keep columns {.unnumbered}  

**Select only the columns you want to remain**  

Put their names in the `select()` command, with no quotation marks. They will appear in the data frame in the order you provide. Note that if you include a column that does not exist, R will return an error (see use of `any_of()` below if you want no error in this situation).  

```{r}
# linelist dataset is piped through select() command, and names() prints just the column names
linelist %>% 
  select(case_id, date_onset, date_hospitalisation, fever) %>% 
  names()  # display the column names
```




### "tidyselect" helper functions {#clean_tidyselect .unnumbered}  

These helper functions exist to make it easy to specify columns to keep, discard, or transform. They are from the package **tidyselect**, which is included in **tidyverse** and underlies how columns are selected in **dplyr** functions.  

For example, if you want to re-order the columns, `everything()` is a useful function to signify "all other columns not yet mentioned". The command below moves columns `date_onset` and `date_hospitalisation` to the beginning (left) of the dataset, but keeps all the other columns afterward. Note that `everything()` is written with empty parentheses:  

```{r}
# move date_onset and date_hospitalisation to beginning
linelist %>% 
  select(date_onset, date_hospitalisation, everything()) %>% 
  names()
```

Here are other "tidyselect" helper functions that also work *within* **dplyr** functions like `select()`, `across()`, and `summarise()`:  

* `everything()`  - all other columns not mentioned  
* `last_col()`    - the last column  
* `where()`       - applies a function to all columns and selects those which are TRUE  
* `contains()`    - columns containing a character string  
  * example: `select(contains("time"))`  
* `starts_with()` - matches to a specified prefix  
  * example: `select(starts_with("date_"))`  
* `ends_with()`   - matches to a specified suffix  
  * example: `select(ends_with("_post"))`  
* `matches()`     - to apply a regular expression (regex)  
  * example: `select(matches("[pt]al"))`  
* `num_range()`   - a numerical range like x01, x02, x03  
* `any_of()`      - matches IF column exists but returns no error if it is not found  
  * example: `select(any_of(date_onset, date_death, cardiac_arrest))`  

In addition, use normal operators such as `c()` to list several columns, `:` for consecutive columns, `!` for opposite, `&` for AND, and `|` for OR.  


Use `where()` to specify logical criteria for columns. If providing a function inside `where()`, do not include the function's empty parentheses. The command below selects columns that are class Numeric.

```{r}
# select columns that are class Numeric
linelist %>% 
  select(where(is.numeric)) %>% 
  names()
```

Use `contains()` to select only columns in which the column name contains a specified character string. `ends_with()` and `starts_with()` provide more nuance.  

```{r}
# select columns containing certain characters
linelist %>% 
  select(contains("date")) %>% 
  names()
```

The function `matches()` works similarly to `contains()` but can be provided a regular expression (see page on [Characters and strings]), such as multiple strings separated by OR bars within the parentheses:  

```{r}
# searched for multiple character matches
linelist %>% 
  select(matches("onset|hosp|fev")) %>%   # note the OR symbol "|"
  names()
```

<span style="color: orange;">**_CAUTION:_** If a column name that you specifically provide does not exist in the data, it can return an error and stop your code. Consider using `any_of()` to cite columns that may or may not exist, especially useful in negative (remove) selections.</span>

Only one of these columns exists, but no error is produced and the code continues without stopping your cleaning chain.  

```{r}
linelist %>% 
  select(any_of(c("date_onset", "village_origin", "village_detection", "village_residence", "village_travel"))) %>% 
  names()
```



### Remove columns {.unnumbered} 

**Indicate which columns to remove** by placing a minus symbol "-" in front of the column name (e.g. `select(-outcome)`), or a vector of column names (as below). All other columns will be retained. 

```{r}
linelist %>% 
  select(-c(date_onset, fever:vomit)) %>% # remove date_onset and all columns from fever to vomit
  names()
```

You can also remove a column using **base** R syntax, by defining it as `NULL`. For example:  

```{r, eval=F}
linelist$date_onset <- NULL   # deletes column with base R syntax 
```



### Standalone {.unnumbered}

`select()` can also be used as an independent command (not in a pipe chain). In this case, the first argument is the original dataframe to be operated upon.  

```{r}
# Create a new linelist with id and age-related columns
linelist_age <- select(linelist, case_id, contains("age"))

# display the column names
names(linelist_age)
```



#### Add to the pipe chain {.unnumbered}  

In the `linelist_raw`, there are a few columns we do not need: `row_num`, `merged_header`, and `x28`. We remove them with a `select()` command in the cleaning pipe chain:  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################

    # remove column
    select(-c(row_num, merged_header, x28))
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Deduplication


See the handbook page on [De-duplication] for extensive options on how to de-duplicate data. Only a very simple row de-duplication example is presented here.  

The package **dplyr** offers the `distinct()` function. This function examines every row and reduce the data frame to only the unique rows. That is, it removes rows that are 100% duplicates.  

When evaluating duplicate rows, it takes into account a range of columns - by default it considers all columns. As shown in the de-duplication page, you can adjust this column range so that the uniqueness of rows is only evaluated in regards to certain columns.  

In this simple example, we just add the empty command `distinct()` to the pipe chain. This ensures there are no rows that are 100% duplicates of other rows (evaluated across all columns).  

We begin with ` nrow(linelist)` rows in `linelist`. 

```{r}
linelist <- linelist %>% 
  distinct()
```

After de-duplication there are ` nrow(linelist)` rows. Any removed rows would have been 100% duplicates of other rows.  

Below, the `distinct()` command is added to the cleaning pipe chain:

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################
    
    # de-duplicate
    distinct()
```





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Column creation and transformation { }


**We recommend using the dplyr function `mutate()` to add a new column, or to modify an existing one.**  

Below is an example of creating a new column with `mutate()`. The syntax is: `mutate(new_column_name = value or transformation)`  

In Stata, this is similar to the command `generate`, but R's `mutate()` can also be used to modify an existing column.  


### New columns {.unnumbered}

The most basic `mutate()` command to create a new column might look like this. It creates a new column `new_col` where the value in every row is 10.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(new_col = 10)
```

You can also reference values in other columns, to perform calculations. Below, a new column `bmi` is created to hold the Body Mass Index (BMI) for each case - as calculated using the formula BMI = kg/m^2, using column `ht_cm` and column `wt_kg`.  

```{r}
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

If creating multiple new columns, separate each with a comma and new line. Below are examples of new columns, including ones that consist of values from other columns combined using `str_glue()` from the **stringr** package (see page on [Characters and strings].  

```{r}
new_col_demo <- linelist %>%                       
  mutate(
    new_var_dup    = case_id,             # new column = duplicate/copy another existing column
    new_var_static = 7,                   # new column = all values the same
    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables
    new_var_paste  = stringr::str_glue("{hospital} on ({date_hospitalisation})") # new column = pasting together values from other columns
    ) %>% 
  select(case_id, hospital, date_hospitalisation, contains("new"))        # show only new columns, for demonstration purposes
```


Review the new columns. For demonstration purposes, only the new columns and the columns used to create them are shown:  


```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(new_col_demo,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: darkgreen;">**_TIP:_** A variation on `mutate()` is the function `transmute()`. This function adds a new column just like `mutate()`, but also drops/removes all other columns that you do not mention within its parentheses.</span>


```{r, eval=F}
# HIDDEN FROM READER
# removes new demo columns created above
# linelist <- linelist %>% 
#   select(-contains("new_var"))
```



### Convert column class {.unnumbered}
  
Columns containing values that are dates, numbers, or logical values (TRUE/FALSE) will only behave as expected if they are correctly classified. There is a difference between "2" of class character and 2 of class numeric!  

There are ways to set column class during the import commands, but this is often cumbersome. See the [R Basics] section on object classes to learn more about converting the class of objects and columns.  

First, let's run some checks on important columns to see if they are the correct class. We also saw this in the beginning when we ran `skim()`.  

Currently, the class of the `age` column is character. To perform quantitative analyses, we need these numbers to be recognized as numeric! 

```{r}
class(linelist$age)
```

The class of the `date_onset` column is also character! To perform analyses, these dates must be recognized as dates! 
 
```{r}
class(linelist$date_onset)
```


To resolve this, use the ability of `mutate()` to re-define a column with a transformation. We define the column as itself, but converted to a different class. Here is a basic example, converting or ensuring that the column `age` is class Numeric:  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age = as.numeric(age))
```

In a similar way, you can use `as.character()` and `as.logical()`. To convert to class Factor, you can use `factor()` from **base** R or `as_factor()` from **forcats**. Read more about this in the [Factors] page.  

You must be careful when converting to class Date. Several methods are explained on the page [Working with dates]. Typically, the raw date values must all be in the same format for conversion to work correctly (e.g "MM/DD/YYYY", or "DD MM YYYY"). After converting to class Date, check your data to confirm that each value was converted correctly.  




### Grouped data {.unnumbered}  

If your data frame is already *grouped* (see page on [Grouping data]), `mutate()` may behave differently than if the data frame is not grouped. Any summarizing functions, like `mean()`, `median()`, `max()`, etc. will calculate by group, not by all the rows.     

```{r, eval=F}
# age normalized to mean of ALL rows
linelist %>% 
  mutate(age_norm = age / mean(age, na.rm=T))

# age normalized to mean of hospital group
linelist %>% 
  group_by(hospital) %>% 
  mutate(age_norm = age / mean(age, na.rm=T))
```

Read more about using `mutate ()` on grouped dataframes in this [tidyverse mutate documentation](https://dplyr.tidyverse.org/reference/mutate.html).  



### Transform multiple columns {#clean_across .unnumbered}


Often to write concise code you want to apply the same transformation to multiple columns at once. A transformation can be applied to multiple columns at once using the `across()` function from the package **dplyr** (also contained within **tidyverse** package). `across()` can be used with any **dplyr** function, but is commonly used within `select()`, `mutate()`, `filter()`, or `summarise()`. See how it is applied to `summarise()` in the page on [Descriptive tables].  

Specify the columns to the argument `.cols = ` and the function(s) to apply to `.fns = `. Any additional arguments to provide to the `.fns` function can be included after a comma, still within `across()`.   

#### `across()` column selection {.unnumbered}  

Specify the columns to the argument `.cols = `. You can name them individually, or use "tidyselect" helper functions. Specify the function to `.fns = `. Note that using the function mode demonstrated below, the function is written *without* its parentheses ( ).  

Here the transformation `as.character()` is applied to specific columns named within `across()`. 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = c(temp, ht_cm, wt_kg), .fns = as.character))
```

The "tidyselect" helper functions are available to assist you in specifying columns. They are detailed above in the section on Selecting and re-ordering columns, and they include: `everything()`, `last_col()`, `where()`, `starts_with()`, `ends_with()`, `contains()`, `matches()`, `num_range()` and `any_of()`.  

Here is an example of how one would change **all columns** to character class:  

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = everything(), .fns = as.character))
```

Convert to character all columns where the name contains the string "date" (note the placement of commas and parentheses):  

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = contains("date"), .fns = as.character))
```

Below, an example of mutating the columns that are currently class POSIXct (a raw datetime class that shows timestamps) - in other words, where the function `is.POSIXct()` evaluates to `TRUE`. Then we want to apply the function `as.Date()` to these columns to convert them to a normal class Date.  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = where(is.POSIXct), .fns = as.Date))
```

* Note that within `across()` we also use the function `where()` as `is.POSIXct` is evaluating to either TRUE or FALSE.  
* Note that `is.POSIXct()` is from the package **lubridate**. Other similar "is" functions like `is.character()`, `is.numeric()`, and `is.logical()` are from **base R**  

#### `across()` functions {.unnumbered}

You can read the documentation with `?across` for details on how to provide functions to `across()`. A few summary points: there are several ways to specify the function(s) to perform on a column and you can even define your own functions:  

* You can provide the function name alone (e.g. `mean` or `as.character`)  
* You can provide the function in **purrr**-style (e.g. `~ mean(.x, na.rm = TRUE)`) (see [this page][Iteration, loops, and lists])  
* You can specify multiple functions by providing a list (e.g. `list(mean = mean, n_miss = ~ sum(is.na(.x))`).  
  * If you provide multiple functions, multiple transformed columns will be returned per input column, with unique names in the format `col_fn`. You can adjust how the new columns are named with the `.names =` argument using **glue** syntax (see page on [Characters and strings]) where `{.col}` and `{.fn}` are shorthand for the input column and function.  
  
  
Here are a few online resources on using `across()`: [creator Hadley Wickham's thoughts/rationale](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-colwise/)




### `coalesce()` {.unnumbered}  

This **dplyr** function finds the first non-missing value at each position. It "fills-in" missing values with the first available value in an order you specify.

Here is an example *outside the context of a data frame*: Let us say you have two vectors, one containing the patient's village of detection and another containing the patient's village of residence. You can use coalesce to pick the first non-missing value for each index:  

```{r}
village_detection <- c("a", "b", NA,  NA)
village_residence <- c("a", "c", "a", "d")

village <- coalesce(village_detection, village_residence)
village    # print
```

This works the same if you provide data frame columns: for each row, the function will assign the new column value with the first non-missing value in the columns you provided (in order provided).

```{r, eval=F}
linelist <- linelist %>% 
  mutate(village = coalesce(village_detection, village_residence))
```

This is an example of a "row-wise" operation. For more complicated row-wise calculations, see the section below on Row-wise calculations.  



### Cumulative math {.unnumbered}

If you want a column to reflect the cumulative sum/mean/min/max etc as assessed down the rows of a dataframe to that point, use the following functions:  

`cumsum()` returns the cumulative sum, as shown below:  

```{r}
sum(c(2,4,15,10))     # returns only one number
cumsum(c(2,4,15,10))  # returns the cumulative sum at each step
```

This can be used in a dataframe when making a new column. For example, to calculate the cumulative number of cases per day in an outbreak, consider code like this:  

```{r, warning=F, message=F}
cumulative_case_counts <- linelist %>%  # begin with case linelist
  count(date_onset) %>%                 # count of rows per day, as column 'n'   
  mutate(cumulative_cases = cumsum(n))  # new column, of the cumulative sum at each row
```

Below are the first 10 rows:  

```{r}
head(cumulative_case_counts, 10)
```

See the page on [Epidemic curves] for how to plot cumulative incidence with the epicurve.  

See also:  
`cumsum()`, `cummean()`, `cummin()`, `cummax()`, `cumany()`, `cumall()`  





### Using **base** R {.unnumbered}  

To define a new column (or re-define a column) using **base** R, write the name of data frame, connected with `$`, to the *new* column (or the column to be modified). Use the assignment operator `<-` to define the new value(s). Remember that when using **base** R you must specify the data frame name before the column name every time (e.g. `dataframe$column`). Here is an example of creating the `bmi` column using **base** R:  

```{r, eval=F}
linelist$bmi = linelist$wt_kg / (linelist$ht_cm / 100) ^ 2)
```




### Add to pipe chain {.unnumbered}  

**Below, a new column is added to the pipe chain and some classes are converted.**  

```{r }
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    # add new column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>% 
  
    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) 
```





## Re-code values

Here are a few scenarios where you need to re-code (change) values:  

* to edit one specific value (e.g. one date with an incorrect year or format)  
* to reconcile values not spelled the same
* to create a new column of categorical values  
* to create a new column of numeric categories (e.g. age categories)  



### Specific values {.unnumbered}  

To change values manually you can use the `recode()` function within the `mutate()` function. 

Imagine there is a nonsensical date in the data (e.g. "2014-14-15"): you could fix the date manually in the raw source data, or, you could write the change into the cleaning pipeline via `mutate()` and `recode()`. The latter is more transparent and reproducible to anyone else seeking to understand or repeat your analysis.  

```{r, eval=F}
# fix incorrect values                   # old value       # new value
linelist <- linelist %>% 
  mutate(date_onset = recode(date_onset, "2014-14-15" = "2014-04-15"))
```

The `mutate()` line above can be read as: "mutate the column `date_onset` to equal the column `date_onset` re-coded so that OLD VALUE is changed to NEW VALUE". Note that this pattern (OLD = NEW) for `recode()` is the opposite of most R patterns (new = old). The R development community is working on revising this.  

**Here is another example re-coding multiple values within one column.** 

In `linelist` the values in the column "hospital" must be cleaned. There are several different spellings and many missing values.

```{r}
table(linelist$hospital, useNA = "always")  # print table of all unique values, including missing  
```

The `recode()` command below re-defines the column "hospital" as the current column "hospital", but with the specified recode changes. Don't forget commas after each!  

```{r}
linelist <- linelist %>% 
  mutate(hospital = recode(hospital,
                     # for reference: OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      ))
```


Now we see the spellings in the `hospital` column have been corrected and consolidated:  

```{r}
table(linelist$hospital, useNA = "always")
```

<span style="color: darkgreen;">**_TIP:_** The number of spaces before and after an equals sign does not matter. Make your code easier to read by aligning the = for all or most rows. Also, consider adding a hashed comment row to clarify for future readers which side is OLD and which side is NEW. </span>  

<span style="color: darkgreen;">**_TIP:_** Sometimes a *blank* character value exists in a dataset (not recognized as R's value for missing - `NA`). You can reference this value with two quotation marks with no space inbetween ("").</span>  




### By logic {.unnumbered}

Below we demonstrate how to re-code values in a column using logic and conditions:  

* Using `replace()`, `ifelse()` and `if_else()` for simple logic
* Using `case_when()` for more complex logic  



### Simple logic {.unnumbered}  


#### `replace()` {.unnumbered}  

To re-code with simple logical criteria, you can use `replace()` within `mutate()`. `replace()` is a function from **base** R. Use a logic condition to specify the rows to change . The general syntax is:  

`mutate(col_to_change = replace(col_to_change, criteria for rows, new value))`.  

One common situation to use `replace()` is **changing just one value in one row, using an unique row identifier**. Below, the gender is changed to "Female" in the row where the column `case_id` is "2195".  

```{r, eval=F}
# Example: change gender of one specific observation to "Female" 
linelist <- linelist %>% 
  mutate(gender = replace(gender, case_id == "2195", "Female"))
```

The equivalent command using **base** R syntax and indexing brackets `[ ]` is below. It reads as "Change the value of the dataframe `linelist`'s column `gender` (for the rows where `linelist`'s column `case_id` has the value  '2195') to 'Female' ".   

```{r, eval=F}
linelist$gender[linelist$case_id == "2195"] <- "Female"
```




#### `ifelse()` and `if_else()` {.unnumbered}  

Another tool for simple logic is `ifelse()` and its partner `if_else()`. However, in most cases for re-coding it is more clear to use `case_when()` (detailed below). These "if else" commands are simplified versions of an `if` and `else` programming statement. The general syntax is:  
`ifelse(condition, value to return if condition evaluates to TRUE, value to return if condition evaluates to FALSE)` 

Below, the column `source_known` is defined. Its value in a given row is set to "known" if the row's value in column `source` is *not* missing. If the value in `source` *is* missing, then the value in `source_known` is set to "unknown".  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(source_known = ifelse(!is.na(source), "known", "unknown"))
```

`if_else()` is a special version from **dplyr** that handles dates. Note that if the 'true' value is a date, the 'false' value must also qualify a date, hence using the special value `NA_real_` instead of just `NA`.

```{r, eval=F}
# Create a date of death column, which is NA if patient has not died.
linelist <- linelist %>% 
  mutate(date_death = if_else(outcome == "Death", date_outcome, NA_real_))
```

**Avoid stringing together many ifelse commands... use `case_when()` instead!** `case_when()` is much easier to read and you'll make fewer errors.  

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "ifelse bad.png"))
```

Outside of the context of a data frame, if you want to have an object used in your code switch its value, consider using `switch()` from **base** R.  




### Complex logic {#clean_case_when .unnumbered}  

Use **dplyr**'s `case_when()` if you are re-coding into many new groups, or if you need to use complex logic statements to re-code values. This function evaluates every row in the data frame, assess whether the rows meets specified criteria, and assigns the correct new value.  

`case_when()` commands consist of statements that have a Right-Hand Side (RHS) and a Left-Hand Side (LHS) separated by a "tilde" `~`. The logic criteria are in the left side and the pursuant values are in the right side of each statement. Statements are separated by commas.  

For example, here we utilize the columns `age` and `age_unit` to create a column `age_years`:  

```{r}
linelist <- linelist %>% 
  mutate(age_years = case_when(
       age_unit == "years"  ~ age,       # if age is given in years
       age_unit == "months" ~ age/12,    # if age is given in months
       is.na(age_unit)      ~ age))      # if age unit is missing, assume years
                                         # any other circumstance, assign NA
```


As each row in the data is evaluated, the criteria are applied/evaluated in the order the `case_when()` statements are written - from top-to-bottom. If the top criteria evaluates to `TRUE` for a given row, the RHS value is assigned, and the remaining criteria are not even tested for that row. Thus, it is best to write the most specific criteria first, and the most general last.  

Sometimes, you may with to write a final statement that assigns a value for all other scenarios not described by one of the previous lines. To do this, place `TRUE` on the left-side, which will capture any row that did not meet any of the previous criteria. The right-side of this statement could be assigned a value like "check me!" or missing.  


<span style="color: red;">**_DANGER:_** **Values on the right-side must all be the same class** - either numeric, character, date, logical, etc. To assign missing (`NA`), you may need to use special variations of `NA` such as `NA_character_`, `NA_real_` (for numeric or POSIX), and `as.Date(NA)`. Read more in [Working with dates].</span>  




### Missing values {.unnumbered} 

Below are special functions for handling missing values in the context of data cleaning.  

See the page on [Missing data] for more detailed tips on identifying and handling missing values. For example, the `is.na()` function which logically tests for missingness.  


**`replace_na()`**  

To change missing values (`NA`) to a specific value, such as "Missing", use the **dplyr** function `replace_na()` within `mutate()`. Note that this is used in the same manner as `recode` above - the name of the variable must be repeated within `replace_na()`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = replace_na(hospital, "Missing"))
```


**fct_explicit_na()**  

This is a function from the **forcats** package. The **forcats** package handles columns of class Factor. Factors are R's way to handle *ordered* values such as `c("First", "Second", "Third")` or to set the order that values (e.g. hospitals) appear in tables and plots. See the page on [Factors].  

If your data are class Factor and you try to convert `NA` to "Missing" by using `replace_na()`, you will get this error: `invalid factor level, NA generated`. You have tried to add "Missing" as a value, when it was not defined as a possible level of the factor, and it was rejected.  

The easiest way to solve this is to use the **forcats** function `fct_explicit_na()` which converts a column to class factor, and converts `NA` values to the character "(Missing)".  

```{r, eval=F}
linelist %>% 
  mutate(hospital = fct_explicit_na(hospital))
```

A slower alternative would be to add the factor level using `fct_expand()` and then convert the missing values.  

**`na_if()`**  

To convert a *specific value to* `NA`, use **dplyr**'s `na_if()`. The command below performs the opposite operation of `replace_na()`. In the example below, any values of "Missing" in the column `hospital` are converted to `NA`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = na_if(hospital, "Missing"))
```

Note: `na_if()` **cannot be used for logic criteria** (e.g. "all values > 99") - use `replace()` or `case_when()` for this:  

```{r, eval=F}
# Convert temperatures above 40 to NA 
linelist <- linelist %>% 
  mutate(temp = replace(temp, temp > 40, NA))

# Convert onset dates earlier than 1 Jan 2000 to missing
linelist <- linelist %>% 
  mutate(date_onset = replace(date_onset, date_onset > as.Date("2000-01-01"), NA))
```




### Cleaning dictionary {.unnumbered}

Use the R package **matchmaker** and its function `match_df()` to clean a data frame with a *cleaning dictionary*.  

1) Create a cleaning dictionary with 3 columns:  
    * A "from" column (the incorrect value)  
    * A "to" column (the correct value)  
    * A column specifying the column for the changes to be applied (or ".global" to apply to all columns)  

Note: .global dictionary entries will be overridden by column-specific dictionary entries.  

```{r, fig.align = "center", out.width = "75%", echo=F}
knitr::include_graphics(here::here("images", "cleaning_dict.png"))
```


2) Import the dictionary file into R. This example can be downloaded via instructions on the [Download handbook and data] page.  

```{r, echo=F}
cleaning_dict <- rio::import(here("data", "case_linelists", "cleaning_dict.csv"))
```

```{r, eval=F}
cleaning_dict <- import("cleaning_dict.csv")
```

3) Pipe the raw linelist to `match_df()`, specifying to `dictionary = ` the cleaning dictionary data frame. The `from = ` argument should be the name of the dictionary column which contains the "old" values, the `by = ` argument should be dictionary column which contains the corresponding "new" values, and the third column lists the column in which to make the change. Use `.global` in the `by = ` column to apply a change across all columns. A fourth dictionary column `order` can be used to specify factor order of new values.  

Read more details in the [package documentation](https://cran.r-project.org/web/packages/matchmaker/vignettes/intro.html) by running `?match_df`. Note this function can take a long time to run for a large dataset.  

```{r}
linelist <- linelist %>%     # provide or pipe your dataset
     matchmaker::match_df(
          dictionary = cleaning_dict,  # name of your dictionary
          from = "from",               # column with values to be replaced (default is col 1)
          to = "to",                   # column with final values (default is col 2)
          by = "col"                   # column with column names (default is col 3)
  )
```

Now scroll to the right to see how values have changed - particularly `gender` (lowercase to uppercase), and all the symptoms columns have been transformed from yes/no to 1/0.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Note that your column names in the cleaning dictionary must correspond to the names *at this point* in your cleaning script. See this [online reference for the linelist package](https://www.repidemicsconsortium.org/linelist/reference/clean_data.html) for more details.





#### Add to pipe chain {.unnumbered}  

**Below, some new columns and column transformations are added to the pipe chain.**  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
   # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
   ###################################################

    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_))
```






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Numeric categories {#num_cats}


Here we describe some special approaches for creating categories from numerical columns. Common examples include age categories, groups of lab values, etc. Here we will discuss:  

* `age_categories()`, from the **epikit** package  
* `cut()`, from **base** R  
* `case_when()`  
* quantile breaks with `quantile()` and `ntile()` 


### Review distribution {.unnumbered}

For this example we will create an `age_cat` column using the `age_years` column.  

```{r}
#check the class of the linelist variable age
class(linelist$age_years)
```

First, examine the distribution of your data, to make appropriate cut-points. See the page on [ggplot basics].  

```{r, out.height='50%'}
# examine the distribution
hist(linelist$age_years)
```

```{r}
summary(linelist$age_years, na.rm=T)
```

<span style="color: orange;">**_CAUTION:_** Sometimes, numeric variables will import as class "character". This occurs if there are non-numeric characters in some of the values, for example an entry of "2 months" for age, or (depending on your R locale settings) if a comma is used in the decimals place (e.g. "4,5" to mean four and one half years)..</span>


<!-- ======================================================= -->
### `age_categories()` {.unnumbered}

With the **epikit** package, you can use the `age_categories()` function to easily categorize and label numeric columns (note: this function can be applied to non-age numeric variables too). As a bonum, the output column is automatically an ordered factor.  

Here are the required inputs:  

* A numeric vector (column)  
* The `breakers = ` argument - provide a numeric vector of break points for the new groups  

First, the simplest example:  

```{r}
# Simple example
################
pacman::p_load(epikit)                    # load package

linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(             # create new column
      age_years,                            # numeric column to make groups from
      breakers = c(0, 5, 10, 15, 20,        # break points
                   30, 40, 50, 60, 70)))

# show table
table(linelist$age_cat, useNA = "always")
```

The break values you specify are by default the lower bounds - that is, they are included in the "higher" group / the groups are "open" on the lower/left side. As shown below, you can add 1 to each break value to achieve groups that are open at the top/right.
 
```{r}
# Include upper ends for the same categories
############################################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 6, 11, 16, 21, 31, 41, 51, 61, 71)))

# show table
table(linelist$age_cat, useNA = "always")
```


You can adjust how the labels are displayed with `separator = `. The default is "-"  

You can adjust how the top numbers are handled, with the `ceiling = ` arguemnt. To set an upper cut-off set `ceiling = TRUE`. In this use, the highest break value provided is a "ceiling" and a category "XX+" is not created. Any values above highest break value (or to `upper = `, if defined) are categorized as `NA`. Below is an example with `ceiling = TRUE`, so that there is no category of XX+ and values above 70 (the highest break value) are assigned as NA.  

```{r}
# With ceiling set to TRUE
##########################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70),
      ceiling = TRUE)) # 70 is ceiling, all above become NA

# show table
table(linelist$age_cat, useNA = "always")
```

Alternatively, instead of `breakers = `, you can provide all of `lower = `, `upper = `, and `by = `:  

* `lower = ` The lowest number you want considered - default is 0  
* `upper = ` The highest number you want considered  
* `by = `    The number of years between groups  

```{r}
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      lower = 0,
      upper = 100,
      by = 10))

# show table
table(linelist$age_cat, useNA = "always")
```


See the function's Help page for more details (enter `?age_categories` in the R console). 


<!-- ======================================================= -->
### `cut()` {.unnumbered}

`cut()` is a **base** R alternative to `age_categories()`, but I think you will see why `age_categories()` was developed to simplify this process. Some notable differences from `age_categories()` are:  

* You do not need to install/load another package  
* You can specify whether groups are open/closed on the right/left  
* You must provide accurate labels yourself  
* If you want 0 included in the lowest group you must specify this  

The basic syntax within `cut()` is to first provide the numeric column to be cut (`age_years`), and then the *breaks* argument, which is a numeric vector `c()` of break points. Using `cut()`, the resulting column is an ordered factor.  

By default, the categorization occurs so that the right/upper side is "open" and inclusive (and the left/lower side is "closed" or exclusive). This is the opposite behavior from the `age_categories()` function. The default labels use the notation "(A, B]", which means A is not included but B is. **Reverse this behavior by providing the `right = TRUE` argument**.   

Thus, by default, "0" values are excluded from the lowest group, and categorized as `NA`! "0" values could be infants coded as age 0 so be careful! To change this, add the argument `include.lowest = TRUE` so that any "0" values will be included in the lowest group. The automatically-generated label for the lowest category will then be "[A],B]". Note that if you include the `include.lowest = TRUE` argument **and** `right = TRUE`, the extreme inclusion will now apply to the *highest* break point value and category, not the lowest.  

You can provide a vector of customized labels using the `labels = ` argument. As these are manually written, be very careful to ensure they are accurate! Check your work using cross-tabulation, as described below. 

An example of `cut()` applied to `age_years` to make the new variable `age_cat` is below:  

```{r}
# Create new variable, by cutting the numeric age variable
# lower break is excluded but upper break is included in each category
linelist <- linelist %>% 
  mutate(
    age_cat = cut(
      age_years,
      breaks = c(0, 5, 10, 15, 20,
                 30, 50, 70, 100),
      include.lowest = TRUE         # include 0 in lowest group
      ))

# tabulate the number of observations per group
table(linelist$age_cat, useNA = "always")
```


**Check your work!!!** Verify that each age value was assigned to the correct category by cross-tabulating the numeric and category columns. Examine assignment of boundary values (e.g. 15, if neighboring categories are 10-15 and 16-20).  

```{r}
# Cross tabulation of the numeric and category columns. 
table("Numeric Values" = linelist$age_years,   # names specified in table for clarity.
      "Categories"     = linelist$age_cat,
      useNA = "always")                        # don't forget to examine NA values
```





**Re-labeling `NA` values**

You may want to assign `NA` values a label such as "Missing". Because the new column is class Factor (restricted values), you cannot simply mutate it with `replace_na()`, as this value will be rejected. Instead, use `fct_explicit_na()` from **forcats** as explained in the [Factors] page.   

```{r}
linelist <- linelist %>% 
  
  # cut() creates age_cat, automatically of class Factor      
  mutate(age_cat = cut(
    age_years,
    breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          
    right = FALSE,
    include.lowest = TRUE,        
    labels = c("0-4", "5-9", "10-14", "15-19", "20-29", "30-49", "50-69", "70-100")),
         
    # make missing values explicit
    age_cat = fct_explicit_na(
      age_cat,
      na_level = "Missing age")  # you can specify the label
  )    

# table to view counts
table(linelist$age_cat, useNA = "always")
```

**Quickly make breaks and labels**  

For a fast way to make breaks and label vectors, use something like below. See the [R basics] page for references on `seq()` and `rep()`.  

```{r, eval=F}
# Make break points from 0 to 90 by 5
age_seq = seq(from = 0, to = 90, by = 5)
age_seq

# Make labels for the above categories, assuming default cut() settings
age_labels = paste0(age_seq + 1, "-", age_seq + 5)
age_labels

# check that both vectors are the same length
length(age_seq) == length(age_labels)
```


Read more about `cut()` in its Help page by entering `?cut` in the R console.  




### Quantile breaks {.unnumbered}  

In common understanding, "quantiles" or "percentiles" typically refer to a value below which a proportion of values fall. For example, the 95th percentile of ages in `linelist` would be the age below which 95% of the age fall.  

However in common speech, "quartiles" and "deciles" can also refer to the *groups of data* as equally divided into 4, or 10 groups (note there will be one more break point than group).    

To get quantile break points, you can use `quantile()` from the **stats** package from **base** R. You provide a numeric vector (e.g. a column in a dataset) and vector of numeric probability values ranging from 0 to 1.0. The break points are returned as a numeric vector. Explore the details of the statistical methodologies by entering `?quantile`.  

* If your input numeric vector has any missing values it is best to set `na.rm = TRUE`  
* Set `names = FALSE` to get an un-named numeric vector  

```{r}
quantile(linelist$age_years,               # specify numeric vector to work on
  probs = c(0, .25, .50, .75, .90, .95),   # specify the percentiles you want
  na.rm = TRUE)                            # ignore missing values 
```

You can use the results of `quantile()` as break points in `age_categories()` or `cut()`. Below we create a new column `deciles` using `cut()` where the breaks are defined using `quantiles()` on `age_years`. Below, we display the results using `tabyl()` from **janitor** so you can see the percentages (see the [Descriptive tables] page). Note how they are not exactly 10% in each group.  

```{r}
linelist %>%                                # begin with linelist
  mutate(deciles = cut(age_years,           # create new column decile as cut() on column age_years
    breaks = quantile(                      # define cut breaks using quantile()
      age_years,                               # operate on age_years
      probs = seq(0, 1, by = 0.1),             # 0.0 to 1.0 by 0.1
      na.rm = TRUE),                           # ignore missing values
    include.lowest = TRUE)) %>%             # for cut() include age 0
  janitor::tabyl(deciles)                   # pipe to table to display
```

### Evenly-sized groups {.unnumbered}  

Another tool to make numeric groups is the the **dplyr** function `ntile()`, which attempts to break your data into n *evenly-sized groups* - *but be aware that unlike with `quantile()` the same value could appear in more than one group.* Provide the numeric vector and then the number of groups. The values in the new column created is just group "numbers" (e.g. 1 to 10), not the range of values themselves as when using `cut()`.  

```{r}
# make groups with ntile()
ntile_data <- linelist %>% 
  mutate(even_groups = ntile(age_years, 10))

# make table of counts and proportions by group
ntile_table <- ntile_data %>% 
  janitor::tabyl(even_groups)
  
# attach min/max values to demonstrate ranges
ntile_ranges <- ntile_data %>% 
  group_by(even_groups) %>% 
  summarise(
    min = min(age_years, na.rm=T),
    max = max(age_years, na.rm=T)
  )

# combine and print - note that values are present in multiple groups
left_join(ntile_table, ntile_ranges, by = "even_groups")
```


<!-- ======================================================= -->
### `case_when()` { .unnumbered}

It is possible to use the **dplyr** function `case_when()` to create categories from a numeric column, but it is easier to use `age_categories()` from **epikit** or `cut()` because these will create an ordered factor automatically. 

If using `case_when()`, please review the proper use as described earlier in the Re-code values section of this page. Also be aware that all right-hand side values must be of the same class. Thus, if you want `NA` on the right-side you should either write "Missing" or use the special `NA` value `NA_character_`.  


### Add to pipe chain {.unnumbered}  

Below, code to create two categorical age columns is added to the cleaning pipe chain:  

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################   
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5)))
```








<!-- ======================================================= -->
## Add rows  

### One-by-one {.unnumbered}  

Adding rows one-by-one manually is tedious but can be done with `add_row()` from **dplyr**. Remember that each column must contain values of only one class (either character, numeric, logical, etc.). So adding a row requires nuance to maintain this. 

```{r, eval=F}
linelist <- linelist %>% 
  add_row(row_num = 666,
          case_id = "abc",
          generation = 4,
          `infection date` = as.Date("2020-10-10"),
          .before = 2)
```

Use `.before` and `.after.` to specify the placement of the row you want to add. `.before = 3` will put the new row before the current 3rd row. The default behavior is to add the row to the end. Columns not specified will be left empty (`NA`).  

The new *row number* may look strange ("...23") but the row numbers in the pre-existing rows *have* changed. So if using the command twice, examine/test the insertion carefully.

If a class you provide is off you will see an error like this:  

```
Error: Can't combine ..1$infection date <date> and ..2$infection date <character>.
```

(when inserting a row with a date value, remember to wrap the date in the function `as.Date()` like `as.Date("2020-10-10")`).


### Bind rows {.unnumbered}  

To combine datasets together by binding the rows of one dataframe to the bottom of another data frame, you can use `bind_rows()` from **dplyr**. This is explained in more detail in the page [Joining data].  




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Filter rows {  }


A typical cleaning step after you have cleaned the columns and re-coded values is to *filter* the data frame for specific rows using the **dplyr** verb `filter()`.  

Within `filter()`, specify the logic that must be `TRUE` for a row in the dataset to be kept. Below we show how to filter rows based on simple and complex logical conditions.  



<!-- ======================================================= -->
### Simple filter {.unnumbered} 

This simple example re-defines the dataframe `linelist` as itself, having filtered the rows to meet a logical condition. **Only the rows where the logical statement within the parentheses evaluates to `TRUE` are kept.**  

In this example, the logical statement is `gender == "f"`, which is asking whether the value in the column `gender` is equal to "f" (case sensitive).   

Before the filter is applied, the number of rows in `linelist` is ` nrow(linelist)`.

```{r, eval=F}
linelist <- linelist %>% 
  filter(gender == "f")   # keep only rows where gender is equal to "f"
```

After the filter is applied, the number of rows in `linelist` is ` linelist %>% filter(gender == "f") %>% nrow()`.


### Filter out missing values {.unnumbered}  

It is fairly common to want to filter out rows that have missing values. Resist the urge to write `filter(!is.na(column) & !is.na(column))` and instead use the **tidyr** function that is custom-built for this purpose: `drop_na()`. If run with empty parentheses, it removes rows with *any* missing values. Alternatively, you can provide names of specific columns to be evaluated for missingness, or use the "tidyselect" helper functions described [above](#clean_tidyselect).  

```{r, eval=F}
linelist %>% 
  drop_na(case_id, age_years)  # drop rows with missing values for case_id or age_years
```

See the page on [Missing data] for many techniques to analyse and manage missingness in your data. 




### Filter by row number {.unnumbered}  

In a data frame or tibble, each row will usually have a "row number" that (when seen in R Viewer) appears to the left of the first column. It is not itself a true column in the data, but it can be used in a `filter()` statement.  

To filter based on "row number", you can use the **dplyr** function `row_number()` with open parentheses as part of a logical filtering statement. Often you will use the `%in%` operator and a range of numbers as part of that logical statement, as shown below. To see the *first* N rows, you can also use the special **dplyr** function `head()`.   

```{r, eval=F}
# View first 100 rows
linelist %>% head(100)     # or use tail() to see the n last rows

# Show row 5 only
linelist %>% filter(row_number() == 5)

# View rows 2 through 20, and three specific columns
linelist %>% filter(row_number() %in% 2:20) %>% select(date_onset, outcome, age)
```

You can also convert the row numbers to a true column by piping your data frame to the **tibble** function `rownames_to_column()` (do not put anything in the parentheses).  


<!-- ======================================================= -->
### Complex filter {.unnumbered} 

More complex logical statements can be constructed using parentheses `( )`, OR `|`, negate `!`, `%in%`, and AND `&` operators. An example is below:  


Note: You can use the `!` operator in front of a logical criteria to negate it. For example, `!is.na(column)` evaluates to true if the column value is *not* missing. Likewise `!column %in% c("a", "b", "c")` evaluates to true if the column value is *not* in the vector.  


#### Examine the data  {.unnumbered}  

Below is a simple one-line command to create a histogram of onset dates. See that a second smaller outbreak from 2012-2013 is also included in this raw dataset. **For our analyses, we want to remove entries from this earlier outbreak.**  

```{r, out.width = "50%"}
hist(linelist$date_onset, breaks = 50)
```


#### How filters handle missing numeric and date values {.unnumbered}  

Can we just filter by `date_onset` to rows after June 2013? **Caution! Applying the code `filter(date_onset > as.Date("2013-06-01")))` would remove any rows in the later epidemic with a missing date of onset!**  

<span style="color: red;">**_DANGER:_** Filtering to greater than (>) or less than (<) a date or number can remove any rows with missing values (`NA`)! This is because `NA` is treated as infinitely large and small.</span>

*(See the page on [Working with dates] for more information on working with dates and the package **lubridate**)*

#### Design the filter {.unnumbered}  

Examine a cross-tabulation to make sure we exclude only the correct rows:  


```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

What other criteria can we filter on to remove the first outbreak (in 2012 & 2013) from the dataset? We see that:  

* The first epidemic  in 2012 & 2013 occurred at Hospital A, Hospital B, and that there were also 10 cases at Port Hospital.  
* Hospitals A & B did *not* have cases in the second epidemic, but Port Hospital did.  

We want to exclude:  

* The ` nrow(linelist %>% filter(hospital %in% c("Hospital A", "Hospital B") | date_onset < as.Date("2013-06-01")))` rows with onset in 2012 and 2013 at either hospital A, B, or Port:  
  * Exclude ` nrow(linelist %>% filter(date_onset < as.Date("2013-06-01")))` rows with onset in 2012 and 2013
  * Exclude ` nrow(linelist %>% filter(hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` rows from Hospitals A & B with missing onset dates  
  * Do **not** exclude ` nrow(linelist %>% filter(!hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` other rows with missing onset dates.  

We start with a linelist of ` `nrow(linelist)`. Here is our filter statement:  

```{r}
linelist <- linelist %>% 
  # keep rows where onset is after 1 June 2013 OR where onset is missing and it was a hospital OTHER than Hospital A or B
  filter(date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))

nrow(linelist)
```

When we re-make the cross-tabulation, we see that Hospitals A & B are removed completely, and the 10 Port Hospital cases from 2012 & 2013 are removed, and all other values are the same - just as we wanted.  
 
```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

Multiple statements can be included within one filter command (separated by commas), or you can always pipe to a separate filter() command for clarity.  


*Note: some readers may notice that it would be easier to just filter by `date_hospitalisation` because it is 100% complete with no missing values. This is true. But `date_onset` is used for purposes of demonstrating a complex filter.* 




### Standalone {.unnumbered}  

Filtering can also be done as a stand-alone command (not part of a pipe chain). Like other **dplyr** verbs, in this case the first argument must be the dataset itself.  

```{r, eval=F}
# dataframe <- filter(dataframe, condition(s) for rows to keep)

linelist <- filter(linelist, !is.na(case_id))
```

You can also use **base** R to subset using square brackets which reflect the [rows, columns] that you want to retain.  

```{r, eval=F}
# dataframe <- dataframe[row conditions, column conditions] (blank means keep all)

linelist <- linelist[!is.na(case_id), ]
```





### Quickly review records {.unnumbered} 

Often you want to quickly review a few records, for only a few columns. The **base** R function `View()` will print a data frame for viewing in your RStudio. 

View the linelist in RStudio:  

```{r, eval=F}
View(linelist)
```

Here are two examples of viewing specific cells (specific rows, and specific columns):  


**With dplyr functions `filter()` and `select()`:**  

Within `View()`, pipe the dataset to `filter()` to keep certain rows, and then to `select()` to keep certain columns. For example, to review onset and hospitalization dates of 3 specific cases:   

```{r, eval=F}
View(linelist %>%
       filter(case_id %in% c("11f8ea", "76b97a", "47a5f5")) %>%
       select(date_onset, date_hospitalisation))
```


You can achieve the same with **base** R syntax, using brackets `[ ]` to subset you want to see. 

```{r, eval=F}
View(linelist[linelist$case_id %in% c("11f8ea", "76b97a", "47a5f5"), c("date_onset", "date_hospitalisation")])
```





#### Add to pipe chain {.unnumbered}  


```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age,
          TRUE ~ NA_real_)) %>% 
  
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5))) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    filter(
          # keep only rows where case_id is not missing
          !is.na(case_id),  
          
          # also filter to keep only the second outbreak
          date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))
```







<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Row-wise calculations  

If you want to perform a calculation within a row, you can use `rowwise()` from **dplyr**. See this online vignette on [row-wise calculations](https://cran.r-project.org/web/packages/dplyr/vignettes/rowwise.html).  
For example, this code applies `rowwise()` and then creates a new column that sums the number of the specified symptom columns that have value "yes", for each row in the linelist. The columns are specified within `sum()` by name within a vector `c()`. `rowwise()` is essentially a special kind of `group_by()`, so it is best to use `ungroup()` when you are done (page on [Grouping data]).  

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_symptoms = sum(c(fever, chills, cough, aches, vomit) == "yes")) %>% 
  ungroup() %>% 
  select(fever, chills, cough, aches, vomit, num_symptoms) # for display
```

  
As you specify the column to evaluate, you may want to use the  "tidyselect" helper functions described in the `select()` section of this page. You just have to make one adjustment (because you are not using them within a **dplyr** function like `select()` or `summarise()`).  

Put the column-specification criteria within the **dplyr** function `c_across()`. This is because `c_across` ([documentation](https://dplyr.tidyverse.org/reference/c_across.html)) is designed to work with `rowwise()` specifically. For example, the following code:  

* Applies `rowwise()` so the following operation (`sum()`) is applied within each row (not summing entire columns)  
* Creates new column `num_NA_dates`, defined for each row as the number of columns (with name containing "date") for which `is.na()` evaluated to TRUE (they are missing data).  
* `ungroup()` to remove the effects of `rowwise()` for subsequent steps  

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_NA_dates = sum(is.na(c_across(contains("date"))))) %>% 
  ungroup() %>% 
  select(num_NA_dates, contains("date")) # for display
```

You could also provide other functions, such as `max()` to get the latest or most recent date for each row:  

```{r}
linelist %>%
  rowwise() %>%
  mutate(latest_date = max(c_across(contains("date")), na.rm=T)) %>% 
  ungroup() %>% 
  select(latest_date, contains("date"))  # for display
```


## Arrange and sort  

Use the **dplyr** function `arrange()` to sort or order the rows by column values.  

Simple list the columns in the order they should be sorted on. Specify `.by_group = TRUE` if you want the sorting to to first occur by any *groupings* applied to the data (see page on [Grouping data]).  

By default, column will be sorted in "ascending" order (which applies to numeric and also to character columns). You can sort a variable in "descending" order by wrapping it with `desc()`.  

Sorting data with `arrange()` is particularly useful when making [Tables for presentation], using `slice()` to take the "top" rows per group, or setting factor level order by order of appearance.  

For example, to sort the our linelist rows by `hospital`, then by `date_onset` in descending order, we would use:  

```{r, eval=F}
linelist %>% 
   arrange(hospital, desc(date_onset))
```


```{r, echo=F}
# HIDDEN
#
# convert one remaining old outbreak row to missing for ease
linelist <- linelist %>% 
  mutate(
    date_hospitalisation = case_when(
      date_hospitalisation < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                         ~ date_hospitalisation),
    date_outcome = case_when(
      date_outcome < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                 ~ date_outcome)
    )

#min(linelist$date_hospitalisation, na.rm=T)
#min(linelist$date_outcome, na.rm=T)
```



```{r echo=F}
# REARRANGE COLUMNS FOR EXPORT
linelist <- linelist %>% 
  select(case_id:gender, age, age_unit, age_years, age_cat, age_cat5, everything())
```

```{r echo=F, eval=F}
# EXPORT CLEANED LINELIST FILE TO "DATA" FOLDER
rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.xlsx"))
rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.rds"))
```
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cleaning.Rmd-->


# Working with dates {}


```{r, out.width=c('50%'), fig.align='center', echo=F, message=F}
knitr::include_graphics(here::here("images", "Dates_500x500.png"))
```


Working with dates in R requires more attention than working with other object classes. Below, we offer some tools and example to make this process less painful. Luckily, dates can be wrangled easily with practice, and with a set of helpful packages such as **lubridate**.  

Upon import of raw data, R often interprets dates as character objects - this means they cannot be used for general date operations such as making time series and calculating time intervals. To make matters more difficult, there are many ways a date can be formatted and you must help R know which part of a date represents what (month, day, hour, etc.). 

Dates in R are their own class of object - the `Date` class. It should be noted that there is also a class that stores objects with date *and* time. Date time objects are formally referred to as `POSIXt`, `POSIXct`, and/or `POSIXlt` classes (the difference isn't important). These objects are informally referred to as *datetime* classes.

* It is important to make R recognize when a column contains dates.  
* Dates are an object class and can be tricky to work with.  
* Here we present several ways to convert date columns to Date class.  


<!-- ======================================================= -->
## Preparation

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for this page. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r dates_packages, warning=F, message=F}
# Checks if package is installed, installs if necessary, and loads package for current session

pacman::p_load(
  lubridate,  # general package for handling and converting dates  
  parsedate,   # has function to "guess" messy dates
  aweek,      # another option for converting dates to weeks, and weeks to dates
  zoo,        # additional date/time functions
  tidyverse,  # data management and visualization  
  rio)        # data import/export
```

### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow along step-by-step, see instruction in the [Download handbook and data] page. We assume the file is in the working directory so no sub-folders are specified in this file path.  

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")

```



<!-- ======================================================= -->
## Current date  

You can get the current "system" date or system datetime of your computer by doing the following with **base** R.  

```{r}
# get the system date - this is a DATE class
Sys.Date()

# get the system time - this is a DATETIME class
Sys.time()
```


With the **lubridate** package these can also be returned with `today()` and `now()`, respectively. `date()` returns the current date and time with weekday and month names.  
  
  

<!-- ======================================================= -->
## Convert to Date  

After importing a dataset into R, date column values may look like "1989/12/30", "05/06/2014", or "13 Jan 2020". In these cases, R is likely still treating these values as Character values. R must be *told* that these values are dates... and what the format of the date is (which part is Day, which is Month, which is Year, etc).  

Once told, R converts these values to class Date. In the background, R will store the dates as numbers (the number of days from its "origin" date 1 Jan 1970). You will not interface with the date number often, but this allows for R to treat dates as continuous variables and to allow special operations such as calculating the distance between dates.  

By default, values of class Date in R are displayed as YYYY-MM-DD. Later in this section we will discuss how to change the display of date values.  

Below we present two approaches to converting a column from character values to class Date.  


<span style="color: darkgreen;">**_TIP:_** You can check the current class of a column with **base** R function `class()`, like `class(linelist$date_onset)`.</span>  

  

### **base** R {.unnumbered}  

`as.Date()` is the standard, **base** R function to convert an object or column to class Date (note capitalization of "D").  

Use of `as.Date()` requires that:  

* You *specify the **existing** format of the raw character date* or the origin date if supplying dates as numbers (see section on Excel dates)  
* If used on a character column, all date values must have the same exact format (if this is not the case, try `parse_date()` from the **parsedate** package)  

**First**, check the class of your column with `class()` from **base** R. If you are unsure or confused about the class of your data (e.g. you see "POSIXct", etc.) it can be easiest to first convert the column to class Character with `as.character()`, and then convert it to class Date.  

**Second**, within the `as.Date()` function, use the `format =` argument to tell R the *current* format of the character date components - which characters refer to the month, the day, and the year, and how they are separated. If your values are already in one of R's standard date formats ("YYYY-MM-DD" or "YYYY/MM/DD") the `format =` argument is not necessary.  

To `format = `, provide a character string (in quotes) that represents the *current* date format using the special "strptime" abbreviations below. For example, if your character dates are currently in the format "DD/MM/YYYY", like "24/04/1968", then you would use `format = "%d/%m/%Y"` to convert the values into dates. **Putting the format in quotation marks is necessary. And don't forget any slashes or dashes!**  

```{r eval=F}
# Convert to class date
linelist <- linelist %>% 
  mutate(date_onset = as.Date(date_of_onset, format = "%d/%m/%Y"))
```

Most of the strptime abbreviations are listed below. You can see the complete list by running `?strptime`.  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds
%z = offset from GMT  
%Z = Time zone (character)  

<span style="color: darkgreen;">**_TIP:_** The `format =` argument of `as.Date()` is *not* telling R the format you want the dates to be, but rather how to identify the date parts as they are *before* you run the command.</span>  

<span style="color: darkgreen;">**_TIP:_** Be sure that in the `format =` argument you use the *date-part separator* (e.g. /, -, or space) that is present in your dates.</span>  

Once the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.



### **lubridate** {.unnumbered}  

Converting character objects to dates can be made easier by using the **lubridate** package. This is a **tidyverse** package designed to make working with dates and times more simple and consistent than in **base** R. For these reasons, **lubridate** is often considered the gold-standard package for dates and time, and is recommended whenever working with them.

The **lubridate** package provides several different helper functions designed to convert character objects to dates in an intuitive, and more lenient way than specifying the format in `as.Date()`. These functions are specific to the rough date format, but allow for a variety of separators, and synonyms for dates (e.g. 01 vs Jan vs January) - they are named after abbreviations of date formats. 


```{r, }
# install/load lubridate 
pacman::p_load(lubridate)
```

The `ymd()` function flexibly converts date values supplied as **year, then month, then day**.  

```{r}
# read date in year-month-day format
ymd("2020-10-11")
ymd("20201011")
```

The `mdy()` function flexibly converts date values supplied as **month, then day, then year**.  

```{r}
# read date in month-day-year format
mdy("10/11/2020")
mdy("Oct 11 20")
```

The `dmy()` function flexibly converts date values supplied as **day, then month, then year**.  

```{r}
# read date in day-month-year format
dmy("11 10 2020")
dmy("11 October 2020")
```

<!-- The `as.character()` and `as.Date()` commands can optionally be combined as:   -->

<!-- ```{r eval=F} -->
<!-- linelist_cleaned$date_of_onset <- as.Date(as.character(linelist_cleaned$date_of_onset), format = "%d/%m/%Y") -->
<!-- ``` -->

If using piping, the conversion of a character column to dates with **lubridate** might look like this:  

```{r, eval=F}
linelist <- linelist %>%
  mutate(date_onset = lubridate::dmy(date_onset))
```

Once complete, you can run `class()` to verify the class of the column  

```{r, eval=F}
# Check the class of the column
class(linelist$date_onset)  
```


Once the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.  

Note that the above functions work best with 4-digit years. 2-digit years can produce unexpected results, as lubridate attempts to guess the century.  

To convert a 2-digit year into a 4-digit year (all in the same century) you can convert to class character and then combine the existing digits with a pre-fix using `str_glue()` from the **stringr** package (see [Characters and strings]). Then convert to date.  

```{r}
two_digit_years <- c("15", "15", "16", "17")
str_glue("20{two_digit_years}")
```



### Combine columns {.unnumbered}  

You can use the **lubridate** functions `make_date()` and `make_datetime()` to combine multiple numeric columns into one date column. For example if you have numeric columns `onset_day`, `onset_month`, and `onset_year` in the data frame `linelist`:  

```{r, eval=F}
linelist <- linelist %>% 
  mutate(onset_date = make_date(year = onset_year, month = onset_month, day = onset_day))
```




<!-- ======================================================= -->
## Excel dates

In the background, most software store dates as numbers. R stores dates from an origin of 1st January, 1970. Thus, if you run `as.numeric(as.Date("1970-01-01))` you will get `0`. 

Microsoft Excel stores dates with an origin of either December 30, 1899 (Windows) or January 1, 1904 (Mac), depending on your operating system. See this [Microsoft guidance](https://docs.microsoft.com/en-us/office/troubleshoot/excel/1900-and-1904-date-system) for more information.  

Excel dates often import into R as these numeric values instead of as characters. If the dataset you imported from Excel shows dates as numbers or characters like "41369"... use `as.Date()` (or **lubridate**'s `as_date()` function) to convert, but **instead of supplying a "format" as above, supply the Excel origin date** to the argument `origin = `.  

This will not work if the Excel date is stored in R as a character type, so be sure to ensure the number is class Numeric!

<span style="color: black;">**_NOTE:_** You should provide the origin date in R's default date format ("YYYY-MM-DD").</span>

```{r, eval = FALSE}
# An example of providing the Excel 'origin date' when converting Excel number dates
data_cleaned <- data %>% 
  mutate(date_onset = as.numeric(date_onset)) %>%   # ensure class is numeric
  mutate(date_onset = as.Date(date_onset, origin = "1899-12-30")) # convert to date using Excel origin
```



<!-- ======================================================= -->
## Messy dates  

The function `parse_date()` from the **parsedate** package attempts to read a "messy" date column containing dates in many different formats and convert the dates to a standard format. You can [read more online about `parse_date()`](https://readr.tidyverse.org/reference/parse_datetime.html).  

For example `parse_date()` would see a vector of the following character dates "03 Jan 2018", "07/03/1982", and "08/20/85" and convert them to class Date as: `2018-01-03`, `1982-03-07`, and `1985-08-20`.  

```{r}
parsedate::parse_date(c("03 January 2018",
                        "07/03/1982",
                        "08/20/85"))
```


```{r eval = FALSE}
# An example using parse_date() on the column date_onset
linelist <- linelist %>%      
  mutate(date_onset = parse_date(date_onset))
```




<!-- ======================================================= -->
## Working with date-time class  

As previously mentioned, R also supports a `datetime` class - a column that contains date **and** time information. As with the `Date` class, these often need to be converted from `character` objects to `datetime` objects. 

### Convert dates with times {.unnumbered}  

A standard `datetime` object is formatted with the date first, which is followed by a time component - for example  _01 Jan 2020, 16:30_. As with dates, there are many ways this can be formatted, and there are numerous levels of precision (hours, minutes, seconds) that can be supplied.  

Luckily, **lubridate** helper functions also exist to help convert these strings to `datetime` objects. These functions are extensions of the date helper functions, with `_h` (only hours supplied), `_hm` (hours and minutes supplied), or `_hms` (hours, minutes, and seconds supplied) appended to the end (e.g. `dmy_hms()`). These can be used as shown:

Convert datetime with only hours to datetime object  

```{r}
ymd_h("2020-01-01 16hrs")
ymd_h("2020-01-01 4PM")
```

Convert datetime with hours and minutes to datetime object  

```{r}
dmy_hm("01 January 2020 16:20")
```

Convert datetime with hours, minutes, and seconds to datetime object  

```{r}
mdy_hms("01 January 2020, 16:20:40")
```

You can supply time zone but it is ignored. See section later in this page on time zones.  

```{r}
mdy_hms("01 January 2020, 16:20:40 PST")

```

When working with a data frame, time and date columns can be combined to create a datetime column using `str_glue()` from **stringr** package and an appropriate **lubridate** function. See the page on [Characters and strings] for details on **stringr**.  

In this example, the `linelist` data frame has a column in format "hours:minutes". To convert this to a datetime we follow a few steps:  

1) Create a "clean" time of admission column with missing values filled-in with the column median. We do this because **lubridate** won't operate on missing values. Combine it with the column `date_hospitalisation`, and then use the function `ymd_hm()` to convert.  

```{r, eval = FALSE}
# packages
pacman::p_load(tidyverse, lubridate, stringr)

# time_admission is a column in hours:minutes
linelist <- linelist %>%
  
  # when time of admission is not given, assign the median admission time
  mutate(
    time_admission_clean = ifelse(
      is.na(time_admission),         # if time is missing
      median(time_admission),        # assign the median
      time_admission                 # if not missing keep as is
  ) %>%
  
    # use str_glue() to combine date and time columns to create one character column
    # and then use ymd_hm() to convert it to datetime
  mutate(
    date_time_of_admission = str_glue("{date_hospitalisation} {time_admission_clean}") %>% 
      ymd_hm()
  )

```

### Convert times alone {.unnumbered}  

If your data contain only a character time (hours and minutes), you can convert and manipulate them as times using `strptime()` from **base** R. For example, to get the difference between two of these times:  

```{r}
# raw character times
time1 <- "13:45" 
time2 <- "15:20"

# Times converted to a datetime class
time1_clean <- strptime(time1, format = "%H:%M")
time2_clean <- strptime(time2, format = "%H:%M")

# Difference is of class "difftime" by default, here converted to numeric hours 
as.numeric(time2_clean - time1_clean)   # difference in hours

```

Note however that without a date value provided, it assumes the date is today. To combine a string date and a string time together see how to use **stringr** in the section just above. Read more about `strptime()` [here](https://rdrr.io/r/base/strptime.html).  

To convert single-digit numbers to double-digits (e.g. to "pad" hours or minutes with leading zeros to achieve 2 digits), see this ["Pad length" section of the Characters and strings page](#str_pad).  


### Extract time {.unnumbered}  

You can extract elements of a time with `hour()`, `minute()`, or `second()` from **lubridate**.  

Here is an example of extracting the hour, and then classifing by part of the day. We begin with the column `time_admission`, which is class Character in format "HH:MM". First, the `strptime()` is used as described above to convert the characters to datetime class. Then, the hour is extracted with `hour()`, returning a number from 0-24. Finally, a column `time_period` is created using logic with `case_when()` to classify rows into Morning/Afternoon/Evening/Night based on their hour of admission.  

```{r}
linelist <- linelist %>%
  mutate(hour_admit = hour(strptime(time_admission, format = "%H:%M"))) %>%
  mutate(time_period = case_when(
    hour_admit > 06 & hour_admit < 12 ~ "Morning",
    hour_admit >= 12 & hour_admit < 17 ~ "Afternoon",
    hour_admit >= 17 & hour_admit < 21 ~ "Evening",
    hour_admit >=21 | hour_admit <= 6 ~ "Night"))
```

To learn more about `case_when()` see the page on [Cleaning data and core functions].  

<!-- ======================================================= -->
## Working with dates   

`lubridate` can also be used for a variety of other functions, such as **extracting aspects of a date/datetime**, **performing date arithmetic**, or **calculating date intervals**

Here we define a date to use for the examples:  

```{r, }
# create object of class Date
example_date <- ymd("2020-03-01")
```

### Extract date components {.unnumbered}  

You can extract common aspects such as month, day, weekday:  

```{r}
month(example_date)  # month number
day(example_date)    # day (number) of the month
wday(example_date)   # day number of the week (1-7)
```

You can also extract time components from a `datetime` object or column. This can be useful if you want to view the distribution of admission times.  

```{r, eval=F}
example_datetime <- ymd_hm("2020-03-01 14:45")

hour(example_datetime)     # extract hour
minute(example_datetime)   # extract minute
second(example_datetime)   # extract second
```

There are several options to retrieve weeks. See the section on Epidemiological weeks below.  

Note that if you are seeking to *display* a date a certain way (e.g. "Jan 2020" or "Thursday 20 March" or "Week 20, 1977") you can do this more flexibly as described in the section on Date display.  


### Date math {.unnumbered}  

You can add certain numbers of days or weeks using their respective function from **lubridate**.  

```{r}
# add 3 days to this date
example_date + days(3)
  
# add 7 weeks and subtract two days from this date
example_date + weeks(7) - days(2)
```

### Date intervals {.unnumbered}  

The difference between dates can be calculated by:  

1. Ensure both dates are of class date  
2. Use subtraction to return the "difftime" difference between the two dates  
3. If necessary, convert the result to numeric class to perform subsequent mathematical calculations  

Below the interval between two dates is calculated and displayed. You can find intervals by using the subtraction "minus" symbol on values that are class Date. Note, however that the class of the returned value is "difftime" as displayed below, and must be converted to numeric. 

```{r}
# find the interval between this date and Feb 20 2020 
output <- example_date - ymd("2020-02-20")
output    # print
class(output)
```

To do subsequent operations on a "difftime", convert it to numeric with `as.numeric()`. 

This can all be brought together to work with data - for example:

```{r, eval = F}
pacman::p_load(lubridate, tidyverse)   # load packages

linelist <- linelist %>%
  
  # convert date of onset from character to date objects by specifying dmy format
  mutate(date_onset = dmy(date_onset),
         date_hospitalisation = dmy(date_hospitalisation)) %>%
  
  # filter out all cases without onset in march
  filter(month(date_onset) == 3) %>%
    
  # find the difference in days between onset and hospitalisation
  mutate(days_onset_to_hosp = date_hospitalisation - date_of_onset)
```



In a data frame context, if either of the above dates is missing, the operation will fail for that row. This will result in an `NA` instead of a numeric value. When using this column for calculations, be sure to set the `na.rm = ` argument to `TRUE`. For example:

```{r, eval = FALSE}
# calculate the median number of days to hospitalisation for all cases where data are available
median(linelist_delay$days_onset_to_hosp, na.rm = T)
```


<!-- ======================================================= -->
## Date display  

Once dates are the correct class, you often want them to display differently, for example to display as "Monday 05 January" instead of "2018-01-05". You may also want to adjust the display in order to then group rows by the date elements displayed - for example to group by month-year.  

### `format()` {.unnumbered}  

Adjust date display with the **base** R function `format()`. This function accepts a character string (in quotes) specifying the *desired* output format in the "%" strptime abbreviations (the same syntax as used in `as.Date()`). Below are most of the common abbreviations.  

Note: using `format()` will convert the values to class Character, so this is generally used towards the end of an analysis or for display purposes only! You can see the complete list by running `?strptime`.  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)  
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds  
%z = offset from GMT  
%Z = Time zone (character)

An example of formatting today's date:  

```{r}
# today's date, with formatting
format(Sys.Date(), format = "%d %B %Y")

# easy way to get full date and time (default formatting)
date()

# formatted combined date, time, and time zone using str_glue() function
str_glue("{format(Sys.Date(), format = '%A, %B %d %Y, %z  %Z, ')}{format(Sys.time(), format = '%H:%M:%S')}")

# Using format to display weeks
format(Sys.Date(), "%Y Week %W")
```

Note that if using `str_glue()`, be aware of that within the expected double quotes " you should only use single quotes (as above).  


### Month-Year {.unnumbered}  

To convert a Date column to Month-year format, we suggest you use the function `as.yearmon()` from the **zoo** package. This converts the date to class "yearmon" and retains the proper ordering. In contrast, using `format(column, "%Y %B")` will convert to class Character and will order the values alphabetically (incorrectly). 

Below, a new column `yearmonth` is created from the column `date_onset`, using the `as.yearmon()` function. The default (correct) ordering of the resulting values are shown in the table.  

```{r}
# create new column 
test_zoo <- linelist %>% 
     mutate(yearmonth = zoo::as.yearmon(date_onset))

# print table
table(test_zoo$yearmon)
```

In contrast, you can see how only using `format()` does achieve the desired display format, but not the correct ordering.  

```{r}
# create new column
test_format <- linelist %>% 
     mutate(yearmonth = format(date_onset, "%b %Y"))

# print table
table(test_format$yearmon)
```

Note: if you are working within a `ggplot()` and want to adjust how dates are *displayed* only, it may be sufficient to provide a strptime format to the `date_labels = ` argument in `scale_x_date()` - you can use `"%b %Y"` or `"%Y %b"`. See the [ggplot tips] page.  


**zoo** also offers the function `as.yearqtr()`, and you can use `scale_x_yearmon()` when using `ggplot()`.  



<!-- ======================================================= -->
## Epidemiological weeks {#dates_epi_wks}

### **lubridate** {.unnumbered}  

See the page on [Grouping data] for more extensive examples of grouping data by date. Below we briefly describe grouping data by weeks.  

We generally recommend using the `floor_date()` function from **lubridate**, with the argument `unit = "week"`. This rounds the date down to the "start" of the week, as defined by the argument `week_start = `. The default week start is 1 (for Mondays) but you can specify any day of the week as the start (e.g. 7 for Sundays). `floor_date()` is versitile and can be used to round down to other time units by setting `unit = ` to "second", "minute", "hour", "day", "month", or "year".  

The returned value is the start date of the week, in Date class. Date class is useful when plotting the data, as it will be easily recognized and ordered correctly by `ggplot()`.

If you are only interested in adjusting dates to *display* by week in a plot, see the section in this page on Date display. For example when plotting an epicurve you can format the date display by providing the desired strptime "%" nomenclature. For example, use "%Y-%W" or "%Y-%U" to return the year and week number (given Monday or Sunday week start, respectively).  

### Weekly counts {.unnumbered}  

See the page on [Grouping data] for a thorough explanation of grouping data with `count()`, `group_by()`, and `summarise()`. A brief example is below.  

1) Create a new 'week' column with `mutate()`, using `floor_date()` with `unit = "week"`  
2) Get counts of rows (cases) per week with `count()`; filter out any cases with missing date  
3) Finish with `complete()` from **tidyr** to ensure that *all* weeks appear in the data - even those with no rows/cases. By default the count values for any "new" rows are NA, but you can make them 0 with the `fill = ` argument, which expects a named list (below, `n` is the name of the counts column).  

```{r}
# Make aggregated dataset of weekly case counts
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%             # remove cases missing onset date
  mutate(weekly_cases = floor_date(   # make new column, week of onset
    date_onset,
    unit = "week")) %>%            
  count(weekly_cases) %>%           # group data by week and count rows per group (creates column 'n')
  tidyr::complete(                  # ensure all weeks are present, even those with no cases reported
    weekly_cases = seq.Date(          # re-define the "weekly_cases" column as a complete sequence,
      from = min(weekly_cases),       # from the minimum date
      to = max(weekly_cases),         # to the maxiumum date
      by = "week"),                   # by weeks
    fill = list(n = 0))             # fill-in NAs in the n counts column with 0
```

Here are the first rows of the resulting data frame:  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_counts, 20), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Epiweek alternatives {.unnumbered}  

Note that **lubridate** also has functions `week()`, `epiweek()`, and `isoweek()`, each of which has slightly different start dates and other nuances. Generally speaking though, `floor_date()` should be all that you need. Read the details for these functions by entering `?week` into the console or reading the documentation [here](https://www.rdocumentation.org/packages/lubridate/versions/1.7.4/topics/week). 


You might consider using the package **aweek** to set epidemiological weeks. You can read more about it [on the RECON website](https://www.repidemicsconsortium.org/aweek/). It has the functions `date2week()` and `week2date()` in which you can set the week start day with `week_start = "Monday"`. This package is easiest if you want "week"-style outputs (e.g. "2020-W12"). Another advantage of **aweek** is that when `date2week()` is applied to a date column, the returned column (week format) is automatically of class Factor and includes levels for all weeks in the time span (this avoids the extra step of `complete()` described above). However, **aweek** does not have the functionality to round dates to other time units such as months, years, etc.  


Another alternative for time series which also works well to show a a "week" format ("2020 W12") is `yearweek()` from the package **tsibble**, as demonstrated in the page on [Time series and outbreak detection].  


<!-- ======================================================= -->
## Converting dates/time zones

When data is present in different time time zones, it can often be important to standardise this data in a unified time zone. This can present a further challenge, as the time zone component of data must be coded manually in most cases.

In R, each *datetime* object has a timezone component. By default, all datetime objects will carry the local time zone for the computer being used - this is generally specific to a *location* rather than a named timezone, as time zones will often change in locations due to daylight savings time. It is not possible to accurately compensate for time zones without a time component of a date, as the event a date column represents cannot be attributed to a specific time, and therefore time shifts measured in hours cannot be reasonably accounted for.

To deal with time zones, there are a number of helper functions in lubridate that can be used to change the time zone of a datetime object from the local time zone to a different time zone. Time zones are set by attributing a valid tz database time zone to the datetime object. A list of these can be found here - if the location you are using data from is not on this list, nearby large cities in the time zone are available and serve the same purpose. 

https://en.wikipedia.org/wiki/List_of_tz_database_time_zones


```{r}
# assign the current time to a column
time_now <- Sys.time()
time_now

# use with_tz() to assign a new timezone to the column, while CHANGING the clock time
time_london_real <- with_tz(time_now, "Europe/London")

# use force_tz() to assign a new timezone to the column, while KEEPING the clock time
time_london_local <- force_tz(time_now, "Europe/London")


# note that as long as the computer that was used to run this code is NOT set to London time,
# there will be a difference in the times 
# (the number of hours difference from the computers time zone to london)
time_london_real - time_london_local

```

This may seem largely abstract, and is often not needed if the user isn't working across time zones.  





<!-- ======================================================= -->
## Lagging and leading calculations  

`lead()` and `lag()` are functions from the **dplyr** package which help find previous (lagged) or subsequent (leading) values in a vector - typically a numeric or date vector. This is useful when doing calculations of change/difference between time units.  


```{r, echo=F}
counts <- import(here("data", "example", "district_weekly_count_data.xlsx")) %>% 
  filter(District == "Nibari") %>% 
  mutate(Date = as.Date(Date),
         week_start = lubridate::floor_date(Date, "week")) %>%
  group_by(week_start) %>% 
  summarize(cases_wk = sum(Cases, na.rm=T)) %>% 
  complete(week_start = seq.Date(min(week_start), max(week_start), by = "week"), fill = list(cases_wk = 0))
```

Let's say you want to calculate the difference in cases between a current week and the previous one. The data are initially provided in weekly counts as shown below.  

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

**When using `lag()` or `lead()` the order of rows in the dataframe is very important! - pay attention to whether your dates/numbers are ascending or descending**  

First, create a new column containing the value of the previous (lagged) week.  

* Control the number of units back/forward with `n = ` (must be a non-negative integer)  
* Use `default = ` to define the value placed in non-existing rows (e.g. the first row for which there is no lagged value). By default this is `NA`.  
* Use `order_by = TRUE` if your the rows are not ordered by your reference column  


```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1))
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Next, create a new column which is the difference between the two cases columns:  

```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1),
         case_diff = cases_wk - cases_prev_wk)
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


You can read more about `lead()` and `lag()` in the documentation [here](https://dplyr.tidyverse.org/reference/lead-lag.html) or by entering `?lag` in your console.  


<!-- ======================================================= -->
## Resources  

**lubridate** [tidyverse page](https://lubridate.tidyverse.org/)  
**lubridate** RStudio [cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)  
R for Data Science page on [dates and times](https://r4ds.had.co.nz/dates-and-times.html)  
[Online tutorial](https://www.statmethods.net/input/dates.html)
[Date formats](https://www.r-bloggers.com/2013/08/date-formats-in-r/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/dates.Rmd-->


# Characters and strings { }  

```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Characters_Strings_1500x500.png"))
```



This page demonstrates use of the **stringr** package to evaluate and handle character values ("strings").  

1. Combine, order, split, arrange - `str_c()`, `str_glue()`, `str_order()`, `str_split()`  
2. Clean and standardise  
    * Adjust length - `str_pad()`, `str_trunc()`, `str_wrap()`  
    * Change case - `str_to_upper()`, `str_to_title()`, `str_to_lower()`, `str_to_sentence()`  
3. Evaluate and extract by position - `str_length()`, `str_sub()`, `word()`  
4. Patterns  
    * Detect and locate - `str_detect()`, `str_subset()`, `str_match()`, `str_extract()`  
    * Modify and replace - `str_sub()`, `str_replace_all()`  
7. Regular expressions ("regex")


For ease of display most examples are shown acting on a short defined character vector, however they can easily be adapted to a column within a data frame.  

This [stringr vignette](
https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html) provided much of the inspiration for this page.  



<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

Install or load the **stringr** and other **tidyverse** packages.  

```{r}
# install/load packages
pacman::p_load(
  stringr,    # many functions for handling strings
  tidyverse,  # for optional data manipulation
  tools)      # alternative for converting to title case

```


### Import data  {.unnumbered}  


In this page we will occassionally reference the cleaned `linelist` of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
## Unite, split, and arrange { }


This section covers:  

* Using `str_c()`, `str_glue()`, and `unite()` to combine strings  
* Using `str_order()` to arrange strings  
* Using `str_split()` and `separate()` to split strings  



<!-- ======================================================= -->
### Combine strings {.unnumbered}

To combine or concatenate multiple strings into one string, we suggest using `str_c` from **stringr**. If you have distinct character values to combine, simply provide them as unique arguments, separated by commas.     

```{r}
str_c("String1", "String2", "String3")
```

The argument `sep = ` inserts a character value between each of the arguments you provided (e.g. inserting a comma, space, or newline `"\n"`)  

```{r}
str_c("String1", "String2", "String3", sep = ", ")
```

The argument `collapse = ` is relevant if you are inputting multiple *vectors* as arguments to `str_c()`. It is used to separate the elements of what would be an *output* vector, such that the output vector only has one long character element.   

The example below shows the combination of two vectors into one (first names and last names). Another similar example might be jurisdictions and their case counts. In this example:  

* The `sep = ` value appears between each first and last name  
* The `collapse = ` value appears between each person  


```{r}
first_names <- c("abdul", "fahruk", "janice") 
last_names  <- c("hussein", "akinleye", "okeke")

# sep displays between the respective input strings, while collapse displays between the elements produced
str_c(first_names, last_names, sep = " ", collapse = ";  ")
```

Note: Depending on your desired display context, when printing such a combined string with newlines, you may need to wrap the whole phrase in `cat()` for the newlines to print properly:  

```{r}
# For newlines to print correctly, the phrase may need to be wrapped in cat()
cat(str_c(first_names, last_names, sep = " ", collapse = ";\n"))
```



<!-- ======================================================= -->
### Dynamic strings {.unnumbered}

Use `str_glue()` to insert dynamic R code into a string. This is a very useful function for creating dynamic plot captions, as demonstrated below.  

* All content goes between double quotation marks `str_glue("")`  
* Any dynamic code or references to pre-defined values are placed within curly brackets `{}` within the double quotation marks. There can be many curly brackets in the same `str_glue()` command.  
* To display character quotes '', use *single* quotes within the surrounding double quotes (e.g. when providing date format - see example below)  
* Tip: You can use `\n` to force a new line  
* Tip: You use `format()` to adjust date display, and use `Sys.Date()` to display the current date  

A simple example, of a dynamic plot caption:  

```{r}
str_glue("Data include {nrow(linelist)} cases and are current to {format(Sys.Date(), '%d %b %Y')}.")
```

An alternative format is to use placeholders within the brackets and define the code in separate arguments at the end of the `str_glue()` function, as below. This can improve code readability if the text is long.

```{r}
str_glue("Linelist as of {current_date}.\nLast case hospitalized on {last_hospital}.\n{n_missing_onset} cases are missing date of onset and not shown",
         current_date = format(Sys.Date(), '%d %b %Y'),
         last_hospital = format(as.Date(max(linelist$date_hospitalisation, na.rm=T)), '%d %b %Y'),
         n_missing_onset = nrow(linelist %>% filter(is.na(date_onset)))
         )

```


**Pulling from a data frame**  

Sometimes, it is useful to pull data from a data frame and have it pasted together in sequence. Below is an example data frame. We will use it to to make a summary statement about the jurisdictions and the new and total case counts.  

```{r}
# make case data frame
case_table <- data.frame(
  zone        = c("Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5"),
  new_cases   = c(3, 0, 7, 0, 15),
  total_cases = c(40, 4, 25, 10, 103)
  )
```

```{r, echo=F}
DT::datatable(case_table, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Use `str_glue_data()`, which is specially made for taking data from data frame rows:  

```{r}
case_table %>% 
  str_glue_data("{zone}: {new_cases} ({total_cases} total cases)")
```


**Combine strings across rows**  

If you are trying to "roll-up" values in a data frame column, e.g. combine values from multiple rows into just one row by pasting them together with a separator, see the section of the [De-duplication] page on ["rolling-up" values](#str_rollup).  

**Data frame to one line**  

You can make the statement appear in one line using `str_c()` (specifying the data frame and column names), and providing `sep = ` and `collapse = ` arguments.  

```{r}
str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  ")
```

You could add the pre-fix text "New Cases:" to the beginning of the statement by wrapping with a separate `str_c()` (if "New Cases:" was within the original `str_c()` it would appear multiple times).  

```{r}
str_c("New Cases: ", str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  "))
```





### Unite columns  {#str_unite .unnumbered}

Within a data frame, bringing together character values from multiple columns can be achieved with `unite()` from **tidyr**. This is the opposite of `separate()`.  

Provide the name of the new united column. Then provide the names of the columns you wish to unite.  

* By default, the separator used in the united column is underscore `_`, but this can be changed with the `sep = ` argument.  
* `remove = ` removes the input columns from the data frame (TRUE by default)  
* `na.rm = ` removes missing values while uniting (FALSE by default)  

Below, we define a mini-data frame to demonstrate with:  

```{r, message = F, warning=F}
df <- data.frame(
  case_ID = c(1:6),
  symptoms  = c("jaundice, fever, chills",     # patient 1
                "chills, aches, pains",        # patient 2 
                "fever",                       # patient 3
                "vomiting, diarrhoea",         # patient 4
                "bleeding from gums, fever",   # patient 5
                "rapid pulse, headache"),      # patient 6
  outcome = c("Recover", "Death", "Death", "Recover", "Recover", "Recover"))
```


```{r}
df_split <- separate(df, symptoms, into = c("sym_1", "sym_2", "sym_3"), extra = "merge")
```

Here is the example data frame:  

```{r, echo=F}
DT::datatable(df_split, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below, we unite the three symptom columns:  

```{r}
df_split %>% 
  unite(
    col = "all_symptoms",         # name of the new united column
    c("sym_1", "sym_2", "sym_3"), # columns to unite
    sep = ", ",                   # separator to use in united column
    remove = TRUE,                # if TRUE, removes input cols from the data frame
    na.rm = TRUE                  # if TRUE, missing values are removed before uniting
  )
```







<!-- ======================================================= -->
### Split {.unnumbered}  

To split a string based on a pattern, use `str_split()`. It evaluates the string(s) and returns a `list` of character vectors consisting of the newly-split values.

The simple example below evaluates one string and splits it into three. By default it returns an object of class `list` with one element (a character vector) for each string initially provided. If `simplify = TRUE` it returns a character matrix.  

In this example, one string is provided, and the function returns a list with one element - a character vector with three values.  

```{r}
str_split(string = "jaundice, fever, chills",
          pattern = ",")
```

If the output is saved, you can then access the nth split value with bracket syntax. To access a specific value you can use syntax like this: `the_returned_object[[1]][2]`, which would access the second value from the first evaluated string ("fever"). See the [R basics] page for more detail on accessing elements.    

```{r}
pt1_symptoms <- str_split("jaundice, fever, chills", ",")

pt1_symptoms[[1]][2]  # extracts 2nd value from 1st (and only) element of the list
```

If multiple strings are provided by `str_split()`, there will be more than one element in the returned list.  

```{r}
symptoms <- c("jaundice, fever, chills",     # patient 1
              "chills, aches, pains",        # patient 2 
              "fever",                       # patient 3
              "vomiting, diarrhoea",         # patient 4
              "bleeding from gums, fever",   # patient 5
              "rapid pulse, headache")       # patient 6

str_split(symptoms, ",")                     # split each patient's symptoms
```


To return a "character matrix" instead, which may be useful if creating data frame columns, set the argument `simplify = TRUE` as shown below:  

```{r}
str_split(symptoms, ",", simplify = TRUE)
```

You can also adjust the number of splits to create with the `n = ` argument. For example, this restricts the number of splits to 2. Any further commas remain within the second values. 

```{r}
str_split(symptoms, ",", simplify = TRUE, n = 2)
```

*Note - the same outputs can be achieved with `str_split_fixed()`, in which you do not give the `simplify` argument, but must instead designate the number of columns (`n`).* 

```{r, eval=F}
str_split_fixed(symptoms, ",", n = 2)
```




### Split columns {.unnumbered}  

If you are trying to split data frame column, it is best to use the `separate()` function from **dplyr**. It is used to split one character column into other columns.  

Let's say we have a simple data frame `df` (defined and united in the [unite section](#str_unite)) containing a `case_ID` column, one character column with many symptoms, and one outcome column. Our goal is to separate the `symptoms` column into many columns - each one containing one symptom.  


```{r, echo=F}
DT::datatable(df, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Assuming the data are piped into `separate()`, first provide the column to be separated. Then provide `into = ` as a vector `c( )` containing the *new* columns names, as shown below.  

* `sep = ` the separator, can be a character, or a number (interpreted as the character position to split at) 
* `remove = ` FALSE by default, removes the input column  
* `convert = ` FALSE by default, will cause string "NA"s to become `NA`  
* `extra = ` this controls what happens if there are more values created by the separation than new columns named.  
     * `extra = "warn"` means you will see a warning but it will drop excess values (**the default**)  
     * `extra = "drop"` means the excess values will be dropped with no warning  
     * **`extra = "merge"` will only split to the number of new columns listed in `into` - *this setting will preserve all your data***  


An example with `extra = "merge"` is below - no data is lost. Two new columns are defined but any third symptoms are left in the second new column:  

```{r}
# third symptoms combined into second new column
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",", extra = "merge")
```

When the default `extra = "drop"` is used below, a warning is given but the third symptoms are lost:  

```{r}
# third symptoms are lost
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",")
```


<span style="color: orange;">**_CAUTION:_** If you do not provide enough `into` values for the new columns, your data may be truncated.</span>  






<!-- ======================================================= -->
### Arrange alphabetically {.unnumbered} 

Several strings can be sorted by alphabetical order. `str_order()` returns the order, while `str_sort()` returns the strings in that order.  

```{r}
# strings
health_zones <- c("Alba", "Takota", "Delta")

# return the alphabetical order
str_order(health_zones)

# return the strings in alphabetical order
str_sort(health_zones)
```

To use a different alphabet, add the argument `locale = `. See the full list of locales by entering `stringi::stri_locale_list()` in the R console.  





<!-- ======================================================= -->
### base R functions {.unnumbered}

It is common to see **base** R functions `paste()` and `paste0()`, which concatenate vectors after converting all parts to character. They act similarly to `str_c()` but the syntax is arguably more complicated - in the parentheses each part is separated by a comma. The parts are either character text (in quotes) or pre-defined code objects (no quotes). For example:

```{r}
n_beds <- 10
n_masks <- 20

paste0("Regional hospital needs ", n_beds, " beds and ", n_masks, " masks.")
```

`sep = ` and `collapse = ` arguments can be specified. `paste()` is simply `paste0()` with a default `sep = " "` (one space).  






## Clean and standardise  


<!-- ======================================================= -->
### Change case {.unnumbered}

Often one must alter the case/capitalization of a string value, for example names of jursidictions. Use `str_to_upper()`, `str_to_lower()`, and `str_to_title()`, from **stringr**, as shown below:  

```{r}
str_to_upper("California")

str_to_lower("California")
```

Using *base** R, the above can also be achieved with `toupper()`, `tolower()`.  


**Title case**  

Transforming the string so each word is capitalized can be achieved with `str_to_title()`:  

```{r}
str_to_title("go to the US state of california ")
```

Use `toTitleCase()` from the **tools** package to achieve more nuanced capitalization (words like "to", "the", and "of" are not capitalized).  

```{r}
tools::toTitleCase("This is the US state of california")
```

You can also use `str_to_sentence()`, which capitalizes only the first letter of the string.

```{r}
str_to_sentence("the patient must be transported")
```



### Pad length  {#str_pad .unnumbered}

Use `str_pad()` to add characters to a string, to a minimum length. By default spaces are added, but you can also pad with other characters using the `pad = ` argument.  


```{r}
# ICD codes of differing length
ICD_codes <- c("R10.13",
               "R10.819",
               "R17")

# ICD codes padded to 7 characters on the right side
str_pad(ICD_codes, 7, "right")

# Pad with periods instead of spaces
str_pad(ICD_codes, 7, "right", pad = ".")
```

For example, to pad numbers with leading zeros (such as for hours or minutes), you can pad the number to minimum length of 2 with `pad = "0"`.

```{r}
# Add leading zeros to two digits (e.g. for times minutes/hours)
str_pad("4", 2, pad = "0") 

# example using a numeric column named "hours"
# hours <- str_pad(hours, 2, pad = "0")
```


### Truncate {.unnumbered} 

`str_trunc()` sets a maximum length for each string. If a string exceeds this length, it is truncated (shortened) and an ellipsis (...) is included to indicate that the string was previously longer. Note that the ellipsis *is* counted in the length. The ellipsis characters can be changed with the argument `ellipsis = `.  The optional `side = ` argument specifies which where the ellipsis will appear within the truncated string ("left", "right", or "center").  

```{r}
original <- "Symptom onset on 4/3/2020 with vomiting"
str_trunc(original, 10, "center")
```


### Standardize length {.unnumbered}

Use `str_trunc()` to set a maximum length, and then use `str_pad()` to expand the very short strings to that truncated length. In the example below, 6 is set as the maximum length (one value is truncated), and then one very short value is padded to achieve length of 6.    

```{r}
# ICD codes of differing length
ICD_codes   <- c("R10.13",
                 "R10.819",
                 "R17")

# truncate to maximum length of 6
ICD_codes_2 <- str_trunc(ICD_codes, 6)
ICD_codes_2

# expand to minimum length of 6
ICD_codes_3 <- str_pad(ICD_codes_2, 6, "right")
ICD_codes_3
```


### Remove leading/trailing whitespace {.unnumbered}  

Use `str_trim()` to remove spaces, newlines (`\n`) or tabs (`\t`) on sides of a string input. Add `"right"` `"left"`, or `"both"` to the command to specify which side to trim (e.g. `str_trim(x, "right")`. 

```{r}
# ID numbers with excess spaces on right
IDs <- c("provA_1852  ", # two excess spaces
         "provA_2345",   # zero excess spaces
         "provA_9460 ")  # one excess space

# IDs trimmed to remove excess spaces on right side only
str_trim(IDs)
```


### Remove repeated whitespace within {.unnumbered}  

Use `str_squish()` to remove repeated spaces that appear *inside* a string. For example, to convert double spaces into single spaces. It also removes spaces, newlines, or tabs on the outside of the string like `str_trim()`.  


```{r}
# original contains excess spaces within string
str_squish("  Pt requires   IV saline\n") 
```

Enter `?str_trim`, `?str_pad` in your R console to see further details.  


### Wrap into paragraphs {.unnumbered}  

Use `str_wrap()` to wrap a long unstructured text into a structured paragraph with fixed line length. Provide the ideal character length for each line, and it applies an algorithm to insert newlines (`\n`) within the paragraph, as seen in the example below.   

```{r}
pt_course <- "Symptom onset 1/4/2020 vomiting chills fever. Pt saw traditional healer in home village on 2/4/2020. On 5/4/2020 pt symptoms worsened and was admitted to Lumta clinic. Sample was taken and pt was transported to regional hospital on 6/4/2020. Pt died at regional hospital on 7/4/2020."

str_wrap(pt_course, 40)
```

The **base** function `cat()` can be wrapped around the above command in order to print the output, displaying the new lines added.  

```{r}
cat(str_wrap(pt_course, 40))
```












<!-- ======================================================= -->
## Handle by position { }


### Extract by character position {.unnumbered}  

Use `str_sub()` to return only a part of a string. The function takes three main arguments:  

1) the character vector(s)  
2) start position  
3) end position  

A few notes on position numbers:  

* If a position number is positive, the position is counted starting from the left end of the string.  
* If a position number is negative, it is counted starting from the right end of the string.  
* Position numbers are inclusive.  
* Positions extending beyond the string will be truncated (removed).  

Below are some examples applied to the string "pneumonia":  

```{r}
# start and end third from left (3rd letter from left)
str_sub("pneumonia", 3, 3)

# 0 is not present
str_sub("pneumonia", 0, 0)

# 6th from left, to the 1st from right
str_sub("pneumonia", 6, -1)

# 5th from right, to the 2nd from right
str_sub("pneumonia", -5, -2)

# 4th from left to a position outside the string
str_sub("pneumonia", 4, 15)
```



### Extract by word position {.unnumbered} 

To extract the nth 'word', use `word()`, also from **stringr**. Provide the string(s), then the first word position to extract, and the last word position to extract.  

By default, the separator between 'words' is assumed to be a space, unless otherwise indicated with `sep = ` (e.g. `sep = "_"` when words are separated by underscores.  


```{r}
# strings to evaluate
chief_complaints <- c("I just got out of the hospital 2 days ago, but still can barely breathe.",
                      "My stomach hurts",
                      "Severe ear pain")

# extract 1st to 3rd words of each string
word(chief_complaints, start = 1, end = 3, sep = " ")
```


### Replace by character position {.unnumbered} 

`str_sub()` paired with the assignment operator (`<-`) can be used to modify a part of a string: 

```{r}
word <- "pneumonia"

# convert the third and fourth characters to X 
str_sub(word, 3, 4) <- "XX"

# print
word
```

An example applied to multiple strings (e.g. a column). Note the expansion in length of "HIV".  

```{r}
words <- c("pneumonia", "tubercolosis", "HIV")

# convert the third and fourth characters to X 
str_sub(words, 3, 4) <- "XX"

words
```



### Evaluate length  {.unnumbered}


```{r}
str_length("abc")
```

Alternatively, use `nchar()` from **base** R







<!-- ======================================================= -->
## Patterns { }

Many **stringr** functions work to detect, locate, extract, match, replace, and split based on a specified *pattern*.  



<!-- ======================================================= -->
### Detect a pattern {.unnumbered}

Use `str_detect()` as below to detect presence/absence of a pattern within a string. First provide the string or vector to search in (`string = `), and then the pattern to look for (`pattern = `). Note that by default the search *is case sensitive*!

```{r}
str_detect(string = "primary school teacher", pattern = "teach")
```

The argument `negate = ` can be included and set to `TRUE` if you want to know if the pattern is NOT present.  
 
```{r}
str_detect(string = "primary school teacher", pattern = "teach", negate = TRUE)
```

To ignore case/capitalization, wrap the pattern within `regex()`, and *within* `regex()` add the argument `ignore_case = TRUE` (or `T` as shorthand).  

```{r}
str_detect(string = "Teacher", pattern = regex("teach", ignore_case = T))
```

When `str_detect()` is applied to a character vector or a data frame column, it will return TRUE or FALSE for each of the values. 

```{r}
# a vector/column of occupations 
occupations <- c("field laborer",
                 "university professor",
                 "primary school teacher & tutor",
                 "tutor",
                 "nurse at regional hospital",
                 "lineworker at Amberdeen Fish Factory",
                 "physican",
                 "cardiologist",
                 "office worker",
                 "food service")

# Detect presence of pattern "teach" in each string - output is vector of TRUE/FALSE
str_detect(occupations, "teach")
```

If you need to count the `TRUE`s, simply `sum()` the output. This counts the number `TRUE`.  

```{r}
sum(str_detect(occupations, "teach"))
```

To search inclusive of multiple terms, include them separated by OR bars (`|`) within the `pattern = ` argument, as shown below:  

```{r}
sum(str_detect(string = occupations, pattern = "teach|professor|tutor"))
```

If you need to build a long list of search terms, you can combine them using `str_c()` and `sep = |`, then define this is a character object, and then reference the vector later more succinctly. The example below includes possible occupation search terms for front-line medical providers.     

```{r}
# search terms
occupation_med_frontline <- str_c("medical", "medicine", "hcw", "healthcare", "home care", "home health",
                                "surgeon", "doctor", "doc", "physician", "surgery", "peds", "pediatrician",
                               "intensivist", "cardiologist", "coroner", "nurse", "nursing", "rn", "lpn",
                               "cna", "pa", "physician assistant", "mental health",
                               "emergency department technician", "resp therapist", "respiratory",
                                "phlebotomist", "pharmacy", "pharmacist", "hospital", "snf", "rehabilitation",
                               "rehab", "activity", "elderly", "subacute", "sub acute",
                                "clinic", "post acute", "therapist", "extended care",
                                "dental", "dential", "dentist", sep = "|")

occupation_med_frontline
```

This command returns the number of occupations which contain any one of the search terms for front-line medical providers (`occupation_med_frontline`):  

```{r}
sum(str_detect(string = occupations, pattern = occupation_med_frontline))
```



**Base R string search functions**  

The **base** function `grepl()` works similarly to `str_detect()`, in that it searches for matches to a pattern and returns a logical vector. The basic syntax is `grepl(pattern, strings_to_search, ignore.case = FALSE, ...)`. One advantage is that the `ignore.case` argument is easier to write (there is no need to involve the `regex()` function).  

Likewise, the **base** functions `sub()` and `gsub()` act similarly to `str_replace()`. Their basic syntax is: `gsub(pattern, replacement, strings_to_search, ignore.case = FALSE)`. `sub()` will replace the first instance of the pattern, whereas `gsub()` will replace all instances of the pattern.  


#### Convert commas to periods {.unnumbered}  

Here is an example of using `gsub()` to convert commas to periods in a vector of numbers. This could be useful if your data come from parts of the world other than the United States or Great Britain.  

The inner `gsub()` which acts first on `lengths` is converting any periods to no space "". The period character  "." has to be "escaped" with two slashes to actually signify a period, because "." in regex means "any character". Then, the result (with only commas) is passed to the outer `gsub()` in which commas are replaced by periods.   

```{r, eval=F}
lengths <- c("2.454,56", "1,2", "6.096,5")

as.numeric(gsub(pattern = ",",                # find commas     
                replacement = ".",            # replace with periods
                x = gsub("\\.", "", lengths)  # vector with other periods removed (periods escaped)
                )
           )                                  # convert outcome to numeric
```





### Replace all {.unnumbered}  

Use `str_replace_all()` as a "find and replace" tool. First, provide the strings to be evaluated to `string = `, then the pattern to be replaced to `pattern = `, and then the replacement value to `replacement = `. The example below replaces all instances of "dead" with "deceased". Note, this IS case sensitive.  

```{r}
outcome <- c("Karl: dead",
            "Samantha: dead",
            "Marco: not dead")

str_replace_all(string = outcome, pattern = "dead", replacement = "deceased")
```

Notes:  

* To replace a pattern with `NA`, use `str_replace_na()`.  
* The function `str_replace()` replaces only the first instance of the pattern within each evaluated string.  





<!-- ======================================================= -->
### Detect within logic {.unnumbered}


**Within `case_when()`**  

`str_detect()` is often used within `case_when()` (from **dplyr**). Let's say `occupations` is a column in the linelist. The `mutate()` below creates a new column called `is_educator` by using conditional logic via `case_when()`. See the page on data cleaning to learn more about `case_when()`.  


```{r, eval=F}
df <- df %>% 
  mutate(is_educator = case_when(
    # term search within occupation, not case sensitive
    str_detect(occupations,
               regex("teach|prof|tutor|university",
                     ignore_case = TRUE))              ~ "Educator",
    # all others
    TRUE                                               ~ "Not an educator"))
```

As a reminder, it may be important to add exclusion criteria to the conditional logic (`negate = F`):  

```{r, eval=F}
df <- df %>% 
  # value in new column is_educator is based on conditional logic
  mutate(is_educator = case_when(
    
    # occupation column must meet 2 criteria to be assigned "Educator":
    # it must have a search term AND NOT any exclusion term
    
    # Must have a search term
    str_detect(occupations,
               regex("teach|prof|tutor|university", ignore_case = T)) &              
    
    # AND must NOT have an exclusion term
    str_detect(occupations,
               regex("admin", ignore_case = T),
               negate = TRUE                        ~ "Educator"
    
    # All rows not meeting above criteria
    TRUE                                            ~ "Not an educator"))
```





<!-- ======================================================= -->
### Locate pattern position {.unnumbered}  

To locate the *first* position of a pattern, use `str_locate()`. It outputs a start and end position.   

```{r}
str_locate("I wish", "sh")
```

Like other `str` functions, there is an "_all" version (`str_locate_all()`) which will return the positions of *all* instances of the pattern within each string. This outputs as a `list`.  

```{r}
phrases <- c("I wish", "I hope", "he hopes", "He hopes")

str_locate(phrases, "h" )     # position of *first* instance of the pattern
str_locate_all(phrases, "h" ) # position of *every* instance of the pattern
```





<!-- ======================================================= -->
### Extract a match {.unnumbered}  

`str_extract_all()` returns the matching patterns themselves, which is most useful when you have offered several patterns via "OR" conditions. For example, looking in the string vector of occupations (see previous tab) for *either* "teach", "prof", or "tutor".

`str_extract_all()` returns a `list` which contains *all matches* for each evaluated string. See below how occupation 3 has two pattern matches within it.  

```{r}
str_extract_all(occupations, "teach|prof|tutor")
```


`str_extract()` extracts *only the first match* in each evaluated string, producing a character vector with one element for each evaluated string. It returns `NA` where there was no match. The `NA`s can be removed by wrapping the returned vector with `na.exclude()`. Note how the second of occupation 3's matches is not shown.  

```{r}
str_extract(occupations, "teach|prof|tutor")
```

<!-- ======================================================= -->
### Subset and count {.unnumbered}  

Aligned functions include `str_subset()` and `str_count()`.  

`str_subset()` returns the actual values which contained the pattern: 

```{r}
str_subset(occupations, "teach|prof|tutor")
```

`str_count()` returns a vector of numbers: the **number of times** a search term appears in each evaluated value.  

```{r}
str_count(occupations, regex("teach|prof|tutor", ignore_case = TRUE))
```












<!-- ======================================================= -->
### Regex groups {.unnumbered}

UNDER CONSTRUCTION






<!-- ======================================================= -->
## Special characters  

**Backslash `\` as escape**  

The backslash `\` is used to "escape" the meaning of the next character. This way, a backslash can be used to have a quote mark display *within* other quote marks (`\"`) - the middle quote mark will not "break" the surrounding quote marks.  

Note - thus, if you want to *display* a backslash, you must escape it's meaning with *another* backslash. So you must write two backslashes `\\` to display one.  

**Special characters**  

Special character | Represents  
----------------- | --------------------------------------------------------------    
`"\\"` | backslash  
`"\n"` | a new line (newline)   
`"\""` | double-quote *within* double quotes  
`'\''` | single-quote *within* single quotes  
`"\`"` | grave accent  
`"\r"` | carriage return  
`"\t"` | tab  
`"\v"` | vertical tab 
`"\b"` | backspace  


Run `?"'"` in the R Console to display a complete list of these special characters (it will appear in the RStudio Help pane). 



<!-- ======================================================= -->
## Regular expressions (regex) 


<!-- ======================================================= -->
## Regex and special characters { } 

Regular expressions, or "regex", is a concise language for describing patterns in strings. If you are not familiar with it, a regular expression can look like an alien language. Here we try to de-mystify this language a little bit.  

*Much of this section is adapted from [this tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432) and [this cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)*. We selectively adapt here knowing that this handbook might be viewed by people without internet access to view the other tutorials.    


A regular expression is often applied to extract specific patterns from "unstructured" text - for example medical notes, chief complaints, patient history, or other free text columns in a data frame  

There are four basic tools one can use to create a basic regular expression:  

1) Character sets  
2) Meta characters  
3) Quantifiers  
4) Groups  


**Character sets**  

Character sets, are a way of expressing listing options for a character match, within brackets. So any a match will be triggered if any of the characters within the brackets are found in the string. For example, to look for vowels one could use this character set: "[aeiou]". Some other common character sets are:  

Character set | Matches for  
----------------- | --------------------------------------------------------------    
`"[A-Z]"` | any single capital letter  
`"[a-z]"` | any single lowercase letter  
`"[0-9]"` | any digit  
`[:alnum:]` | any alphanumeric character  
`[:digit:]` | any numeric digit  
`[:alpha:]` | any letter (upper or lowercase)  
`[:upper:]` | any uppercase letter  
`[:lower:]` | any lowercase letter  


Character sets can be combined within one bracket (no spaces!), such as `"[A-Za-z]"` (any upper or lowercase letter), or another example `"[t-z0-5]"` (lowercase t through z OR number 0 through 5).  



**Meta characters**  

Meta characters are shorthand for character sets. Some of the important ones are listed below:  

Meta character | Represents  
----------------- | --------------------------------------------------------------    
`"\\s"` | a single space  
`"\\w"` | any single alphanumeric character (A-Z, a-z, or 0-9)  
`"\\d"` | any single numeric digit (0-9)  


**Quantifiers**  

Typically you do not want to search for a match on only one character. Quantifiers allow you to designate the length of letters/numbers to allow for the match.  

Quantifiers are numbers written within curly brackets `{ }` *after* the character they are quantifying, for example,  

* `"A{2}"` will return instances of **two** capital A letters.  
* `"A{2,4}"` will return instances of **between two and four** capital A letters *(do not put spaces!)*.  
* `"A{2,}"` will return instances of **two or more** capital A letters.  
* `"A+"` will return instances of **one or more** capital A letters (group extended until a different character is encountered).  
* Precede with an `*` asterisk to return **zero or more** matches (useful if you are not sure the pattern is present)  


Using the `+` plus symbol as a quantifier, the match will occur until a different character is encountered. For example, this expression will return all *words* (alpha characters: `"[A-Za-z]+"`  


```{r}
# test string for quantifiers
test <- "A-AA-AAA-AAAA"
```

When a quantifier of {2} is used, only pairs of consecutive A's are returned. Two pairs are identified within `AAAA`.  

```{r}
str_extract_all(test, "A{2}")
```

When a quantifier of {2,4} is used, groups of consecutive A's that are two to four in length are returned.  

```{r}
str_extract_all(test, "A{2,4}")
```

With the quantifier `+`, groups of **one or more** are returned:  

```{r}
str_extract_all(test, "A+")
```

**Relative position**  

These express requirements for what precedes or follows a pattern. For example, to extract sentences, "two numbers that are followed by a period" (`""`).  (?<=\\.)\\s(?=[A-Z]) 

```{r}
str_extract_all(test, "")
```

Position statement | Matches to  
----------------- | --------------------------------------------------------------    
`"(?<=b)a"` | "a" that **is preceded** by a "b"  
`"(?<!b)a"` | "a" that **is NOT preceded** by a "b"  
`"a(?=b)"` | "a" that **is followed** by a "b"  
`"a(?!b)"` | "a" that **is NOT followed** by a "b"  





**Groups**  

Capturing groups in your regular expression is a way to have a more organized output upon extraction.  




**Regex examples**  

Below is a free text for the examples. We will try to extract useful information from it using a regular expression search term.  

```{r}
pt_note <- "Patient arrived at Broward Hospital emergency ward at 18:00 on 6/12/2005. Patient presented with radiating abdominal pain from LR quadrant. Patient skin was pale, cool, and clammy. Patient temperature was 99.8 degrees farinheit. Patient pulse rate was 100 bpm and thready. Respiratory rate was 29 per minute."
```

This expression matches to all words (any character until hitting non-character such as a space):  

```{r}
str_extract_all(pt_note, "[A-Za-z]+")
```

The expression `"[0-9]{1,2}"` matches to consecutive numbers that are 1 or 2 digits in length. It could also be written `"\\d{1,2}"`, or `"[:digit:]{1,2}"`.  

```{r}
str_extract_all(pt_note, "[0-9]{1,2}")
```

<!-- This expression will extract all sentences (assuming first letter is capitalized, and the sentence ends with a period). The pattern reads in English as: "A capital letter followed by some lowercase letters, a space, some letters, a space,     -->

<!-- ```{r} -->
<!-- str_extract_all(pt_note, "[A-Z][a-z]+\\s\\w+\\s\\d{1,2}\\s\\w+\\s*\\w*") -->
<!-- ``` -->


You can view a useful list of regex expressions and tips on page 2 of [this cheatsheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)  

Also see this [tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432).  




<!-- ======================================================= -->
## Resources { }

A reference sheet for **stringr** functions can be found [here](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)


A vignette on **stringr** can be found [here](
https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/characters_strings.Rmd-->

# Factors {}


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Factors_1500x500.png"))
```

In R, *factors* are a class of data that allow for ordered categories with a fixed set of acceptable values.  

Typically, you would convert a column from character or numeric class to a factor if you want to set an intrinsic order to the values ("*levels*") so they can be displayed non-alphabetically in plots and tables. Another common use of factors is to standardise the legends of plots so they do not fluctuate if certain values are temporarily absent from the data.   

This page demonstrates use of functions from the package **forcats** (a short name for "**For** **cat**egorical variables") and some **base** R functions. We also touch upon the use of **lubridate** and **aweek** for special factor cases related to epidemiological weeks.  

A complete list of **forcats** functions can be found online [here](https://forcats.tidyverse.org/reference/index.html). Below we demonstrate some of the most common ones.  


<!-- ======================================================= -->
## Preparation  

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # import/export
  here,          # filepaths
  lubridate,     # working with dates
  forcats,       # factors
  aweek,         # create epiweeks with automatic factor levels
  janitor,       # tables
  tidyverse      # data mgmt and viz
  )
```



### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# import your dataset
linelist <- import("linelist_cleaned.rds")
```


### New categorical variable {#fct_newcat .unnumbered}  

For demonstration in this page we will use a common scenario - the creation of a new categorical variable.

Note that if you convert a numeric column to class factor, you will not be able to calculate numeric statistics on it.  

#### Create column {.unnumbered}  

We use the existing column `days_onset_hosp` (days from symptom onset to hospital admission) and create a new column `delay_cat` by classifying each row into one of several categories. We do this with the **dplyr** function `case_when()`, which sequentially applies logical criteria (right-side) to each row and returns the corresponding left-side value for the new column `delay_cat`. Read more about `case_when()` in [Cleaning data and core functions].  

```{r}
linelist <- linelist %>% 
  mutate(delay_cat = case_when(
    # criteria                                   # new value if TRUE
    days_onset_hosp < 2                        ~ "<2 days",
    days_onset_hosp >= 2 & days_onset_hosp < 5 ~ "2-5 days",
    days_onset_hosp >= 5                       ~ ">5 days",
    is.na(days_onset_hosp)                     ~ NA_character_,
    TRUE                                       ~ "Check me"))  
```


#### Default value order {.unnumbered}  

As created with `case_when()`, the new column `delay_cat` is a categorical column of class Character - *not* yet a factor. Thus, in a frequency table, we see that the unique values appear in a default alpha-numeric order - an order that does not make much intuitive sense:  

```{r}
table(linelist$delay_cat, useNA = "always")
```

Likewise, if we make a bar plot, the values also appear in this order on the x-axis (see the [ggplot basics] page for more on **ggplot2** - the most common visualization package in R).  

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```



## Convert to factor  

To convert a character or numeric column to class *factor*, you can use any function from the **forcats** package (many are detailed [below](#fct_adjust)). They will convert to class factor and then also perform or allow certain ordering of the levels - for example using `fct_relevel()` lets you manually specify the level order. The function `as_factor()` simply converts the class without any further capabilities.  

The **base** R function `factor()` converts a column to factor and allows you to manually specify the order of the levels, as a character vector to its `levels = ` argument.  

Below we use `mutate()` and `fct_relevel()` to convert the column `delay_cat` from class character to class factor. The column `delay_cat` is created in the [Preparation](#fct_newcat) section above. 

```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat))
```

*The unique "values" in this column are now considered "levels" of the factor.*  The levels have an *order*, which can be printed with the **base** R function `levels()`, or alternatively viewed in a count table via `table()` from **base** R or `tabyl()` from **janitor**. By default, the order of the levels will be alpha-numeric, as before. Note that `NA` is not a factor level.  

```{r}
levels(linelist$delay_cat)
```

The function `fct_relevel()` has the additional utility of allowing you to manually specify the level order. Simply write the level values in order, in quotation marks, separated by commas, as shown below. Note that the spelling must exactly match the values. If you want to create levels that do not exist in the data, use [`fct_expand()` instead](#fct_add)).  

```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", "2-5 days", ">5 days"))
```

We can now see that the levels are ordered, as specified in the previous command, in a sensible order.  

```{r}
levels(linelist$delay_cat)
```

Now the plot order makes more intuitive sense as well.  

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```


## Add or drop levels  

### Add {#fct_add .unnumbered}
If you need to add levels to a factor, you can do this with `fct_expand()`. Just write the column name followed by the new levels (separated by commas). By tabulating the values, we can see the new levels and the zero counts. You can use `table()` from **base** R, or `tabyl()` from **janitor**:  

```{r}
linelist %>% 
  mutate(delay_cat = fct_expand(delay_cat, "Not admitted to hospital", "Transfer to other jurisdiction")) %>% 
  tabyl(delay_cat)   # print table
```


Note: there is a special **forcats** function to easily add missing values (`NA`) as a level. See the section on [Missing values](#fct_missing) below.  


### Drop {.unnumbered}  

If you use `fct_drop()`, the "unused" levels with zero counts will be dropped from the set of levels. The levels we added above ("Not admitted to a hospital") exists as a level but no rows actually have those values. So they will be dropped by applying `fct_drop()` to our factor column:  

```{r}
linelist %>% 
  mutate(delay_cat = fct_drop(delay_cat)) %>% 
  tabyl(delay_cat)
```




## Adjust level order {#fct_adjust} 

The package **forcats** offers useful functions to easily adjust the order of a factor's levels (after a column been defined as class factor): 

These functions can be applied to a factor column in two contexts:  

1) To the column in the data frame, as usual, so the transformation is available for any subsequent use of the data  
2) *Inside of a plot*, so that the change is applied only within the plot  



### Manually {.unnumbered} 

This function is used to manually order the factor levels. If used on a non-factor column, the column will first be converted to class factor.  

Within the parentheses first provide the factor column name, then provide either:  

* All the levels in the desired order (as a character vector `c()`), or  
* One level and it's corrected placement using the `after = ` argument  

Here is an example of redefining the column `delay_cat` (which is already class Factor) and specifying all the desired order of levels.  

```{r}
# re-define level order
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days")))
```

If you only want to move one level, you can specify it to `fct_relevel()` alone and give a number to the `after = ` argument to indicate where in the order it should be. For example, the command below shifts "<2 days" to the second position: 

```{r, eval=F}
# re-define level order
linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 1)) %>% 
  tabyl(delay_cat)
```




### Within a plot {.unnumbered}  

The **forcats** commands can be used to set the level order in the data frame, or only within a plot. By using the command to "wrap around" the column name *within* the `ggplot()` plotting command, you can reverse/relevel/etc. the transformation will only apply within that plot.  

Below, two plots are created with `ggplot()` (see the [ggplot basics] page). In the first, the `delay_cat` column is mapped to the x-axis of the plot, with it's default level order as in the data `linelist`. In the second example it is wrapped within `fct_relevel()` and the order is changed in the plot.  

```{r, echo =F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("2-5 days", "<2 days", ">5 days")))

```



```{r, warning=F, message=F, out.width = c('50%', '50%'), fig.show='hold'}
# Alpha-numeric default order - no adjustment within ggplot
ggplot(data = linelist)+
    geom_bar(mapping = aes(x = delay_cat))

# Factor level order adjusted within ggplot
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days"))))
```

Note that default x-axis title is now quite complicated - you can overwrite this title with the **ggplot2** `labs()` argument.  




### Reverse {.unnumbered}  

It is rather common that you want to reverse the level order. Simply wrap the factor with `fct_rev()`.  

Note that if you want to reverse *only* a plot legend but not the actual factor levels, you can do that with `guides()` (see [ggplot tips]).  




### By frequency {.unnumbered}  

To order by frequency that the value appears in the data, use `fct_infreq()`. Any missing values (`NA`) will automatically be included at the end, unless they are converted to an explicit level (see [this section](#fct_missing)). You can reverse the order by further wrapping with `fct_rev()`.  

This function can be used within a `ggplot()`, as shown below.  

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# ordered by frequency
ggplot(data = linelist, aes(x = fct_infreq(delay_cat)))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by frequency")

# reversed frequency
ggplot(data = linelist, aes(x = fct_rev(fct_infreq(delay_cat))))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Reverse of order by frequency")
```


### By appearance {.unnumbered}  

Use `fct_inorder()` to set the level order to match the order of appearance in the data, starting from the first row. This can be useful if you first carefully `arrange()` the data in the data frame, and then use this to set the factor order.  




### By summary statistic of another column {.unnumbered}  

You can use `fct_reorder()` to order the levels of one column *by a summary statistic of another column*. Visually, this can result in pleasing plots where the bars/points ascend or descend steadily across the plot.  

In the examples below, the x-axis is `delay_cat`, and the y-axis is numeric column `ct_blood` (cycle-threshold value). Box plots show the CT value distribution by `delay_cat` group. We want to order the box plots in ascending order by the group median CT value. 

In the first example below, the default order alpha-numeric level order is used. You can see the box plot heights are jumbled and not in any particular order. In the second example, the `delay_cat` column (mapped to the x-axis) has been wrapped in `fct_reorder()`, the column `ct_blood` is given as the second argument, and "median" is given as the third argument (you could also use "max", "mean", "min", etc). Thus, the order of the levels of `delay_cat` will now reflect ascending median CT values of each `delay_cat` group's median CT value. This is reflected in the second plot - the box plots have been re-arranged to ascend. Note how `NA` (missing) will appear at the end, unless converted to an explicit level.  

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# boxplots ordered by original factor levels
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = delay_cat,
        y = ct_blood, 
        fill = delay_cat))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by original alpha-numeric levels")+
  theme_classic()+
  theme(legend.position = "none")


# boxplots ordered by median CT value
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = fct_reorder(delay_cat, ct_blood, "median"),
        y = ct_blood,
        fill = delay_cat))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by median CT value in group")+
  theme_classic()+
  theme(legend.position = "none")
```

Note in this example above there are no steps required prior to the `ggplot()` call - the grouping and calculations are all done internally to the ggplot command.  


### By "end" value {.unnumbered}  

Use `fct_reorder2()` for grouped line plots. It orders the levels (and therefore the *legend*) to align with the vertical ordering of the lines at the "end" of the plot. Technically speaking, it "orders by the y-values associated with the largest x values."  

For example, if you have lines showing case counts by hospital over time, you can apply `fct_reorder2()` to the `color = ` argument within `aes()`, such that the vertical order of hospitals appearing in the legend aligns with the order of lines at the terminal end of the plot. Read more in the [online documentation](https://forcats.tidyverse.org/reference/fct_reorder.html).  

```{r, warning=F, message=F}
epidemic_data <- linelist %>%         # begin with the linelist   
    filter(date_onset < as.Date("2014-09-21")) %>%    # cut-off date, for visual clarity
    count(                                            # get case counts per week and by hospital
      epiweek = lubridate::floor_date(date_onset, "week"),  
      hospital                                            
    ) 
  
ggplot(data = epidemic_data)+                       # start plot
  geom_line(                                        # make lines
    aes(
      x = epiweek,                                  # x-axis epiweek
      y = n,                                        # height is number of cases per week
      color = fct_reorder2(hospital, epiweek, n)))+ # data grouped and colored by hospital, with factor order by height at end of plot
  labs(title = "Factor levels (and legend display) by line height at end of plot",
       color = "Hospital")                          # change legend title
```




## Missing values {#fct_missing}  

If you have `NA` values in your factor column, you can easily convert them to a named level such as "Missing" with `fct_explicit_na()`. The `NA` values are converted to "(Missing)" at the end of the level order by default. You can adjust the level name with the argument `na_level = `.  

Below, this opertation is performed on the column `delay_cat` and a table is printed with `tabyl()` with `NA` converted to "Missing delay".  

```{r}
linelist %>% 
  mutate(delay_cat = fct_explicit_na(delay_cat, na_level = "Missing delay")) %>% 
  tabyl(delay_cat)
```





## Combine levels  


### Manually {.unnumbered}  

You can adjust the level displays manually manually with `fct_recode()`. This is like the **dplyr** function `recode()` (see [Cleaning data and core functions]), but it allows the creation of new factor levels. If you use the simple `recode()` on a factor, new re-coded values will be rejected unless they have already been set as permissible levels. 

This tool can also be used to "combine" levels, by assigning multiple levels the same re-coded value. Just be careful to not lose information! Consider doing these combining steps in a new column (not over-writing the existing column).  

`fct_recode()` has a different syntax than `recode()`. `recode()` uses `OLD = NEW`, whereas `fct_recode()` uses `NEW = OLD`.     

The current levels of `delay_cat` are:  
```{r, echo=F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 0))
```


```{r}
levels(linelist$delay_cat)
```

The new levels are created using syntax `fct_recode(column, "new" = "old", "new" = "old", "new" = "old")` and printed:  

```{r}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 2 days" = "<2 days",
    "2 to 5 days"      = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```

Here they are manually combined with `fct_recode()`. Note there is no error raised at the creation of a new level "Less than 5 days".  


```{r, warning=F, message=F}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 5 days" = "<2 days",
    "Less than 5 days" = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```





### Reduce into "Other" {.unnumbered}  

You can use `fct_other()` to manually assign factor levels to an "Other" level. Below, all levels in the column `hospital`, aside from "Port Hospital" and "Central Hospital", are combined into "Other". You can provide a vector to either `keep = `, or `drop = `. You can change the display of the "Other" level with `other_level = `.  

```{r}
linelist %>%    
  mutate(hospital = fct_other(                      # adjust levels
    hospital,
    keep = c("Port Hospital", "Central Hospital"),  # keep these separate
    other_level = "Other Hospital")) %>%            # All others as "Other Hospital"
  tabyl(hospital)                                   # print table

```




### Reduce by frequency {.unnumbered}

You can combine the least-frequent factor levels automatically using `fct_lump()`.  

To "lump" together many low-frequency levels into an "Other" group, do one of the following:  

* Set `n = ` as the number of groups you want to keep. The n most-frequent levels will be kept, and all others will combine into "Other".  
* Set `prop = ` as the threshold frequency proportion for levels above which you want to keep. All other values will combine into "Other".  

You can change the display of the "Other" level with `other_level = `. Below, all but the two most-frequent hospitals are combined into "Other Hospital".  

```{r, warning=F, message=F}
linelist %>%    
  mutate(hospital = fct_lump(                      # adjust levels
    hospital,
    n = 2,                                          # keep top 2 levels
    other_level = "Other Hospital")) %>%            # all others as "Other Hospital"
  tabyl(hospital)                                   # print table

```




, warn
## Show all levels  

One benefit of using factors is to standardise the appearance of plot legends and tables, regardless of which values are actually present in a dataset. 

If you are preparing many figures (e.g. for multiple jurisdictions) you will want the legends and tables to appear identically even with varying levels of data completion or data composition.  

### In plots {.unnumbered}  

In a `ggplot()` figure, simply add the argument `drop = FALSE` in the relevant `scale_xxxx()` function. All factor levels will be displayed, regardless of whether they are present in the data. If your factor column levels are displayed using `fill = `, then in scale_fill_discrete() you include `drop = FALSE`, as shown below. If your levels are displayed with `x = ` (to the x-axis) `color = ` or `size = ` you would provide this to `scale_color_discrete()` or `scale_size_discrete()` accordingly.  

This example is a stacked bar plot of age category, by hospital. Adding `scale_fill_discrete(drop = FALSE)` ensures that all age groups appear in the legend, even if not present in the data. 

```{r}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = hospital, fill = age_cat)) +
  scale_fill_discrete(drop = FALSE)+                        # show all age groups in the legend, even those not present
  labs(
    title = "All age groups will appear in legend, even if not present in data")
```

### In tables {.unnumbered}  

Both the **base** R `table()` and `tabyl()` from **janitor** will show all factor levels (even unused levels).  

If you use `count()` or `summarise()` from **dplyr** to make a table, add the argument `.drop = FALSE` to include counts for all factor levels even those unused.  

Read more in the [Descriptive tables] page, or at the [scale_discrete documentation](https://ggplot2.tidyverse.org/reference/scale_discrete.html), or the [count() documentation](https://dplyr.tidyverse.org/reference/count.html). You can see another example in the [Contact tracing] page.  


## Epiweeks  

Please see the extensive discussion of how to create epidemiological weeks in the [Grouping data] page.  
Please also see the [Working with dates] page for tips on how to create and format epidemiological weeks.  


### Epiweeks in a plot {.unnumbered}  

If your goal is to create epiweeks to display in a plot, you can do this simply with **lubridate**'s `floor_date()`, as explained in the [Grouping data] page. The values returned will be of class Date with format YYYY-MM-DD. If you use this column in a plot, the dates will naturally order correctly, and you do not need to worry about levels or converting to class Factor. See the `ggplot()` histogram of onset dates below.  

In this approach, you can adjust the *display* of the dates on an axis with `scale_x_date()`. See the page on [Epidemic curves] for more information. You can specify a "strptime" display format to the `date_labels = ` argument of `scale_x_date()`. These formats use "%" placeholders and are covered in the [Working with dates] page. Use "%Y" to represent a 4-digit year, and either "%W" or "%U" to represent the week number (Monday or Sunday weeks respectively).  

```{r, warning=F, message=F}
linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week")) %>%  # create week column
  ggplot()+                                                  # begin ggplot
  geom_histogram(mapping = aes(x = epiweek_date))+           # histogram of date of onset
  scale_x_date(date_labels = "%Y-W%W")                       # adjust disply of dates to be YYYY-WWw
```


### Epiweeks in the data {.unnumbered}  

However, if your purpose in factoring is *not* to plot, you can approach this one of two ways:  

1) *For fine control over the display*, convert the **lubridate** epiweek column (YYYY-MM-DD) to the desired display format (YYYY-WWw) *within the data frame itself*, and then convert it to class Factor.  

First, use `format()` from **base** R to convert the date display from YYYY-MM-DD to YYYY-Www display (see the [Working with dates] page). In this process the class will be converted to character. Then, convert from character to class Factor with `factor()`.  


```{r}
linelist <- linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week"),       # create epiweeks (YYYY-MM-DD)
         epiweek_formatted = format(epiweek_date, "%Y-W%W"),  # Convert to display (YYYY-WWw)
         epiweek_formatted = factor(epiweek_formatted))       # Convert to factor

# Display levels
levels(linelist$epiweek_formatted)
```

<span style="color: red;">**_DANGER:_** If you place the weeks ahead of the years ("Www-YYYY") ("%W-%Y"), the default alpha-numeric level ordering will be incorrect (e.g. 01-2015 will be before 35-2014). You could need to manually adjust the order, which would be a long painful process.</span>  

2) *For fast default display*, use the **aweek** package and it's function `date2week()`. You can set the `week_start = ` day, and if you set `factor = TRUE` then the output column is an ordered factor. As a bonus, the factor includes levels for *all* possible weeks in the span - even if there are no cases that week.  

```{r, eval=F}
df <- linelist %>% 
  mutate(epiweek = date2week(date_onset, week_start = "Monday", factor = TRUE))

levels(df$epiweek)
```

See the [Working with dates] page for more information about **aweek**. It also offers the reverse function `week2date()`.  



<!-- ======================================================= -->
## Resources {} 

R for Data Science page on [factors](https://r4ds.had.co.nz/factors.html)  
[aweek package vignette](https://cran.r-project.org/web/packages/aweek/vignettes/introduction.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/factors.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Pivoting data {}

```{r, warning=F, message=F, out.height = c('50%'), fig.align="center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pivoting", "Pivoting_500x500.png"))

#knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_bar.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```



When managing data, *pivoting* can be understood to refer to one of two processes:  

1. The creation of *pivot tables*, which are tables of statistics that summarise the data of a more extensive table  
2. The conversion of a table from **long** to **wide** format, or vice versa. 

**In this page, we will focus on the latter definition.** The former is a crucial step in data analysis, and is covered elsewhere in the [Grouping data] and [Descriptive tables] pages. 

This page discusses the formats of data. It is useful to be aware of the idea of "tidy data", in which each variable has it's own column, each observation has it's own row, and each value has it's own cell. More about this topic can be found [at this online chapter in R for Data Science](https://r4ds.had.co.nz/tidy-data.html). 





## Preparation  

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse)    # data management + ggplot2 graphics
```



### Import data {.unnumbered}


### Malaria count data {-}  

In this page, we will use a fictional dataset of daily malaria cases, by facility and age group. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/malaria_facility_count_data.rds' class='download-button'>click here to download (as .rds file)<span></a>. Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()
```

```{r, eval=F}
# Import data
count_data <- import("malaria_facility_count_data.rds")
```

The first 50 rows are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Linelist case data {-}  

In the later part of this page, we will also use the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# import your dataset
linelist <- import("linelist_cleaned.xlsx")
```







<!-- ======================================================= -->
## Wide-to-long {}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```


<!-- ======================================================= -->
### "Wide" format {.unnumbered}

Data are often entered and stored in a "wide" format - where a subject's characteristics or responses are stored in a single row. While this may be useful for presentation, it is not ideal for some types of analysis.  

Let us take the `count_data` dataset imported in the Preparation section above as an example. You can see that each row represents a "facility-day". The actual case counts (the right-most columns) are stored in a "wide" format such that the information for every age group on a given facility-day is stored in a single row.  

```{r, echo=F}
DT::datatable(count_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

Each observation in this dataset refers to the malaria counts at one of 65 facilities on a given date, ranging from ` count_data$data_date %>% min()` to ` count_data$data_date %>% max()`. These facilities are located in one `Province` (North) and four `District`s (Spring, Bolo, Dingo, and Barnard). The dataset provides the overall counts of malaria, as well as age-specific counts in each of three age groups - <4 years, 5-14 years, and 15 years and older.

"Wide" data like this are not adhering to "tidy data" standards, because the column headers do not actually represent "variables" - they represent *values* of a hypothetical "age group" variable. 


This format can be useful for presenting the information in a table, or for entering data (e.g. in Excel) from case report forms. However, in the analysis stage, these data typically should be transformed to a "longer" format more aligned with "tidy data" standards. The plotting R package **ggplot2** in particular works best when data are in a "long" format.  


Visualising the *total* malaria counts over time poses no difficulty with the data in it's current format:

```{r, warning=F, message=F}
ggplot(count_data) +
  geom_col(aes(x = data_date, y = malaria_tot), width = 1)
```

However, what if we wanted to display the relative contributions of each age group to this total count? In this case, we need to ensure that the variable of interest (age group), appears in the dataset in a single column that can be passed to `{ggplot2}`'s "mapping aesthetics" `aes()` argument.




<!-- ======================================================= -->
### `pivot_longer()` {.unnumbered}

The **tidyr** function `pivot_longer()` makes data "longer". **tidyr** is part of the **tidyverse** of R packages.  

It accepts a range of columns to transform (specified to `cols = `). Therefore, it can operate on only a part of a dataset. This is useful for the malaria data, as we only want to pivot the case count columns.  

In this process, you will end up with two "new" columns - one with the categories (the former column names), and one with the corresponding values (e.g. case counts). You can accept the default names for these new columns, or you can specify your own to `names_to = ` and `values_to = ` respectively.  

Let's see `pivot_longer()` in action... 



### Standard pivoting {.unnumbered}  

We want to use **tidyr**'s `pivot_longer()` function to convert the "wide" data to a "long" format. Specifically, to convert the four numeric columns with data on malaria counts to two new columns: one which holds the *age groups* and one which holds the corresponding *values*.  

```{r, eval=F}
df_long <- count_data %>% 
  pivot_longer(
    cols = c(`malaria_rdt_0-4`, `malaria_rdt_5-14`, `malaria_rdt_15`, `malaria_tot`)
  )

df_long
```

Notice that the newly created data frame (`df_long`) has more rows (12,152 vs 3,038); it has become *longer*. In fact, it is precisely four times as long, because each row in the original dataset now represents four rows in df_long, one for each of the malaria count observations (<4y, 5-14y, 15y+, and total).

In addition to becoming longer, the new dataset has fewer columns (8 vs 10), as the data previously stored in four columns (those beginning with the prefix `malaria_`) is now stored in two. 

Since the names of these four columns all begin with the prefix `malaria_`, we could have made use of the handy "tidyselect" function `starts_with()` to achieve the same result (see the page [Cleaning data and core functions] for more of these helper functions).  

```{r}
# provide column with a tidyselect helper function
count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_")
  )
```

or by position: 

```{r, eval=F}
# provide columns by position
count_data %>% 
  pivot_longer(
    cols = 6:9
  )
```

or by named range:

```{r, eval=F}
# provide range of consecutive columns
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_tot
  )
```



These two new columns are given the default names of `name` and `value`, but we can override these defaults to provide more meaningful names, which can help remember what is stored within, using the `names_to` and `values_to` arguments. Let's use the names `age_group` and `counts`:

```{r}
df_long <- 
  count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_"),
    names_to = "age_group",
    values_to = "counts"
  )

df_long
```

We can now pass this new dataset to `{ggplot2}`, and map the new column `count` to the y-axis and new column `age_group` to the `fill = ` argument (the column internal color). This will display the malaria counts in a stacked bar chart, by age group:

```{r, warning=F, message=F}
ggplot(data = df_long) +
  geom_col(
    mapping = aes(x = data_date, y = counts, fill = age_group),
    width = 1
  )
```

Examine this new plot, and compare it with the plot we created earlier - *what has gone wrong?*  

We have encountered a common problem when wrangling surveillance data - we have also included the total counts from the `malaria_tot` column, so the magnitude of each bar in the plot is twice as high as it should be. 

We can handle this in a number of ways. We could simply filter these totals from the dataset before we pass it to `ggplot()`:

```{r, warning=F, message=F}
df_long %>% 
  filter(age_group != "malaria_tot") %>% 
  ggplot() +
  geom_col(
    aes(x = data_date, y = counts, fill = age_group),
    width = 1
  )
```

Alternatively, we could have excluded this variable when we ran `pivot_longer()`, thereby maintaining it in the dataset as a separate variable. See how its values "expand" to fill the new rows. 

```{r, warning=F, message=F}
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_rdt_15,   # does not include the totals column
    names_to = "age_group",
    values_to = "counts"
  )
```





### Pivoting data of multiple classes {.unnumbered}

The above example works well in situations in which all the columns you want to "pivot longer" are of the same class (character, numeric, logical...). 

However, there will be many cases when, as a field epidemiologist, you will be working with data that was prepared by non-specialists and which follow their own non-standard logic - as Hadley Wickham noted (referencing Tolstoy) in his [seminal article](https://vita.had.co.nz/papers/tidy-data.pdf) on **Tidy Data** principles: "Like families, tidy datasets are all alike but every messy dataset is messy in its own way."

One particularly common problem you will encounter will be the need to pivot columns that contain different classes of data. This pivot will result in storing these different data types in a single column, which is not a good situation. There are various approaches one can take to separate out the mess this creates, but there is an important step you can take using `pivot_longer()` to avoid creating such a situation yourself.

Take a situation in which there have been a series of observations at different time steps for each of three items A, B and C. Examples of such items could be individuals (e.g. contacts of an Ebola case being traced each day for 21 days) or remote village health posts being monitored once per year to ensure they are still functional. Let's use the contact tracing example. Imagine that the data are stored as follows:


```{r, message=FALSE, echo=F}

df <- 
  tibble::tribble(
     ~id,   ~obs1_date, ~obs1_status,   ~obs2_date, ~obs2_status,   ~obs3_date, ~obs3_status,
     "A", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",     "Unwell",
     "B", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy",
     "C", "2021-04-23",    "Missing", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy"
     ) 

DT::datatable(df, rownames = FALSE)

```

As can be seen, the data are a bit complicated. Each row stores information about one item, but with the time series running further and further away to the right as time progresses. Moreover, the column classes alternate between date and character values.  

One particularly bad example of this encountered by this author involved cholera surveillance data, in which 8 new columns of observations were added *each day* over the course of __4 years__. Simply opening the Excel file in which these data were stored took >10 minuntes on my laptop!

In order to work with these data, we need to transform the data frame to long format, but keeping the separation between a `date` column and a `character` (status) column, for each observation for each item. If we don't, we might end up with a mixture of variable types in a single column (a very big "no-no" when it comes to data management and tidy data):

```{r}
df %>% 
  pivot_longer(
    cols = -id,
    names_to = c("observation")
  )

```

Above, our pivot has merged *dates* and *characters* into a single `value` column. R will react by converting the entire column to class character, and the utility of the dates is lost.  

To prevent this situation, we can take advantage of the syntax structure of the original column names. There is a common naming structure, with the observation number, an underscore, and then either "status" or "date". We can leverage this syntax to keep these two data types in separate columns after the pivot. 

We do this by:  

* Providing a character vector to the `names_to = ` argument, with the second item being (`".value"` ). This special term indicates that the pivoted columns will be split based on a character in their name...  
* You must also provide the "splitting" character to the `names_sep = ` argument. In this case, it is the underscore "_".  

Thus, the naming and split of new columns is based around the underscore in the existing variable names.  

```{r}

df_long <- 
  df %>% 
  pivot_longer(
    cols = -id,
    names_to = c("observation", ".value"),
    names_sep = "_"
  )

df_long

```

__Finishing touches__:

Note that the `date` column is currently in *character* class - we can easily convert this into it's proper date class using the `mutate()` and `as_date()` functions described in the [Working with dates] page.  

We may also want to convert the `observation` column to a `numeric` format by dropping the "obs" prefix and converting to numeric. We cando this with `str_remove_all()` from the **stringr** package (see the [Characters and strings] page).  

```{r}

df_long <- 
  df_long %>% 
  mutate(
    date = date %>% lubridate::as_date(),
    observation = 
      observation %>% 
      str_remove_all("obs") %>% 
      as.numeric()
  )

df_long

```

And now, we can start to work with the data in this format, e.g. by plotting a descriptive heat tile:  

```{r}
ggplot(data = df_long, mapping = aes(x = date, y = id, fill = status)) +
  geom_tile(colour = "black") +
  scale_fill_manual(
    values = 
      c("Healthy" = "lightgreen", 
        "Unwell" = "red", 
        "Missing" = "orange")
  )

```





<!-- ======================================================= -->
## Long-to-wide {}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```


In some instances, we may wish to convert a dataset to a wider format. For this, we can use the `pivot_wider()` function.

A typical use-case is when we want to transform the results of an analysis into a format which is more digestible for the reader (such as a [Table for presentation][Tables for presentation]). Usually, this involves transforming a dataset in which information for one subject is are spread over multiple rows into a format in which that information is stored in a single row.

### Data {.unnumbered}

For this section of the page, we will use the case linelist (see the [Preparation](#pivot_prep) section), which contains one row per case.  

Here are the first 50 rows:  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Suppose that we want to know the counts of individuals in the different age groups, by gender:

```{r}
df_wide <- 
  linelist %>% 
  count(age_cat, gender)

df_wide
```

This gives us a long dataset that is great for producing visualisations in **ggplot2**, but not ideal for presentation in a table:

```{r}
ggplot(df_wide) +
  geom_col(aes(x = age_cat, y = n, fill = gender))
```

### Pivot wider {.unnumbered}  

Therefore, we can use `pivot_wider()` to transform the data into a better format for inclusion as tables in our reports.  

The argument `names_from` specifies the column *from* which to generate the new column *names*, while the argument `values_from` specifies the column *from* which to take the *values* to populate the cells. The argument `id_cols = ` is optional, but can be provided a vector of column names that should not be pivoted, and will thus identify each row.  

```{r}
table_wide <- 
  df_wide %>% 
  pivot_wider(
    id_cols = age_cat,
    names_from = gender,
    values_from = n
  )

table_wide
```

This table is much more reader-friendly, and therefore better for inclusion in our reports. You can convert into a pretty table with several packages including **flextable** and **knitr**. This process is elaborated in the page [Tables for presentation].  

```{r}
table_wide %>% 
  janitor::adorn_totals(c("row", "col")) %>% # adds row and column totals
  knitr::kable() %>% 
  kableExtra::row_spec(row = 10, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

---


<!-- ======================================================= -->
## Fill 

In some situations after a `pivot`, and more commonly after a `bind`, we are left with gaps in some cells that we would like to fill.  

<!-- ======================================================= -->
### Data {.unnumbered}

For example, take two datasets, each with observations for the measurement number, the name of the facility, and the case count at that time. However, the second dataset also has a variable `Year`. 

```{r}
df1 <- 
  tibble::tribble(
       ~Measurement, ~Facility, ~Cases,
                  1,  "Hosp 1",     66,
                  2,  "Hosp 1",     26,
                  3,  "Hosp 1",      8,
                  1,  "Hosp 2",     71,
                  2,  "Hosp 2",     62,
                  3,  "Hosp 2",     70,
                  1,  "Hosp 3",     47,
                  2,  "Hosp 3",     70,
                  3,  "Hosp 3",     38,
       )

df1 

df2 <- 
  tibble::tribble(
    ~Year, ~Measurement, ~Facility, ~Cases,
     2000,            1,  "Hosp 4",     82,
     2001,            2,  "Hosp 4",     87,
     2002,            3,  "Hosp 4",     46
  )

df2
```


When we perform a `bind_rows()` to join the two datasets together, the `Year` variable is filled with `NA` for those rows where there was no prior information (i.e. the first dataset):


```{r}
df_combined <- 
  bind_rows(df1, df2) %>% 
  arrange(Measurement, Facility)

df_combined

```

<!-- ======================================================= -->
### `fill()` {.unnumbered}

In this case, `Year` is a useful variable to include, particularly if we want to explore trends over time. Therefore, we use `fill()` to *fill* in those empty cells, by specifying the column to fill and the direction (in this case **up**):

```{r}
df_combined %>% 
  fill(Year, .direction = "up")
```

Alternatively, we can rearrange the data so that we would need to fill in a downward direction:

```{r}
df_combined <- 
  df_combined %>% 
  arrange(Measurement, desc(Facility))

df_combined

df_combined <- 
  df_combined %>% 
  fill(Year, .direction = "down")

df_combined
```

We now have a useful dataset for plotting:

```{r}
ggplot(df_combined) +
  aes(Year, Cases, fill = Facility) +
  geom_col()
```

But less useful for presenting in a table, so let's practice converting this long, untidy dataframe into a wider, tidy dataframe:

```{r}
df_combined %>% 
  pivot_wider(
    id_cols = c(Measurement, Facility),
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  arrange(Facility) %>% 
  janitor::adorn_totals(c("row", "col")) %>% 
  knitr::kable() %>% 
  kableExtra::row_spec(row = 5, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

N.B. In this case, we had to specify to only include the three variables `Facility`, `Year`, and `Cases` as the additional variable `Measurement` would interfere with the creation of the table:

```{r}
df_combined %>% 
  pivot_wider(
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  knitr::kable()
```

## Resources  

Here is a helpful [tutorial](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/pivoting.Rmd-->


# Grouping data { }  


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Grouping_1500x500.png"))
```


This page covers how to group and aggregate data for descriptive analysis. It makes use of the **tidyverse** family of packages for common and easy-to-use functions. 


Grouping data is a core component of data management and analysis. Grouped data statistically summarised by group, and can be plotted by group. Functions from the **dplyr** package (part of the **tidyverse**) make grouping and subsequent operations quite easy.  

This page will address the following topics:  

* Group data with the `group_by()` function  
* Un-group data  
* `summarise()` grouped data with statistics  
* The difference between `count()` and `tally()`  
* `arrange()` applied to grouped data  
* `filter()` applied to grouped data  
* `mutate()` applied to grouped data  
* `select()` applied to grouped data  
* The **base** R `aggregate()` command as an alternative  




<!-- ======================================================= -->
## Preparation {  }
     
### Load packages {.unnumbered}  
     
This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r}
pacman::p_load(
  rio,       # to import data
  here,      # to locate files
  tidyverse, # to clean, handle, and plot the data (includes dplyr)
  janitor)   # adding total rows and columns
```




### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

```{r, echo=F}
linelist <- rio::import(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```


The first 50 rows of `linelist`:  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Grouping {  }
     
The function `group_by()` from **dplyr** groups the rows by the unique values in the column specified to it. If multiple columns are specified, rows are grouped by the unique combinations of values across the columns. Each unique value (or combination of values) constitutes a group. Subsequent changes to the dataset or calculations can then be performed within the context of each group.  

For example, the command below takes the `linelist` and groups the rows by unique values in the column `outcome`, saving the output as a new data frame `ll_by_outcome`. The grouping column(s) are placed inside the parentheses of the function `group_by()`.  

```{r}
ll_by_outcome <- linelist %>% 
  group_by(outcome)
```

**Note that there is no perceptible change to the dataset** after running `group_by()`, *until* another **dplyr** verb such as `mutate()`, `summarise()`, or `arrange()` is applied on the "grouped" data frame.  

You can however "see" the groupings by printing the data frame. When you print a grouped data frame, you will see it has been transformed into a [`tibble` class object](https://tibble.tidyverse.org/) which, when printed, displays which groupings have been applied and how many groups there are - written just above the header row.  

```{r}
# print to see which groups are active
ll_by_outcome
```


### Unique groups {.unnumbered}  

**The groups created reflect each unique combination of values across the grouping columns.** 

To see the groups *and the number of rows in each group*, pass the grouped data to `tally()`. To see just the unique groups without counts you can pass to `group_keys()`.  

See below that there are **three** unique values in the grouping column `outcome`: "Death", "Recover", and `NA`. See that there were ` nrow(linelist %>% filter(outcome == "Death"))` deaths, ` nrow(linelist %>% filter(outcome == "Recover"))` recoveries, and ` nrow(linelist %>% filter(is.na(outcome)))` with no outcome recorded.

```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally()
```


You can group by more than one column. Below, the data frame is grouped by `outcome` and `gender`, and then tallied. Note how each unique combination of `outcome` and `gender` is registered as its own group - including missing values for either column.   

```{r}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally()
```

### New columns {.unnumbered} 

You can also create a new grouping column *within* the `group_by()` statement. This is equivalent to calling `mutate()` before the `group_by()`. For a quick tabulation this style can be handy, but for more clarity in your code consider creating this column in its own `mutate()` step and then piping to `group_by()`.

```{r}
# group dat based on a binary column created *within* the group_by() command
linelist %>% 
  group_by(
    age_class = ifelse(age >= 18, "adult", "child")) %>% 
  tally(sort = T)
```

### Add/drop grouping columns {.unnumbered}  

By default, if you run `group_by()` on data that are already grouped, the old groups will be removed and the new one(s) will apply. If you want to add new groups to the existing ones, include the argument `.add = TRUE`.  

````{r, eval=F}
# Grouped by outcome
by_outcome <- linelist %>% 
  group_by(outcome)

# Add grouping by gender in addition
by_outcome_gender <- by_outcome %>% 
  group_by(gender, .add = TRUE)
```


** Keep all groups**  

If you group on a column of class factor there may be levels of the factor that are not currently present in the data. If you group on this column, by default those non-present levels are dropped and not included as groups. To change this so that all levels appear as groups (even if not present in the data), set `.drop = FALSE` in your `group_by()` command.  


## Un-group  

Data that have been grouped will remain grouped until specifically ungrouped via `ungroup()`. If you forget to ungroup, it can lead to incorrect calculations! Below is an example of removing all groupings:  

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup()
```

You can also remove grouping for only specific columns, by placing the column name inside `ungroup()`.  

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup(gender) # remove the grouping by gender, leave grouping by outcome
```


<span style="color: black;">**_NOTE:_** The verb `count()` automatically ungroups the data after counting.</span>



## Summarise {#group_summarise} 

See the **dplyr** section of the [Descriptive tables] page for a detailed description of how to produce summary tables with `summarise()`. Here we briefly address how its behavior changes when applied to grouped data.  

The **dplyr** function `summarise()` (or `summarize()`) takes a data frame and converts it into a *new* summary data frame, with columns containing summary statistics that you define. On an ungrouped data frame, the summary statistics will be calculated from all rows. Applying `summarise()` to grouped data produces those summary statistics *for each group*.  

The syntax of `summarise()` is such that you provide the name(s) of the **new** summary column(s), an equals sign, and then a statistical function to apply to the data, as shown below. For example, `min()`, `max()`, `median()`, or `sd()`. Within the statistical function, list the column to be operated on and any relevant argument (e.g. `na.rm = TRUE`). You can use `sum()` to count the number of rows that meet a logical criteria (with double equals `==`).   

Below is an example of `summarise()` applied *without grouped data*. The statistics returned are produced from the entire dataset.     

```{r}
# summary statistics on ungrouped linelist
linelist %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males  = sum(gender == "m", na.rm=T))
```

In contrast, below is the same `summarise()` statement applied to grouped data. The statistics are calculated for each `outcome` group. Note how grouping columns will carry over into the new data frame.    

```{r}
# summary statistics on grouped linelist
linelist %>% 
  group_by(outcome) %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males    = sum(gender == "m", na.rm=T))
```

<span style="color: darkgreen;">**_TIP:_** The summarise function works with both UK and US spelling - `summarise()` and `summarize()` call the same function.</span>




## Counts and tallies  

`count()` and `tally()` provide similar functionality but are different. Read more about the distinction between `tally()` and `count()` [here](https://dplyr.tidyverse.org/reference/tally.html)    

### `tally()` {.unnumbered}  

`tally()` is shorthand for `summarise(n = n())`, and *does not* group data. Thus, to achieve grouped tallys it must follow a `group_by()` command. You can add `sort = TRUE` to see the largest groups first.    

```{r}
linelist %>% 
  tally()
```


```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally(sort = TRUE)
```


### `count()`  {.unnumbered}  

In contrast, `count()` does the following:  

1) applies `group_by()` on the specified column(s)  
2) applies `summarise()` and returns column `n` with the number of rows per group  
3) applies `ungroup()`  

```{r}
linelist %>% 
  count(outcome)
```

Just like with `group_by()` you can create a new column within the `count()` command:  

```{r}
linelist %>% 
  count(age_class = ifelse(age >= 18, "adult", "child"), sort = T)
```


`count()` can be called multiple times, with the functionality "rolling up". For example, to summarise the number of hospitals present for each gender, run the following. Note, the name of the final column is changed from default "n" for clarity (with `name  = `).  

```{r}
linelist %>% 
  # produce counts by unique outcome-gender groups
  count(gender, hospital) %>% 
  # gather rows by gender (3) and count number of hospitals per gender (6)
  count(gender, name = "hospitals per gender" ) 
```


### Add counts {.unnumbered}  

In contrast to `count()` and `summarise()`, you can use `add_count()` to *add* a new column `n` with the counts of rows per group *while retaining all the other data frame columns*.   

This means that a group's count number, in the new column `n`, will be printed in each row of the group. For demonstration purposes, we add this column and then re-arrange the columns for easier viewing. See the section below on [filter on group size](#group_filter_grp_size) for another example.  


```{r}
linelist %>% 
  as_tibble() %>%                   # convert to tibble for nicer printing 
  add_count(hospital) %>%           # add column n with counts by hospital
  select(hospital, n, everything()) # re-arrange for demo purposes
```



### Add totals {.unnumbered} 

To easily add total *sum* rows or columns after using `tally()` or `count()`, see the **janitor** section of the [Descriptive tables](#tbl_janitor) page. This package offers functions like `adorn_totals()` and `adorn_percentages()` to add totals and convert to show percentages. Below is a brief example:  

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts of two columns
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions with column denominator
  adorn_pct_formatting() %>%                  # convert proportions to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```


To add more complex totals rows that involve summary statistics other than *sums*, see [this section of the Descriptive Tables page](#tbl_dplyr_totals).  



## Grouping by date  

When grouping data by date, you must have (or create) a column for the date unit of interest - for example "day", "epiweek", "month", etc. You can make this column using `floor_date()` from **lubridate**, as explained in the [Epidemiological weeks section](#dates_epi_wks) of the [Working with dates] page. Once you have this column, you can use `count()` from **dplyr** to group the rows by those unique date values and achieve aggregate counts. 

One additional step common for date situations, is to "fill-in" any dates in the sequence that are not present in the data. Use `complete()` from **tidyr** so that the aggregated date series is *complete* including *all possible date units* within the range. Without this step, a week with no cases reported might not appear in your data!  

Within `complete()` you *re-define* your date column as a *sequence* of dates `seq.Date()` from the minimum to the maximum  - thus the dates are expanded. By default, the case count values in any new "expanded" rows will be `NA`. You can set them to 0 using the `fill = ` argument of `complete()`, which expects a named list (if your counts column is named `n`, provide `fill = list(n = 0)`. See `?complete` for details and the [Working with dates](#dates_epi_wks) page for an example.  



### Linelist cases into days  {.unnumbered}  

Here is an example of grouping cases into days *without* using `complete()`. Note the first rows skip over dates with no cases.  

```{r}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%        # remove that were missing date_onset
  count(date_onset)              # count number of rows per unique date
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below we add the `complete()` command to ensure every day in the range is represented.

```{r, eval=F}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%                 # remove case missing date_onset
  count(date_onset) %>%                   # count number of rows per unique date
  complete(                               # ensure all days appear even if no cases
    date_onset = seq.Date(                # re-define date colume as daily sequence of dates
      from = min(date_onset, na.rm=T), 
      to = max(date_onset, na.rm=T),
      by = "day"),
    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) 
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Linelist cases into weeks {.unnumbered}  


The same principle can be applied for weeks. First create a new column that is the week of the case using `floor_date()` with `unit = "week"`. Then, use `count()` as above to achieve weekly case counts. Finish with `complete()` to ensure that all weeks are represented, even if they contain no cases.

```{r}
# Make dataset of weekly case counts
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%                 # remove cases missing date_onset
  mutate(week = lubridate::floor_date(date_onset, unit = "week")) %>%  # new column of week of onset
  count(week) %>%                         # group data by week and count rows per group
  complete(                               # ensure all days appear even if no cases
    week = seq.Date(                      # re-define date colume as daily sequence of dates
      from = min(week, na.rm=T), 
      to = max(week, na.rm=T),
      by = "week"),
    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) 
```

Here are the first 50 rows of the resulting data frame:  

```{r message=FALSE, echo=F}
DT::datatable(weekly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Linelist cases into months {.unnumbered}

To aggregate cases into months, again use `floor_date()` from the **lubridate** package, but with the argument `unit = "months"`. This rounds each date down to the 1st of its month. The output will be class Date. Note that in the `complete()` step we also use `by = "months"`.  


```{r}
# Make dataset of monthly case counts
monthly_counts <- linelist %>% 
  drop_na(date_onset) %>% 
  mutate(month = lubridate::floor_date(date_onset, unit = "months")) %>%  # new column, 1st of month of onset
  count(month) %>%                          # count cases by month
  complete(
    month = seq.Date(
      min(month, na.rm=T),     # include all months with no cases reported
      max(month, na.rm=T),
      by="month"),
    fill = list(n = 0))
```

```{r message=FALSE, echo=F}
DT::datatable(monthly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Daily counts into weeks {.unnumbered}

To aggregate daily counts into weekly counts, use `floor_date()` as above. However, use `group_by()` and `summarize()` instead of `count()` because you need to `sum()` daily case counts instead of just counting the number of rows per week.



#### Daily counts into months {.unnumbered}

To aggregate daily counts into months counts, use `floor_date()` with `unit = "month"` as above. However, use `group_by()` and `summarize()` instead of `count()` because you need to `sum()` daily case counts instead of just counting the number of rows per month.  




## Arranging grouped data

Using the **dplyr** verb `arrange()` to order the rows in a data frame behaves the same when the data are grouped, *unless* you set the argument `.by_group =TRUE`. In this case the rows are ordered first by the grouping columns and then by any other columns you specify to `arrange()`.   



## Filter on grouped data

### `filter()` {.unnumbered}

When applied in conjunction with functions that evaluate the data frame (like `max()`, `min()`, `mean()`), these functions will now be applied to the groups. For example, if you want to filter and keep rows where patients are above the median age, this will now apply per group - filtering to keep rows above the *group's* median age. 




### Slice rows per group {.unnumbered} 

The **dplyr** function `slice()`, which [filters rows based on their position](https://dplyr.tidyverse.org/reference/slice.html) in the data, can also be applied per group. Remember to account for sorting the data within each group to get the desired "slice".  

For example, to retrieve only the latest 5 admissions from each hospital:  

1) Group the linelist by column `hospital`  
2) Arrange the records from latest to earliest `date_hospitalisation` *within each hospital group*  
3) Slice to retrieve the first 5 rows from each hospital  

```{r,}
linelist %>%
  group_by(hospital) %>%
  arrange(hospital, date_hospitalisation) %>%
  slice_head(n = 5) %>% 
  arrange(hospital) %>%                            # for display
  select(case_id, hospital, date_hospitalisation)  # for display
```

`slice_head()` - selects n rows from the top  
`slice_tail()` - selects n rows from the end  
`slice_sample()` - randomly selects n rows  
`slice_min()` - selects n rows with highest values in `order_by = ` column, use `with_ties = TRUE` to keep ties  
`slice_max()` - selects n rows with lowest values in `order_by = ` column, use `with_ties = TRUE` to keep ties  

See the [De-duplication] page for more examples and detail on `slice()`.  




### Filter on group size {#group_filter_grp_size .unnumbered} 

The function `add_count()` adds a column `n` to the original data giving the number of rows in that row's group. 

Shown below, `add_count()` is applied to the column `hospital`, so the values in the new column `n` reflect the number of rows in that row's hospital group. Note how values in column `n` are repeated. In the example below, the column name `n` could be changed using `name = ` within `add_count()`. For demonstration purposes we re-arrange the columns with `select()`.  


```{r}
linelist %>% 
  as_tibble() %>% 
  add_count(hospital) %>%          # add "number of rows admitted to same hospital as this row" 
  select(hospital, n, everything())
```

It then becomes easy to filter for case rows who were hospitalized at a "small" hospital, say, a hospital that admitted fewer than 500 patients:  

```{r, eval=F}
linelist %>% 
  add_count(hospital) %>% 
  filter(n < 500)
```





## Mutate on grouped data  

To retain all columns and rows (not summarise) and *add a new column containing group statistics*, use `mutate()` after `group_by()` instead of `summarise()`. 

This is useful if you want group statistics in the original dataset *with all other columns present* - e.g. for calculations that compare one row to its group.  

For example, this code below calculates the difference between a row's delay-to-admission and the median delay for their hospital. The steps are:  

1) Group the data by hospital  
2) Use the column `days_onset_hosp` (delay to hospitalisation) to create a new column containing the mean delay at the hospital of *that row*  
3) Calculate the difference between the two columns  

We `select()` only certain columns to display, for demonstration purposes.  

```{r}
linelist %>% 
  # group data by hospital (no change to linelist yet)
  group_by(hospital) %>% 
  
  # new columns
  mutate(
    # mean days to admission per hospital (rounded to 1 decimal)
    group_delay_admit = round(mean(days_onset_hosp, na.rm=T), 1),
    
    # difference between row's delay and mean delay at their hospital (rounded to 1 decimal)
    diff_to_group     = round(days_onset_hosp - group_delay_admit, 1)) %>%
  
  # select certain rows only - for demonstration/viewing purposes
  select(case_id, hospital, days_onset_hosp, group_delay_admit, diff_to_group)
```



## Select on grouped data  

The verb `select()` works on grouped data, but the grouping columns are always included (even if not mentioned in `select()`). If you do not want these grouping columns, use `ungroup()` first.  










<!-- ======================================================= -->
## Resources {  }

Here are some useful resources for more information:  

You can perform any summary function on grouped data; see the [RStudio data transformation cheat sheet](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf)  

The Data Carpentry page on [**dplyr**](https://datacarpentry.org/R-genomics/04-dplyr.html)  
The **tidyverse** reference pages on [group_by()](https://dplyr.tidyverse.org/reference/group_by.html) and [grouping](https://dplyr.tidyverse.org/articles/grouping.html)  

This page on [Data manipulation](https://itsalocke.com/files/DataManipulationinR.pdf)  

[Summarize with conditions in dplyr](https://stackoverflow.com/questions/23528862/summarize-with-conditions-in-dplyr)  






```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/grouping.Rmd-->


# Joining data { }  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
```

*Above: an animated example of a left join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))*  


This page describes ways to "join", "match", "link" "bind", and otherwise combine data frames.  

It is uncommon that your epidemiological analysis or workflow does not involve multiple sources of data, and the linkage of multiple datasets. Perhaps you need to connect laboratory data to patient clinical outcomes, or Google mobility data to infectious disease trends, or even a dataset at one stage of analysis to a transformed version of itself.

In this page we demonstrate code to:  

* Conduct *joins* of two data frames such that rows are matched based on common values in identifier columns  
* Join two data frames based on *probabilistic* (likely) matches between values  
* Expand a data frame by directly *binding* or ("appending") rows or columns from another data frame  


<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,            # import and export
  here,           # locate files 
  tidyverse,      # data management and visualisation
  RecordLinkage,  # probabilistic matches
  fastLink        # probabilistic matches
)
```



### Import data {.unnumbered}

To begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
### Example datasets {.unnumbered}

In the joining section below, we will use the following datasets:  

1) A "miniature" version of the case `linelist`, containing only the columns `case_id`, `date_onset`, and `hospital`, and only the first 10 rows  
2) A separate data frame named `hosp_info`, which contains more details about each hospital  

In the section on probabilistic matching, we will use two different small datasets. The code to create those datasets is given in that section.  




#### "Miniature" case linelist {#joins_llmini .unnumbered}  

Below is the the miniature case linelist, which contains only 10 rows and only columns `case_id`, `date_onset`, and `hospital`.  

```{r}
linelist_mini <- linelist %>%                 # start with original linelist
  select(case_id, date_onset, hospital) %>%   # select columns
  head(10)                                    # only take the first 10 rows
```

```{r message=FALSE, echo=F}
DT::datatable(linelist_mini, rownames = FALSE, options = list(pageLength = nrow(10)))
```




#### Hospital information data frame {#joins_hosp_info .unnumbered}  

Below is the code to create a separate data frame with additional information about seven hospitals (the catchment population, and the level of care available). Note that the name "Military Hospital" belongs to two different hospitals - one a primary level serving 10000 residents and the other a secondary level serving 50280 residents.  

```{r}
# Make the hospital information data frame
hosp_info = data.frame(
  hosp_name     = c("central hospital", "military", "military", "port", "St. Mark's", "ignace", "sisters"),
  catchment_pop = c(1950280, 40500, 10000, 50280, 12000, 5000, 4200),
  level         = c("Tertiary", "Secondary", "Primary", "Secondary", "Secondary", "Primary", "Primary")
)
```

Here is this data frame:  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(hosp_info, rownames = FALSE, options = list(pageLength = nrow(hosp_info)))
```





<!-- ======================================================= -->
### Pre-cleaning {.unnumbered}

Traditional joins (non-probabilistic) are case-sensitive and require exact character matches between values in the two data frames. To demonstrate some of the cleaning steps you might need to do before initiating a join, we will clean and align the `linelist_mini` and `hosp_info` datasets now.  

**Identify differences**  

We need the values of the `hosp_name` column in the `hosp_info` data frame to match the values of the `hospital` column in the `linelist_mini` data frame.  

Here are the values in the `linelist_mini` data frame, printed with the **base** R function `unique()`:  

```{r}
unique(linelist_mini$hospital)
```

and here are the values in the `hosp_info` data frame:  

```{r}
unique(hosp_info$hosp_name)
```

You can see that while some of the hospitals exist in both data frames, there are many differences in spelling.  



**Align values**  

We begin by cleaning the values in the `hosp_info` data frame. As explained in the [Cleaning data and core functions] page, we can re-code values with logical criteria using **dplyr**'s `case_when()` function. For the four hospitals that exist in both data frames we change the values to align with the values in `linelist_mini`. The other hospitals we leave the values as they are (`TRUE ~ hosp_name`).   

<span style="color: orange;">**_CAUTION:_** Typically when cleaning one should create a new column (e.g. `hosp_name_clean`), but for ease of demonstration we show modification of the old column</span>

```{r}
hosp_info <- hosp_info %>% 
  mutate(
    hosp_name = case_when(
      # criteria                         # new value
      hosp_name == "military"          ~ "Military Hospital",
      hosp_name == "port"              ~ "Port Hospital",
      hosp_name == "St. Mark's"        ~ "St. Mark's Maternity Hospital (SMMH)",
      hosp_name == "central hospital"  ~ "Central Hospital",
      TRUE                             ~ hosp_name
      )
    )
```

The hospital names that appear in both data frames are aligned. There are two hospitals in `hosp_info` that are not present in `linelist_mini` - we will deal with these later, in the join.  

```{r}
unique(hosp_info$hosp_name)
```

Prior to a join, it is often easiest to convert a column to all lowercase or all uppercase. If you need to convert all values in a column to UPPER or lower case, use `mutate()` and wrap the column with one of these functions from **stringr**, as shown in the page on [Characters and strings].  

`str_to_upper()`  
`str_to_upper()`  
`str_to_title()`  




<!-- ======================================================= -->
## **dplyr** joins { }

The **dplyr** package offers several different join functions. **dplyr** is included in the **tidyverse** package. These join functions are described below, with simple use cases.  

Many thanks to [https://github.com/gadenbuie](https://github.com/gadenbuie/tidyexplain/tree/master/images) for the informative gifs!  




<!-- ======================================================= -->
### General syntax {.unnumbered}

The join commands can be run as standalone commands to join two data frames into a new object, or they can be used within a pipe chain (`%>%`) to merge one data frame into another as it is being cleaned or otherwise modified.  

In the example below, the function `left_join()` is used as a standalone command to create the a new `joined_data` data frame. The inputs are data frames 1 and 2 (`df1` and `df2`). The first data frame listed is the baseline data frame, and the second one listed is joined *to* it.  

The third argument `by = ` is where you specify the columns in each data frame that will be used to aligns the rows in the two data frames. If the names of these columns are different, provide them within a `c()` vector as shown below, where the rows are matched on the basis of common values between the column `ID` in `df1` and the column `identifier` in `df2`.   

```{r, eval=F}
# Join based on common values between column "ID" (first data frame) and column "identifier" (second data frame)
joined_data <- left_join(df1, df2, by = c("ID" = "identifier"))
```

If the `by` columns in both data frames have the exact same name, you can just provide this one name, within quotes.  

```{r, eval=F}
# Joint based on common values in column "ID" in both data frames
joined_data <- left_join(df1, df2, by = "ID")
```

If you are joining the data frames based on common values across multiple fields, list these fields within the `c()` vector. This example joins rows if the values in three columns in each dataset align exactly.  

```{r, eval=F}
# join based on same first name, last name, and age
joined_data <- left_join(df1, df2, by = c("name" = "firstname", "surname" = "lastname", "Age" = "age"))
```


The join commands can also be run within a pipe chain. This will modify the data frame being piped. 

In the example below, `df1` is is passed through the pipes, `df2` is joined to it, and `df` is thus modified and re-defined.  

```{r eval=F}
df1 <- df1 %>%
  filter(date_onset < as.Date("2020-03-05")) %>% # miscellaneous cleaning 
  left_join(df2, by = c("ID" = "identifier"))    # join df2 to df1
```


<span style="color: orange;">**_CAUTION:_** Joins are case-specific! Therefore it is useful to convert all values to lowercase or uppercase prior to joining. See the page on characters/strings.</span>





<!-- ======================================================= -->
### Left and right joins {.unnumbered}  

**A left or right join is commonly used to add information to a data frame** - new information is added only to rows that already existed in the baseline data frame. These are common joins in epidemiological work as they are used to add information from one dataset into another. 

In using these joins, the written order of the data frames in the command is important*.  

* In a *left join*, the *first* data frame written is the baseline  
* In a *right join*, the *second* data frame written is the baseline  

**All rows of the baseline data frame are kept.** Information in the other (secondary) data frame is joined to the baseline data frame *only if there is a match via the identifier column(s)*. In addition:  

* Rows in the secondary data frame that do not match are dropped.  
* If there are many baseline rows that match to one row in the secondary data frame (many-to-one), the secondary information is added to *each matching baseline row*.  
* If a baseline row matches to multiple rows in the secondary data frame (one-to-many), all combinations are given, meaning *new rows may be added to your returned data frame!*  

Animated examples of left and right joins ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
knitr::include_graphics(here::here("images", "right-join.gif"))
```

**Example**  

Below is the output of a `left_join()` of `hosp_info` (secondary data frame, [view here](#joins_hosp_info))  *into* `linelist_mini` (baseline data frame, [view here](#joins_llmini)). The original `linelist_mini` has ` nrow(linelist_mini)` rows. The modified `linelist_mini` is displayed. Note the following:  

* Two new columns, `catchment_pop` and `level` have been added on the left side of `linelist_mini`  
* All original rows of the baseline data frame `linelist_mini` are kept  
* Any original rows of `linelist_mini` for "Military Hospital" are duplicated because it matched to *two* rows in the secondary data frame, so both combinations are returned  
* The join identifier column of the secondary dataset (`hosp_name`) has disappeared because it is redundant with the identifier column in the primary dataset (`hospital`)  
* When a baseline row did not match to any secondary row (e.g. when `hospital` is "Other" or "Missing"), `NA` (blank) fills in the columns from the secondary data frame  
* Rows in the secondary data frame with no match to the baseline data frame ("sisters" and "ignace" hospitals) were dropped  


```{r, eval=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```





#### "Should I use a right join, or a left join?" {.unnumbered}  

To answer the above question, ask yourself "which data frame should retain all of its rows?" - use this one as the baseline. A *left join* keep all the rows in the first data frame written in the command, whereas a *right join* keeps all the rows in the second data frame.  

The two commands below achieve the same output - 10 rows of `hosp_info` joined *into* a `linelist_mini` baseline, but they use different joins. The result is that the column order will differ based on whether `hosp_info` arrives from the right (in the left join) or arrives from the left (in the right join). The order of the rows may also shift accordingly. But both of these consequences can be subsequently addressed, using `select()` to re-order columns or `arrange()` to sort rows.  

```{r, eval=F}
# The two commands below achieve the same data, but with differently ordered rows and columns
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name"))
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital"))
```

Here is the result of `hosp_info` into `linelist_mini` via a left join (new columns incoming from the right)

```{r message=FALSE, echo=F}
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Here is the result of `hosp_info` into  `linelist_mini` via a right join (new columns incoming from the left)  

```{r message=FALSE, echo=F}
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Also consider whether your use-case is within a pipe chain (`%>%`). If the dataset in the pipes is the baseline, you will likely use a left join to add data to it.


<!-- ======================================================= -->
### Full join {.unnumbered} 

**A full join is the most *inclusive* of the joins** - it returns all rows from both data frames.  

If there are any rows present in one and not the other (where no match was found), the data frame will include them and become longer. `NA` missing values are used to fill-in any gaps created. As you join, watch the number of columns and rows carefully to troubleshoot case-sensitivity and exact character matches. 

The "baseline" data frame is the one written first in the command. Adjustment of this will not impact which records are returned by the join, but it can impact the resulting column order, row order, and which identifier columns are retained.  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "full-join.gif"))
```

Animated example of a full join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

**Example**  

Below is the output of a `full_join()` of `hosp_info` (originally ` nrow(hosp_info)`, [view here](#joins_hosp_info))  *into* `linelist_mini` (originally ` nrow(linelist_mini)`, [view here](#joins_llmini)). Note the following:  

* All baseline rows are kept (`linelist_mini`)  
* Rows in the secondary that do not match to the baseline are kept ("ignace" and "sisters"), with values in the corresponding baseline columns  `case_id` and `onset` filled in with missing values  
* Likewise, rows in the baseline data frame that do not match to the secondary ("Other" and "Missing") are kept, with secondary columns ` catchment_pop` and `level` filled-in with missing values  
* In the case of one-to-many or many-to-one matches (e.g. rows for "Military Hospital"), all possible combinations are returned (lengthening the final data frame)  
* Only the identifier column from the baseline is kept (`hospital`)  


```{r, eval=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 15))
```





<!-- ======================================================= -->
### Inner join {.unnumbered} 

**An inner join is the most *restrictive* of the joins** - it returns only rows with matches across both data frames.  
This means that the number of rows in the baseline data frame may actually *reduce*. Adjustment of which data frame is the "baseline" (written first in the function) will not impact which rows are returned, but it will impact the column order, row order, and which identifier columns are retained.   


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "inner-join.gif"))
```

Animated example of an inner join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))


**Example**  

Below is the output of an `inner_join()` of `linelist_mini` (baseline) with `hosp_info` (secondary). Note the following:  

* Baseline rows with no match to the secondary data are removed (rows where `hospital` is "Missing" or "Other")  
* Likewise, rows from the secondary data frame that had no match in the baseline are removed (rows where `hosp_name` is "sisters" or "ignace")  
* Only the identifier column from the baseline is kept (`hospital`)  


```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```


```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```






<!-- ======================================================= -->
### Semi join {.unnumbered} 

A semi join is a "filtering join" which uses another dataset *not to add rows or columns, but to perform filtering*.  

A **semi-join keeps all observations in the baseline data frame that have a match in the secondary data frame** (but does not add new columns nor duplicate any rows for multiple matches). Read more about these "filtering" joins [here](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "semi-join.gif"))
```

Animated example of a semi join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))

As an example, the below code returns rows from the `hosp_info` data frame that have matches in `linelist_mini` based on hospital name.  

```{r}
hosp_info %>% 
  semi_join(linelist_mini, by = c("hosp_name" = "hospital"))
```



<!-- ======================================================= -->
### Anti join {.unnumbered} 

**The anti join is another "filtering join" that returns rows in the baseline data frame that *do not* have a match in the secondary data frame.**  

Read more about filtering joins [here](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0).  

Common scenarios for an anti-join include identifying records not present in another data frame, troubleshooting spelling in a join (reviewing records that *should have* matched), and examining records that were excluded after another join.  

**As with `right_join()` and `left_join()`, the *baseline* data frame (listed first) is important**. The returned rows are from the baseline data frame only. Notice in the gif below that row in the secondary data frame (purple row 4) is not returned even though it does not match with the baseline.  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "anti-join.gif"))
```

Animated example of an anti join ([image source](https://github.com/gadenbuie/tidyexplain/tree/master/images))


#### Simple `anti_join()` example {.unnumbered}  

For a simple example, let's find the `hosp_info` hospitals that do not have any cases present in `linelist_mini`. We list `hosp_info` first, as the baseline data frame. The hospitals which are not present in `linelist_mini` are returned.  

```{r, eval=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital"))
```

```{r message=FALSE, echo=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```


#### Complex `anti_join()` example {.unnumbered}  

For another example, let us say we ran an `inner_join()` between `linelist_mini` and `hosp_info`. This returns only a subset of the original `linelist_mini` records, as some are not present in `hosp_info`.  

```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```

To review the `linelist_mini` records that were excluded during the inner join, we can run an anti-join with the same settings (`linelist_mini` as the baseline).  

```{r, eval = F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```


To see the `hosp_info` records that were excluded in the inner join, we could also run an anti-join with `hosp_info` as the baseline data frame.  



<!-- ======================================================= -->
## Probabalistic matching { }

If you do not have a unique identifier common across datasets to join on, consider using a probabilistic matching algorithm. This would find matches between records based on similarity (e.g. Jaro–Winkler string distance, or numeric distance).  Below is a simple example using the package **fastLink** .  

**Load packages**  

```{r}
pacman::p_load(
  tidyverse,      # data manipulation and visualization
  fastLink        # record matching
  )
```


Here are two small example datasets that we will use to demonstrate the probabilistic matching (`cases` and `test_results`):  

Here is the code used to make the datasets:  


```{r}
# make datasets

cases <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural"
)

results <- tribble(
  ~gender,  ~first,     ~middle,     ~last,          ~yr, ~mon, ~day, ~district, ~result,
  "M",      "Amir",     NA,          "Khan",         1989, 11,   22,  "River", "positive",
  "M",      "Tony",   "B",         "Smith",          1970, 09,   19,  "River", "positive",
  "F",      "Maria",    "Contreras", "Rodriguez",    1972, 04,   15,  "Cty",   "negative",
  "F",      "Betty",    "Castel",   "Chase",        1954,  03,   30,  "City",  "positive",
  "F",      "Andrea",   NA,          "Kumaraswamy",  2001, 01,   05,  "Rural", "positive",      
  "F",      "Caroline", NA,          "Wang",         1988, 12,   11,  "Rural", "negative",
  "F",      "Trang",    NA,          "Nguyen",       1981, 06,   10,  "Rural", "positive",
  "M",      "Olivier" , "Laurent",   "De Bordeaux",  NA,   NA,   NA,  "River", "positive",
  "M",      "Mike",     "Murphy",    "O'Callaghan",  1969, 04,   12,  "Rural", "negative",
  "F",      "Cassidy",  "Jones",     "Davis",        1980, 07,   02,  "City",  "positive",
  "M",      "Mohammad", NA,          "Ali",          1942, 01,   17,  "City",  "negative",
  NA,       "Jose",     "Sanchez",   "Lopez",        1995, 01,   06,  "City",  "negative",
  "M",      "Abubakar", NA,          "Abullahi",     1960, 01,   01,  "River", "positive",
  "F",      "Maria",    "Salinas",   "Contreras",    1955, 03,   03,  "River", "positive"
  )

```


**The `cases` dataset has 9 records** of patients who are awaiting test results.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(cases, rownames = FALSE, options = list(pageLength = nrow(cases), scrollX=T), class = 'white-space: nowrap')
```



**The `test_results` dataset** has 14 records and contains the column `result`, which we want to add to the records in `cases` based on probabilistic matching of records.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(results, rownames = FALSE, options = list(pageLength = nrow(results), scrollX=T), class = 'white-space: nowrap')
```

### Probabilistic matching {.unnumbered}  

The `fastLink()` function from the **fastLink** package can be used to apply a matching algorithm. Here is the basic information. You can read more detail by entering `?fastLink` in your console.  

* Define the two data frames for comparison to arguments `dfA = ` and `dfB = `  
* In `varnames = ` give all column names to be used for matching. They must all exist in both `dfA` and `dfB`.  
* In `stringdist.match = ` give columns from those in `varnames` to be evaluated on string "distance".  
* In `numeric.match = ` give columns from those in `varnames` to be evaluated on numeric distance.  
* Missing values are ignored  
* By default, each row in either data frame is matched to at most one row in the other data frame. If you want to see all the evaluated matches, set `dedupe.matches = FALSE`. The deduplication is done using Winkler's linear assignment solution.  

*Tip: split one date column into three separate numeric columns using `day()`, `month()`, and `year()` from **lubridate** package*  

The default threshold for matches is 0.94 (`threshold.match = `) but you can adjust it higher or lower. If you define the threshold, consider that higher thresholds could yield more false-negatives (rows that do not match which actually should match) and likewise a lower threshold could yield more false-positive matches.  

Below, the data are matched on string distance across the name and district columns, and on numeric distance for year, month, and day of birth. A match threshold of 95% probability is set.  


```{r, message=F, warning=F}
fl_output <- fastLink::fastLink(
  dfA = cases,
  dfB = results,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district"),
  stringdist.match = c("first", "middle", "last", "district"),
  numeric.match = c("yr", "mon", "day"),
  threshold.match = 0.95)
```

**Review matches**  

We defined the object returned from `fastLink()` as `fl_output`. It is of class `list`, and it actually contains several data frames within it, detailing the results of the matching. One of these data frames is `matches`, which contains the most likely matches across `cases` and `results`. You can access this "matches" data frame with `fl_output$matches`. Below, it is saved as `my_matches` for ease of accessing later.    

When `my_matches` is printed, you see two column vectors: the pairs of row numbers/indices (also called "rownames") in `cases` ("inds.a") and in `results` ("inds.b") representing the best matches. If a row number from a datafrane is missing, then no match was found in the other data frame at the specified match threshold.    

```{r}
# print matches
my_matches <- fl_output$matches
my_matches
```

Things to note:  

* Matches occurred despite slight differences in name spelling and dates of birth:  
  * "Tony B. Smith" matched to "Anthony B Smith"  
  * "Maria Rodriguez" matched to "Marialisa Rodrigues"  
  * "Betty Chase" matched to "Elizabeth Chase"  
  * "Olivier Laurent De Bordeaux" matched to "Oliver Laurent De Bordow" (missing date of birth ignored)  
* One row from `cases` (for "Blessing Adebayo", row 9) had no good match in `results`, so it is not present in `my_matches`.  




**Join based on the probabilistic matches**  

To use these matches to join `results` to `cases`, one strategy is:  

1) Use `left_join()` to join `my_matches` to `cases` (matching rownames in `cases` to "inds.a" in `my_matches`)  
2) Then use another `left_join()` to join `results` to `cases` (matching the newly-acquired "inds.b" in `cases` to rownames in `results`)  

Before the joins, we should clean the three data frames:  

* Both `dfA` and `dfB` should have their row numbers ("rowname") converted to a proper column.  
* Both the columns in `my_matches` are converted to class character, so they can be joined to the character rownames  

```{r}
# Clean data prior to joining
#############################

# convert cases rownames to a column 
cases_clean <- cases %>% rownames_to_column()

# convert test_results rownames to a column
results_clean <- results %>% rownames_to_column()  

# convert all columns in matches dataset to character, so they can be joined to the rownames
matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))



# Join matches to dfA, then add dfB
###################################
# column "inds.b" is added to dfA
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))

# column(s) from dfB are added 
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```

As performed using the code above, the resulting data frame `complete` will contain *all* columns from both `cases` and `results`. Many will be appended with suffixes ".x" and ".y", because the column names would otherwise be duplicated.  

```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```

Alternatively, to achieve only the "original" 9 records in `cases` with the new column(s) from `results`, use `select()` on `results` before the joins, so that it contains only rownames and the columns that you want to add to `cases` (e.g. the column `result`).  

```{r}
cases_clean <- cases %>% rownames_to_column()

results_clean <- results %>%
  rownames_to_column() %>% 
  select(rowname, result)    # select only certain columns 

matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))

# joins
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```


```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```


If you want to subset either dataset to only the rows that matched, you can use the codes below:  

```{r}
cases_matched <- cases[my_matches$inds.a,]  # Rows in cases that matched to a row in results
results_matched <- results[my_matches$inds.b,]  # Rows in results that matched to a row in cases
```

Or, to see only the rows that did **not** match:  

```{r}
cases_not_matched <- cases[!rownames(cases) %in% my_matches$inds.a,]  # Rows in cases that did NOT match to a row in results
results_not_matched <- results[!rownames(results) %in% my_matches$inds.b,]  # Rows in results that did NOT match to a row in cases
```


### Probabilistic deduplication {.unnumbered}  

Probabilistic matching can be used to deduplicate a dataset as well. See the page on deduplication for other methods of deduplication.  

Here we began with the `cases` dataset, but are now calling it `cases_dup`, as it has 2 additional rows that could be duplicates of previous rows:
See "Tony" with "Anthony", and "Marialisa Rodrigues" with "Maria Rodriguez".  

```{r, echo=F}
## Add duplicates
#cases_dup <- rbind(cases, cases[sample(1:nrow(cases), 3, replace = FALSE),])

cases_dup <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural",
  "M",     "Tony",   "B.",        "Smith",         1970, 09, 19,      "River", 
  "F",     "Maria",  "Contreras", "Rodriguez",     1972, 04, 15,      "River",
)

```

```{r message=FALSE, echo=F}
DT::datatable(cases_dup, rownames = FALSE, options = list(pageLength = nrow(cases_dup)))
```


Run `fastLink()` like before, but compare the `cases_dup` data frame to itself. When the two data frames provided are identical, the function assumes you want to de-duplicate. Note we do not specify `stringdist.match = ` or `numeric.match = ` as we did previously.  

```{r, message = F, warning = F}
## Run fastLink on the same dataset
dedupe_output <- fastLink(
  dfA = cases_dup,
  dfB = cases_dup,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district")
)
```

Now, you can review the potential duplicates with `getMatches()`. Provide the data frame as both `dfA = ` and `dfB = `, and provide the output of the `fastLink()` function as `fl.out = `.  `fl.out` must be of class `fastLink.dedupe`, or in other words, the result of `fastLink()`.  


```{r}
## Run getMatches()
cases_dedupe <- getMatches(
  dfA = cases_dup,
  dfB = cases_dup,
  fl.out = dedupe_output)
```

See the right-most column, which indicates the duplicate IDs - the final two rows are identified as being likely duplicates of rows 2 and 3.  

```{r message=FALSE, echo=F}
DT::datatable(cases_dedupe, rownames = FALSE, options = list(pageLength = nrow(cases_dedupe)))
```

To return the row numbers of rows which are likely duplicates, you can count the number of rows per unique value in the `dedupe.ids` column, and then filter to keep only those with more than one row. In this case this leaves rows 2 and 3.  

```{r}
cases_dedupe %>% 
  count(dedupe.ids) %>% 
  filter(n > 1)
```

To inspect the whole rows of the likely duplicates, put the row number in this command:  

```{r}
# displays row 2 and all likely duplicates of it
cases_dedupe[cases_dedupe$dedupe.ids == 2,]   
```



## Binding and aligning  

Another method of combining two data frames is "binding" them together. You can also think of this as "appending" or "adding" rows or columns.  

This section will also discuss how to "align" the order of rows of one data frame to the order in another data frame. This topic is discussed below in the section on Binding columns.  



### Bind rows {.unnumbered}

To bind rows of one data frame to the bottom of another data frame, use `bind_rows()` from **dplyr**. It is very inclusive, so any column present in either data frame will be included in the output. A few notes:  

* Unlike the **base** R version `row.bind()`, **dplyr**'s `bind_rows()` does not require that the order of columns be the same in both data frames. As long as the column names are spelled identically, it will align them correctly.  
* You can optionally specify the argument `.id = `. Provide a character column name. This will produce a new column that serves to identify which data frame each row originally came from.  
* You can use `bind_rows()` on a `list` of similarly-structured data frames to combine them into one data frame. See an example in the [Iteration, loops, and lists] page involving the import of multiple linelists with **purrr**.  

One common example of row binding is to bind a "total" row onto a descriptive table made with **dplyr**'s `summarise()` function. Below we create a table of case counts and median CT values by hospital with a total row.  

The function `summarise()` is used on data grouped by hospital to return a summary data frame by hospital. But the function `summarise()` does not automatically produce a "totals" row, so we create it by summarising the data *again*, but with the data not grouped by hospital. This produces a second data frame of just one row. We can then bind these data frames together to achieve the final table.  

See other worked examples like this in the [Descriptive tables] and [Tables for presentation] pages.  


```{r}
# Create core table
###################
hosp_summary <- linelist %>% 
  group_by(hospital) %>%                        # Group data by hospital
  summarise(                                    # Create new summary columns of indicators of interest
    cases = n(),                                  # Number of rows per hospital-outcome group     
    ct_value_med = median(ct_blood, na.rm=T))     # median CT value per group
```

Here is the `hosp_summary` data frame:  

```{r message=FALSE, echo=F}
DT::datatable(hosp_summary, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Create a data frame with the "total" statistics (*not grouped by hospital*). This will return just one row.  

```{r}
# create totals
###############
totals <- linelist %>% 
  summarise(
    cases = n(),                               # Number of rows for whole dataset     
    ct_value_med = median(ct_blood, na.rm=T))  # Median CT for whole dataset
```

And below is that `totals` data frame. Note how there are only two columns. These columns are also in `hosp_summary`, but there is one column in `hosp_summary` that is not in `totals` (`hospital`).  

```{r message=FALSE, echo=F}
DT::datatable(totals, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Now we can bind the rows together with `bind_rows()`.  

```{r}
# Bind data frames together
combined <- bind_rows(hosp_summary, totals)
```

Now we can view the result. See how in the final row, an empty `NA` value fills in for the column `hospital` that was not in `hosp_summary`. As explained in the [Tables for presentation] page, you could "fill-in" this cell with "Total" using `replace_na()`.  

```{r message=FALSE, echo=F}
DT::datatable(combined, rownames = FALSE, options = list(pageLength = nrow(10)))
```


### Bind columns {.unnumbered}

There is a similar **dplyr** function `bind_cols()` which you can use to combine two data frames sideways. Note that rows are matched to each other *by position* (not like a *join* above) - for example the 12th row in each data frame will be aligned.  

For an example, we bind several summary tables together. In order to do this, we also demonstrate how to re-arrange the order of rows in one data frame to match the order in another data frame, with `match()`.    

Here we define `case_info` as a summary data frame of linelist cases, by hospital, with the number of cases and the number of deaths.


```{r}
# Case information
case_info <- linelist %>% 
  group_by(hospital) %>% 
  summarise(
    cases = n(),
    deaths = sum(outcome == "Death", na.rm=T)
  )
```

```{r message=FALSE, echo=F}
DT::datatable(case_info, rownames = FALSE, options = list(pageLength = nrow(10)))
```

And let's say that here is a different data frame `contact_fu` containing information on the percent of exposed contacts investigated and "followed-up", again by hospital. 

```{r}
contact_fu <- data.frame(
  hospital = c("St. Mark's Maternity Hospital (SMMH)", "Military Hospital", "Missing", "Central Hospital", "Port Hospital", "Other"),
  investigated = c("80%", "82%", NA, "78%", "64%", "55%"),
  per_fu = c("60%", "25%", NA, "20%", "75%", "80%")
)
```

```{r message=FALSE, echo=F}
DT::datatable(contact_fu, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Note that the hospitals are the same, but are in different orders in each data frame. The easiest solution would be to use a `left_join()` on the `hospital` column, but you could also use `bind_cols()` with one extra step.  

#### Use `match()` to align ordering {.unnumbered}  

Because the row orders are different, a simple `bind_cols()` command would result in a mis-match of data. To fix this we can use `match()` from **base** R to align the rows of a data frame in the same order as in another. We assume for this approach that there are no duplicate values in either data frame.  

When we use `match()`, the syntax is `match(TARGET ORDER VECTOR, DATA FRAME COLUMN TO CHANGE)`, where the first argument is the desired order (either a stand-alone vector, or in this case a column in a data frame), and the second argument is the data frame column in the data frame that will be re-ordered. The output of `match()` is a vector of numbers representing the correct position ordering. You can read more with `?match`.  

```{r}
match(case_info$hospital, contact_fu$hospital)
```

You can use this numeric vector to re-order the data frame - place it within subset brackets `[ ]` *before the comma*. Read more about **base** R bracket subset syntax in the [R basics] page. The command below creates a new data frame, defined as the old one in which the rows are ordered in the numeric vector above.  

```{r}
contact_fu_aligned <- contact_fu[match(case_info$hospital, contact_fu$hospital),]
```


```{r message=FALSE, echo=F}
DT::datatable(contact_fu_aligned, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Now we can bind the data frame columns together, with the correct row order. Note that some columns are duplicated and will require cleaning with `rename()`. Read more aboout `bind_rows()` [here](https://dplyr.tidyverse.org/reference/bind.html).  

```{r}
bind_cols(case_info, contact_fu)
```

A **base** R alternative to `bind_cols` is `cbind()`, which performs the same operation.  




<!-- ======================================================= -->
## Resources { }

The [tidyverse page on joins](https://dplyr.tidyverse.org/reference/join.html)  

The [R for Data Science page on relational data](https://r4ds.had.co.nz/relational-data.html)  

Th [tidyverse page on dplyr](https://dplyr.tidyverse.org/reference/bind.html) on binding  

A vignette on [fastLink](https://github.com/kosukeimai/fastLink) at the package's Github page  

Publication describing methodology of [fastLink](https://imai.fas.harvard.edu/research/files/linkage.pdf)  

Publication describing [RecordLinkage package](https://journal.r-project.org/archive/2010/RJ-2010-017/RJ-2010-017.pdf)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/joining_matching.Rmd-->


# De-duplication {}  

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "deduplication.png"))
```

This page covers the following de-duplication techniques:  

1. Identifying and removing duplicate rows  
2. "Slicing" rows to keep only certain rows (e.g. min or max) from each group of rows  
3. "Rolling-up", or combining values from multiple rows into one row  


<!-- ======================================================= -->
## Preparation { }


### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,   # deduplication, grouping, and slicing functions
  janitor,     # function for reviewing duplicates
  stringr)      # for string searches, can be used in "rolling-up" values
```

### Import data {.unnumbered}

For demonstration, we will use an example dataset that is created with the R code below.  

The data are records of COVID-19 phone encounters, including encounters with contacts and with cases. The columns include `recordID` (computer-generated), `personID`, `name`, `date` of encounter, `time` of encounter, the `purpose` of the encounter (either to interview as a case or as a contact), and `symptoms_ever` (whether the person in that encounter reported *ever* having symptoms).  

Here is the code to create the `obs` dataset:  

```{r}
obs <- data.frame(
  recordID  = c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18),
  personID  = c(1,1,2,2,3,2,4,5,6,7,2,1,3,3,4,5,5,7,8),
  name      = c("adam", "adam", "amrish", "amrish", "mariah", "amrish", "nikhil", "brian", "smita", "raquel", "amrish",
                "adam", "mariah", "mariah", "nikhil", "brian", "brian", "raquel", "natalie"),
  date      = c("1/1/2020", "1/1/2020", "2/1/2020", "2/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020","5/1/2020", "2/1/2020",
                "5/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "7/1/2020", "7/1/2020", "7/1/2020"),
  time      = c("09:00", "09:00", "14:20", "14:20", "12:00", "16:10", "13:01", "15:20", "14:20", "12:30", "10:24",
                "09:40", "07:25", "08:32", "15:36", "15:31", "07:59", "11:13", "17:12"),
  encounter = c(1,1,1,1,1,3,1,1,1,1,2,
                2,2,3,2,2,3,2,1),
  purpose   = c("contact", "contact", "contact", "contact", "case", "case", "contact", "contact", "contact", "contact", "contact",
                "case", "contact", "contact", "contact", "contact", "case", "contact", "case"),
  symptoms_ever = c(NA, NA, "No", "No", "No", "Yes", "Yes", "No", "Yes", NA, "Yes",
                    "No", "No", "No", "Yes", "Yes", "No","No", "No")) %>% 
  mutate(date = as.Date(date, format = "%d/%m/%Y"))
```


#### Here is the data frame {#dedup_data .unnumbered}  

Use the filter boxes along the top to review the encounters for each person.  

```{r message=FALSE, echo=F}
DT::datatable(obs, rownames = FALSE, filter = "top", options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```


A few things to note as you review the data:  

* The first two records are 100% complete duplicates including duplicate `recordID` (must be a computer glitch!)  
* The second two rows are duplicates, in all columns *except for `recordID`*  
* Several people had multiple phone encounters, at various dates and times, and as contacts and/or cases  
* At each encounter, the person was asked if they had **ever** had symptoms, and some of this information is missing.  


And here is a quick summary of the people and the purposes of their encounters, using `tabyl()` from **janitor**:  

```{r}
obs %>% 
  tabyl(name, purpose)
```
<!-- ======================================================= -->
## Deduplication { }


This section describes how to review and remove duplicate rows in a data frame. It also show how to handle duplicate elements in a vector.  


<!-- ======================================================= -->
### Examine duplicate rows {.unnumbered}  


To quickly review rows that have duplicates, you can use `get_dupes()` from the **janitor** package. *By default*, all columns are considered when duplicates are evaluated - rows returned by the function are 100% duplicates considering the values in *all* columns.  

In the `obs` data frame, the first two rows are *100% duplicates* - they have the same value in every column (including the `recordID` column, which is *supposed* to be unique - it must be some computer glitch). The returned data frame automatically includes a new column `dupe_count` on the right side, showing the number of rows with that combination of duplicate values. 

```{r, eval=F}
# 100% duplicates across all columns
obs %>% 
  janitor::get_dupes()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes() %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data)  

However, if we choose to ignore `recordID`, the 3rd and 4th rows rows are also duplicates of each other. That is, they have the same values in all columns *except* for `recordID`. You can specify specific columns to be ignored in the function using a `-` minus symbol.  

```{r, eval=F}
# Duplicates when column recordID is not considered
obs %>% 
  janitor::get_dupes(-recordID)         # if multiple columns, wrap them in c()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(-recordID) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

You can also positively specify the columns to consider. Below, only rows that have the same values in the `name` and `purpose` columns are returned. Notice how "amrish" now has `dupe_count` equal to 3 to reflect his three "contact" encounters.  

*Scroll left for more rows**  

```{r, eval=F}
# duplicates based on name and purpose columns ONLY
obs %>% 
  janitor::get_dupes(name, purpose)
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(name, purpose) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 7, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  

See `?get_dupes` for more details, or see this [online reference](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  






<!-- ======================================================= -->
### Keep only unique rows  {.unnumbered}


To keep only unique rows of a data frame, use `distinct()` from **dplyr** (as demonstrated in the [Cleaning data and core functions] page). Rows that are duplicates are removed such that only the first of such rows is kept. By default, "first" means the highest `rownumber` (order of rows top-to-bottom). Only unique rows remain.  

In the example below, we run `distinct()` such that the column `recordID` is excluded from consideration - thus **two duplicate rows are removed**. The first row (for "adam") was 100% duplicated and has been removed. Also row 3 (for "amrish") was a duplicate in every column *except* `recordID` (which is not being considered) and so is also removed. The `obs` dataset n is now ` nrow(obs)-2`, not ` nrow(obs)` rows).  

*Scroll to the left to see the entire data frame*  


```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) 

# if outside pipes, include the data as first argument 
# distinct(obs)
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">**_CAUTION:_** If using `distinct()` on grouped data, the function will apply to each group.</span>


**Deduplicate based on specific columns**  

You can also specify columns to be the basis for de-duplication. In this way, the de-duplication only applies to rows that are duplicates within the specified columns. Unless you set `.keep_all = TRUE`, all columns not mentioned will be dropped.  

In the example below, the de-duplication only applies to rows that have identical values for `name` and `purpose` columns. Thus, "brian" has only 2 rows instead of 3 - his *first* "contact" encounter and his only "case" encounter. To adjust so that brian's *latest* encounter of each purpose is kept, see the tab on Slicing within groups.  

*Scroll to the left to see the entire data frame*  

```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name)                                  # arrange for easier viewing
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  

<!-- ======================================================= -->
### Deduplicate elements in a vector {.unnumbered}  


The function `duplicated()` from **base** R will evaluate a vector (column) and return a logical vector of the same length (TRUE/FALSE). The first time a value appears, it will return FALSE (not a duplicate), and subsequent times that value appears it will return TRUE. Note how `NA` is treated the same as any other value.    

```{r}
x <- c(1, 1, 2, NA, NA, 4, 5, 4, 4, 1, 2)
duplicated(x)
```

To return only the duplicated elements, you can use brackets to subset the original vector: 

```{r}
x[duplicated(x)]
```

To return only the unique elements, use `unique()` from **base** R. To remove `NA`s from the output, nest `na.omit()` within `unique()`.  

```{r}
unique(x)           # alternatively, use x[!duplicated(x)]
unique(na.omit(x))  # remove NAs 
```


<!-- ======================================================= -->
### Using **base** R {.unnumbered}

**To return duplicate rows**  

In **base** R, you can also see which rows are 100% duplicates in a data frame `df` with the command `duplicated(df)` (returns a logical vector of the rows).  

Thus, you can also use the base subset `[ ]` on the data frame to see the *duplicated* rows with `df[duplicated(df),]` (don't forget the comma, meaning that you want to see all columns!). 

**To return unique rows**  

See the notes above. To see the *unique* rows you add the logical negator `!` in front of the `duplicated()` function:  
`df[!duplicated(df),]`  


**To return rows that are duplicates of only certain columns**  

Subset the `df` that is *within the `duplicated()` parentheses*, so this function will operate on only certain columns of the `df`.  

To specify the columns, provide column numbers or names after a comma (remember, all this is *within* the `duplicated()` function).  

Be sure to keep the comma `,` *outside* after the `duplicated()` function as well! 

For example, to evaluate only columns 2 through 5 for duplicates:  `df[!duplicated(df[, 2:5]),]`  
To evaluate only columns `name` and `purpose` for duplicates: `df[!duplicated(df[, c("name", "purpose)]),]`  





<!-- ======================================================= -->
## Slicing { }


To "slice" a data frame to apply a filter on the rows by row number/position. This becomes particularly useful if you have multiple rows per functional group (e.g. per "person") and you only want to keep one or some of them. 

The basic `slice()` function accepts numbers and returns rows in those positions. If the numbers provided are positive, only they are returned. If negative, those rows are *not* returned. Numbers must be either all positive or all negative.     

```{r}
obs %>% slice(4)  # return the 4th row
```

```{r}
obs %>% slice(c(2,4))  # return rows 2 and 4
#obs %>% slice(c(2:4))  # return rows 2 through 4
```


See the [original data](#dedup_data). 

There are several variations:  These should be provided with a column and a number of rows to return (to `n = `).  

* `slice_min()` and `slice_max()`  keep only the row(s) with the minimium or maximum value(s) of the specified column. This also works to return the "min" and "max" of ordered factors.    
* `slice_head()` and `slice_tail()` - keep only the *first* or *last* row(s).  
* `slice_sample()`  - keep only a random sample of the rows.  


```{r}
obs %>% slice_max(encounter, n = 1)  # return rows with the largest encounter number
```

Use arguments `n = ` or `prop = ` to specify the number or proportion of rows to keep. If not using the function in a pipe chain, provide the data argument first (e.g. `slice(data, n = 2)`). See `?slice` for more information. 

Other arguments:  

`.order_by = ` used in `slice_min()` and `slice_max()` this is a column to order by before slicing.  
`with_ties = ` TRUE by default, meaning ties are kept.  
`.preserve = ` FALSE by default. If TRUE then the grouping structure is re-calculated after slicing.  
`weight_by = ` Optional, numeric column to weight by (bigger number more likely to get sampled).  Also `replace = ` for whether sampling is done with/without replacement.  

<span style="color: darkgreen;">**_TIP:_** When using `slice_max()` and `slice_min()`, be sure to specify/write the `n = `  (e.g. `n = 2`, not just `2`). Otherwise you may get an error `Error: `...` is not empty.` </span>

<span style="color: black;">**_NOTE:_** You may encounter the function [`top_n()`](https://dplyr.tidyverse.org/reference/top_n.html), which has been superseded by the `slice` functions.</span>

 


<!-- ======================================================= -->
### Slice with groups  {.unnumbered}

The `slice_*()` functions can be very useful if applied to a grouped data frame because the slice operation is performed on each group separately. Use the **function** `group_by()` in conjunction with `slice()` to group the data to take a slice from each group.  

This is helpful for de-duplication if you have multiple rows per person but only want to keep one of them. You first use `group_by()` with key columns that are the same per person, and then use a slice function on a column that will differ among the grouped rows.  

In the example below, to keep only the *latest* encounter *per person*, we group the rows by `name` and then use `slice_max()` with `n = 1` on the `date` column. Be aware! To apply a function like `slice_max()` on dates, the date column must be class Date.   

By default, "ties" (e.g. same date in this scenario) are kept, and we would still get multiple rows for some people (e.g. adam). To avoid this we set `with_ties = FALSE`. We get back only one row per person.  

<span style="color: orange;">**_CAUTION:_** If using `arrange()`, specify `.by_group = TRUE` to have the data arranged within each group.</span>

<span style="color: red;">**_DANGER:_** If `with_ties = FALSE`, the first row of a tie is kept. This may be deceptive. See how for Mariah, she has two encounters on her latest date (6 Jan) and the first (earliest) one was kept. Likely, we want to keep her later encounter on that day. See how to "break" these ties in the next example. </span>  




```{r, eval=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) # if there's a tie (of date), take the first row
```

```{r message=FALSE, echo=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) %>%  # if there's a tie (of date), take the first row
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

Above, for example we can see that only Amrish's row on 5 Jan was kept, and only Brian's row on 7 Jan was kept. See the [original data](#dedup_data).  


**Breaking "ties"**  

Multiple slice statements can be run to "break ties". In this case, if a person has multiple encounters on their latest *date*, the encounter with the latest *time* is kept (`lubridate::hm()` is used to convert the character times to a sortable time class).  
Note how now, the one row kept for "Mariah" on 6 Jan is encounter 3 from 08:32, not encounter 2 at 07:25.  

```{r, eval=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE)
```

```{r message=FALSE, echo=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE) %>% 
  
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

*In the example above, it would also have been possible to slice by `encounter` number, but we showed the slice on `date` and `time` for example purposes.*  

<span style="color: darkgreen;">**_TIP:_** To use `slice_max()` or `slice_min()` on a "character" column, mutate it to an *ordered* factor class!</span>

See the [original data](#dedup_data).  


<!-- ======================================================= -->
### Keep all but mark them  {.unnumbered}

If you want to keep all records but mark only some for analysis, consider a two-step approach utilizing a unique recordID/encounter number:  

1) Reduce/slice the orginal data frame to only the rows for analysis. Save/retain this reduced data frame.  
2) In the original data frame, mark rows as appropriate with `case_when()`, based on whether their record unique identifier (recordID in this example) is present in the reduced data frame.  


```{r}
# 1. Define data frame of rows to keep for analysis
obs_keep <- obs %>%
  group_by(name) %>%
  slice_max(encounter, n = 1, with_ties = FALSE) # keep only latest encounter per person


# 2. Mark original data frame
obs_marked <- obs %>%

  # make new dup_record column
  mutate(dup_record = case_when(
    
    # if record is in obs_keep data frame
    recordID %in% obs_keep$recordID ~ "For analysis", 
    
    # all else marked as "Ignore" for analysis purposes
    TRUE                            ~ "Ignore"))

# print
obs_marked
```


```{r, echo=F}
DT::datatable(obs_marked, rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  

<!-- ======================================================= -->
### Calculate row completeness {.unnumbered} 

Create a column that contains a metric for the row's completeness (non-missingness). This could be helpful when deciding which rows to prioritize over others when de-duplicating/slicing.  

In this example, "key" columns over which you want to measure completeness are saved in a vector of column names.  

Then the new column `key_completeness` is created with `mutate()`. The new value in each row is defined as a calculated fraction: the number of non-missing values in that row among the key columns, divided by the number of key columns.  

This involves the function `rowSums()` from **base** R. Also used is `.`, which within piping refers to the data frame at that point in the pipe (in this case, it is being subset with brackets `[]`).  

*Scroll to the right to see more rows**  

```{r, eval=F}
# create a "key variable completeness" column
# this is a *proportion* of the columns designated as "key_cols" that have non-missing values

key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) 
```

```{r message=FALSE, echo=F}
key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

See the [original data](#dedup_data).  




<!-- ======================================================= -->
## Roll-up values {#str_rollup}


This section describes:  

1) How to "roll-up" values from multiple rows into just one row, with some variations  
2) Once you have "rolled-up" values, how to overwrite/prioritize the values in each cell  

This tab uses the example dataset from the Preparation tab.  



<!-- ======================================================= -->
### Roll-up values into one row {.unnumbered}  

The code example below uses `group_by()` and `summarise()` to group rows by person, and then paste together all unique values within the grouped rows. Thus, you get one summary row per person. A few notes:  

* A suffix is appended to all new columns ("_roll" in this example)  
* If you want to show only unique values per cell, then wrap the `na.omit()` with `unique()`  
* `na.omit()` removes `NA` values, but if this is not desired it can be removed `paste0(.x)`...  



```{r, eval=F}
# "Roll-up" values into one row per group (per "personID") 
cases_rolled <- obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                           # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) # function is defined which combines non-NA values
```

The result is one row per group (`ID`), with entries arranged by date and pasted together. *Scroll to the left to see more rows*    

```{r message=FALSE, echo=F}
# "Roll-up" values into one row per group (per "personID") 
obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                                # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) %>%  # function is defined which combines non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

See the [original data](#dedup_data).  


**This variation shows unique values only:**  

```{r}
# Variation - show unique values only 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) # function is defined which combines unique non-NA values
```

```{r message=FALSE, echo=F}
# Variation - show unique values only 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) %>%  # function is defined which combines unique non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


**This variation appends a suffix to each column.**  
In this case "_roll" to signify that it has been rolled:  

```{r, eval=F}
# Variation - suffix added to column names 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) # _roll is appended to column names
```

```{r message=FALSE, echo=F}
# display the linelist data as a table
# Variation - suffix added to column names 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) %>%  # _roll is appended to column names
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Overwrite values/hierarchy {.unnumbered} 


If you then want to evaluate all of the rolled values, and keep only a specific value (e.g. "best" or "maximum" value), you can use `mutate()` across the desired columns, to implement `case_when()`, which uses `str_detect()` from the **stringr** package to sequentially look for string patterns and overwrite the cell content.  

```{r}
# CLEAN CASES
#############
cases_clean <- cases_rolled %>% 
    
    # clean Yes-No-Unknown vars: replace text with "highest" value present in the string
    mutate(across(c(contains("symptoms_ever")),                     # operates on specified columns (Y/N/U)
             list(mod = ~case_when(                                 # adds suffix "_mod" to new cols; implements case_when()
               
               str_detect(.x, "Yes")       ~ "Yes",                 # if "Yes" is detected, then cell value converts to yes
               str_detect(.x, "No")        ~ "No",                  # then, if "No" is detected, then cell value converts to no
               str_detect(.x, "Unknown")   ~ "Unknown",             # then, if "Unknown" is detected, then cell value converts to Unknown
               TRUE                        ~ as.character(.x)))),   # then, if anything else if it kept as is
      .keep = "unused")                                             # old columns removed, leaving only _mod columns
```


Now you can see in the column `symptoms_ever` that if the person EVER said "Yes" to symptoms, then only "Yes" is displayed.  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(cases_clean, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap')
```


See the [original data](#dedup_data).  


## Probabilistic de-duplication  

Sometimes, you may want to identify "likely" duplicates based on similarity (e.g. string "distance") across several columns such as name, age, sex, date of birth, etc. You can apply a probabilistic matching algorithm to identify likely duplicates.  

See the page on [Joining data] for an explanation on this method. The section on Probabilistic Matching contains an example of applying these algorithms to compare a data frame to *itself*, thus performing probabilistic de-duplication.  



<!-- ======================================================= -->
## Resources { }

Much of the information in this page is adapted from these resources and vignettes online:  

[datanovia](https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/)

[dplyr tidyverse reference](https://dplyr.tidyverse.org/reference/slice.html)  

[cran janitor vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/deduplication.Rmd-->


# Iteration, loops, and lists { }  

Epidemiologists often are faced with repeating analyses on subgroups such as countries, districts, or age groups. These are but a few of the many situations involving *iteration*. Coding your iterative operations using the approaches below will help you perform such repetitive tasks faster, reduce the chance of error, and reduce code length.  

This page will introduce two approaches to iterative operations - using *for loops* and using the package **purrr**.  

1) *for loops* iterate code across a series of inputs, but are less common in R than in other programming languages. Nevertheless, we introduce them here as a learning tool and reference  
2) The **purrr** package is the **tidyverse** approach to iterative operations - it works by "mapping" a function across many inputs (values, columns, datasets, etc.)  

Along the way, we'll show examples like:  

* Importing and exporting multiple files  
* Creating epicurves for multiple jurisdictions  
* Running T-tests for several columns in a data frame  

In the **purrr** [section](#iter_purrr) we will also provide several examples of creating and handling `lists`.  



## Preparation {  }
     
     
### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
     rio,         # import/export
     here,        # file locator
     purrr,       # iteration
     tidyverse    # data management and visualization
)
```


### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.


```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



<!-- ======================================================= -->

## *for loops* {  }

### *for loops* in R {#iter_loops .unnumbered}  

*for loops* are not emphasized in R, but are common in other programming languages. As a beginner, they can be helpful to learn and practice with because they are easier to "explore", "de-bug", and otherwise grasp exactly what is happening for each iteration, especially when you are not yet comfortable writing your own functions.  

You may move quickly through *for loops* to iterating with mapped functions with **purrr** (see [section below](#iter_purrr)).  


### Core components {.unnumbered}   

A *for loop* has three core parts:  
     
1) The **sequence** of items to iterate through  
2) The **operations** to conduct per item in the sequence  
3) The **container** for the results (optional)  

The basic syntax is: `for (item in sequence) {do operations using item}`. Note the parentheses and the curly brackets. The results could be printed to console, or stored in a container R object.   

A simple *for loop* example is below.   

```{r}
for (num in c(1,2,3,4,5)) {  # the SEQUENCE is defined (numbers 1 to 5) and loop is opened with "{"
  print(num + 2)             # The OPERATIONS (add two to each sequence number and print)
}                            # The loop is closed with "}"                            
                             # There is no "container" in this example
```



### Sequence {.unnumbered}  

This is the "for" part of a *for loop* - the operations will run "for" each item in the sequence. The sequence can be a series of values (e.g. names of jurisdictions, diseases, column names, list elements, etc), or it can be a series of consecutive numbers (e.g. 1,2,3,4,5). Each approach has their own utilities, described below.  

The basic structure of a sequence statement is `item in vector`.  

* You can write any character or word in place of "item" (e.g. "i", "num", "hosp", "district", etc.). The value of this "item" changes with each iteration of the loop, proceeding through each value in the vector.  
* The *vector* could be of character values, column names, or perhaps a sequence of numbers - these are the values that will change with each iteration. You can use them within the *for loop* operations using the "item" term.  

**Example: sequence of character values**  

In this example, a loop is performed for each value in a pre-defined character vector of hospital names.  

```{r}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)
hospital_names # print
```

We have chosen the term `hosp` to represent values from the vector `hospital_names`. For the first iteration of the loop, the value of `hosp` will be ` hospital_names[[1]]`. For the second loop it will be ` hospital_names[[2]]`. And so on...  

```{r, eval=F}
# a 'for loop' with character sequence

for (hosp in hospital_names){       # sequence
  
       # OPERATIONS HERE
  }
```

**Example: sequence of column names**  
     
This is a variation on the character sequence above, in which the names of an existing R object are extracted and become the vector. For example, the column names of a data frame. Conveniently, in the operations code of the *for loop*, the column names can be used to *index* (subset) their original data frame  

Below, the sequence is the `names()` (column names) of the `linelist` data frame. Our "item" name is `col`, which will represent each column name as the loops proceeds.  

For purposes of example, we include operations code inside the *for loop*, which is run for every value in the sequence. In this code, the sequence values (column names) are used to *index* (subset) `linelist`, one-at-a-time. As taught in the [R basics] page, double branckets `[[ ]]` are used to subset. The resulting column is passed to `is.na()`, then to `sum()` to produce the number of values in the column that are missing. The result is printed to the console - one number for each column.  

A note on indexing with column names - whenever referencing the column itself *do not just write "col"!* `col` represents just the character column name! To refer to the entire column you must use the column name as an *index* on `linelist` via `linelist[[col]]`.  

```{r}
for (col in names(linelist)){        # loop runs for each column in linelist; column name represented by "col" 
  
  # Example operations code - print number of missing values in column
  print(sum(is.na(linelist[[col]])))  # linelist is indexed by current value of "col"
     
}
```



**Sequence of numbers**  
     
In this approach, the sequence is a series of consecutive numbers. Thus, the value of the "item" is not a character value (e.g. "Central Hospital" or "date_onset") but is a number. This is useful for looping through data frames, as you can use the "item" number inside the *for loop* to index the data frame by *row number*.  

For example, let's say that you want to loop through every row in your data frame and extract certain information. Your "items" would be numeric row numbers. Often, "items" in this case are written as `i`.  

The *for loop* process could be explained in words as "for every item in a sequence of numbers from 1 to the total number of rows in my data frame, do X". For the first iteration of the loop, the value of "item" `i` would be 1. For the second iteration, `i` would be 2, etc.  

Here is what the sequence looks like in code: `for (i in 1:nrow(linelist)) {OPERATIONS CODE}` where `i` represents the "item" and `1:nrow(linelist)` produces a sequence of consecutive numbers from 1 through the number of rows in `linelist`.  


```{r, eval=F}
for (i in 1:nrow(linelist)) {  # use on a data frame
  # OPERATIONS HERE
}  
```

If you want the sequence to be numbers, but you are starting from a vector (not a data frame), use the shortcut `seq_along()` to return a sequence of numbers for each element in the vector. For example, `for (i in seq_along(hospital_names) {OPERATIONS CODE}`.  

The below code actually returns numbers, which would become the value of `i` in their respective loop.     

```{r}
seq_along(hospital_names)  # use on a named vector
```

One advantage of using numbers in the sequence is that is easy to also use the `i` number to index a *container* that stores the loop outputs. There is an example of this in the Operations section below.  

### Operations  {.unnumbered}  

This is code within the curly brackets `{ }` of the *for loop*. You want this code to run for each "item" in the *sequence*. Therefore, be careful that every part of your code that changes by the "item" is correctly coded such that it actually changes! E.g. remember to use `[[ ]]` for indexing.  

In the example below, we iterate through each row in the `linelist`. The `gender` and `age` values of each row are pasted together and stored in the container character vector `cases_demographics`. Note how we also use indexing `[[i]]` to save the loop output to the correct position in the "container" vector.  

```{r}
# create container to store results - a character vector
cases_demographics <- vector(mode = "character", length = nrow(linelist))

# the for loop
for (i in 1:nrow(linelist)){
  
  # OPERATIONS
  # extract values from linelist for row i, using brackets for indexing
  row_gender  <- linelist$gender[[i]]
  row_age     <- linelist$age_years[[i]]    # don't forget to index!
     
  # combine gender-age and store in container vector at indexed location
  cases_demographics[[i]] <- str_c(row_gender, row_age, sep = ",") 

}  # end for loop


# display first 10 rows of container
head(cases_demographics, 10)
```


### Container {.unnumbered}

Sometimes the results of your *for loop* will be printed to the console or RStudio Plots pane. Other times, you will want to store the outputs in a "container" for later use. Such a container could be a vector, a data frame, or even a list.  

It is most efficient to create the container for the results *before* even beginning the *for loop*. In practice, this means creating an empty vector, data frame, or list. These can be created with the functions `vector()` for vectors or lists, or with `matrix()` and `data.frame()` for a data frame. 

**Empty vector**  

Use `vector()` and specify the `mode = ` based on the expected class of the objects you will insert - either "double" (to hold numbers), "character", or "logical". You should also set the `length = ` in advance. This should be the length of your *for loop* sequence.  

Say you want to store the median delay-to-admission for each hospital. You would use "double" and set the length to be the number of expected outputs (the number of unique hospitals in the data set).  

```{r}
delays <- vector(
  mode = "double",                            # we expect to store numbers
  length = length(unique(linelist$hospital))) # the number of unique hospitals in the dataset
```

**Empty data frame**  
     
You can make an empty data frame by specifying the number of rows and columns like this:  
     
```{r, eval=F}
delays <- data.frame(matrix(ncol = 2, nrow = 3))
```


**Empty list**  
     
You may want store some plots created by a *for loop* in a list. A list is like vector, but holds other R objects within it that can be of different classes. Items in a list could be a single number, a dataframe, a vector, and even another list.  

You actually initialize an empty list using the same `vector()` command as above, but with `mode = "list"`. Specify the length however you wish.  

```{r, eval=F}
plots <- vector(mode = "list", length = 16)
```




### Printing {.unnumbered}  

Note that to print from within a *for loop* you will likely need to explicitly wrap with the function `print()`.  

In this example below, the sequence is an explicit character vector, which is used to subset the linelist by hospital. The results are not stored in a container, but rather are printed to console with the `print()` function.    

```{r}
for (hosp in hospital_names){ 
     hospital_cases <- linelist %>% filter(hospital == hosp)
     print(nrow(hospital_cases))
}
```


### Testing your for loop {.unnumbered}

To test your loop, you can run a command to make a temporary assignment of the "item", such as `i <- 10` or `hosp <- "Central Hospital"`. Do this *outside the loop* and then run your operations code only (the code within the curly brackets) to see if the expected results are produced.  




### Looping plots {.unnumbered}

To put all three components together (container, sequence, and operations) let's try to plot an epicurve for each hospital (see page on [Epidemic curves]).  

We can make a nice epicurve of *all* the cases by gender using the **incidence2** package as below:  

```{r, warning=F, message=F}
# create 'incidence' object
outbreak <- incidence2::incidence(   
     x = linelist,                   # dataframe - complete linelist
     date_index = date_onset,        # date column
     interval = "week",              # aggregate counts weekly
     groups = gender,                # group values by gender
     na_as_group = TRUE)             # missing gender is own group

# plot epi curve
plot(outbreak,                       # name of incidence object
     fill = "gender",                # color bars by gender
     color = "black",                # outline color of bars
     title = "Outbreak of ALL cases" # title
     )
```

To produce a separate plot for each hospital's cases, we can put this epicurve code within a *for loop*. 

First, we save a named vector of the unique hospital names, `hospital_names`. The *for loop* will run once for each of these names: `for (hosp in hospital_names)`. Each iteration of the *for loop*, the current hospital name from the vector will be represented as `hosp` for use within the loop.  

Within the loop operations, you can write R code as normal, but use the "item" (`hosp` in this case) knowing that its value will be changing. Within this loop:  
     
* A `filter()` is applied to `linelist`, such that column `hospital` must equal the current value of `hosp`  
* The incidence object is created on the filtered linelist  
* The plot for the current hospital is created, with an auto-adjusting title that uses `hosp`  
* The plot for the current hospital is temporarily saved and then printed  
* The loop then moves onward to repeat with the next hospital in `hospital_names`  

```{r, out.width='50%', message = F}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)

# for each name ("hosp") in hospital_names, create and print the epi curve
for (hosp in hospital_names) {
     
     # create incidence object specific to the current hospital
     outbreak_hosp <- incidence2::incidence(
          x = linelist %>% filter(hospital == hosp),   # linelist is filtered to the current hospital
          date_index = date_onset,
          interval = "week", 
          groups = gender,
          na_as_group = TRUE
     )
     
     # Create and save the plot. Title automatically adjusts to the current hospital
     plot_hosp <- plot(
       outbreak_hosp,
       fill = "gender",
       color = "black",
       title = stringr::str_glue("Epidemic of cases admitted to {hosp}")
     )
     
     # print the plot for the current hospital
     print(plot_hosp)
     
} # end the for loop when it has been run for every hospital in hospital_names 
```



### Tracking progress of a loop {.unnumbered} 

A loop with many iterations can run for many minutes or even hours. Thus, it can be helpful to print the progress to the R console. The `if` statement below can be placed *within* the loop operations to print every 100th number. Just adjust it so that `i` is the "item" in your loop.  

```{r, eval=F}
# loop with code to print progress every 100 iterations
for (i in seq_len(nrow(linelist))){

  # print progress
  if(i %% 100==0){    # The %% operator is the remainder
    print(i)

}
```





<!-- ======================================================= -->
## **purrr** and lists {#iter_purrr}
     
Another approach to iterative operations is the **purrr** package - it is the **tidyverse** approach to iteration.  

If you are faced with performing the same task several times, it is probably worth creating a generalised solution that you can use across many inputs. For example, producing plots for multiple jurisdictions, or importing and combining many files.  

There are also a few other advantages to **purrr** - you can use it with pipes `%>%`, it handles errors better than normal *for loops*, and the syntax is quite clean and simple! If you are using a *for loop*, you can probably do it more clearly and succinctly with **purrr**! 
   
Keep in mind that **purrr** is a *functional programming tool*. That is, the operations that are to be iteratively applied are wrapped up into *functions*. See the [Writing functions] page to learn how to write your own functions.  

**purrr** is also almost entirely based around *lists* and *vectors* - so think about it as applying a function to each element of that list/vector!
     
### Load packages {.unnumbered}  
     
**purrr** is part of the **tidyverse**, so there is no need to install/load a separate package.  

```{r}
pacman::p_load(
     rio,            # import/export
     here,           # relative filepaths
     tidyverse,      # data mgmt and viz
     writexl,        # write Excel file with multiple sheets
     readxl          # import Excel with multiple sheets
)
```


### `map()` {.unnumbered}  

One core **purrr** function is `map()`, which "maps" (applies) a function to each input element of a list/vector you provide.  

The basic syntax is `map(.x = SEQUENCE, .f = FUNCTION, OTHER ARGUMENTS)`. In a bit more detail:  
     
* `.x = ` are the *inputs* upon which the `.f` function will be iteratively applied - e.g. a vector of jurisdiction names, columns in a data frame, or a list of data frames  
* `.f = ` is the *function* to apply to each element of the `.x` input - it could be a function like `print()` that already exists, or a custom function that you define. The function is often written after a tilde `~` (details below). 

A few more notes on syntax:  
     
* If the function needs no further arguments specified, it can be written with no parentheses and no tilde (e.g. `.f = mean`). To provide arguments that will be the same value for each iteration, provide them within `map()` but outside the `.f = ` argument, such as the `na.rm = T` in `map(.x = my_list, .f = mean, na.rm=T)`.  
* You can use `.x` (or simply `.`) *within* the `.f = ` function as a placeholder for the `.x` value of that iteration  
* Use tilde syntax (`~`) to have greater control over the function - write the function as normal with parentheses, such as: `map(.x = my_list, .f = ~mean(., na.rm = T))`. Use this syntax particularly if the value of an argument will change each iteration, or if it is the value `.x` itself (see examples below)  


**The output of using `map()` is a *list*** - a list is an object class like a vector but whose elements can be of different classes. So, a list produced by `map()` could contain many data frames, or many vectors, many single values, or even many lists! There are alternative versions of `map()` explained below that produce other types of outputs (e.g. `map_dfr()` to produce a data frame, `map_chr()` to produce character vectors, and `map_dbl()` to produce numeric vectors).  

#### Example - import and combine Excel sheets {#iter_combined .unnumbered}  

**Let's demonstrate with a common epidemiologist task:** - *You want to import an Excel workbook with case data, but the data are split across different named sheets in the workbook. How do you efficiently import and combine the sheets into one data frame?*  

Let's say we are sent the below Excel workbook. Each sheet contains cases from a given hospital.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "hospital_linelists_excel_sheets.png"))
```

Here is one approach that uses `map()`:  
     
1) `map()` the function `import()` so that it runs for each Excel sheet  
2) Combine the imported data frames into one using `bind_rows()`  
3) Along the way, preserve the original sheet name for each row, storing this information in a new column in the final data frame  

First, we need to extract the sheet names and save them. We provide the Excel workbook's file path to the function `excel_sheets()` from the package **readxl**, which extracts the sheet names. We store them in a character vector called `sheet_names`.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

```

```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")
```

Here are the names:  

```{r}
sheet_names
```

Now that we have this vector of names, `map()` can provide them one-by-one to the function `import()`. In this example, the `sheet_names` are `.x` and `import()` is the function `.f`.  

Recall from the [Import and export] page that when used on Excel workbooks, `import()` can accept the argument `which = ` specifying the sheet to import. Within the `.f` function `import()`, we provide `which = .x`, whose value will change with each iteration through the vector `sheet_names` - first "Central Hospital", then "Military Hospital", etc.  

Of note - because we have used `map()`, the data in each Excel sheet will be saved as a separate data frame within a list. We want each of these list elements (data frames) to have a *name*, so before we pass `sheet_names` to `map()` we pass it through `set_names()` from **purrr**, which ensures that each list element gets the appropriate name.  

We save the output list as `combined`.  

```{r, echo=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x))
```

```{r, eval=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import("hospital_linelists.xlsx", which = .x))
```

When we inspect output, we see that the data from each Excel sheet is saved in the list with a name. This is good, but we are not quite finished.  


```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_list.png"))
```

Lastly, we use the function `bind_rows()` (from **dplyr**) which accepts the list of similarly-structured data frames and combines them into one data frame. To create a new column from the list element *names*, we use the argument `.id = ` and provide it with the desired name for the new column.  

Below is the whole sequence of commands:  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x)) %>% 
  bind_rows(.id = "origin_sheet")
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")  # extract sheet names
 
combined <- sheet_names %>%                                     # begin with sheet names
  purrr::set_names() %>%                                        # set their names
  map(.f = ~import("hospital_linelists.xlsx", which = .x)) %>%  # iterate, import, save in list
  bind_rows(.id = "origin_sheet") # combine list of data frames, preserving origin in new column  
```

And now we have one data frame with a column containing the sheet of origin!  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_df.png"))
```

There are variations of `map()` that you should be aware of. For example, `map_dfr()` returns a data frame, not a list. Thus, we could have used it for the task above and not have had to bind rows. But then we would not have been able to capture which sheet (hospital) each case came from.  

Other variations include `map_chr()`, `map_dbl()`. These are very useful functions for two reasons. Firstly. they automatically convert the output of an iterative function into a vector (not a list). Secondly, they can explicitly control the class that the data comes back in - you ensure that your data comes back as a character vector with `map_chr()`, or numeric vector with `map_dbl()`. Lets return to these later in the section!

The functions `map_at()` and `map_if()` are also very useful for iteration - they allow you to specify which elements of a list you should iterate at! These work by simply applying a vector of indexes/names (in the case of `map_at()`) or a logical test (in the case of `map_if()`).  

Lets use an example where we didn't want to read the first sheet of hospital data. We use `map_at()` instead of `map()`, and specify the `.at = ` argument to `c(-1)` which means to *not* use the first element of `.x`. Alternatively, you can provide a vector of positive numbers, or names, to `.at = ` to specify which elements to use.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclude the first sheet
     map_at(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x),
            .at = c(-1))
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclude the first sheet
     map_at(.f = ~import( "hospital_linelists.xlsx", which = .x),
            .at = c(-1))
```

Note that the first sheet name will still appear as an element of the output list - but it is only a single character name (not a data frame). You would need to remove this element before binding rows. We will cover how to remove and modify list elements in a later section.  


### Split dataset and export {.unnumbered}  

Below, we give an example of how to split a dataset into parts and then use `map()` iteration to export each part as a separate Excel sheet, or as a separate CSV file.  

#### Split dataset {.unnumbered}  

Let's say we have the complete case `linelist` as a data frame, and we now want to create a separate linelist for each hospital and export each as a separate CSV file. Below, we do the following steps:  
     
Use `group_split()` (from **dplyr**) to split the `linelist` data frame by unique values in column `hospital`. The output is a list containing one data frame per hospital subset.  

```{r}
linelist_split <- linelist %>% 
     group_split(hospital)
```

We can run `View(linelist_split)` and see that this list contains 6 data frames ("tibbles"), each representing the cases from one hospital. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split.png"))
```

However, note that the data frames in the list do not have names by default! We want each to have a name, and then to use that name when saving the CSV file.  

One approach to extracting the names is to use `pull()` (from **dplyr**) to extract the `hospital` column from each data frame in the list. Then, to be safe, we convert the values to character and then use `unique()` to get the name for that particular data frame. All of these steps are applied to each data frame via `map()`.  

```{r}
names(linelist_split) <- linelist_split %>%   # Assign to names of listed data frames 
     # Extract the names by doing the following to each data frame: 
     map(.f = ~pull(.x, hospital)) %>%        # Pull out hospital column
     map(.f = ~as.character(.x)) %>%          # Convert to character, just in case
     map(.f = ~unique(.x))                    # Take the unique hospital name
```

We can now see that each of the list elements has a name. These names can be accessed via `names(linelist_split)`.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split_named.png"))
```

```{r}
names(linelist_split)
```


##### More than one `group_split()` column {.unnumbered}  

If you wanted to split the linelist by *more than one grouping column*, such as to produce subset linelist by intersection of hospital AND gender, you will need a different approach to naming the list elements. This involves collecting the unique "group keys" using `group_keys()` from **dplyr** - they are returned as a data frame. Then you can combine the group keys into values with `unite()` as shown below, and assign these conglomerate names to `linelist_split`.  

```{r}
# split linelist by unique hospital-gender combinations
linelist_split <- linelist %>% 
     group_split(hospital, gender)

# extract group_keys() as a dataframe
groupings <- linelist %>% 
     group_by(hospital, gender) %>%       
     group_keys()

groupings      # show unique groupings 
```

Now we combine the groupings together, separated by dashes, and assign them as the names of list elements in `linelist_split`. This takes some extra lines as we replace `NA` with "Missing", use `unite()` from **dplyr** to combine the column values together (separated by dashes), and then convert into an un-named vector so it can be used as names of `linelist_split`.  

```{r, eval=F}
# Combine into one name value 
names(linelist_split) <- groupings %>% 
     mutate(across(everything(), replace_na, "Missing")) %>%  # replace NA with "Missing" in all columns
     unite("combined", sep = "-") %>%                         # Unite all column values into one
     setNames(NULL) %>% 
     as_vector() %>% 
     as.list()
```



#### Export as Excel sheets {.unnumbered}  

To export the hospital linelists as *an Excel workbook with one linelist per sheet*, we can just provide the named list `linelist_split` to the `write_xlsx()` function from the **writexl** package. This has the ability to save one Excel workbook with multiple sheets. The list element names are automatically applied as the sheet names.  

```{r, eval=F}
linelist_split %>% 
     writexl::write_xlsx(path = here("data", "hospital_linelists.xlsx"))
```

You can now open the Excel file and see that each hospital has its own sheet.  

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_sheets.png"))
```

#### Export as CSV files {.unnumbered}  

It is a bit more complex command, but you can also export each hospital-specific linelist as a separate CSV file, with a file name specific to the hospital.  

Again we use `map()`: we take the vector of list element names (shown above) and use `map()` to iterate through them, applying `export()` (from the **rio** package, see [Import and export] page) on the data frame in the list `linelist_split` that has that name. We also use the name to create a unique file name. Here is how it works:  
     
* We begin with the vector of character names, passed to `map()` as `.x`  
* The `.f` function is `export()` , which requires a data frame and a file path to write to  
* The input `.x` (the hospital name) is used *within* `.f` to extract/index that specific element of `linelist_split` list. This results in only one data frame at a time being provided to `export()`.  
* For example, when `map()` iterates for "Military Hospital", then `linelist_split[[.x]]` is actually `linelist_split[["Military Hospital"]]`, thus returning the second element of `linelist_split` - which is all the cases from Military Hospital.  
* The file path provided to `export()` is dynamic via use of `str_glue()` (see [Characters and strings] page):  
     * `here()` is used to get the base of the file path and specify the "data" folder (note single quotes to not interrupt the `str_glue()` double quotes)  
* Then a slash `/`, and then again the `.x` which prints the current hospital name to make the file identifiable  
* Finally the extension ".csv" which `export()` uses to create a CSV file  

```{r, eval=F, message = F, warning=F}
names(linelist_split) %>%
     map(.f = ~export(linelist_split[[.x]], file = str_glue("{here('data')}/{.x}.csv")))
```
Now you can see that each file is saved in the "data" folder of the R Project "Epi_R_handbook"!  
     
```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_csv.png"))
```



### Custom functions {.unnumbered}  

You may want to create your own function to provide to `map()`.  

Let's say we want to create epidemic curves for each hospital's cases. To do this using **purrr**, our `.f` function can be `ggplot()` and extensions with `+` as usual. As the output of `map()` is always a list, the plots are stored in a list. Because they are plots, they can be extracted and plotted with the `ggarrange()` function from the **ggpubr** package ([documentation](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html)).  


```{r, message = F, warning=F}

# load package for plotting elements from list
pacman::p_load(ggpubr)

# map across the vector of 6 hospital "names" (created earlier)
# use the ggplot function specified
# output is a list with 6 ggplots

hospital_names <- unique(linelist$hospital)

my_plots <- map(
  .x = hospital_names,
  .f = ~ggplot(data = linelist %>% filter(hospital == .x)) +
                geom_histogram(aes(x = date_onset)) +
                labs(title = .x)
)

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```

If this `map()` code looks too messy, you can achieve the same result by saving your specific `ggplot()` command as a custom user-defined function, for example we can name it `make_epicurve())`. This function is then used within the `map()`. `.x` will be iteratively replaced by the hospital name, and used as `hosp_name` in the `make_epicurve()` function. See the page on [Writing functions].

```{r, eval=F}
# Create function
make_epicurve <- function(hosp_name){
  
  ggplot(data = linelist %>% filter(hospital == hosp_name)) +
    geom_histogram(aes(x = date_onset)) +
    theme_classic()+
    labs(title = hosp_name)
  
}
```

```{r, eval=F}
# mapping
my_plots <- map(hospital_names, ~make_epicurve(hosp_name = .x))

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```




### Mapping a function across columns {.unnumbered}  

Another common use-case is to map a function across many columns. Below, we `map()` the function `t.test()` across numeric columns in the data frame `linelist`, comparing the numeric values by gender.  

Recall from the page on [Simple statistical tests] that `t.test()` can take inputs in a formula format, such as `t.test(numeric column ~ binary column)`. In this example, we do the following:    
     
* The numeric columns of interest are selected from `linelist` - these become the `.x` inputs to `map()`  
* The function `t.test()` is supplied as the `.f` function, which is applied to each numeric column  
* Within the parentheses of `t.test()`:  
  * the first `~` precedes the `.f` that `map()` will iterate over `.x`  
  * the `.x` represents the current column being supplied to the function `t.test()`  
  * the second `~` is part of the t-test equation described above  
  * the `t.test()` function expects a binary column on the right-hand side of the equation. We supply the vector `linelist$gender` independently and statically (note that it is not included in `select()`).  

`map()` returns a list, so the output is a list of t-test results - one list element for each numeric column analysed.  

```{r}
# Results are saved as a list
t.test_results <- linelist %>% 
  select(age, wt_kg, ht_cm, ct_blood, temp) %>%  # keep only some numeric columns to map across
  map(.f = ~t.test(.x ~ linelist$gender))        # t.test function, with equation NUMERIC ~ CATEGORICAL
```

Here is what the list `t.test_results` looks like when opened (Viewed) in RStudio. We have highlighted parts that are important for the examples in this page.  

* You can see at the top that the whole list is named `t.test_results` and has five elements. Those five elements are named `age`, `wt_km`, `ht_cm`, `ct_blood`, `temp` after each variable that was used in a t-test with `gender` from the `linelist`.  
* Each of those five elements are themselves lists, with elements within them such as `p.value` and `conf.int`. Some of these elements like `p.value` are single numbers, whereas some such as `estimate` consist of two or more elements (`mean in group f` and `mean in group m`).  

```{r, out.height="150%", echo=F}
knitr::include_graphics(here::here("images", "purrr_ttest.png"))
```


Note: Remember that if you want to apply a function to only certain columns in a data frame, you can also simply use `mutate()` and `across()`, as explained in the [Cleaning data and core functions] page. Below is an example of applying `as.character()` to only the "age" columns. Note the placement of the parentheses and commas.  

```{r, eval=F}
# convert columns with column name containing "age" to class Character
linelist <- linelist %>% 
  mutate(across(.cols = contains("age"), .fns = as.character))  
```


### Extract from lists {.unnumbered}  

As `map()` produces an output of class List, we will spend some time discussing how to extract data from lists using accompanying **purrr** functions. To demonstrate this, we will use the list `t.test_results` from the previous section. This is a list of 5 lists - each of the 5 lists contains the results of a t-test between a column from `linelist` data frame and its binary column `gender`. See the image in the section above for a visual of the list structure.  

#### Names of elements {.unnumbered}  

To extract the names of the elements themselves, simply use `names()` from **base** R. In this case, we use `names()` on `t.test_results` to return the names of each sub-list, which are the names of the 5 variables that had t-tests performed.  

```{r}
names(t.test_results)
```

#### Elements by name or position {.unnumbered}  

To extract list elements by name or by position you can use brackets `[[ ]]` as described in the [R basics] page. Below we use double brackets to index the list `t.tests_results` and display the first element which is the results of the t-test on `age`.  

```{r}
t.test_results[[1]] # first element by position
t.test_results[[1]]["p.value"] # return element named "p.value" from first element  
```

However, below we will demonstrate use of the simple and flexible **purrr** functions `map()` and `pluck()` to achieve the same outcomes.  

#### `pluck()` {.unnumbered}  

`pluck()` pulls out elements by name or by position. For example - to extract the t-test results for age, you can use `pluck()` like this:  

```{r}
t.test_results %>% 
  pluck("age")        # alternatively, use pluck(1)
```

Index deeper levels by specifying the further levels with commas. The below extracts the element named "p.value" from the list `age` within the list `t.test_results`. You can also use numbers instead of character names.    

```{r}
t.test_results %>% 
  pluck("age", "p.value")
```

You can extract such inner elements from *all* first-level elements by using `map()` to run the `pluck()` function across each first-level element. For example, the below code extracts the "p.value" elements from all lists within `t.test_results`. The list of t-test results is the `.x` iterated across, `pluck()` is the `.f` function being iterated, and the value "p-value" is provided to the function.     

```{r}
t.test_results %>%
  map(pluck, "p.value")   # return every p-value
```

As another alternative, `map()` offers a shorthand where you can write the element name in quotes, and it will pluck it out. If you use `map()` the output will be a list, whereas if you use `map_chr()` it will be a named character vector and if you use `map_dbl()` it will be a named numeric vector.  

```{r}
t.test_results %>% 
  map_dbl("p.value")   # return p-values as a named numeric vector
```

You can read more about `pluck()` in it's **purrr** [documentation](https://purrr.tidyverse.org/reference/pluck.html). It has a sibling function `chuck()` that will return an error instead of NULL if an element does not exist.  



### Convert list to data frame {.unnumbered}  

This is a complex topic - see the Resources section for more complete tutorials. Nevertheless, we will demonstrate converting the list of t-test results into a data frame. We will create a data frame with columns for the variable, its p-value, and the means from the two groups (male and female).  

Here are some of the new approaches and functions that will be used:  

* The function `tibble()` will be used to create a tibble (like a data frame)  
  * We surround the `tibble()` function with curly brackets `{ }` to prevent the entire `t.test_results` from being stored as the first tibble column  
* Within `tibble()`, each column is created explicitly, similar to the syntax of `mutate()`:  
  * The `.` represents `t.test_results`
  * To create a column with the t-test variable names (the names of each list element) we use `names()` as described above  
  * To create a column with the p-values we use `map_dbl()` as described above to pull the `p.value` elements and convert them to a numeric vector  

```{r}
t.test_results %>% {
  tibble(
    variables = names(.),
    p         = map_dbl(., "p.value"))
  }
```

But now let's add columns containing the means for each group (males and females).  

We would need to extract the element `estimate`, but this actually contains *two* elements within it (`mean in group f` and `mean in group m`). So, it cannot be simplified into a vector with `map_chr()` or `map_dbl()`. Instead, we use `map()`, which used within `tibble()` will create *a column of class list within the tibble*! Yes, this is possible!  

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimate"))}
```

Once you have this list column, there are several **tidyr** functions (part of **tidyverse**) that help you "rectangle" or "un-nest" these "nested list" columns. Read more about them [here](), or by running `vignette("rectangle")`. In brief:  

* `unnest_wider()` - gives each element of a list-column its own column  
* `unnest_longer()` - gives each element of a list-column its own row
* `hoist()` - acts like `unnest_wider()` but you specify which elements to unnest  

Below, we pass the tibble to `unnest_wider()` specifying the tibble's `means` column (which is a nested list). The result is that `means` is replaced by two new columns, each reflecting the two elements that were previously in each `means` cell.  

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimate")
    )} %>% 
  unnest_wider(means)
```



### Discard, keep, and compact lists {.unnumbered}  

Because working with **purrr** so often involves lists, we will briefly explore some **purrr** functions to modify lists. See the Resources section for more complete tutorials on **purrr** functions.    

* `list_modify()` has many uses, one of which can be to remove a list element  
* `keep()` retains the elements specified to `.p = `, or where a function supplied to `.p = ` evaluates to TRUE  
* `discard()` removes the elements specified to `.p`, or where a function supplied to `.p = ` evaluates to TRUE  
* `compact()` removes all empty elements  

Here are some examples using the `combined` list created in the section above on [using map() to import and combine multiple files](#iter_combined) (it contains 6 case linelist data frames):    

Elements can be removed by name with `list_modify()` and setting the name equal to `NULL`.  

```{r, eval=F}
combined %>% 
  list_modify("Central Hospital" = NULL)   # remove list element by name
```

You can also remove elements by criteria, by providing a "predicate" equation to `.p = ` (an equation that evaluates to either TRUE or FALSE). Place a tilde `~` before the function and use `.x` to represent the list element. Using `keep()` the list elements that evaluate to TRUE will be kept. Inversely, if using `discard()` the list elements that evaluate to TRUE will be removed.  

```{r, eval=F}
# keep only list elements with more than 500 rows
combined %>% 
  keep(.p = ~nrow(.x) > 500)  
```

In the below example, list elements are discarded if their class are not data frames.  

```{r, eval=F}
# Discard list elements that are not data frames
combined %>% 
  discard(.p = ~class(.x) != "data.frame")
```

Your predicate function can also reference elements/columns within each list item. For example, below, list elements where the mean of column `ct_blood` is over 25 are discarded.  

```{r, eval=F}
# keep only list elements where ct_blood column mean is over 25
combined %>% 
  discard(.p = ~mean(.x$ct_blood) > 25)  
```

This command would remove all empty list elements:  

```{r, eval=F}
# Remove all empty list elements
combined %>% 
  compact()
```



### `pmap()` {.unnumbered}

THIS SECTION IS UNDER CONSTRUCTION  



## Apply functions  

The "apply" family of functions is a **base** R alternative to **purrr** for iterative operations. You can read more about them [here](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family).  






<!-- ======================================================= -->
## Resources { }

[for loops with Data Carpentry](https://datacarpentry.org/semester-biology/materials/for-loops-R/)  

The [R for Data Science page on iteration](https://r4ds.had.co.nz/iteration.html#iteration)  

[Vignette on write/read Excel files](https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/)  

A purrr [tutorial](https://jennybc.github.io/purrr-tutorial/index.html) by jennybc 

Another purrr [tutorial](http://www.rebeccabarter.com/blog/2019-08-19_purrr/) by Rebecca Barter  

A purrr [tutorial](http://zevross.com/blog/2019/06/11/the-power-of-three-purrr-poseful-iteration-in-r-with-map-pmap-and-imap/) on map, pmap, and imap  

[purrr cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/thumbnails/purrr-cheatsheet-thumbs.png)

[purrr tips and tricks](https://www.emilhvitfeldt.com/post/2018-01-08-purrr-tips-and-tricks/)

[keep and discard](https://hookedondata.org/going-off-the-map/#keep-and-discard)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/iteration.Rmd-->

# (PART) Analysis {.unnumbered}

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_analysis.Rmd-->

# Descriptive tables { }

```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "descriptive_tables.png"))
```

This page demonstrates the use of **janitor**, **dplyr**, **gtsummary**, **rstatix**, and **base** R to summarise data and create tables with descriptive statistics. 

*This page covers how to *create* the underlying tables, whereas the [Tables for presentation] page covers how to nicely format and print them.*  

Each of these packages has advantages and disadvantages in the areas of code simplicity, accessibility of outputs, quality of printed outputs. Use this page to decide which approach works for your scenario.  


You have several choices when producing tabulation and cross-tabulation summary tables. Some of the factors to consider include code simplicity, customizeability, the desired output (printed to R console, as data frame, or as "pretty" .png/.jpeg/.html image), and ease of post-processing. Consider the points below as you choose the tool for your situation.

* Use `tabyl()` from **janitor** to produce and "adorn" tabulations and cross-tabulations  
* Use `get_summary_stats()` from **rstatix** to easily generate data frames of numeric summary statistics for multiple columns and/or groups  
* Use `summarise()` and `count()` from **dplyr** for more complex statistics, tidy data frame outputs, or preparing data for `ggplot()`  
* Use `tbl_summary()` from **gtsummary** to produce detailed publication-ready tables  
* Use `table()` from **base** R if you do not have access to the above packages  


<!-- ======================================================= -->
## Preparation {  }


### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r, warning=F, message=F}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics 
  gtsummary,    # summary statistics and tests
  rstatix,      # summary statistics and statistical tests
  janitor,      # adding totals and percents to tables
  scales,       # easily convert proportions to percents  
  flextable     # converting tables to pretty images
  )
```

### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Browse data {  }

### **skimr** package {.unnumbered}

By using the **skimr** package, you can get a detailed and aesthetically pleasing overview of each of the variables in your dataset. Read more about **skimr** at its [github page](https://github.com/ropensci/skimr).  

Below, the function `skim()` is applied to the entire `linelist` data frame. An overview of the data frame and a summary of every column (by class) is produced.    

```{r eval=F}
## get information about each variable in a dataset 
skim(linelist)
```

```{r  echo=F}
# sparkline histograms not showing correctly, so avoiding them.
skim_without_charts(linelist)
```



You can also use the `summary()` function, from **base** R, to get information about an entire dataset, but this output can be more difficult to read than using **skimr**. Therefore the output is not shown below, to conserve page space.  

```{r, eval=F}
## get information about each column in a dataset 
summary(linelist)
```


### Summary statistics {.unnumbered} 

You can use **base** R functions to return summary statistics on a numeric column. You can return most of the useful summary statistics for a numeric column using `summary()`, as below. Note that the data frame name must also be specified as shown below.  

```{r}
summary(linelist$age_years)
```

You can access and save one specific part of it with index brackets [ ]:  

```{r}
summary(linelist$age_years)[[2]]            # return only the 2nd element
# equivalent, alternative to above by element name
# summary(linelist$age_years)[["1st Qu."]]  
```

You can return individual statistics with **base** R functions like `max()`, `min()`, `median()`, `mean()`, `quantile()`, `sd()`, and `range()`. See the [R basics] page for a complete list.  

<span style="color: orange;">**_CAUTION:_** If your data contain missing values, R wants you to know this and so will return `NA` unless you specify to the above mathematical functions that you want R to ignore missing values, via the argument `na.rm = TRUE`.</span>


You can use the `get_summary_stats()` function from **rstatix** to return summary statistics *in a data frame format*. This can be helpful for performing subsequent operations or plotting on the numbers. See the [Simple statistical tests] page for more details on the **rstatix** package and its functions.  

```{r}
linelist %>% 
  get_summary_stats(
    age, wt_kg, ht_cm, ct_blood, temp,  # columns to calculate for
    type = "common")                    # summary stats to return

```





## **janitor** package {#tbl_janitor}  

The **janitor** packages offers the `tabyl()` function to produce tabulations and cross-tabulations, which can be "adorned" or modified with helper functions to display percents, proportions, counts, etc.  

Below, we pipe the `linelist` data frame to **janitor** functions and print the result. If desired, you can also save the resulting tables with the assignment operator `<-`.  

### Simple tabyl {.unnumbered}  

The default use of `tabyl()` on a specific column produces the unique values, counts, and column-wise "percents" (actually proportions). The proportions may have many digits. You can adjust the number of decimals with `adorn_rounding()` as described below.   

```{r}
linelist %>% tabyl(age_cat)
```
As you can see above, if there are missing values they display in a row labeled `<NA>`. You can suppress them with `show_na = FALSE`. If there are no missing values, this row will not appear. If there are missing values, all proportions are given as both raw (denominator inclusive of `NA` counts) and "valid" (denominator excludes `NA` counts).  

If the column is class Factor and only certain levels are present in your data, all levels will still appear in the table. You can suppress this feature by specifying `show_missing_levels = FALSE`. Read more on the [Factors] page.  

### Cross-tabulation {.unnumbered}  

Cross-tabulation counts are achieved by adding one or more additional columns within `tabyl()`. Note that now only counts are returned - proportions and percents can be added with additional steps shown below.  

```{r}
linelist %>% tabyl(age_cat, gender)
```

### "Adorning" the tabyl {#tbl_adorn .unnumbered}  

Use **janitor**'s "adorn" functions to add totals or convert to proportions, percents, or otherwise adjust the display. Often, you will pipe the tabyl through several of these functions.  


Function           | Outcome                          
-------------------|--------------------------------
`adorn_totals()`   | Adds totals (`where = ` "row", "col", or "both"). Set `name =` for "Total".  
`adorn_percentages()` | Convert counts to proportions, with `denominator = ` "row", "col", or "all"  
`adorn_pct_formatting()` | Converts proportions to percents. Specify `digits =`. Remove the "%" symbol with `affix_sign = FALSE`.  
`adorn_rounding()` | To round proportions to `digits =` places. To round percents use `adorn_pct_formatting()` with `digits = `.  
`adorn_ns()` | Add counts to a table of proportions or percents. Indicate `position =` "rear" to show counts in parentheses, or "front" to put the percents in parentheses.  
`adorn_title()` | Add string via arguments `row_name = ` and/or `col_name = `  

Be conscious of the order you apply the above functions. Below are some examples.  

A simple one-way table with percents instead of the default proportions.  

```{r}
linelist %>%               # case linelist
  tabyl(age_cat) %>%       # tabulate counts and proportions by age category
  adorn_pct_formatting()   # convert proportions to percents
```

A cross-tabulation with a total row and row percents.  

```{r}
linelist %>%                                  
  tabyl(age_cat, gender) %>%                  # counts by age and gender
  adorn_totals(where = "row") %>%             # add total row
  adorn_percentages(denominator = "row") %>%  # convert counts to proportions
  adorn_pct_formatting(digits = 1)            # convert proportions to percents
```

A cross-tabulation adjusted so that both counts and percents are displayed.  

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions
  adorn_pct_formatting() %>%                  # convert to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```



### Printing the tabyl {.unnumbered}

By default, the tabyl will print raw to your R console.  

Alternatively, you can pass the tabyl to **flextable**  or similar package to print as a "pretty" image in the RStudio Viewer, which could be exported as .png, .jpeg, .html, etc. This is discussed in the page [Tables for presentation]. Note that if printing in this manner and using `adorn_titles()`, you must specify `placement = "combined"`.

```{r}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% # this is necessary to print as image
  flextable::flextable() %>%    # convert to pretty image
  flextable::autofit()          # format to one line per row 

```


### Use on other tables {.unnumbered}  

You can use **janitor**'s `adorn_*()` functions on other tables, such as those created by `summarise()` and `count()` from **dplyr**, or `table()` from **base** R. Simply pipe the table to the desired **janitor** function. For example:  

```{r}
linelist %>% 
  count(hospital) %>%   # dplyr function
  adorn_totals()        # janitor function
```


### Saving the tabyl {.unnumbered}  

If you convert the table to a "pretty" image with a package like **flextable**, you can save it with functions from that package - like `save_as_html()`, `save_as_word()`, `save_as_ppt()`, and `save_as_image()` from **flextable** (as discussed more extensively in the [Tables for presentation] page). Below, the table is saved as a Word document, in which it can be further hand-edited.  

```{r, eval=F}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% 
  flextable::flextable() %>%                     # convert to image
  flextable::autofit() %>%                       # ensure only one line per row
  flextable::save_as_docx(path = "tabyl.docx")   # save as Word document to filepath
```

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "tabyl_word.png"))
```

### Statistics {#janitor_age_out_stats .unnumbered}  

You can apply statistical tests on tabyls, like `chisq.test()` or `fisher.test()` from the **stats** package, as shown below. Note missing values are not allowed so they are excluded from the tabyl with `show_na = FALSE`.  

```{r, warning=F, message=F}
age_by_outcome <- linelist %>% 
  tabyl(age_cat, outcome, show_na = FALSE) 

chisq.test(age_by_outcome)
```

See the page on [Simple statistical tests] for more code and tips about statistics.  

### Other tips {.unnumbered}  

* Include the argument `na.rm = TRUE` to exclude missing values from any of the above calculations.  
* If applying any `adorn_*()` helper functions to tables not created by `tabyl()`, you can specify particular column(s) to apply them to like  `adorn_percentage(,,,c(cases,deaths))` (specify them to the 4th unnamed argument). The syntax is not simple. Consider using `summarise()` instead.  
* You can read more detail in the [janitor page](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) and this [tabyl vignette](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html).  




## **dplyr** package   

**dplyr** is part of the **tidyverse** packages and is an very common data management tool. Creating tables with **dplyr** functions `summarise()` and `count()` is a useful approach to calculating summary statistics, summarize *by group*, or pass tables to `ggplot()`. 

`summarise()` creates a *new, summary data frame*. If the data are *ungrouped*, it will return a one-row dataframe with the specified summary statistics of the entire data frame. If the data are *grouped*, the new data frame will have one row per *group* (see [Grouping data] page).  

Within the `summarise()` parentheses, you provide the names of each new summary column followed by an equals sign and a statistical function to apply.  

<span style="color: darkgreen;">**_TIP:_** The summarise function works with both UK and US spelling (`summarise()` and `summarize()`).</span>

### Get counts {.unnumbered}  

The most simple function to apply within `summarise()` is `n()`. Leave the parentheses empty to count the number of rows.  

```{r}
linelist %>%                 # begin with linelist
  summarise(n_rows = n())    # return new summary dataframe with column n_rows
```

This gets more interesting if we have grouped the data beforehand.  

```{r}
linelist %>% 
  group_by(age_cat) %>%     # group data by unique values in column age_cat
  summarise(n_rows = n())   # return number of rows *per group*
```

The above command can be shortened by using the `count()` function instead. `count()` does the following:  

1) Groups the data by the columns provided to it  
2) Summarises them with `n()` (creating column `n`)  
3) Un-groups the data  

```{r}
linelist %>% 
  count(age_cat)
```

You can change the name of the counts column from the default `n` to something else by specifying it to `name = `.  

Tabulating counts of two or more grouping columns are still returned in “long” format, with the counts in the `n` column. See the page on [Pivoting data] to learn about "long" and "wide" data formats.  

```{r}
linelist %>% 
  count(age_cat, outcome)
```


### Show all levels {.unnumbered}  

If you are tabling a column of class *factor* you can ensure that *all* levels are shown (not just the levels with values in the data) by adding `.drop = FALSE` into the `summarise()` or `count()` command.  

This technique is useful to standardise your tables/plots. For example if you are creating figures for multiple sub-groups, or repeatedly creating the figure for routine reports. In each of these circumstances, the presence of values in the data may fluctuate, but you can define levels that remain constant.  

See the page on [Factors] for more information.  




### Proportions {#tbl_dplyr_prop .unnumbered}  

Proportions can be added by piping the table to `mutate()` to create a new column. Define the new column as the counts column (`n` by default) divided by the `sum()` of the counts column (this will return a proportion).  

Note that in this case, `sum()` in the `mutate()` command will return the sum of the whole column `n` for use as the proportion denominator. As explained [in the Grouping data page](#group_summarise), *if* `sum()` is used in *grouped* data (e.g. if the `mutate()` immediately followed a `group_by()` command), it will return sums *by group*. As stated just above, `count()` finishes its actions by *ungrouping*. Thus, in this scenario we get full column proportions. 

To easily display percents, you can wrap the proportion in the function `percent()` from the package **scales** (note this convert to class character).  

```{r}
age_summary <- linelist %>% 
  count(age_cat) %>%                     # group and count by gender (produces "n" column)
  mutate(                                # create percent of column - note the denominator
    percent = scales::percent(n / sum(n))) 

# print
age_summary
```

Below is a method to calculate proportions *within groups*. It relies on different levels of data grouping being selectively applied and removed. First, the data are grouped on `outcome` via `group_by()`. Then, `count()` is applied. This function further groups the data by `age_cat` and returns counts for each `outcome`-`age-cat` combination. Importantly - as it finishes its process, `count()` also *ungroups* the `age_cat` grouping, so the only remaining data grouping is the original grouping by `outcome`. Thus, the final step of calculating proportions (denominator `sum(n)`) is still grouped by `outcome`.  

```{r}
age_by_outcome <- linelist %>%                  # begin with linelist
  group_by(outcome) %>%                         # group by outcome 
  count(age_cat) %>%                            # group and count by age_cat, and then remove age_cat grouping
  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```




### Plotting {.unnumbered}  

To display a "long" table output like the above with `ggplot()` is relatively straight-forward. The data are naturally in "long" format, which is naturally accepted by `ggplot()`. See further examples in the pages [ggplot basics] and [ggplot tips].  

```{r, warning=F, message=F}
linelist %>%                      # begin with linelist
  count(age_cat, outcome) %>%     # group and tabulate counts by two columns
  ggplot()+                       # pass new data frame to ggplot
    geom_col(                     # create bar plot
      mapping = aes(   
        x = outcome,              # map outcome to x-axis
        fill = age_cat,           # map age_cat to the fill
        y = n))                   # map the counts column `n` to the height
```


### Summary statistics {.unnumbered}  

One major advantage of **dplyr** and `summarise()` is the ability to return more advanced statistical summaries like `median()`, `mean()`, `max()`, `min()`, `sd()` (standard deviation), and percentiles. You can also use `sum()` to return the number of rows that meet certain logical criteria. As above, these outputs can be produced for the whole data frame set, or by group.  

The syntax is the same - within the `summarise()` parentheses you provide the names of each new summary column followed by an equals sign and a statistical function to apply. Within the statistical function, give the column(s) to be operated on and any relevant arguments (e.g. `na.rm = TRUE` for most mathematical functions). 

You can also use `sum()` to return the number of rows that meet a logical criteria. The expression within is counted if it evaluates to `TRUE`. For example:  

* `sum(age_years < 18, na.rm=T)`  
* `sum(gender == "male", na.rm=T)`  
* `sum(response %in% c("Likely", "Very Likely"))`  

Below, `linelist` data are summarised to describe the days delay from symptom onset to hospital admission (column `days_onset_hosp`), by hospital.  

```{r}
summary_table <- linelist %>%                                        # begin with linelist, save out as new object
  group_by(hospital) %>%                                             # group all calculations by hospital
  summarise(                                                         # only the below summary columns will be returned
    cases       = n(),                                                # number of rows per group
    delay_max   = max(days_onset_hosp, na.rm = T),                    # max delay
    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # mean delay, rounded
    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # standard deviation of delays, rounded
    delay_3     = sum(days_onset_hosp >= 3, na.rm = T),               # number of rows with delay of 3 or more days
    pct_delay_3 = scales::percent(delay_3 / cases)                    # convert previously-defined delay column to percent 
  )

summary_table  # print
```


Some tips:  

* Use `sum()` with a logic statement to "count" rows that meet certain criteria (`==`)  
* Note the use of `na.rm = TRUE` within mathematical functions like `sum()`, otherwise `NA` will be returned if there are any missing values  
* Use the function `percent()` from the **scales** package to easily convert to percents  
  * Set `accuracy = ` to 0.1 or 0.01 to ensure 1 or 2 decimal places respectively  
* Use `round()` from **base** R to specify decimals  
* To calculate these statistics on the entire dataset, use `summarise()` without `group_by()`  
* You may create columns for the purposes of later calculations (e.g. denominators) that you eventually drop from your data frame with `select()`.  


### Conditional statistics {.unnumbered}  

You may want to return *conditional statistics* - e.g. the maximum of rows that meet certain criteria. This can be done by subsetting the column with brackets `[ ]`. The example below returns the maximum temperature for patients classified having or not having fever. Be aware however - it may be more appropriate to add another column to the `group_by()` command and `pivot_wider()` (as demonstrated [below](#tbls_pivot_wider)).  


```{r}
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    max_temp_fvr = max(temp[fever == "yes"], na.rm = T),
    max_temp_no = max(temp[fever == "no"], na.rm = T)
  )
```



### Glueing together {.unnumbered}  

The function `str_glue()` from **stringr** is useful to combine values from several columns into one new column. In this context this is typically used *after* the `summarise()` command.  

In the [Characters and strings] page, various options for combining columns are discussed, including `unite()`, and `paste0()`. In this use case, we advocate for `str_glue()` because it is more flexible than `unite()` and has more simple syntax than `paste0()`.  

Below, the `summary_table` data frame (created above) is mutated such that columns `delay_mean` and `delay_sd` are combined, parentheses formating is added to the new column, and their respective old columns are removed.  

Then, to make the table more presentable, a total row is added with `adorn_totals()` from **janitor** (which ignores non-numeric columns). Lastly, we use `select()` from **dplyr** to both re-order and rename to nicer column names.  

Now you could pass to **flextable** and print the table to Word, .png, .jpeg, .html, Powerpoint, RMarkdown, etc.! (see the [Tables for presentation] page).  

```{r}
summary_table %>% 
  mutate(delay = str_glue("{delay_mean} ({delay_sd})")) %>%  # combine and format other values
  select(-c(delay_mean, delay_sd)) %>%                       # remove two old columns   
  adorn_totals(where = "row") %>%                            # add total row
  select(                                                    # order and rename cols
    "Hospital Name"   = hospital,
    "Cases"           = cases,
    "Max delay"       = delay_max,
    "Mean (sd)"       = delay,
    "Delay 3+ days"   = delay_3,
    "% delay 3+ days" = pct_delay_3
    )
```

#### Percentiles {.unnumbered}  

*Percentiles* and quantiles in **dplyr** deserve a special mention. To return quantiles, use `quantile()` with the defaults or specify the value(s) you would like with `probs = `.

```{r}
# get default percentile values of age (0%, 25%, 50%, 75%, 100%)
linelist %>% 
  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))

# get manually-specified percentile values of age (5%, 50%, 75%, 98%)
linelist %>% 
  summarise(
    age_percentiles = quantile(
      age_years,
      probs = c(.05, 0.5, 0.75, 0.98), 
      na.rm=TRUE)
    )
```

If you want to return quantiles *by group*, you may encounter long and less useful outputs if you simply add another column to `group_by()`. So, try this approach instead - create a column for each quantile level desired.  

```{r}
# get manually-specified percentile values of age (5%, 50%, 75%, 98%)
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    p05 = quantile(age_years, probs = 0.05, na.rm=T),
    p50 = quantile(age_years, probs = 0.5, na.rm=T),
    p75 = quantile(age_years, probs = 0.75, na.rm=T),
    p98 = quantile(age_years, probs = 0.98, na.rm=T)
    )
```

While **dplyr** `summarise()` certainly offers more fine control, you may find that all the summary statistics you need can be produced with `get_summary_stat()` from the **rstatix** package. If operating on grouped data, if will return 0%, 25%, 50%, 75%, and 100%. If applied to ungrouped data, you can specify the percentiles with `probs = c(.05, .5, .75, .98)`.  


```{r}
linelist %>% 
  group_by(hospital) %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```

```{r}
linelist %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```



### Summarise aggregated data {.unnumbered}  

*If you begin with aggregated data*, using `n()` return the number of *rows*, not the sum of the aggregated counts. To get sums, use `sum()` on the data's counts column.  

For example, let's say you are beginning with the data frame of counts below, called `linelist_agg` - it shows in "long" format the case counts by outcome and gender.  

Below we create this example data frame of `linelist` case counts by outcome and gender (missing values removed for clarity).  

```{r}
linelist_agg <- linelist %>% 
  drop_na(gender, outcome) %>% 
  count(outcome, gender)

linelist_agg
```

To sum the counts (in column `n`) by group you can use `summarise()` but set the new column equal to `sum(n, na.rm=T)`. To add a conditional element to the sum operation, you can use the subset bracket [ ] syntax on the counts column.  

```{r}
linelist_agg %>% 
  group_by(outcome) %>% 
  summarise(
    total_cases  = sum(n, na.rm=T),
    male_cases   = sum(n[gender == "m"], na.rm=T),
    female_cases = sum(n[gender == "f"], na.rm=T))
```




### `across()` multiple columns {.unnumbered}  

You can use `summarise()` across multiple columns using `across()`. This makes life easier when you want to calculate the same statistics for many columns. Place `across()` within `summarise()` and specify the following:  

* `.cols = ` as either a vector of column names `c()` or "tidyselect" helper functions (explained below)  
* `.fns = ` the function to perform (no parentheses) - you can provide multiple within a `list()`

Below, `mean()` is applied to several numeric columns. A vector of columns are named explicitly to `.cols = ` and a single function `mean` is specified (no parentheses) to `.fns = `. Any additional arguments for the function (e.g. `na.rm=TRUE`) are provided after `.fns = `, separated by a comma.  

It can be difficult to get the order of parentheses and commas correct when using `across()`. Remember that within `across()` you must include the columns, the functions, and any extra arguments needed for the functions. 

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columns
                   .fns = mean,                               # function
                   na.rm=T))                                  # extra arguments
```

Multiple functions can be run at once. Below the functions `mean` and `sd` are provided to `.fns = ` within a `list()`. You have the opportunity to provide character names (e.g. "mean" and "sd") which are appended in the new column names.  

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columns
                   .fns = list("mean" = mean, "sd" = sd),    # multiple functions 
                   na.rm=T))                                 # extra arguments
```

Here are those "tidyselect" helper functions you can provide to `.cols = ` to select columns:  

* `everything()`  - all other columns not mentioned  
* `last_col()`    - the last column  
* `where()`       - applies a function to all columns and selects those which are TRUE  
* `starts_with()` - matches to a specified prefix. Example: `starts_with("date")`
* `ends_with()`   - matches to a specified suffix. Example: `ends_with("_end")`  
* `contains()`    - columns containing a character string. Example: `contains("time")` 
* `matches()`     - to apply a regular expression (regex). Example: `contains("[pt]al")`  
* `num_range()`   - 
* `any_of()`      - matches if column is named. Useful if the name might not exist. Example: `any_of(date_onset, date_death, cardiac_arrest)`  


For example, to return the mean of every numeric column use `where()` and provide the function `as.numeric()` (without parentheses). All this remains within the `across()` command.  

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(
    .cols = where(is.numeric),  # all numeric columns in the data frame
    .fns = mean,
    na.rm=T))
```


### Pivot wider {#tbls_pivot_wider .unnumbered}

If you prefer your table in "wide" format you can transform it using the **tidyr** `pivot_wider()` function. You will likely need to re-name the columns with `rename()`. For more information see the page on [Pivoting data].  

The example below begins with the "long" table `age_by_outcome` from the [proportions section](#tbl_dplyr_prop). We create it again and print, for clarity:  

```{r}
age_by_outcome <- linelist %>%                  # begin with linelist
  group_by(outcome) %>%                         # group by outcome 
  count(age_cat) %>%                            # group and count by age_cat, and then remove age_cat grouping
  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

To pivot wider, we create the new columns from the *values* in the existing column `age_cat` (by setting `names_from = age_cat`). We also specify that the new table values will come from the existing column `n`, with `values_from = n`. The columns not mentioned in our pivoting command (`outcome`) will remain unchanged on the far left side.  

```{r}
age_by_outcome %>% 
  select(-percent) %>%   # keep only counts for simplicity
  pivot_wider(names_from = age_cat, values_from = n)  
```


### Total rows {#tbl_dplyr_totals .unnumbered}  

When `summarise()` operates on grouped data it does not automatically produce "total" statistics. Below, two approaches to adding a total row are presented:  

#### **janitor**'s `adorn_totals()` {.unnumbered}  

If your table consists only of counts or proportions/percents that can be summed into a total, then you can add *sum* totals using **janitor**'s `adorn_totals()` as described in the section above. Note that this function can only sum the numeric columns - if you want to calculate other total summary statistics see the next approach with **dplyr**.  

Below, `linelist` is grouped by gender and summarised into a table that described the number of cases with known outcome, deaths, and recovered. Piping the table to `adorn_totals()` adds a total row at the bottom reflecting the sum of each column. The further `adorn_*()` functions adjust the display as noted in the code.  

```{r}
linelist %>% 
  group_by(gender) %>%
  summarise(
    known_outcome = sum(!is.na(outcome)),           # Number of rows in group where outcome is not missing
    n_death  = sum(outcome == "Death", na.rm=T),    # Number of rows in group where outcome is Death
    n_recover = sum(outcome == "Recover", na.rm=T), # Number of rows in group where outcome is Recovered
  ) %>% 
  adorn_totals() %>%                                # Adorn total row (sums of each numeric column)
  adorn_percentages("col") %>%                      # Get column proportions
  adorn_pct_formatting() %>%                        # Convert proportions to percents
  adorn_ns(position = "front")                      # display % and counts (with counts in front)
```

#### `summarise()` on "total" data and then `bind_rows()` {.unnumbered}  

If your table consists of summary statistics such as `median()`, `mean()`, etc, the `adorn_totals()` approach shown above will *not* be sufficient. Instead, to get summary statistics for the entire dataset you must calculate them with a separate `summarise()` command and then bind the results to the original grouped summary table. To do the binding you can use `bind_rows()` from **dplyr** s described in the [Joining data] page. Below is an example:  

You can make a summary table of outcome *by hospital* with `group_by()` and `summarise()` like this:  

```{r, warning=F, message=F}
by_hospital <- linelist %>% 
  filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T))               # median CT value per group
  
by_hospital # print table
```

To get the totals, run the same `summarise()` command but only group the data by outcome (not by hospital), like this:  

```{r}
totals <- linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # These statistics are now by outcome only     
        ct_value = median(ct_blood, na.rm=T))

totals # print table
```

We can bind these two data frames together. Note that `by_hospital` has 4 columns whereas `totals` has 3 columns. By using `bind_rows()`, the columns are combined by name, and any extra space is filled in with `NA` (e.g the column `hospital` values for the two new `totals` rows). After binding the rows, we convert these empty spaces to "Total"  using `replace_na()` (see [Cleaning data and core functions] page).  

```{r}
table_long <- bind_rows(by_hospital, totals) %>% 
  mutate(hospital = replace_na(hospital, "Total"))
```

Here is the new table with "Total" rows at the bottom.  

```{r, message=FALSE, echo=F}
DT::datatable(table_long, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

This table is in a "long" format, which may be what you want. *Optionally*, you can *pivot* this table *wider* to make it more readable. See the section on pivoting wider above, and the [Pivoting data] page. You can also add more columns, and arrange it nicely. This code is below.  

```{r}
table_long %>% 
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known)                                  # Arrange rows from lowest to highest (Total row at bottom)

```

And then you can print this nicely as an image - below is the output printed with **flextable**. You can read more in depth about this example and how to achieve this "pretty" table in the [Tables for presentation] page.  

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make pretty images of tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  colformat_num(., j = c(4,7), digits = 1) %>% 
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 6, bold = TRUE, part = "body")


table
```



## **gtsummary** package {#tbl_gt}   

If you want to print your summary statistics in a pretty, publication-ready graphic, you can use the **gtsummary** package and its function `tbl_summary()`. The code can seem complex at first, but the outputs look very nice and print to your RStudio Viewer panel as an HTML image. Read a [vignette here](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html).    

You can also add the results of statistical tests to **gtsummary** tables. This process is described in the **gtsummary** section of the [Simple statistical tests](#stats_gt) page.  

To introduce `tbl_summary()` we will show the most basic behavior first, which actually produces a large and beautiful table. Then, we will examine in detail how to make adjustments and more tailored tables. 



### Summary table {.unnumbered}

The default behavior of `tbl_summary()` is quite incredible - it takes the columns you provide and creates a summary table in one command. The function prints statistics appropriate to the column class: median and inter-quartile range (IQR) for numeric columns, and counts (%) for categorical columns. Missing values are converted to "Unknown". Footnotes are added to the bottom to explain the statistics, while the total N is shown at the top.  

```{r, warning=F, message=F}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>%  # keep only the columns of interest
  tbl_summary()                                                  # default
```


### Adjustments {.unnumbered}  

Now we will explain how the function works and how to make adjustments. The key arguments are detailed below: 

**`by = `**  
You can stratify your table by a column (e.g. by `outcome`), creating a 2-way table.  

**`statistic = `**  
Use an equations to specify which statistics to show and how to display them. There are two sides to the equation, separated by a tilde `~`. On the right side, in quotes, is the statistical display desired, and on the left are the columns to which that display will apply.  

* The right side of the equation uses the syntax of `str_glue()` from **stringr** (see [Characters and Strings]), with the desired display string in quotes and the statistics themselves within curly brackets. You can include statistics like "n" (for counts), "N" (for denominator), "mean", "median", "sd", "max", "min", percentiles as "p##" like "p25", or percent of total as "p". See `?tbl_summary` for details.  
* For the left side of the equation, you can specify columns by name (e.g. `age` or `c(age, gender)`) or using helpers such as `all_continuous()`, `all_categorical()`, `contains()`, `starts_with()`, etc.  

A simple example of a `statistic = ` equation might look like below, to only print the mean of column `age_years`:  

```{r}
linelist %>% 
  select(age_years) %>%         # keep only columns of interest 
  tbl_summary(                  # create summary table
    statistic = age_years ~ "{mean}") # print mean of age
```

A slightly more complex equation might look like `"({min}, {max})"`, incorporating the max and min values within parentheses and separated by a comma:  

```{r}
linelist %>% 
  select(age_years) %>%                       # keep only columns of interest 
  tbl_summary(                                # create summary table
    statistic = age_years ~ "({min}, {max})") # print min and max of age
```

You can also differentiate syntax for separate columns or types of columns. In the more complex example below, the value provided to `statistc = ` is a **list** indicating that for all continuous columns the table should print mean with standard deviation in parentheses, while for all categorical columns it should print the n, denominator, and percent.  

**`digits = `**  
Adjust the digits and rounding. Optionally, this can be specified to be for continuous columns only (as below).  

**`label = `**  
Adjust how the column name should be displayed. Provide the column name and its desired label separated by a tilde. The default is the column name.  

**`missing_text = `**  
Adjust how missing values are displayed. The default is "Unknown".  

**`type = `**  
This is used to adjust how many levels of the statistics are shown. The syntax is similar to `statistic = ` in that you provide an equation with columns on the left and a value on the right. Two common scenarios include:  

* `type = all_categorical() ~ "categorical"` Forces dichotomous columns (e.g. `fever` yes/no) to show all levels instead of only the “yes” row  
* `type = all_continuous() ~ "continuous2"` Allows multi-line statistics per variable, as shown in a later section  

In the example below, each of these arguments is used to modify the original summary table:  

```{r}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>% # keep only columns of interest
  tbl_summary(     
    by = outcome,                                               # stratify entire table by outcome
    statistic = list(all_continuous() ~ "{mean} ({sd})",        # stats and format for continuous columns
                     all_categorical() ~ "{n} / {N} ({p}%)"),   # stats and format for categorical columns
    digits = all_continuous() ~ 1,                              # rounding for continuous columns
    type   = all_categorical() ~ "categorical",                 # force all categorical levels to display
    label  = list(                                              # display labels for column names
      outcome   ~ "Outcome",                           
      age_years ~ "Age (years)",
      gender    ~ "Gender",
      temp      ~ "Temperature",
      hospital  ~ "Hospital"),
    missing_text = "Missing"                                    # how missing values should display
  )
```



### Multi-line stats for continuous variables {.unnumbered}  

If you want to print multiple lines of statistics for continuous variables, you can indicate this by setting the `type = ` to "continuous2".  You can combine all of the previously shown elements in one table by choosing which statistics you want to show. To do this you need to tell the function that you want to get a table back by entering the type as “continuous2”. The number of missing values is shown as "Unknown".

```{r}
linelist %>% 
  select(age_years, temp) %>%                      # keep only columns of interest
  tbl_summary(                                     # create summary table
    type = all_continuous() ~ "continuous2",       # indicate that you want to print multiple statistics 
    statistic = all_continuous() ~ c(
      "{mean} ({sd})",                             # line 1: mean and SD
      "{median} ({p25}, {p75})",                   # line 2: median and IQR
      "{min}, {max}")                              # line 3: min and max
    )
```
There are many other ways to modify these tables, including adding p-values, adjusting color and headings, etc. Many of these are described in the documentation (enter `?tbl_summary` in Console), and some are given in the section on [statistical tests](https://epirhandbook.com/simple-statistical-tests.html).  







## **base** R   

You can use the function `table()` to tabulate and cross-tabulate columns. Unlike the options above, you must specify the dataframe each time you reference a column name, as shown below.  

<span style="color: orange;">**_CAUTION:_** `NA` (missing) values will **not** be tabulated unless you include the argument `useNA = "always"` (which could also be set to "no" or "ifany").</span>

<span style="color: darkgreen;">**_TIP:_** You can use the `%$%` from **magrittr** to remove the need for repeating data frame calls within **base** functions. For example the below could be written `linelist %$% table(outcome, useNA = "always")` </span>

```{r}
table(linelist$outcome, useNA = "always")
```

Multiple columns can be cross-tabulated by listing them one after the other, separated by commas. Optionally, you can assign each column a "name" like `Outcome = linelist$outcome`.  

```{r}
age_by_outcome <- table(linelist$age_cat, linelist$outcome, useNA = "always") # save table as object
age_by_outcome   # print table
```

### Proportions {.unnumbered}  

To return proportions, passing the above table to the function `prop.table()`. Use the `margins = ` argument to specify whether you want the proportions to be of rows (1), of columns (2), or of the whole table (3). For clarity, we pipe the table to the `round()` function from **base** R, specifying 2 digits.   

```{r}
# get proportions of table defined above, by rows, rounded
prop.table(age_by_outcome, 1) %>% round(2)
```

### Totals {.unnumbered}  

To add row and column totals, pass the table to `addmargins()`. This works for both counts and proportions.  

```{r}
addmargins(age_by_outcome)
```

### Convert to data frame {.unnumbered}  

Converting a `table()` object directly to a data frame is not straight-forward. One approach is demonstrated below:  

1) Create the table, *without using* `useNA = "always"`. Instead convert `NA` values to "(Missing)" with `fct_explicit_na()` from **forcats**.  
2) Add totals (optional) by piping to `addmargins()`  
3) Pipe to the **base** R function `as.data.frame.matrix()`  
4) Pipe the table to the **tibble** function `rownames_to_column()`, specifying the name for the first column  
5) Print, View, or export as desired. In this example we use `flextable()` from package **flextable** as described in the [Tables for presentation] page. This will print to the RStudio viewer pane as a pretty HTML image.  

```{r, warning=F, message=F}
table(fct_explicit_na(linelist$age_cat), fct_explicit_na(linelist$outcome)) %>% 
  addmargins() %>% 
  as.data.frame.matrix() %>% 
  tibble::rownames_to_column(var = "Age Category") %>% 
  flextable::flextable()
```




<!-- ======================================================= -->

## Resources {  }

Much of the information in this page is adapted from these resources and vignettes online:  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)  

[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_descriptive.Rmd-->

# Simple statistical tests { }


This page demonstrates how to conduct simple statistical tests using **base** R, **rstatix**, and **gtsummary**.  

* T-test  
* Shapiro-Wilk test  
* Wilcoxon rank sum test  
* Kruskal-Wallis test  
* Chi-squared test  
* Correlations between numeric variables  

...many other tests can be performed, but we showcase just these common ones and link to further documentation.  

Each of the above packages bring certain advantages and disadvantages:  

* Use **base** R functions to print a statistical outputs to the R Console  
* Use **rstatix** functions to return results in a data frame, or if you want tests to run by group  
* Use **gtsummary** if you want to quickly print publication-ready tables  



<!-- ======================================================= -->
## Preparation {  }


### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  rstatix,      # statistics
  corrr,        # correlation analayis for numeric variables
  janitor,      # adding totals and percents to tables
  flextable     # converting tables to HTML
  )
```

### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





## **base** R {}

You can use **base** R functions to conduct statistical tests. The commands are relatively simple and results will print to the R Console for simple viewing. However, the outputs are usually lists and so are harder to manipulate if you want to use the results in subsequent operations. 

### T-tests {.unnumbered} 

A [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), also called "Student's t-Test", is typically used to determine if there is a significant difference between the means of some numeric variable between two groups. Here we'll show the syntax to do this test depending on whether the columns are in the same data frame.

**Syntax 1:** This is the syntax when your numeric and categorical columns are in the same data frame. Provide the numeric column on the left side of the equation and the categorical column on the right side. Specify the dataset to `data = `. Optionally, set `paired = TRUE`, and `conf.level = ` (0.95 default), and `alternative = ` (either "two.sided", "less", or "greater"). Enter `?t.test` for more details.  

```{r}
## compare mean age by outcome group with a t-test
t.test(age_years ~ gender, data = linelist)
```

**Syntax 2:** You can compare two separate numeric vectors using this alternative syntax. For example, if the two columns are in different data sets.  

```{r, eval=F}
t.test(df1$age_years, df2$age_years)
```

You can also use a t-test to determine whether a sample mean is significantly different from some specific value. Here we conduct a one-sample t-test with the known/hypothesized population mean as `mu = `:  

```{r, eval=F}
t.test(linelist$age_years, mu = 45)
```

### Shapiro-Wilk test {.unnumbered}  

The [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) can be used to determine whether a sample came from a normally-distributed population (an assumption of many other tests and analysis, such as the t-test). However, this can only be used on a sample between 3 and 5000 observations. For larger samples a [quantile-quantile plot](https://ggplot2.tidyverse.org/reference/geom_qq.html) may be helpful. 


```{r, eval=F}
shapiro.test(linelist$age_years)
```

### Wilcoxon rank sum test {.unnumbered}

The Wilcoxon rank sum test, also called the [Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), is often used to help determine if two numeric samples are from the same distribution when their populations are not normally distributed or have unequal variance.

```{r wilcox_base}

## compare age distribution by outcome group with a wilcox test
wilcox.test(age_years ~ outcome, data = linelist)

```


### Kruskal-Wallis test {.unnumbered}


The [Kruskal-Wallis test](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance) is an extension of the Wilcoxon rank sum test that can be used to test for differences in the distribution of more than two samples. When only two samples are used it gives identical results to the Wilcoxon rank sum test. 

```{r }

## compare age distribution by outcome group with a kruskal-wallis test
kruskal.test(age_years ~ outcome, linelist)

```

### Chi-squared test {.unnumbered} 

[Pearson's Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test) is used in testing for significant differences between categorical croups. 

```{r}

## compare the proportions in each group with a chi-squared test
chisq.test(linelist$gender, linelist$outcome)

```



## **rstatix** package {}

The **rstatix** package offers the ability to run statistical tests and retrieve results in a "pipe-friendly" framework. The results are automatically in a data frame so that you can perform subsequent operations on the results. It is also easy to group the data being passed into the functions, so that the statistics are run for each group.  


### Summary statistics {.unnumbered}  

The function `get_summary_stats()` is a quick way to return summary statistics. Simply pipe your dataset to this function and provide the columns to analyse. If no columns are specified, the statistics are calculated for all columns.  

By default, a full range of summary statistics are returned: n, max, min, median, 25%ile, 75%ile, IQR, median absolute deviation (mad), mean, standard deviation, standard error, and a confidence interval of the mean. 


```{r}
linelist %>%
  rstatix::get_summary_stats(age, temp)
```

You can specify a subset of summary statistics to return by providing one of the following values to `type = `: "full", "common", "robust", "five_number", "mean_sd", "mean_se", "mean_ci", "median_iqr", "median_mad", "quantile", "mean", "median", "min", "max".  

It can be used with grouped data as well, such that a row is returned for each grouping-variable:  

```{r}
linelist %>%
  group_by(hospital) %>%
  rstatix::get_summary_stats(age, temp, type = "common")
```

You can also use **rstatix** to conduct statistical tests:  

### T-test {.unnumbered}  

Use a formula syntax to specify the numeric and categorical columns:  

```{r}
linelist %>% 
  t_test(age_years ~ gender)
```

Or use `~ 1` and specify `mu = ` for a one-sample T-test. This can also be done by group.  

```{r}
linelist %>% 
  t_test(age_years ~ 1, mu = 30)
```

If applicable, the statistical tests can be done by group, as shown below:  

```{r}
linelist %>% 
  group_by(gender) %>% 
  t_test(age_years ~ 1, mu = 18)
```

### Shapiro-Wilk test {.unnumbered}  

As stated above, sample size must be between 3 and 5000.  

```{r}
linelist %>% 
  head(500) %>%            # first 500 rows of case linelist, for example only
  shapiro_test(age_years)
```

### Wilcoxon rank sum test {.unnumbered}  

```{r}
linelist %>% 
  wilcox_test(age_years ~ gender)
```


### Kruskal-Wallis test {.unnumbered}  

Also known as the Mann-Whitney U test.  

```{r}
linelist %>% 
  kruskal_test(age_years ~ outcome)
```


### Chi-squared test {.unnumbered}  

The chi-square test function accepts a table, so first we create a cross-tabulation. There are many ways to create a cross-tabulation (see [Descriptive tables]) but here we use `tabyl()` from **janitor** and remove the left-most column of value labels before passing to `chisq_test()`.  

```{r}
linelist %>% 
  tabyl(gender, outcome) %>% 
  select(-1) %>% 
  chisq_test()

```

Many many more functions and statistical tests can be run with **rstatix** functions. See the documentation for **rstatix** [online here](https://github.com/kassambara/rstatix) or by entering ?rstatix.  





## `gtsummary` package {#stats_gt}

Use **gtsummary** if you are looking to add the results of a statistical test to a pretty table that was created with this package (as described in the **gtsummary** section of the [Descriptive tables](#tbl_gt) page).  

Performing statistical tests of comparison with `tbl_summary` is done by adding the 
`add_p` function to a table and specifying which test to use. It is possible to get p-values corrected for multiple testing by using the
`add_q` function. Run `?tbl_summary` for details.  

### Chi-squared test {.unnumbered}

Compare the proportions of a categorical variable in two groups. The default statistical test for `add_p()` when applied to a categorical variable is to perform a chi-squared test of independence with continuity correction, but if any expected call count is below 5 then a Fisher's exact test is used. 

```{r chi_gt}
linelist %>% 
  select(gender, outcome) %>%    # keep variables of interest
  tbl_summary(by = outcome) %>%  # produce summary table and specify grouping variable
  add_p()                        # specify what test to perform
```


### T-tests {.unnumbered} 

Compare the difference in means for a continuous variable in two groups. 
For example, compare the mean age by patient outcome. 

```{r ttest_gt}

linelist %>% 
  select(age_years, outcome) %>%             # keep variables of interest
  tbl_summary(                               # produce summary table
    statistic = age_years ~ "{mean} ({sd})", # specify what statistics to show
    by = outcome) %>%                        # specify the grouping variable
  add_p(age_years ~ "t.test")                # specify what tests to perform


```

### Wilcoxon rank sum test{.unnumbered}

Compare the distribution of a continuous variable in two groups. The default 
is to use the Wilcoxon rank sum test and the median (IQR) when comparing two 
groups. However for non-normally distributed data or comparing multiple groups, 
the Kruskal-wallis test is more appropriate. 

```{r wilcox_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (this is default so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "wilcox.test")                     # specify what test to perform (default so could leave brackets empty)


```

### Kruskal-wallis test {.unnumbered}

Compare the distribution of a continuous variable in two or more groups, 
regardless of whether the data is normally distributed. 

```{r kruskal_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (default, so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "kruskal.test")                    # specify what test to perform


```




<!-- ## `dplyr` package {} -->

<!-- Performing statistical tests in `dplyr` alone is very dense, again because it  -->
<!-- does not fit within the tidy-data framework. It requires using `purrr` to create -->
<!-- a list of dataframes for each of the subgroups you want to compare. See the page on [Iteration, loops, and lists] to learn about **purrr**.   -->

<!-- An easier alternative may be the `rstatix` package.  -->

<!-- ### T-tests {.unnumbered}  -->

<!-- ```{r ttest_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the mean age for the death group -->
<!--     Death_mean = map(Death, ~mean(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_sd = map(Death, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the mean age for the recover group -->
<!--     Recover_mean = map(Recover, ~mean(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_sd = map(Recover, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## using both grouped data sets compare mean age with a t-test -->
<!--     ## keep only the p.value -->
<!--     t_test = map2(Death, Recover, ~t.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->


<!-- ### Wilcoxon rank sum test {.unnumbered} -->

<!-- ```{r wilcox_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using both grouped data sets compare age distribution with a wilcox test -->
<!--     ## keep only the p.value -->
<!--     wilcox = map2(Death, Recover, ~wilcox.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Kruskal-wallis test {.unnumbered} -->


<!-- ```{r kruskal_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using the original data set compare age distribution with a kruskal test -->
<!--     ## keep only the p.value -->
<!--     kruskal = kruskal.test(linelist$age, linelist$outcome)$p.value -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Chi-squared test {.unnumbered}  -->


<!-- ```{r} -->
<!-- linelist %>%  -->
<!--   ## do everything by gender  -->
<!--   group_by(outcome) %>%  -->
<!--   ## count the variable of interest -->
<!--   count(gender) %>%  -->
<!--   ## calculate proportion  -->
<!--   ## note that the denominator here is the sum of each gender -->
<!--   mutate(percentage = n / sum(n) * 100) %>%  -->
<!--   pivot_wider(names_from = outcome, values_from = c(n, percentage)) %>%  -->
<!--   filter(!is.na(gender)) %>%  -->
<!--   mutate(pval = chisq.test(linelist$gender, linelist$outcome)$p.value) -->
<!-- ``` -->


<!-- ======================================================= -->

## Correlations 

Correlation between numeric variables can be investigated using the **tidyverse**  
**corrr** package. It allows you to compute correlations using Pearson, Kendall
tau or Spearman rho. The package creates a table and also has a function to 
automatically plot the values. 

```{r, warning=F, message=F}

correlation_tab <- linelist %>% 
  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %>%   # keep numeric variables of interest
  correlate()      # create correlation table (using default pearson)

correlation_tab    # print

## remove duplicate entries (the table above is mirrored) 
correlation_tab <- correlation_tab %>% 
  shave()

## view correlation table 
correlation_tab

## plot correlations 
rplot(correlation_tab)
```


<!-- ======================================================= -->

## Resources {  }

Much of the information in this page is adapted from these resources and vignettes online:  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)
[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)
[corrr](https://corrr.tidymodels.org/articles/using-corrr.html)
[sthda correlation](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/stat_tests.Rmd-->

# Univariate and multivariable regression { }

<!-- ======================================================= -->

This page demonstrates the use of **base** R regression functions such as `glm()` and the **gtsummary** package to 
look at associations between variables (e.g. odds ratios, risk ratios and hazard
ratios). It also uses functions like `tidy()` from the **broom** package to clean-up regression outputs.  

1.  Univariate: two-by-two tables 
2.  Stratified: mantel-haenszel estimates  
3.  Multivariable: variable selection, model selection, final table
4.  Forest plots

For Cox proportional hazard regression, see the [Survival analysis] page.  

<span style="color: black;">**_NOTE:_** We use the term *multivariable* to refer to a regression with multiple explanatory variables. In this sense a *multivariate* model would be a regression with several outcomes - see this [editorial](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518362/) for detail </span> 

<!-- ======================================================= -->

## Preparation {  }


### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse,    # data management + ggplot2 graphics, 
  stringr,      # manipulate text strings 
  purrr,        # loop over objects in a tidy way
  gtsummary,    # summary statistics and tests 
  broom,        # tidy up results from regressions
  lmtest,       # likelihood-ratio tests
  parameters,   # alternative to tidy up results from regressions
  see          # alternative to visualise forest plots
  )
```

### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

### Clean data {.unnumbered}

#### Store explanatory variables {.unnumbered}  

We store the names of the explanatory columns as a character vector. This will be referenced later.  

```{r}
## define variables of interest 
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")
```


#### Convert to 1's and 0's  {.unnumbered}   

Below we convert the explanatory columns from "yes"/"no", "m"/"f", and "dead"/"alive" to **1 / 0**, to cooperate with the expectations of logistic regression models. To do this efficiently, used `across()` from **dplyr** to transform multiple columns at one time. The function we apply to each column is `case_when()` (also **dplyr**) which applies logic to convert specified values to 1's and 0's. See sections on `across()` and `case_when()` in the [Cleaning data and core functions page](#clean_across)).  

Note: the "." below represents the column that is being processed by `across()` at that moment.

```{r}
## convert dichotomous variables to 0/1 
linelist <- linelist %>%  
  mutate(across(                                      
    .cols = all_of(c(explanatory_vars, "outcome")),  ## for each column listed and "outcome"
    .fns = ~case_when(                              
      . %in% c("m", "yes", "Death")   ~ 1,           ## recode male, yes and death to 1
      . %in% c("f", "no",  "Recover") ~ 0,           ## female, no and recover to 0
      TRUE                            ~ NA_real_)    ## otherwise set to missing
    )
  )

       
      
```

#### Drop rows with missing values {.unnumbered}  

To drop rows with missing values, can use the **tidyr** function `drop_na()`. However, we only want to do this for rows that are missing values in the columns of interest.  

The first thing we must to is make sure our `explanatory_vars` vector includes the column `age` (`age` would have produced an error in the previous `case_when()` operation, which was only for dichotomous variables). Then we pipe the `linelist` to `drop_na()` to remove any rows with missing values in the `outcome` column or any of the `explanatory_vars` columns.  

Before running the code, the number of rows in the `linelist` is ` nrow(linelist)`.  

```{r}
## add in age_category to the explanatory vars 
explanatory_vars <- c(explanatory_vars, "age_cat")

## drop rows with missing information for variables of interest 
linelist <- linelist %>% 
  drop_na(any_of(c("outcome", explanatory_vars)))

```

The number of rows remaining in `linelist` is ` nrow(linelist)`.  


<!-- ======================================================= -->

## Univariate {  }

Just like in the page on [Descriptive tables](https://epirhandbook.com/descriptive-tables.html), your use case will determine which R package you use. We present two options for doing univariate analysis:  

* Use functions available in **base** R to quickly print results to the console. Use the **broom** package to tidy up the outputs.  
* Use the **gtsummary** package to model and get publication-ready outputs  



<!-- ======================================================= -->

### **base** R {.unnumbered}

#### Linear regression {.unnumbered}  

The **base** R function `lm()` perform linear regression, assessing the relationship between numeric response and explanatory variables that are assumed to have a linear relationship.  

Provide the equation as a formula, with the response and explanatory column names separated by a tilde `~`. Also, specify the dataset to `data = `. Define the model results as an R object, to use later.    

```{r lin_reg}
lm_results <- lm(ht_cm ~ age, data = linelist)
```

You can then run `summary()` on the model results to see the coefficients (Estimates), P-value, residuals, and other measures.  

```{r lin_reg_res}
summary(lm_results)
```

Alternatively you can use the `tidy()` function from the **broom** package to pull 
the results in to a table. What the results tell us is that for each year increase in age the height increases
by 3.5 cm and this is statistically significant. 

```{r lin_reg_res_tidy}
tidy(lm_results)
```

You can then also use this regression to add it to a **ggplot**, to do this we 
first pull the points for the observed data and the fitted line in to one data frame 
using the `augment()` function from **broom**. 

```{r lin_reg_res_plot}

## pull the regression points and observed data in to one dataset
points <- augment(lm_results)

## plot the data using age as the x-axis 
ggplot(points, aes(x = age)) + 
  ## add points for height 
  geom_point(aes(y = ht_cm)) + 
  ## add your regression line 
  geom_line(aes(y = .fitted), colour = "red")

```

It is also possible to add a simple linear regression straight straight in **ggplot** 
using the `geom_smooth()` function. 

```{r geom_smooth}

## add your data to a plot 
 ggplot(linelist, aes(x = age, y = ht_cm)) + 
  ## show points
  geom_point() + 
  ## add a linear regression 
  geom_smooth(method = "lm", se = FALSE)
```

See the Resource section at the end of this chapter for more detailed tutorials.  


#### Logistic regression {.unnumbered}  

The function `glm()` from the **stats** package (part of **base** R) is used to fit Generalized Linear Models (GLM).  

`glm()` can be used for univariate and multivariable logistic regression (e.g. to get Odds Ratios). Here are the core parts:  

```{r, eval=F}
# arguments for glm()
glm(formula, family, data, weights, subset, ...)
```

* `formula = ` The model is provided to `glm()` as an equation, with the outcome on the left and explanatory variables on the right of a tilde `~`.  
* `family = ` This determines the type of model to run. For logistic regression, use `family = "binomial"`, for poisson use `family = "poisson"`. Other examples are in the table below.  
* `data = ` Specify your data frame  


If necessary, you can also specify the link function via the syntax `family = familytype(link = "linkfunction"))`. You can read more in the documentation about other families and optional arguments such as `weights = ` and `subset = ` (`?glm`).  



Family                 | Default link function 
-----------------------|-------------------------------------------  
`"binomial"` | `(link = "logit")`  
`"gaussian"` | `(link = "identity")`  
`"Gamma"` | `(link = "inverse")`  
`"inverse.gaussian"` | `(link = "1/mu^2")`  
`"poisson"` | `(link = "log")`  
`"quasi"` | `(link = "identity", variance = "constant")`  
`"quasibinomial"` | `(link = "logit")`  
`"quasipoisson"` | `(link = "log")`  


When running `glm()` it is most common to save the results as a named R object. Then you can print the results to your console using `summary()` as shown below, or perform other operations on the results (e.g. exponentiate).  

If you need to run a negative binomial regression you can use the **MASS** package; the `glm.nb()` uses the same syntax as `glm()`. 
For a walk-through of different regressions, see the [UCLA stats page](https://stats.idre.ucla.edu/other/dae/). 

#### Univariate `glm()` {.unnumbered}

In this example we are assessing the association between different age categories and the outcome of death (coded as 1 in the Preparation section). Below is a univariate model of `outcome` by `age_cat`. We save the model output as `model` and then print it with `summary()` to the console. Note the estimates provided are the *log odds* and that the baseline level is the first factor level of `age_cat` ("0-4").  

```{r}
model <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
summary(model)
```

To alter the baseline level of a given variable, ensure the column is class Factor and move the desired level to the first position with `fct_relevel()` (see page on [Factors]). For example, below we take column `age_cat` and set "20-29" as the baseline before piping the modified data frame into `glm()`.  

```{r}
linelist %>% 
  mutate(age_cat = fct_relevel(age_cat, "20-29", after = 0)) %>% 
  glm(formula = outcome ~ age_cat, family = "binomial") %>% 
  summary()
```

#### Printing results {.unnumbered}

For most uses, several modifications must be made to the above outputs. The function `tidy()` from the package **broom** is convenient for making the model results presentable.  

Here we demonstrate how to combine model outputs with a table of counts.  

1) Get the *exponentiated* log odds ratio estimates and confidence intervals by passing the model to `tidy()` and setting `exponentiate = TRUE` and `conf.int = TRUE`.  

```{r odds_base_single}

model <- glm(outcome ~ age_cat, family = "binomial", data = linelist) %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE) %>%        # exponentiate and produce CIs
  mutate(across(where(is.numeric), round, digits = 2))  # round all numeric columns
```

Below is the outputted tibble `model`:  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(model, rownames = FALSE, options = list(pageLength = nrow(model), scrollX=T), class = 'white-space: nowrap' )
```

2) Combine these model results with a table of counts. Below, we create the a counts cross-table with the `tabyl()` function from **janitor**, as covered in the [Descriptive tables] page.  

```{r}
counts_table <- linelist %>% 
  janitor::tabyl(age_cat, outcome)
```


<!-- * Group rows by outcome, and get counts by age category   -->
<!-- * Pivot wider so the column are `age_cat`, `0`, and `1`   -->
<!-- * Remove row for `NA` `age_cat`, if applicable, to align with the model results   -->

<!-- ```{r} -->
<!-- counts_table <- linelist %>%  -->
<!--   filter(!is.na(outcome) & !is.na(age_cat)) %>%    # ensure outcome and age_cat are present  -->
<!--   group_by(outcome) %>%                            # get counts of variable of interest grouped by outcome -->
<!--   count(age_cat) %>%   ## gets number or rows by unique outcome-age category combinations   -->
<!--   pivot_wider(names_from = outcome, values_from = n)    ## spread data to wide format (as in cross-tabulation) -->

<!-- ``` -->


Here is what this `counts_table` data frame looks like:  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(counts_table, rownames = FALSE, options = list(pageLength = nrow(counts_table), scrollX=T), class = 'white-space: nowrap' )
```

Now we can bind the `counts_table` and the `model` results together horizontally with `bind_cols()` (**dplyr**). Remember that with `bind_cols()` the rows in the two data frames must be aligned perfectly. In this code, because we are binding within a pipe chain, we use `.` to represent the piped object `counts_table` as we bind it to `model`. To finish the process, we use `select()` to pick the desired columns and their order, and finally apply the **base** R `round()` function across all numeric columns to specify 2 decimal places.  

```{r, message=F, warning=F}
combined <- counts_table %>%           # begin with table of counts
  bind_cols(., model) %>%              # combine with the outputs of the regression 
  select(term, 2:3, estimate,          # select and re-order cols
         conf.low, conf.high, p.value) %>% 
  mutate(across(where(is.numeric), round, digits = 2)) ## round to 2 decimal places
```

Here is what the combined data frame looks like, printed nicely as an image with a function from **flextable**. The [Tables for presentation] explains how to customize such tables with **flextable**, or or you can use numerous other packages such as **knitr** or **GT**.  

```{r}
combined <- combined %>% 
  flextable::qflextable()
```


#### Looping multiple univariate models {.unnumbered}  

Below we present a method using `glm()` and `tidy()` for a more simple approach, see the section on **gtsummary**.  

To run the models on several exposure variables to produce univariate odds ratios (i.e. not controlling for each other), you can use the approach below. It uses `str_c()` from **stringr** to create univariate formulas (see [Characters and strings]), runs the `glm()` regression on each formula, passes each `glm()` output to `tidy()` and finally collapses all the model outputs together with `bind_rows()` from **tidyr**. This approach uses `map()` from the package **purrr** to iterate - see the page on [Iteration, loops, and lists] for more information on this tool.  

1) Create a vector of column names of the explanatory variables. We already have this as `explanatory_vars` from the Preparation section of this page.  

2) Use `str_c()` to create multiple string formulas, with `outcome` on the left, and a column name from `explanatory_vars` on the right. The period `.` substitutes for the column name in `explanatory_vars`.  

```{r}
explanatory_vars %>% str_c("outcome ~ ", .)
```

3) Pass these string formulas to `map()` and set `~glm()` as the function to apply to each input. Within `glm()`, set the regression formula as `as.formula(.x)` where `.x` will be replaced by the string formula defined in the step above. `map()` will loop over each of the string formulas, running regressions for each one.  

4) The outputs of this first `map()` are passed to a second `map()` command, which applies `tidy()` to the regression outputs.  

5) Finally the output of the second `map()` (a list of tidied data frames) is condensed with `bind_rows()`, resulting in one data frame with all the univariate results.  


```{r odds_base_multiple}

models <- explanatory_vars %>%       # begin with variables of interest
  str_c("outcome ~ ", .) %>%         # combine each variable into formula ("outcome ~ variable of interest")
  
  # iterate through each univariate formula
  map(                               
    .f = ~glm(                       # pass the formulas one-by-one to glm()
      formula = as.formula(.x),      # within glm(), the string formula is .x
      family = "binomial",           # specify type of glm (logistic)
      data = linelist)) %>%          # dataset
  
  # tidy up each of the glm regression outputs from above
  map(
    .f = ~tidy(
      .x, 
      exponentiate = TRUE,           # exponentiate 
      conf.int = TRUE)) %>%          # return confidence intervals
  
  # collapse the list of regression outputs in to one data frame
  bind_rows() %>% 
  
  # round all numeric columns
  mutate(across(where(is.numeric), round, digits = 2))
```

This time, the end object `models` is longer because it now represents the combined results of several univariate regressions. Click through to see all the rows of `model`.  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(models, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

As before, we can create a counts table from the `linelist` for each explanatory variable, bind it to `models`, and make a nice table. We begin with the variables, and iterate through them with `map()`. We iterate through a user-defined function which involves creating a counts table with **dplyr** functions. Then the results are combined and bound with the `models` model results.  


```{r, warning=F, message=F}

## for each explanatory variable
univ_tab_base <- explanatory_vars %>% 
  map(.f = 
    ~{linelist %>%                ## begin with linelist
        group_by(outcome) %>%     ## group data set by outcome
        count(.data[[.x]]) %>%    ## produce counts for variable of interest
        pivot_wider(              ## spread to wide format (as in cross-tabulation)
          names_from = outcome,
          values_from = n) %>% 
        drop_na(.data[[.x]]) %>%         ## drop rows with missings
        rename("variable" = .x) %>%      ## change variable of interest column to "variable"
        mutate(variable = as.character(variable))} ## convert to character, else non-dichotomous (categorical) variables come out as factor and cant be merged
      ) %>% 
  
  ## collapse the list of count outputs in to one data frame
  bind_rows() %>% 
  
  ## merge with the outputs of the regression 
  bind_cols(., models) %>% 
  
  ## only keep columns interested in 
  select(term, 2:3, estimate, conf.low, conf.high, p.value) %>% 
  
  ## round decimal places
  mutate(across(where(is.numeric), round, digits = 2))

```

Below is what the data frame looks like. See the page on [Tables for presentation] for ideas on how to further convert this table to pretty HTML output (e.g. with **flextable**).  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(univ_tab_base, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### **gtsummary** package {#reg_gt_uni .unnumbered}

Below we present the use of `tbl_uvregression()` from the **gtsummary** package. Just like in the page on [Descriptive tables](https://epirhandbook.com/descriptive-tables.html), **gtsummary** functions do a good job of running statistics *and* producing professional-looking outputs. This function produces a table of univariate regression results.  

We select only the necessary columns from the `linelist` (explanatory variables and the outcome variable) and  pipe them into `tbl_uvregression()`. We are going to run univariate regression on each of the columns we defined as `explanatory_vars` in the data Preparation section (gender, fever, chills, cough, aches, vomit, and age_cat).  

Within the function itself, we provide the `method = ` as `glm` (no quotes), the `y = ` outcome column (`outcome`), specify to `method.args = ` that we want to run logistic regression via `family = binomial`, and we tell it to exponentiate the results.  

The output is HTML and contains the counts

```{r odds_gt, message=F, warning=F}

univ_tab <- linelist %>% 
  dplyr::select(explanatory_vars, outcome) %>% ## select variables of interest

  tbl_uvregression(                         ## produce univariate table
    method = glm,                           ## define regression want to run (generalised linear model)
    y = outcome,                            ## define outcome variable
    method.args = list(family = binomial),  ## define what type of glm want to run (logistic)
    exponentiate = TRUE                     ## exponentiate to produce odds ratios (rather than log odds)
  )

## view univariate results table 
univ_tab
```

There are many modifications you can make to this table output, such as adjusting the text labels, bolding rows by their p-value, etc. See tutorials [here](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html) and elsewhere online.  



<!-- ======================================================= -->

## Stratified {  }

Stratified analysis is currently still being worked on for **gtsummary**, 
this page will be updated in due course. 




## Multivariable  

For multivariable analysis, we again present two approaches:  

* `glm()` and `tidy()`  
* **gtsummary** package  

The workflow is similar for each and only the last step of pulling together a final table is different.


### Conduct multivariable {.unnumbered}  


Here we use `glm()` but add more variables to the right side of the equation, separated by plus symbols (`+`). 


To run the model with all of our explanatory variables we would run:  

```{r}
mv_reg <- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = "binomial", data = linelist)

summary(mv_reg)
```

If you want to include two variables and an interaction between them you can separate them with an asterisk `*` instead of a `+`. Separate them with a colon `:` if you are only specifying the interaction. For example:  

```{r, eval=F}
glm(outcome ~ gender + age_cat * fever, family = "binomial", data = linelist)
```


*Optionally*, you can use this code to leverage the pre-defined vector of column names and re-create the above command using `str_c()`. This might be useful if your explanatory variable names are changing, or you don't want to type them all out again.  

```{r mv_regression}

## run a regression with all variables of interest 
mv_reg <- explanatory_vars %>%  ## begin with vector of explanatory column names
  str_c(collapse = "+") %>%     ## combine all names of the variables of interest separated by a plus
  str_c("outcome ~ ", .) %>%    ## combine the names of variables of interest with outcome in formula style
  glm(family = "binomial",      ## define type of glm as logistic,
      data = linelist)          ## define your dataset
```


#### Building the model {.unnumbered}  

You can build your model step-by-step, saving various models that include certain explanatory variables. You can compare these models with likelihood-ratio tests using `lrtest()` from the package **lmtest**, as below:  

<span style="color: black;">**_NOTE:_** Using **base** `anova(model1, model2, test = "Chisq)` produces the same results </span> 

```{r}
model1 <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
model2 <- glm(outcome ~ age_cat + gender, family = "binomial", data = linelist)

lmtest::lrtest(model1, model2)
```

Another option is to take the model object and apply the `step()` function from the **stats** package. Specify which variable selection direction you want use when building the model.      

```{r}
## choose a model using forward selection based on AIC
## you can also do "backward" or "both" by adjusting the direction
final_mv_reg <- mv_reg %>%
  step(direction = "forward", trace = FALSE)
```


You can also turn off scientific notation in your R session, for clarity:  

```{r}
options(scipen=999)
```

As described in the section on univariate analysis, pass the model output to `tidy()` to exponentiate the log odds and CIs. Finally we round all numeric columns to two decimal places. Scroll through to see all the rows.  

```{r mv_regression_base}

mv_tab_base <- final_mv_reg %>% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %>%  ## get a tidy dataframe of estimates 
  mutate(across(where(is.numeric), round, digits = 2))          ## round 
```

Here is what the resulting data frame looks like: 

```{r, message=FALSE, echo=F}
DT::datatable(mv_tab_base, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### Combine univariate and multivariable {.unnumbered}

#### Combine with **gtsummary**  {.unnumbered}  

The **gtsummary** package provides the `tbl_regression()` function, which will 
take the outputs from a regression (`glm()` in this case) and produce an nice 
summary table. 

```{r mv_regression_gt}
## show results table of final regression 
mv_tab <- tbl_regression(final_mv_reg, exponentiate = TRUE)
```

Let's see the table:  

```{r}
mv_tab
```

You can also combine several different output tables produced by **gtsummary** with 
the `tbl_merge()` function. We now combine the multivariable results with the **gtsummary** *univariate* results that we created [above](#reg_gt_uni):  

```{r}
## combine with univariate results 
tbl_merge(
  tbls = list(univ_tab, mv_tab),                          # combine
  tab_spanner = c("**Univariate**", "**Multivariable**")) # set header names
```



#### Combine with **dplyr** {.unnumbered}  

An alternative way of combining the `glm()`/`tidy()` univariate and multivariable outputs is with the **dplyr** join functions.  

* Join the univariate results from earlier (`univ_tab_base`, which contains counts) with the tidied multivariable results `mv_tab_base`  
* Use `select()` to keep only the columns we want, specify their order, and re-name them  
* Use `round()` with two decimal places on all the column that are class Double  

```{r, warning=F, message=F}
## combine univariate and multivariable tables 
left_join(univ_tab_base, mv_tab_base, by = "term") %>% 
  ## choose columns and rename them
  select( # new name =  old name
    "characteristic" = term, 
    "recovered"      = "0", 
    "dead"           = "1", 
    "univ_or"        = estimate.x, 
    "univ_ci_low"    = conf.low.x, 
    "univ_ci_high"   = conf.high.x,
    "univ_pval"      = p.value.x, 
    "mv_or"          = estimate.y, 
    "mvv_ci_low"     = conf.low.y, 
    "mv_ci_high"     = conf.high.y,
    "mv_pval"        = p.value.y 
  ) %>% 
  mutate(across(where(is.double), round, 2))   

```




<!-- ======================================================= -->

## Forest plot {  }

This section shows how to produce a plot with the outputs of your regression.
There are two options, you can build a plot yourself using **ggplot2** or use a 
meta-package called **easystats** (a package that includes many packages).  

See the page on [ggplot basics] if you are unfamiliar with the **ggplot2** plotting package.  


<!-- ======================================================= -->

### **ggplot2** package {.unnumbered}

You can build a forest plot with `ggplot()` by plotting elements of the multivariable regression results. Add the layers of the plots using these "geoms":  

* estimates with `geom_point()`  
* confidence intervals with `geom_errorbar()`  
* a vertical line at OR = 1 with `geom_vline()`  

Before plotting, you may want to use `fct_relevel()` from the **forcats** package to set the order of the variables/levels on the y-axis. `ggplot()` may display them in alpha-numeric order which would not work well for these age category values ("30" would appear before "5"). See the page on [Factors] for more details.  

```{r ggplot_forest}

## remove the intercept term from your multivariable results
mv_tab_base %>% 
  
  #set order of levels to appear along y-axis
  mutate(term = fct_relevel(
    term,
    "vomit", "gender", "fever", "cough", "chills", "aches",
    "age_cat5-9", "age_cat10-14", "age_cat15-19", "age_cat20-29",
    "age_cat30-49", "age_cat50-69", "age_cat70+")) %>%
  
  # remove "intercept" row from plot
  filter(term != "(Intercept)") %>% 
  
  ## plot with variable on the y axis and estimate (OR) on the x axis
  ggplot(aes(x = estimate, y = term)) +
  
  ## show the estimate as a point
  geom_point() + 
  
  ## add in an error bar for the confidence intervals
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  
  ## show where OR = 1 is for reference as a dashed line
  geom_vline(xintercept = 1, linetype = "dashed")
  
```


<!-- ======================================================= -->

### **easystats** packages {.unnumbered}

An alternative, if you do not want to the fine level of control that **ggplot2** provides, is to use a combination of **easystats** packages.  

The function `model_parameters()` from the **parameters** package does the equivalent
of the **broom** package function `tidy()`. The **see** package then accepts those outputs
and creates a default forest plot as a `ggplot()` object. 

```{r easystats_forest}
pacman::p_load(easystats)

## remove the intercept term from your multivariable results
final_mv_reg %>% 
  model_parameters(exponentiate = TRUE) %>% 
  plot()
  
```


<!-- ======================================================= -->

## Resources {  }

The content of this page was informed by these resources and vignettes online:  

[Linear regression in R](https://www.datacamp.com/community/tutorials/linear-regression-R)  

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)  

[UCLA stats page](https://stats.idre.ucla.edu/other/dae/)  

[sthda stepwise regression](http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/)   

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/regression.Rmd-->


# Missing data { }

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "missingness.png"))
knitr::include_graphics(here::here("images", "missingness_overview.png"))
```

This page will cover how to:  

1) Assess missingness  
2) Filter out rows by missingness  
3) Plot missingness over time  
4) Handle how `NA` is displayed in plots  
5) Perform missing value imputation: MCAR, MAR, MNAR  



<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # import/export
  tidyverse,     # data mgmt and viz
  naniar,        # assess and visualize missingness
  mice           # missing data imputation
)
```


### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Convert missing on import {.unnumbered}  

When importing your data, be aware of values that should be classified as missing. For example, 99, 999, "Missing", blank cells (""), or cells with an empty space (" "). You can convert these to `NA` (R's version of missing data) during the data import command.  
See the page on importing page section on [Missing data](#import_missing) for details, as the exact syntax varies by file type.  


<!-- ======================================================= -->
## Missing values in R { }

Below we explore ways that missingness is presented and assessed in R, along with some adjacent values and functions.  

### `NA` {.unnumbered}  

In R, missing values are represented by a reserved (special) value - `NA`. Note that this is typed *without* quotes. "NA" is different and is just a normal character value (also a Beatles lyric from the song Hey Jude).  

Your data may have other ways of representing missingness, such as "99", or "Missing", or "Unknown" - you may even have empty character value "" which looks "blank", or a single space " ". Be aware of these and consider whether to [convert them to `NA` during import](#import_missing) or during data cleaning with `na_if()`.  

In your data cleaning, you may also want to convert the other way - changing all `NA` to "Missing" or similar with `replace_na()` or with `fct_explicit_na()` for factors.  




### Versions of `NA` {.unnumbered}  

Most of the time, `NA` represents a missing value and everything works fine. However, in some circumstances you may encounter the need for *variations* of `NA` specific to an object class (character, numeric, etc). This will be rare, but you should be aware.    
The typical scenario for this is when creating a new column with the **dplyr** function `case_when()`. As described in the [Cleaning data and core functions](#clean_case_when) page, this function evaluates every row in the data frame, assess whether the rows meets specified logical criteria (right side of the code), and assigns the correct new value (left side of the code). *Importantly: all values on the right side must be the same class*.  

```{r, eval=F}
linelist <- linelist %>% 
  
  # Create new "age_years" column from "age" column
  mutate(age_years = case_when(
    age_unit == "years"  ~ age,       # if age is given in years, assign original value
    age_unit == "months" ~ age/12,    # if age is given in months, divide by 12
    is.na(age_unit)      ~ age,       # if age UNIT is missing, assume years
    TRUE                 ~ NA_real_)) # any other circumstance, assign missing
```

If you want `NA` on the right side, you may need to specify one of the special `NA` options listed below. If the other right side values are character, consider using "Missing" instead or otherwise use `NA_character_`. If they are all numeric, use `NA_real_`. If they are all dates or logical, you can use `NA`.  

* `NA` - use for dates or logical TRUE/FALSE 
* `NA_character_` - use for characters  
* `NA_real_`  - use for numeric

Again, it is not likely you will encounter these variations *unless* you are using `case_when()` to create a new column. See the [R documentation on NA](https://stat.ethz.ch/R-manual/R-devel/library/base/html/NA.html) for more information. 





### `NULL` {.unnumbered}  

`NULL` is another reserved value in R. It is the logical representation of a statement that is neither true nor false. It is returned by expressions or functions whose values are undefined. Generally do not assign NULL as a value, unless writing functions or perhaps writing a [**shiny** app][Dashboards with Shiny] to return `NULL` in specific scenarios.  

Null-ness can be assessed using `is.null()` and conversion can made with `as.null()`.  

See this [blog post](https://www.r-bloggers.com/2010/04/r-na-vs-null/) on the difference between `NULL` and `NA`.  




### `NaN` {.unnumbered}  

Impossible values are represented by the special value `NaN`. An example of this is when you force R to divide 0 by 0. You can assess this with `is.nan()`. You may also encounter complementary functions including `is.infinite()` and `is.finite()`.  


### `Inf` {.unnumbered}  

`Inf` represents an infinite value, such as when you divide a number by 0.  

As an example of how this might impact your work: let's say you have a vector/column `z` that contains these values: `z <- c(1, 22, NA, Inf, NaN, 5)`

If you want to use `max()` on the column to find the highest value, you can use the `na.rm = TRUE` to remove the `NA` from the calculation, but the `Inf` and `NaN` remain and `Inf` will be returned. To resolve this, you can use brackets `[ ]` and `is.finite()` to subset such that only finite values are used for the calculation: `max(z[is.finite(z)])`.  

```{r, eval=F}
z <- c(1, 22, NA, Inf, NaN, 5)
max(z)                           # returns NA
max(z, na.rm=T)                  # returns Inf
max(z[is.finite(z)])             # returns 22
```


### Examples {.unnumbered}  


R command | Outcome
----------|--------------
`5 / 0` | `Inf`  
`0 / 0` | `NaN`  
`5 / NA` | `NA`  
`5 / Inf | `0`  
`NA - 5` | `NA`  
`Inf / 5` | `Inf`  
`class(NA)` | "logical"  
`class(NaN)` | "numeric"  
`class(Inf)` | "numeric"  
`class(NULL)` | "NULL"  

"NAs introduced by coercion" is a common warning message. This can happen if you attempt to make an illegal conversion like inserting a character value into a vector that is otherwise numeric.  

```{r}
as.numeric(c("10", "20", "thirty", "40"))
```

`NULL` is ignored in a vector.  

```{r}
my_vector <- c(25, NA, 10, NULL)  # define
my_vector                         # print
```


Variance of one number results in `NA`.  

```{r}
var(22)
```


<!-- ======================================================= -->
## Useful functions { }

The following are useful **base** R functions when assessing or handling missing values:  


### `is.na()` and `!is.na()` {.unnumbered}  

Use `is.na()`to identify missing values, or use its opposite (with `!` in front) to identify non-missing values. These both return a logical value (`TRUE` or `FALSE`). Remember that you can `sum()` the resulting vector to count the number `TRUE`, e.g. `sum(is.na(linelist$date_outcome))`.    

```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)
is.na(my_vector)
!is.na(my_vector)
sum(is.na(my_vector))
```


### `na.omit()` {.unnumbered}  

This function, if applied to a data frame, will remove rows with *any* missing values. It is also from **base** R.  
If applied to a vector, it will remove `NA` values from the vector it is applied to. For example:  

```{r}
na.omit(my_vector)
```

### `drop_na()` {.unnumbered}  

This is a **tidyr** function that is useful in a [data cleaning pipeline][Cleaning data and core functions]. If run with the parentheses empty, it removes rows with *any* missing values. If column names are specified in the parentheses, rows with missing values in those columns will be dropped. You can also use "tidyselect" syntax to specify the columns.  

```{r, eval=F}
linelist %>% 
  drop_na(case_id, date_onset, age) # drops rows missing values for any of these columns
```


### `na.rm = TRUE` {.unnumbered}  

When you run a mathematical function such as `max()`, `min()`, `sum()` or `mean()`, if there are any `NA` values present the returned value will be `NA`. This default behavior is intentional, so that you are alerted if any of your data are missing.  

You can avoid this by removing missing values from the calculation. To do this, include the argument `na.rm = TRUE` ("na.rm" stands for "remove `NA`").  


```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)

mean(my_vector)     

mean(my_vector, na.rm = TRUE)
```



<!-- ======================================================= -->
## Assess missingness in a data frame { }

You can use the package **naniar** to assess and visualize missingness in the data frame `linelist`.  

```{r}
# install and/or load package
pacman::p_load(naniar)
```

### Quantifying missingness {.unnumbered}

To find the percent of all values that are missing use `pct_miss()`. Use `n_miss()` to get the number of missing values.  

```{r}
# percent of ALL data frame values that are missing
pct_miss(linelist)
```

The two functions below return the percent of rows with any missing value, or that are entirely complete, respectively. Remember that `NA` means missing, and that ``""` or `" "` will not be counted as missing.  

```{r}
# Percent of rows with any value missing
pct_miss_case(linelist)   # use n_complete() for counts
```

```{r}
# Percent of rows that are complete (no values missing)  
pct_complete_case(linelist) # use n_complete() for counts
```



### Visualizing missingness {.unnumbered}  

The `gg_miss_var()` function will show you the number (or %) of missing values in each column. A few nuances:  

* You can add a column name (not in quote) to the argument `facet = ` to see the plot by groups  
* By default, counts are shown instead of percents, change this with `show_pct = TRUE`  
* You can add axis and title labels as for a normal `ggplot()` with `+ labs(...)`  


```{r}
gg_miss_var(linelist, show_pct = TRUE)
```

Here the data are piped `%>%` into the function. The `facet = ` argument is also used to split the data.  

```{r}
linelist %>% 
  gg_miss_var(show_pct = TRUE, facet = outcome)
```


You can use `vis_miss()` to visualize the data frame as a heatmap, showing whether each value is missing or not. You can also `select()` certain columns from the data frame and provide only those columns to the function.    

```{r}
# Heatplot of missingness across the entire data frame  
vis_miss(linelist)
```


### Explore and visualize missingness relationships {.unnumbered} 

How do you visualize something that is not there??? By default, `ggplot()` removes points with missing values from plots.  

**naniar** offers a solution via `geom_miss_point()`. When creating a scatterplot of two columns, records with one of the values missing and the other value present are shown by setting the missing values to 10% lower than the lowest value in the column, and coloring them distinctly.  

In the scatterplot below, the red dots are records where the value for one column is present but the value for the other column is missing. This allows you to see the distribution of missing values in relation to the non-missing values.  



```{r}
ggplot(
  data = linelist,
  mapping = aes(x = age_years, y = temp)) +     
  geom_miss_point()
```

To assess missingness in the data frame *stratified by another column*, consider `gg_miss_fct()`, which returns a heatmap of percent missingness in the data frame *by a factor/categorical (or date) column*:  

```{r}
gg_miss_fct(linelist, age_cat5)
```


This function can also be used with a date column to see how missingness has changed over time:  

```{r}
gg_miss_fct(linelist, date_onset)
```




### "Shadow" columns {.unnumbered}

Another way to visualize missingness in one column by values in a second column is using the "shadow" that **naniar** can create. `bind_shadow()` creates a binary `NA`/not `NA` column for every existing column, and binds all these new columns to the original dataset with the appendix "_NA". This doubles the number of columns - see below:  


```{r}
shadowed_linelist <- linelist %>% 
  bind_shadow()

names(shadowed_linelist)
```

These "shadow" columns can be used to plot the proportion of values that are missing, by any another column.  

For example, the plot below shows the proportion of records missing `days_onset_hosp` (number of days from symptom onset to hospitalisation), by that record's value in `date_hospitalisation`. Essentially, you are plotting the density of the x-axis column, but stratifying the results (`color = `) by a shadow column of interest. This analysis works best if the x-axis is a numeric or date column.  


```{r, message = F}
ggplot(data = shadowed_linelist,          # data frame with shadow columns
  mapping = aes(x = date_hospitalisation, # numeric or date column
                colour = age_years_NA)) + # shadow column of interest
  geom_density()                          # plots the density curves
```

You can also use these "shadow" columns to stratify a statistical summary, as shown below:

```{r}
linelist %>%
  bind_shadow() %>%                # create the shows cols
  group_by(date_outcome_NA) %>%    # shadow col for stratifying
  summarise(across(
    .cols = age_years,             # variable of interest for calculations
    .fns = list("mean" = mean,     # stats to calculate
                "sd" = sd,
                "var" = var,
                "min" = min,
                "max" = max),  
    na.rm = TRUE))                 # other arguments for the stat calculations
```


An alternative way to plot the proportion of a column's values that are missing over time is shown below. It does *not* involve **naniar**. This example shows percent of weekly observations that are missing).  

1) Aggregate the data into a useful time unit (days, weeks, etc.), summarizing the proportion of observations with `NA` (and any other values of interest)  
2) Plot the proportion missing as a line using `ggplot()`  

Below, we take the linelist, add a new column for week, group the data by week, and then calculate the percent of that week's records where the value is missing. (note: if you want % of 7 days the calculation would be slightly different).  

```{r}
outcome_missing <- linelist %>%
  mutate(week = lubridate::floor_date(date_onset, "week")) %>%   # create new week column
  group_by(week) %>%                                             # group the rows by week
  summarise(                                                     # summarize each week
    n_obs = n(),                                                  # number of records
    
    outcome_missing = sum(is.na(outcome) | outcome == ""),        # number of records missing the value
    outcome_p_miss  = outcome_missing / n_obs,                    # proportion of records missing the value
  
    outcome_dead    = sum(outcome == "Death", na.rm=T),           # number of records as dead
    outcome_p_dead  = outcome_dead / n_obs) %>%                   # proportion of records as dead
  
  tidyr::pivot_longer(-week, names_to = "statistic") %>%         # pivot all columns except week, to long format for ggplot
  filter(stringr::str_detect(statistic, "_p_"))                  # keep only the proportion values
```

Then we plot the proportion missing as a line, by week. The [ggplot basics] page if you are unfamiliar with the **ggplot2** plotting package.  

```{r, message=F, warning=F}
ggplot(data = outcome_missing)+
    geom_line(
      mapping = aes(x = week, y = value, group = statistic, color = statistic),
      size = 2,
      stat = "identity")+
    labs(title = "Weekly outcomes",
         x = "Week",
         y = "Proportion of weekly records") + 
     scale_color_discrete(
       name = "",
       labels = c("Died", "Missing outcome"))+
    scale_y_continuous(breaks = c(seq(0,1,0.1)))+
  theme_minimal()+
  theme(legend.position = "bottom")
```





<!-- ======================================================= -->
## Using data with missing values  


### Filter out rows with missing values {.unnumbered}

To quickly remove rows with missing values, use the **dplyr** function `drop_na()`.  

The original `linelist` has ` nrow(linelist)` rows. The adjusted number of rows is shown below:  

```{r}
linelist %>% 
  drop_na() %>%     # remove rows with ANY missing values
  nrow()
```

You can specify to drop rows with missingness in certain columns:  

```{r}
linelist %>% 
  drop_na(date_onset) %>% # remove rows missing date_onset 
  nrow()
```

You can list columns one after the other, or use ["tidyselect" helper functions](#clean_tidyselect):  

```{r}
linelist %>% 
  drop_na(contains("date")) %>% # remove rows missing values in any "date" column 
  nrow()
```



<!-- ======================================================= -->
### Handling `NA` in `ggplot()` {.unnumbered}

It is often wise to report the number of values excluded from a plot in a caption. Below is an example:  

In `ggplot()`, you can add `labs()` and within it a `caption = `. In the caption, you can use `str_glue()` from **stringr** package to paste values together into a sentence dynamically so they will adjust to the data. An example is below:  

* Note the use of `\n` for a new line.  
* Note that if multiple column would contribute to values not being plotted (e.g. age or sex if those are reflected in the plot), then you must filter on those columns as well to correctly calculate the number not shown.  

```{r, eval=F}
labs(
  title = "",
  y = "",
  x = "",
  caption  = stringr::str_glue(
  "n = {nrow(central_data)} from Central Hospital;
  {nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown."))  
```

Sometimes, it can be easier to save the string as an object in commands prior to the `ggplot()` command, and simply reference the named string object within the `str_glue()`.  


<!-- ======================================================= -->
### `NA` in factors {.unnumbered}

If your column of interest is a factor, use `fct_explicit_na()` from the **forcats** package to convert `NA` values to a character value. See more detail in the [Factors] page. By default, the new value is "(Missing)" but this can be adjusted via the `na_level =` argument.   

```{r}
pacman::p_load(forcats)   # load package

linelist <- linelist %>% 
  mutate(gender = fct_explicit_na(gender, na_level = "Missing"))

levels(linelist$gender)
```



<!-- ======================================================= -->
## Imputation { }


Sometimes, when analyzing your data, it will be important to "fill in the gaps" and impute missing data While you can always simply analyze a dataset after removing all missing values, this can cause problems in many ways. Here are two examples: 

1) By removing all observations with missing values or variables with a large amount of missing data, you might reduce your power or ability to do some types of analysis. For example, as we discovered earlier, only a small fraction of the observations in our linelist dataset have no missing data across all of our variables. If we removed the majority of our dataset we'd be losing a lot of information! And, most of our variables have some amount of missing data--for most analysis it's probably not reasonable to drop every variable that has a lot of missing data either.

2) Depending on why your data is missing, analysis of only non-missing data might lead to biased or misleading results. For example, as we learned earlier we are missing data for some patients about whether they've had some important symptoms like fever or cough. But, as one possibility, maybe that information wasn't recorded for people that just obviously weren't very sick. In that case, if we just removed these observations we'd be excluding some of the healthiest people in our dataset and that might really bias any results.

It's important to think about why your data might be missing in addition to seeing how much is missing. Doing this can help you decide how important it might be to impute missing data, and also which method of imputing missing data might be best in your situation.

### Types of missing data {.unnumbered}

Here are three general types of missing data:

1) **Missing Completely at Random** (MCAR). This means that there is no relationship between the probability of data being missing and any of the other variables in your data. The probability of being missing is the same for all cases This is a rare situation. But, if you have strong reason to believe your data is MCAR analyzing only non-missing data without imputing won't bias your results (although you may lose some power). [TODO: consider discussing statistical tests for MCAR]

2) **Missing at Random** (MAR). This name is actually a bit misleading as MAR means that your data is missing in a systematic, predictable way based on the other information you have. For example, maybe every observation in our dataset with a missing value for fever was actually not recorded because every patient with chills and and aches was just assumed to have a fever so their temperature was never taken. If true, we could easily predict that every missing observation with chills and aches has a fever as well and use this information to impute our missing data. In practice, this is more of a spectrum. Maybe if a patient had both chills and aches they were more likely to have a fever as well if they didn't have their temperature taken, but not always. This is still predictable even if it isn't perfectly predictable. This is a common type of missing data 

3) **Missing not at Random** (MNAR). Sometimes, this is also called **Not Missing at Random** (NMAR). This assumes that the probability of a value being missing is NOT systematic or predictable using the other information we have but also isn't missing randomly. In this situation data is missing for unknown reasons or for reasons you don't have any information about. For example, in our dataset maybe information on age is missing because some very elderly patients either don't know or refuse to say how old they are. In this situation, missing data on age is related to the value itself (and thus isn't random) and isn't predictable based on the other information we have. MNAR is complex and often the best way of dealing with this is to try to collect more data or information about why the data is missing rather than attempt to impute it. 

In general, imputing MCAR data is often fairly simple, while MNAR is very challenging if not impossible. Many of the common data imputation methods assume MAR. 

### Useful packages {.unnumbered}

Some useful packages for imputing missing data are Mmisc, missForest (which uses random forests to impute missing data), and mice (Multivariate Imputation by Chained Equations). For this section we'll just use the mice package, which implements a variety of techniques. The maintainer of the mice package has published an online book about imputing missing data that goes into more detail here (https://stefvanbuuren.name/fimd/).  

Here is the code to load the mice package:

```{r}
pacman::p_load(mice)
```

### Mean Imputation {.unnumbered}

Sometimes if you are doing a simple analysis or you have strong reason to think you can assume MCAR, you can simply set missing numerical values to the mean of that variable. Perhaps we can assume that missing temperature measurements in our dataset were either MCAR or were just normal values. Here is the code to create a new variable that replaces missing temperature values with the mean temperature value in our dataset. However, in many situations replacing data with the mean can lead to bias, so be careful.

```{r}
linelist <- linelist %>%
  mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))
```

You could also do a similar process for replacing categorical data with a specific value. For our dataset, imagine you knew that all observations with a missing value for their outcome (which can be "Death" or "Recover") were actually people that died (note: this is not actually true for this dataset):

```{r}
linelist <- linelist %>%
  mutate(outcome_replace_na_with_death = replace_na(outcome, "Death"))
```

### Regression imputation {.unnumbered}

A somewhat more advanced method is to use some sort of statistical model to predict what a missing value is likely to be and replace it with the predicted value. Here is an example of creating predicted values for all the observations where temperature is missing, but age and fever are not, using simple linear regression using fever status and age in years as predictors. In practice you'd want to use a better model than this sort of simple approach.

```{r, warning=F, message=F}
simple_temperature_model_fit <- lm(temp ~ fever + age_years, data = linelist)

#using our simple temperature model to predict values just for the observations where temp is missing
predictions_for_missing_temps <- predict(simple_temperature_model_fit,
                                        newdata = linelist %>% filter(is.na(temp))) 
```

Or, using the same modeling approach through the mice package to create imputed values for the missing temperature observations:

```{r}
model_dataset <- linelist %>%
  select(temp, fever, age_years)  

temp_imputed <- mice(model_dataset,
                            method = "norm.predict",
                            seed = 1,
                            m = 1,
                            print = F)

temp_imputed_values <- temp_imputed$imp$temp

```


This is the same type of approach by some more advanced methods like using the missForest package to replace missing data with predicted values. In that case, the prediction model is a random forest instead of a linear regression. You can use other types of models to do this as well. However, while this approach works well under MCAR you should be a bit careful if you believe MAR or MNAR more accurately describes your situation. The quality of your imputation will depend on how good your prediction model is and even with a very good model the variability of your imputed data may be underestimated. 

### LOCF and BOCF {.unnumbered}

Last observation carried forward (LOCF) and baseline observation carried forward (BOCF) are imputation methods for time series/longitudinal data. The idea is to take the previous observed value as a replacement for the missing data. When multiple values are missing in succession, the method searches for the last observed value.

The `fill()` function from the **tidyr** package can be used for both LOCF and BOCF imputation (however, other packages such as **HMISC**, **zoo**, and **data.table** also include methods for doing this). To show the `fill()` syntax we'll make up a simple time series dataset containing the number of cases of a disease for each quarter of the years 2000 and 2001. However, the year value for subsequent quarters after Q1 are missing so we'll need to impute them. The `fill()` junction is also demonstrated in the [Pivoting data] page.  

```{r}
#creating our simple dataset
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",    2000,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",      NA,    21001,
  "Q1",    2001,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",      NA,    50197)

#imputing the missing year values:
disease %>% fill(year)

```

Note: make sure your data are sorted correctly before using the `fill()` function. `fill()`  defaults to filling "down" but you can also impute values in different directions by changing the `.direction` parameter. We can make a similar dataset where the year value is recorded only at the end of the year and missing for earlier quarters: 

```{r}
#creating our slightly different dataset
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",      NA,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",    2000,    21001,
  "Q1",      NA,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",    2001,    50197)

#imputing the missing year values in the "up" direction:
disease %>% fill(year, .direction = "up")

```
In this example, LOCF and BOCF are clearly the right things to do, but in more complicated situations it may be harder to decide if these methods are appropriate. For example, you may have missing laboratory values for a hospital patient after the first day. Sometimes, this can mean the lab values didn't change...but it could also mean the patient recovered and their values would be very different after the first day! Use these methods with caution.


### Multiple Imputation {.unnumbered}

The online book we mentioned earlier by the author of the mice package (https://stefvanbuuren.name/fimd/) contains a detailed explanation of multiple imputation and why you'd want to use it. But, here is a basic explanation of the method:

When you do multiple imputation, you create multiple datasets with the missing values imputed to plausible data values (depending on your research data you might want to create more or less of these imputed datasets, but the mice package sets the default number to 5). The difference is that rather than a single, specific value each imputed value is drawn from an estimated distribution (so it includes some randomness). As a result, each of these datasets will have slightly different different imputed values (however, the non-missing data will be the same in each of these imputed datasets). You still use some sort of predictive model to do the imputation in each of these new datasets (mice has many options for prediction methods including *Predictive Mean Matching*, *logistic regression*, and *random forest*) but the mice package can take care of many of the modeling details. 

Then, once you have created these new imputed datasets, you can apply then apply whatever statistical model or analysis you were planning to do for each of these new imputed datasets and pool the results of these models together. This works very well to reduce bias in both MCAR and many MAR settings and often results in more accurate standard error estimates.

Here is an example of applying the Multiple Imputation process to predict temperature in our linelist dataset using a age and fever status (our simplified model_dataset from above):  

```{r}
# imputing missing values for all variables in our model_dataset, and creating 10 new imputed datasets
multiple_imputation = mice(
  model_dataset,
  seed = 1,
  m = 10,
  print = FALSE) 

model_fit <- with(multiple_imputation, lm(temp ~ age_years + fever))

base::summary(mice::pool(model_fit))
```

Here we used the mice default method of imputation, which is Predictive Mean Matching. We then used these imputed datasets to separately estimate and then pool results from simple linear regressions on each of these datasets. There are many details we've glossed over and many settings you can adjust during the Multiple Imputation process while using the mice package. For example, you won't always have numerical data and might need to use other imputation methods (you can still use the mice package for many other types of data and methods). But, for a more robust analysis when missing data is a significant concern, Multiple Imputation is good solution that isn't always much more work than doing a complete case analysis. 





<!-- ======================================================= -->
## Resources { }

Vignette on the [naniar package](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)

Gallery of [missing value visualizations](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html)

[Online book](https://stefvanbuuren.name/fimd/) about multiple imputation in R by the maintainer of the **mice** package 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/missing_data.Rmd-->


# Standardised rates { }  

This page will show you two ways to standardize an outcome, such as hospitalizations or mortality, by characteristics such as age and sex. 

* Using **dsr** package 
* Using **PHEindicatormethods** package  

We begin by extensively demonstrating the processes of data preparation/cleaning/joining, as this is common when combining population data from multiple countries, standard population data, deaths, etc.  

## Overview  

There are two main ways to standardize: direct and indirect standardization.
Let's say we would like to the standardize mortality rate by age and sex for country A and country B, and compare the standardized rates between these countries.

* For direct standardization, you will have to know the number of the at-risk population and the number of deaths for each stratum of age and sex, for country A and country B. One stratum in our example could be females between ages 15-44.  
* For indirect standardization, you only need to know the total number of deaths and the age- and sex structure of each country. This option is therefore feasible if age- and sex-specific mortality rates or population numbers are not available. Indirect standardization is furthermore preferable in case of small numbers per stratum, as estimates in direct standardization would be influenced by substantial sampling variation. 

<!-- ======================================================= -->
## Preparation {  }

To show how standardization is done, we will use fictitious population counts and death counts from  country A and country B, by age (in 5 year categories) and sex (female, male). To make the datasets ready for use, we will perform the following preparation steps:  

1. Load packages  
2. Load datasets  
3. Join the population and death data from the two countries
4. Pivot longer so there is one row per age-sex stratum
5. Clean the reference population (world standard population) and join it to the country data  

In your scenario, your data may come in a different format. Perhaps your data are by province, city, or other catchment area. You may have one row for each death and information on age and sex for each (or a significant proportion) of these deaths. In this case, see the pages on [Grouping data], [Pivoting data], and [Descriptive tables] to create a dataset with event and population counts per age-sex stratum.  

We also need a reference population, the standard population. For the purposes of this exercise we will use the `world_standard_population_by_sex`. The World standard population is based on the populations of 46 countries and was developed in 1960. There are many "standard" populations - as one example, the website of [NHS Scotland](https://www.opendata.nhs.scot/dataset/standard-populations) is quite informative on the European Standard Population, World Standard Population and Scotland Standard Population. 

<!-- ======================================================= -->
### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
     rio,                 # import/export data
     here,                # locate files
     tidyverse,           # data management and visualization
     stringr,             # cleaning characters and strings
     frailtypack,         # needed for dsr, for frailty models
     dsr,                 # standardise rates
     PHEindicatormethods) # alternative for rate standardisation
```


<span style="color: orange;">**_CAUTION:_** If you have a newer version of R, the **dsr** package cannot be directly downloaded from CRAN. However, it is still available from the CRAN archive. You can install and use this one. </span>

For non-Mac users:  

```{r, eval=F} 
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

```{r, eval=FALSE}
# Other solution that may work
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="http:/cran.us.r.project.org")
```

For Mac users:  

```{r, eval=FALSE}
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="https://mac.R-project.org")
```




### Load population data {.unnumbered}  

See the [Download handbook and data] page for instructions on how to download all the example data in the handbook. You can import the Standardisation page data directly into R from our Github repository by running the following `import()` commands:  

```{r, eval=F}
# import demographics for country A directly from Github
A_demo <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics.csv")

# import deaths for country A directly from Github
A_deaths <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryA.csv")

# import demographics for country B directly from Github
B_demo <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics_2.csv")

# import deaths for country B directly from Github
B_deaths <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryB.csv")

# import demographics for country B directly from Github
standard_pop_data <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/world_standard_population_by_sex.csv")

```


First we load the demographic data (counts of males and females by 5-year age category) for the two countries that we will be comparing, "Country A" and "Country B".  

```{r, echo=F}
# Country A
A_demo <- rio::import(here::here("data", "standardization", "country_demographics.csv")) %>% 
     mutate(Country = "A") %>% 
     select(Country, everything()) %>% # re-arrange
     mutate(age_cat5 = str_replace_all(age_cat5, "\\+", "")) # remove + symbols
```

```{r, eval=F}
# Country A
A_demo <- import("country_demographics.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(A_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
# Country B
B_demo <- rio::import(here::here("data", "standardization", "country_demographics_2.csv")) %>% 
     mutate(Country = "B") %>% 
     select(Country, everything()) # re-arrange
```

```{r, eval=F}
# Country B
B_demo <- import("country_demographics_2.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(B_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





### Load death counts {.unnumbered}  

Conveniently, we also have the counts of deaths during the time period of interest, by age and sex. Each country's counts are in a separate file, shown below.   

```{r, echo=F}
A_males <- c(224, 257, 251, 245, 334, 245, 154, 189, 334, 342, 565, 432, 543, 432, 245, 543, 234, 354) # for males of country A
B_males <- c(34, 37, 51, 145, 434, 120, 100, 143, 307, 354, 463, 639, 706, 232, 275, 543, 234, 274) # for males of country B
A_females <- c(194, 254, 232, 214, 316, 224, 163, 167, 354, 354, 463, 574, 493, 295, 175, 380, 177, 392) # for females of country A
B_females <- c(54, 24, 32, 154, 276, 254, 123, 164, 254, 354, 453, 654, 435, 354, 165, 432, 287, 395) # for females of country B

age_cat5 <- c("0-4", "5-9", "10-14", "15-19", "20-24", "25-29",  "30-34", "35-39", "40-44",
                                                                                "45-49", "50-54", "55-59",
                                                                                "60-64", "65-69", "70-74",
                                                                                "75-79", "80-84", "85")
A_deaths <- data.frame(Country = "A", AgeCat = age_cat5, Male = A_males, Female = A_females)
B_deaths <- data.frame(Country = "B", AgeCat = age_cat5, Male = B_males, Female = B_females)
```

Deaths in Country A
```{r message=FALSE, echo=F}
DT::datatable(A_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Deaths in Country B

```{r message=FALSE, echo=F}
DT::datatable(B_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
rio::export(A_deaths, here::here("data", "standardization", "deaths_countryA.csv"))
rio::export(B_deaths, here::here("data", "standardization", "deaths_countryB.csv"))
```



### Clean populations and deaths {.unnumbered}  


We need to join and transform these data in the following ways:  

* Combine country populations into one dataset and pivot "long" so that each age-sex stratum is one row  
* Combine country death counts into one dataset and pivot "long" so each age-sex stratum is one row  
* Join the deaths to the populations  

First, we combine the country populations datasets, pivot longer, and do minor cleaning. See the page on [Pivoting data] for more detail.  

```{r}
pop_countries <- A_demo %>%  # begin with country A dataset
     bind_rows(B_demo) %>%        # bind rows, because cols are identically named
     pivot_longer(                       # pivot longer
          cols = c(m, f),                   # columns to combine into one
          names_to = "Sex",                 # name for new column containing the category ("m" or "f") 
          values_to = "Population") %>%     # name for new column containing the numeric values pivoted
     mutate(Sex = recode(Sex,            # re-code values for clarity
          "m" = "Male",
          "f" = "Female"))
```

The combined population data now look like this (click through to see countries A and B):  

```{r message=FALSE, echo=F}
DT::datatable(pop_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

And now we perform similar operations on the two deaths datasets.

```{r}
deaths_countries <- A_deaths %>%    # begin with country A deaths dataset
     bind_rows(B_deaths) %>%        # bind rows with B dataset, because cols are identically named
     pivot_longer(                  # pivot longer
          cols = c(Male, Female),        # column to transform into one
          names_to = "Sex",              # name for new column containing the category ("m" or "f") 
          values_to = "Deaths") %>%      # name for new column containing the numeric values pivoted
     rename(age_cat5 = AgeCat)      # rename for clarity
```

The deaths data now look like this, and contain data from both countries: 

```{r message=FALSE, echo=F}
DT::datatable(deaths_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


We now join the deaths and population data based on common columns `Country`, `age_cat5`, and `Sex`. This adds the column `Deaths`.  

```{r}
country_data <- pop_countries %>% 
     left_join(deaths_countries, by = c("Country", "age_cat5", "Sex"))
```

We can now classify `Sex`, `age_cat5`, and `Country` as factors and set the level order using `fct_relevel()` function from the **forcats** package, as described in the page on [Factors]. Note, classifying the factor levels doesn't visibly change the data, but the `arrange()` command does sort it by Country, age category, and sex.  

```{r, warning=F, message=F}
country_data <- country_data %>% 
  mutate(
    Country = fct_relevel(Country, "A", "B"),
      
    Sex = fct_relevel(Sex, "Male", "Female"),
        
    age_cat5 = fct_relevel(
      age_cat5,
      "0-4", "5-9", "10-14", "15-19",
      "20-24", "25-29",  "30-34", "35-39",
      "40-44", "45-49", "50-54", "55-59",
      "60-64", "65-69", "70-74",
      "75-79", "80-84", "85")) %>% 
          
  arrange(Country, age_cat5, Sex)

```

```{r message=FALSE, echo=F}
DT::datatable(country_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">**_CAUTION:_** If you have few deaths per stratum, consider using 10-, or 15-year categories, instead of 5-year categories for age.</span>




### Load reference population {.unnumbered}  

Lastly, for the direct standardisation, we import the reference population (world "standard population" by sex)

```{r, echo=F}
# Reference population
standard_pop_data <- rio::import(here::here("data", "standardization", "world_standard_population_by_sex.csv")) %>% 
     rename(age_cat5 = AgeGroup)
```

```{r, eval=F}
# Reference population
standard_pop_data <- import("world_standard_population_by_sex.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(standard_pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Clean reference population {.unnumbered}

The age category values in the `country_data` and `standard_pop_data` data frames will need to be aligned.  

Currently, the values of the column `age_cat5` from the `standard_pop_data` data frame contain the word "years" and "plus", while those of the `country_data` data frame do not. We will have to make the age category values match. We use `str_replace_all()` from the **stringr** package, as described in the page on [Characters and strings], to replace these patterns with no space `""`.  

Furthermore, the package **dsr** expects that in the standard population, the column containing counts will be called `"pop"`. So we rename that column accordingly.  

```{r}
# Remove specific string from column values
standard_pop_clean <- standard_pop_data %>%
     mutate(
          age_cat5 = str_replace_all(age_cat5, "years", ""),   # remove "year"
          age_cat5 = str_replace_all(age_cat5, "plus", ""),    # remove "plus"
          age_cat5 = str_replace_all(age_cat5, " ", "")) %>%   # remove " " space
     
     rename(pop = WorldStandardPopulation)   # change col name to "pop", as this is expected by dsr package
```

<span style="color: orange;">**_CAUTION:_** If you try to use `str_replace_all()` to remove a plus *symbol*, it won't work because it is a special symbol. "Escape" the specialnes by putting two back slashes in front, as in `str_replace_call(column, "\\+", "")`. </span>

### Create dataset with standard population {#standard_all .unnumbered}  

Finally, the package **PHEindicatormethods**, detailed [below](#standard_phe), expects the standard populations joined to the country event and population counts. So, we will create a dataset `all_data` for that purpose.  

```{r}
all_data <- left_join(country_data, standard_pop_clean, by=c("age_cat5", "Sex"))
```

This complete dataset looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(all_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## **dsr** package {  }
 
Below we demonstrate calculating and comparing directly standardized rates using the **dsr** package. The **dsr** package allows you to calculate and compare directly standardized rates (no indirectly standardized rates!).
  
In the data Preparation section, we made separate datasets for country counts and standard population:  

1) the `country_data` object, which is a population table with the number of population and number of deaths per stratum per country  
2) the `standard_pop_clean` object, containing the number of population per stratum for our reference population, the World Standard Population  

We will use these separate datasets for the **dsr** approach.  


<!-- ======================================================= -->
### Standardized rates {.unnumbered}

Below, we calculate rates per country directly standardized for age and sex. We use the `dsr()` function. 

Of note - `dsr()` expects one data frame for the country populations and event counts (deaths), *and a **separate** data frame with the reference population*. It also expects that in this reference population dataset the unit-time column name is "pop" (we assured this in the data Preparation section).  

There are many arguments, as annotated in the code below. Notably, `event = ` is set to the column `Deaths`, and the `fu = ` ("follow-up") is set to the `Population` column. We set the subgroups of comparison as the column `Country` and we standardize based on `age_cat5` and `Sex`. These last two columns are not assigned a particular named argument. See `?dsr` for details. 

```{r, warning=F, message=F}
# Calculate rates per country directly standardized for age and sex
mortality_rate <- dsr::dsr(
     data = country_data,  # specify object containing number of deaths per stratum
     event = Deaths,       # column containing number of deaths per stratum 
     fu = Population,      # column containing number of population per stratum
     subgroup = Country,   # units we would like to compare
     age_cat5,             # other columns - rates will be standardized by these
     Sex,
     refdata = standard_pop_clean, # reference population data frame, with column called pop
     method = "gamma",      # method to calculate 95% CI
     sig = 0.95,            # significance level
     mp = 100000,           # we want rates per 100.000 population
     decimals = 2)          # number of decimals)


# Print output as nice-looking HTML table
knitr::kable(mortality_rate) # show mortality rate before and after direct standardization
```

Above, we see that while country A had a lower crude mortality rate than country B, it has a higher standardized rate after direct age and sex standardization.




<!-- ======================================================= -->
### Standardized rate ratios {.unnumbered}

```{r,warning=F, message=F}
# Calculate RR
mortality_rr <- dsr::dsrr(
     data = country_data, # specify object containing number of deaths per stratum
     event = Deaths,      # column containing number of deaths per stratum 
     fu = Population,     # column containing number of population per stratum
     subgroup = Country,  # units we would like to compare
     age_cat5,
     Sex,                 # characteristics to which we would like to standardize 
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",      # reference for comparison
     estimate = "ratio",  # type of estimate
     sig = 0.95,          # significance level
     mp = 100000,         # we want rates per 100.000 population
     decimals = 2)        # number of decimals

# Print table
knitr::kable(mortality_rr) 
```

The standardized mortality rate is 1.22 times higher in country A compared to country B (95% CI 1.17-1.27).

<!-- ======================================================= -->
### Standardized rate difference {.unnumbered}

```{r, warning=F, message=F}
# Calculate RD
mortality_rd <- dsr::dsrr(
     data = country_data,       # specify object containing number of deaths per stratum
     event = Deaths,            # column containing number of deaths per stratum 
     fu = Population,           # column containing number of population per stratum
     subgroup = Country,        # units we would like to compare
     age_cat5,                  # characteristics to which we would like to standardize
     Sex,                        
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",            # reference for comparison
     estimate = "difference",   # type of estimate
     sig = 0.95,                # significance level
     mp = 100000,               # we want rates per 100.000 population
     decimals = 2)              # number of decimals

# Print table
knitr::kable(mortality_rd) 
```

Country A has 4.24 additional deaths per 100.000 population (95% CI 3.24-5.24) compared to country A.







<!-- ======================================================= -->
## **PHEindicatormethods** package {#standard_phe  }

Another way of calculating standardized rates is with the **PHEindicatormethods** package. This package allows you to calculate directly as well as indirectly standardized rates. We will show both.  

This section will use the `all_data` data frame created at the end of the Preparation section. This data frame includes the country populations, death events, and the world standard reference population. You can view it [here](#standard_all).  



<!-- ======================================================= -->
### Directly standardized rates {.unnumbered}

Below, we first group the data by Country and then pass it to the function `phe_dsr()` to get directly standardized rates per country.

Of note - the reference (standard) population can be provided as a **column within the country-specific data frame** or as a **separate vector**. If provided within the country-specific data frame, you have to set `stdpoptype = "field"`. If provided as a vector, set `stdpoptype = "vector"`. In the latter case, you have to make sure the ordering of rows by strata is similar in both the country-specific data frame and the reference population, as records will be matched by position. In our example below, we provided the reference population as a column within the country-specific data frame.

See the help with `?phr_dsr` or the links in the References section for more information.  

```{r}
# Calculate rates per country directly standardized for age and sex
mortality_ds_rate_phe <- all_data %>%
     group_by(Country) %>%
     PHEindicatormethods::phe_dsr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          stdpop = pop,               # standard populations for each stratum
          stdpoptype = "field")       # either "vector" for a standalone vector or "field" meaning std populations are in the data  

# Print table
knitr::kable(mortality_ds_rate_phe)
```

<!-- ======================================================= -->
### Indirectly standardized rates {#standard_indirect .unnumbered}

For indirect standardization, you need a reference population with the number of deaths and number of population per stratum. In this example, we will be calculating rates for country A *using country B as the reference population*, as the `standard_pop_clean` reference population does not include number of deaths per stratum. 

Below, we first create the reference population from country B. Then, we pass mortality and population data for country A, combine it with the reference population, and pass it to the function `phe_isr()`, to get indirectly standardized rates. Of course, you can do it also vice versa.

Of note - in our example below, the reference population is provided as a separate data frame. In this case, we make sure that `x = `, `n = `, `x_ref = ` and `n_ref = ` vectors are all ordered by the same standardization category (stratum) values as that in our country-specific data frame, as records will be matched by position.

See the help with `?phr_isr` or the links in the References section for more information.  

```{r}
# Create reference population
refpopCountryB <- country_data %>% 
  filter(Country == "B") 

# Calculate rates for country A indirectly standardized by age and sex
mortality_is_rate_phe_A <- country_data %>%
     filter(Country == "A") %>%
     PHEindicatormethods::phe_isr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          x_ref = refpopCountryB$Deaths,  # reference number of deaths for each stratum
          n_ref = refpopCountryB$Population)  # reference population for each stratum

# Print table
knitr::kable(mortality_is_rate_phe_A)
```

<!-- ======================================================= -->
## Resources {  }

If you would like to see another reproducible example using **dsr** please see [this vignette]( https://mran.microsoft.com/snapshot/2020-02-12/web/packages/dsr/vignettes/dsr.html)  

For another example using **PHEindicatormethods**, please go to [this website](https://mran.microsoft.com/snapshot/2018-10-22/web/packages/PHEindicatormethods/vignettes/IntroductiontoPHEindicatormethods.html)  

See the **PHEindicatormethods** [reference pdf file](https://cran.r-project.org/web/packages/PHEindicatormethods/PHEindicatormethods.pdf)  


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/standardization.Rmd-->


# Moving averages { }  

```{r, out.width=c("100%"), echo=F}
knitr::include_graphics(here::here("images", "moving_avg_epicurve.png"))
```


This page will cover two methods to calculate and visualize moving averages:  

1) Calculate with the **slider** package  
2) Calculate *within* a `ggplot()` command with the **tidyquant** package  



<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages. 


```{r}
pacman::p_load(
  tidyverse,      # for data management and viz
  slider,         # for calculating moving averages
  tidyquant       # for calculating moving averages within ggplot
)
```


### Import data {.unnumbered}

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
## Calculate with **slider** {  }

**Use this approach to calculate a moving average in a data frame prior to plotting.**  

The **slider** package provides several "sliding window" functions to compute rolling averages, cumulative sums, rolling regressions, etc. It treats a data frame as a vector of rows, allowing iteration row-wise over a data frame.   

Here are some of the common functions:  

* `slide_dbl()` - iterates through a *numeric* (hence "_dbl") column performing an operation using a sliding window  
  * `slide_sum()` - rolling sum shortcut function for `slide_dbl()`  
  * `slide_mean()` - rolling average shortcut function for `slide_dbl()` 
* `slide_index_dbl()` - applies the rolling window on a numeric column using a separate column to *index* the window progression (useful if rolling by date with some dates absent)  
  * `slide_index_sum()` - rolling sum shortcut function with indexing  
  * `slide_index_mean()` - rolling mean shortcut function with indexing  
  
The **slider** package has many other functions that are covered in the Resources section of this page. We briefly touch upon the most common.  

**Core arguments**  

* `.x`, the first argument by default, is the vector to iterate over and to apply the function to  
* `.i = ` for the "index" versions of the **slider** functions - provide a column to "index" the roll on (see section [below](#roll_index))  
* `.f = `, the second argument by default, either:  
  * A function, written without parentheses, like `mean`, or  
  * A formula, which will be converted into a function. For example `~ .x - mean(.x)` will return the result of the current value minus the mean of the window's value  
  
* For more details see this [reference material](https://davisvaughan.github.io/slider/reference/slide.html)



**Window size**  

Specify the size of the window by using either `.before`, `.after`, or both arguments:   

* `.before = ` - Provide an integer  
* `.after = ` - Provide an integer  
* `.complete = ` - Set this to `TRUE` if you only want calculation performed on complete windows  

For example, to achieve a 7-day window including the current value and the six previous, use `.before = 6`. To achieve a "centered" window provide the same number to both `.before = ` and `.after = `.    

By default, `.complete = ` will be FALSE so if the full window of rows does not exist, the functions will use available rows to perform the calculation. Setting to TRUE restricts so calculations are only performed on complete windows.  

**Expanding window**  

To achieve *cumulative* operations, set the `.before = ` argument to `Inf`. This will conduct the operation on the current value and all coming before.  





### Rolling by date  {#roll_index .unnumbered}  

The most likely use-case of a rolling calculation in applied epidemiology is to examine a metric *over time*. For example, a rolling measurement of case incidence, based on daily case counts. 

If you have clean time series data with values for every date, you may be OK to use `slide_dbl()`, as demonstrated here in the [Time series and outbreak detection](#timeseries_moving) page.  

However, in many applied epidemiology circumstances you may have dates absent from your data, where there are no events recorded. In these cases, it is best to use the "index" versions of the **slider** functions.  


### Indexed data {.unnumbered}  

Below, we show an example using `slide_index_dbl()` on the case linelist. Let us say that our objective is to calculate a rolling 7-day incidence - the sum of cases using a rolling 7-day window. If you are looking for an example of rolling average, see the section below on [grouped rolling](#roll_slider_group).    

To begin, the dataset `daily_counts` is created to reflect the daily case counts from the `linelist`, as calculated with `count()` from **dplyr**.  

```{r}
# make dataset of daily counts
daily_counts <- linelist %>% 
  count(date_hospitalisation, name = "new_cases")
```


Here is the `daily_counts` data frame - there are ` nrow(daily_counts)` rows, each day is represented by one row, but especially early in the epidemic *some days are not present (there were no cases admitted on those days)*.  


```{r, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 6, scrollX=T) )
```



It is crucial to recognize that a standard rolling function (like `slide_dbl()` would use a window of 7 *rows*, not 7 *days*. So, if there are any absent dates, some windows will actually extend more than 7 calendar days!  

A "smart" rolling window can be achieved with `slide_index_dbl()`. The "index" means that the function uses a *separate column* as an "index" for the rolling window. The window is not simply based on the rows of the data frame.  

If the index column is a date, you have the added ability to specify the window extent to `.before = ` and/or `.after = ` in units of **lubridate** `days()` or `months()`. If you do these things, the function will include absent days in the windows as if they were there (as `NA` values).  

Let's show a comparison. Below, we calculate rolling 7-day case incidence with regular and indexed windows.  


```{r}
rolling <- daily_counts %>% 
  mutate(                                # create new columns
    # Using slide_dbl()
    ###################
    reg_7day = slide_dbl(
      new_cases,                         # calculate on new_cases
      .f = ~sum(.x, na.rm = T),          # function is sum() with missing values removed
      .before = 6),                      # window is the ROW and 6 prior ROWS
    
    # Using slide_index_dbl()
    #########################
    indexed_7day = slide_index_dbl(
        new_cases,                       # calculate on new_cases
        .i = date_hospitalisation,       # indexed with date_onset 
        .f = ~sum(.x, na.rm = TRUE),     # function is sum() with missing values removed
        .before = days(6))               # window is the DAY and 6 prior DAYS
    )

```

Observe how in the regular column for the first 7 rows the count steadily increases *despite the rows not being within 7 days of each other*! The adjacent "indexed" column accounts for these absent calendar days, so its 7-day sums are much lower, at least in this period of the epidemic when the cases a farther between.  

```{r, echo=F}
DT::datatable(rolling, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```



Now you can plot these data using `ggplot()`:  

```{r}
ggplot(data = rolling)+
  geom_line(mapping = aes(x = date_hospitalisation, y = indexed_7day), size = 1)
```




<!-- ### Rolling by month {.unnumbered}   -->

<!-- If you want to calculate statistics by month (e.g. sum, mean, max) you can do this with **dplyr** as described in the [Grouping data] page. Simply create a "month" column, group the data, and run your calculations with `summarise()`.   -->

<!-- If however, you want to calculate rolling statistics over several months (e.g a 2-month rolling window), you can use the `slide_period()` function from **slider**.   -->

<!-- ```{r} -->
<!-- monthly_mean = function(data){ -->
<!--   summarise(data, mean = mean(new_cases, na.rm=T)) -->
<!-- } -->

<!-- linelist %>%  -->
<!--   count(date_hospitalisation, name = "new_cases") %>%  -->
<!--   mutate( -->
<!--     slide_period_dfr( -->
<!--       new_cases,  -->
<!--       .i = date_hospitalisation, -->
<!--       .period = "month", -->
<!--       .f = monthly_mean))  #~mean(.x, na.rm=T))) -->

<!--       #values_col = new_cases, -->
<!--       #index_col = date_hospitalisation -->
<!--     )) -->



<!-- ``` -->


### Rolling by group {#roll_slider_group .unnumbered}  

If you group your data prior to using a **slider** function, the sliding windows will be applied by group. Be careful to arrange your rows in the desired order *by group*.  

Each time a new group begins, the sliding window will re-start. Therefore, one nuance to be aware of is that if your data are grouped *and* you have set `.complete = TRUE`, you will have empty values at each transition between groups. As the function moved downward through the rows, every transition in the grouping column will re-start the accrual of the minimum window size to allow a calculation.  

See handbook page on [Grouping data] for details on grouping data.

Below, we count linelist cases by date *and* by hospital. Then we arrange the rows in ascending order, first ordering by hospital and then within that by date. Next we set `group_by()`. Then we can create our new rolling average. 


```{r}
grouped_roll <- linelist %>%

  count(hospital, date_hospitalisation, name = "new_cases") %>% 

  arrange(hospital, date_hospitalisation) %>%   # arrange rows by hospital and then by date
  
  group_by(hospital) %>%              # group by hospital 
    
  mutate(                             # rolling average  
    mean_7day_hosp = slide_index_dbl(
      .x = new_cases,                 # the count of cases per hospital-day
      .i = date_hospitalisation,      # index on date of admission
      .f = mean,                      # use mean()                   
      .before = days(6)               # use the day and the 6 days prior
      )
  )

```

Here is the new dataset:  

```{r, echo=F}
DT::datatable(grouped_roll, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```


We can now plot the moving averages, displaying the data by group by specifying `~ hospital` to `facet_wrap()` in `ggplot()`. For fun, we plot two geometries - a `geom_col()` showing the daily case counts and a `geom_line()` showing the 7-day moving average.  


```{r, warning=F, message=F}
ggplot(data = grouped_roll)+
  geom_col(                       # plot daly case counts as grey bars
    mapping = aes(
      x = date_hospitalisation,
      y = new_cases),
    fill = "grey",
    width = 1)+
  geom_line(                      # plot rolling average as line colored by hospital
    mapping = aes(
      x = date_hospitalisation,
      y = mean_7day_hosp,
      color = hospital),
    size = 1)+
  facet_wrap(~hospital, ncol = 2)+ # create mini-plots per hospital
  theme_classic()+                 # simplify background  
  theme(legend.position = "none")+ # remove legend
  labs(                            # add plot labels
    title = "7-day rolling average of daily case incidence",
    x = "Date of admission",
    y = "Case incidence")
```


<span style="color: red;">**_DANGER:_** If you get an error saying *"slide() was deprecated in tsibble 0.9.0 and is now defunct. Please use slider::slide() instead."*, it means that the `slide()` function from the **tsibble** package is masking the `slide()` function from **slider** package. Fix this by specifying the package in the command, such as `slider::slide_dbl()`.</span>




<!-- You can group the data prior to using a **slider** function. For example, if you want to calculate the same 7-day rolling sum as above, but by hospital. above rolling mean delay from symptom onset to hospital admission (column `days_onset_hosp`).   -->

<!-- You can group the data by the month of symptom onset using **lubridate**'s `floor_date()` as described in the [Grouping data] page. Then, use `slide_index_dbl()` as before but set your window extent using `months()` (also from **lubridate**).  -->

<!-- f you want a rolling average by *months*, you can use **lubridate** to group the data by month, and then apply `slide_index_dbl()` as below shown for a three-month rolling average:   -->

<!-- ```{r} -->
<!-- months_delay <- linelist %>% -->
<!--   arrange(date_onset) %>%    # drop rows missing date of onset -->
<!--   group_by(hospital) %>%  -->
<!--   #group_by(month_onset = floor_date(date_onset, "month")) %>% # create and group by month of onset  -->
<!--   mutate( -->
<!--     delay_7d = slide_index_dbl( -->
<!--       days_onset_hosp,                  # calculate avg based on value in new_cases column -->
<!--       .i = date_onset,                 # index column is date_onset, so non-present dates are included in 7day window  -->
<!--       .f = ~mean(.x, na.rm = TRUE),     # function is mean() with missing values removed -->
<!--       .before = days(7)), -->

<!--     delay_month = slide_index_dbl( -->
<!--       days_onset_hosp,                  # calculate avg based on value in new_cases column -->
<!--       .i = date_onset,                 # index column is date_onset, so non-present dates are included in 7day window  -->
<!--       .f = ~mean(.x, na.rm = TRUE),     # function is mean() with missing values removed -->
<!--       .before = months(1)))               # window is the month and the prior month -->


<!-- # window is the month and the prior month -->

<!-- ``` -->

<!-- ```{r} -->
<!-- ggplot(data = months_delay, mapping = aes(x = month_onset))+ -->
<!--   geom_line(mapping = aes(y = )) -->

<!-- ``` -->






<!-- ======================================================= -->
## Calculate with **tidyquant** within `ggplot()` {  }

The package **tidyquant** offers another approach to calculating moving averages - this time from *within* a `ggplot()` command itself.  

Below the `linelist` data are counted by date of onset, and this is plotted as a faded line (`alpha` < 1). Overlaid on top is a line created with `geom_ma()` from the package **tidyquant**, with a set window of 7 days (`n = 7`) with specified color and thickness.  

By default `geom_ma()` uses a simple moving average (`ma_fun = "SMA"`), but other types can be specified, such as:  

* "EMA" - exponential moving average (more weight to recent observations)  
* "WMA" - weighted moving average (`wts` are used to weight observations in the moving average)  
* Others can be found in the function documentation  

```{r}
linelist %>% 
  count(date_onset) %>%                 # count cases per day
  drop_na(date_onset) %>%               # remove cases missing onset date
  ggplot(aes(x = date_onset, y = n))+   # start ggplot
    geom_line(                          # plot raw values
      size = 1,
      alpha = 0.2                       # semi-transparent line
      )+             
    tidyquant::geom_ma(                 # plot moving average
      n = 7,           
      size = 1,
      color = "blue")+ 
  theme_minimal()                       # simple background
```

See this [vignette](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html) for more details on the options available within **tidyquant**.  


<!-- ## Rolling regression  -->

<!-- ```{r} -->
<!-- a <- linelist %>% -->
<!--   separate(time_admission, into = c("hour", "minute"), sep = ":") %>%  -->
<!--   count(days_onset_hosp, hour) %>%  -->
<!--   mutate(reg_admit_hour = slide(., ~lm(days_onset_hosp ~ hour), .before = 3, .complete = T)) %>%  -->
<!--   mutate(coeff = reg_admit_hour[[1]]) -->

<!-- ggplot()+ -->
<!--   geom_point(aes(x = hour, y = days_onset_hosp)) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- linelist %>%  -->
<!--   mutate( -->

<!--   ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Resources {  }


See the helpful online [vignette for the **slider** package](https://cran.r-project.org/web/packages/slider/vignettes/slider.html)  

The **slider** [github page](https://github.com/DavisVaughan/slider)

A **slider** [vignette](https://davisvaughan.github.io/slider/articles/slider.html)  

[tidyquant vignette](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html)

If your use case requires that you “skip over” weekends and even holidays, you might like **almanac** package.



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/moving_average.Rmd-->


# Time series and outbreak detection { }  

<!-- ======================================================= -->
## Overview {  }

This tab demonstrates the use of several packages for time series analysis. 
It primarily relies on packages from the [**tidyverts**](https://tidyverts.org/) 
family, but will also use the RECON [**trending**](https://github.com/reconhub/trending) 
package to fit models that are more appropriate for infectious disease epidemiology. 

Note in the below example we use a dataset from the **surveillance** package 
on Campylobacter in Germany (see the [data chapter](https://epirhandbook.com/download-handbook-and-data.html), 
of the handbook for details). However, if you wanted to run the same code on a dataset
with multiple countries or other strata, then there is an example code template for this in the 
[r4epis github repo](https://github.com/R4EPI/epitsa). 

Topics covered include:  

1.  Time series data 
2.  Descriptive analysis 
3.  Fitting regressions
4.  Relation of two time series 
5.  Outbreak detection
6.  Interrupted time series


<!-- ======================================================= -->
## Preparation {  }

### Packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics](https://epirhandbook.com/r-basics.html) for more information on R packages.  

```{r load_packages}
pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets
               slider,       # for calculating moving averages
               imputeTS,     # for filling in missing values
               feasts,       # for time series decomposition and autocorrelation
               forecast,     # fit sin and cosin terms to data (note: must load after feasts)
               trending,     # fit and assess models 
               tmaptools,    # for getting geocoordinates (lon/lat) based on place names
               ecmwfr,       # for interacting with copernicus sateliate CDS API
               stars,        # for reading in .nc (climate data) files
               units,        # for defining units of measurement (climate data)
               yardstick,    # for looking at model accuracy
               surveillance  # for aberration detection
               )
``` 

### Load data {.unnumbered}

You can download all the data used in this handbook via the instructions in the [Download handbook and data] page.  

The example dataset used in this section is weekly counts of campylobacter cases reported in Germany between 2001 and 2011. <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>
	You can click here to download<span> this data file (.xlsx).</span></a> 

This dataset is a reduced version of the dataset available in the [**surveillance**](https://cran.r-project.org/web/packages/surveillance/) package. 
(for details load the surveillance package and see `?campyDE`)

Import these data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).

```{r read_data_hide, echo=F}
# import the counts into R
counts <- rio::import(here::here("data", "time_series", "campylobacter_germany.xlsx"))
```

```{r read_data_show, eval=F}
# import the counts into R
counts <- rio::import("campylobacter_germany.xlsx")
```

The first 10 rows of the counts are displayed below.

```{r inspect_data, message=FALSE, echo=F}
# display the counts data as a table
DT::datatable(head(counts, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Clean data {.unnumbered}

The code below makes sure that the date column is in the appropriate format. 
For this tab we will be using the **tsibble** package and so the `yearweek` 
function will be used to create a calendar week variable. There are several other
ways of doing this (see the [Working with dates](https://epirhandbook.com/working-with-dates.html)
page for details), however for time series its best to keep within one framework (**tsibble**). 

```{r clean_data}

## ensure the date column is in the appropriate format
counts$date <- as.Date(counts$date)

## create a calendar week variable 
## fitting ISO definitons of weeks starting on a monday
counts <- counts %>% 
     mutate(epiweek = yearweek(date, week_start = 1))

```

### Download climate data {.unnumbered} 

In the *relation of two time series* section of this page, we will be comparing 
campylobacter case counts to climate data. 

Climate data for anywhere in the world can be downloaded from the EU's Copernicus 
Satellite. These are not exact measurements, but based on a model (similar to 
interpolation), however the benefit is global hourly coverage as well as forecasts.  

You can download each of these climate data files from the [Download handbook and data] page.  

For purposes of demonstration here, we will show R code to use the **ecmwfr** package to pull these data from the Copernicus 
climate data store. You will need to create a free account in order for this to 
work. The package website has a useful [walkthrough](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds)
of how to do this. Below is example code of how to go about doing this, once you 
have the appropriate API keys. You have to replace the X's below with your account
IDs. You will need to download one year of data at a time otherwise the server times-out. 

If you are not sure of the coordinates for a location you want to download data 
for, you can use the **tmaptools** package to pull the coordinates off open street
maps. An alternative option is the [**photon**](https://github.com/rCarto/photon)
package, however this has not been released on to CRAN yet; the nice thing about 
**photon** is that it provides more contextual data for when there are several 
matches for your search.

```{r weather_data, eval = FALSE}

## retrieve location coordinates
coords <- geocode_OSM("Germany", geometry = "point")

## pull together long/lats in format for ERA-5 querying (bounding box) 
## (as just want a single point can repeat coords)
request_coords <- str_glue_data(coords$coords, "{y}/{x}/{y}/{x}")


## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)
## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app
## https://github.com/bluegreen-labs/ecmwfr

## set up key for weather data 
wf_set_key(user = "XXXXX",
           key = "XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX",
           service = "cds") 

## run for each year of interest (otherwise server times out)
for (i in 2002:2011) {
  
  ## pull together a query 
  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax
  ## change request to a list using addin button above (python to list)
  ## Target is the name of the output file!!
  request <- request <- list(
    product_type = "reanalysis",
    format = "netcdf",
    variable = c("2m_temperature", "total_precipitation"),
    year = c(i),
    month = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    day = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12",
            "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
            "25", "26", "27", "28", "29", "30", "31"),
    time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00", "06:00", "07:00",
             "08:00", "09:00", "10:00", "11:00", "12:00", "13:00", "14:00", "15:00",
             "16:00", "17:00", "18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
    area = request_coords,
    dataset_short_name = "reanalysis-era5-single-levels",
    target = paste0("germany_weather", i, ".nc")
  )
  
  ## download the file and store it in the current working directory
  file <- wf_request(user     = "XXXXX",  # user ID (for authentication)
                     request  = request,  # the request
                     transfer = TRUE,     # download the file
                     path     = here::here("data", "Weather")) ## path to save the data
  }

```

### Load climate data {.unnumbered}

Whether you downloaded the climate data via our handbook, or used the code above, you now should have 10 years of ".nc" climate data files stored in the same folder on your computer.  

Use the code below to import these files into R with the **stars** package. 

```{r read_climate, warning = FALSE, message = FALSE}

## define path to weather folder 
file_paths <- list.files(
  here::here("data", "time_series", "weather"), # replace with your own file path 
  full.names = TRUE)

## only keep those with the current name of interest 
file_paths <- file_paths[str_detect(file_paths, "germany")]

## read in all the files as a stars object 
data <- stars::read_stars(file_paths)
```

Once these files have been imported as the object `data`, we will convert them to a data frame.  

```{r}
## change to a data frame 
temp_data <- as_tibble(data) %>% 
  ## add in variables and correct units
  mutate(
    ## create an calendar week variable 
    epiweek = tsibble::yearweek(time), 
    ## create a date variable (start of calendar week)
    date = as.Date(epiweek),
    ## change temperature from kelvin to celsius
    t2m = set_units(t2m, celsius), 
    ## change precipitation from metres to millimetres 
    tp  = set_units(tp, mm)) %>% 
  ## group by week (keep the date too though)
  group_by(epiweek, date) %>% 
  ## get the average per week
  summarise(t2m = as.numeric(mean(t2m)), 
            tp = as.numeric(mean(tp)))

```




<!-- ======================================================= -->
## Time series data {  }

There are a number of different packages for structuring and handling time series
data. As said, we will focus on the **tidyverts** family of packages and so will
use the **tsibble** package to define our time series object. Having a data set
defined as a time series object means it is much easier to structure our analysis. 

To do this we use the `tsibble()` function and specify the "index", i.e. the variable
specifying the time unit of interest. In our case this is the `epiweek` variable. 

If we had a data set with weekly counts by province, for example, we would also 
be able to specify the grouping variable using the `key = ` argument. 
This would allow us to do analysis for each group. 


```{r ts_object}

## define time series object 
counts <- tsibble(counts, index = epiweek)

```

Looking at `class(counts)` tells you that on top of being a tidy data frame 
("tbl_df", "tbl", "data.frame"), it has the additional properties of a time series
data frame ("tbl_ts"). 

You can take a quick look at your data by using **ggplot2**. We see from the plot that
there is a clear seasonal pattern, and that there are no missings. However, there
seems to be an issue with reporting at the beginning of each year; cases drop 
in the last week of the year and then increase for the first week of the next year. 

```{r basic_plot}

## plot a line graph of cases by week
ggplot(counts, aes(x = epiweek, y = case)) + 
     geom_line()

```


<span style="color: red;">**_DANGER:_** Most datasets aren't as clean as this example. 
You will need to check for duplicates and missings as below. </span>

<!-- ======================================================= -->
### Duplicates {.unnumbered}

**tsibble** does not allow duplicate observations. So each row will need to be
unique, or unique within the group (`key` variable). 
The package has a few functions that help to identify duplicates. These include
`are_duplicated()` which gives you a TRUE/FALSE vector of whether the row is a 
duplicate, and `duplicates()` which gives you a data frame of the duplicated rows. 

See the page on [De-duplication](https://epirhandbook.com/de-duplication.html)
for more details on how to select rows you want. 

```{r duplicates, eval = FALSE}

## get a vector of TRUE/FALSE whether rows are duplicates
are_duplicated(counts, index = epiweek) 

## get a data frame of any duplicated rows 
duplicates(counts, index = epiweek) 

```

<!-- ======================================================= -->
### Missings {.unnumbered}

We saw from our brief inspection above that there are no missings, but we also 
saw there seems to be a problem with reporting delay around new year. 
One way to address this problem could be to set these values to missing and then 
to impute values. The simplest form of time series imputation is to draw
a straight line between the last non-missing and the next non-missing value. 
To do this we will use the **imputeTS** package function `na_interpolation()`. 

See the [Missing data](https://epirhandbook.com/missing-data.html) page for other options for imputation.  

Another alternative would be to calculate a moving average, to try and smooth
over these apparent reporting issues (see next section, and the page on [Moving averages](https://epirhandbook.com/moving-averages.html)). 

```{r missings}

## create a variable with missings instead of weeks with reporting issues
counts <- counts %>% 
     mutate(case_miss = if_else(
          ## if epiweek contains 52, 53, 1 or 2
          str_detect(epiweek, "W51|W52|W53|W01|W02"), 
          ## then set to missing 
          NA_real_, 
          ## otherwise keep the value in case
          case
     ))

## alternatively interpolate missings by linear trend 
## between two nearest adjacent points
counts <- counts %>% 
  mutate(case_int = imputeTS::na_interpolation(case_miss)
         )

## to check what values have been imputed compared to the original
ggplot_na_imputations(counts$case_miss, counts$case_int) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```




<!-- ======================================================= -->
## Descriptive analysis {  }



<!-- ======================================================= -->
### Moving averages {#timeseries_moving .unnumbered}

If data is very noisy (counts jumping up and down) then it can be helpful to 
calculate a moving average. In the example below, for each week we calculate the 
average number of cases from the four previous weeks. This smooths the data, to 
make it more interpretable. In our case this does not really add much, so we will
stick to the interpolated data for further analysis. 
See the [Moving averages](https://epirhandbook.com/moving-averages.html) page for more detail. 

```{r moving_averages}

## create a moving average variable (deals with missings)
counts <- counts %>% 
     ## create the ma_4w variable 
     ## slide over each row of the case variable
     mutate(ma_4wk = slider::slide_dbl(case, 
                               ## for each row calculate the name
                               ~ mean(.x, na.rm = TRUE),
                               ## use the four previous weeks
                               .before = 4))

## make a quick visualisation of the difference 
ggplot(counts, aes(x = epiweek)) + 
     geom_line(aes(y = case)) + 
     geom_line(aes(y = ma_4wk), colour = "red")

```


<!-- ======================================================= -->
### Periodicity {.unnumbered}

Below we define a custom function to create a periodogram. See the [Writing functions] page for information about how to write functions in R.  

First, the function is defined. Its arguments include a dataset with a column `counts`, `start_week = ` which is the first week of the dataset, a number to indicate how many periods per year (e.g. 52, 12), and lastly the output style (see details in the code below).  


```{r periodogram}
## Function arguments
#####################
## x is a dataset
## counts is variable with count data or rates within x 
## start_week is the first week in your dataset
## period is how many units in a year 
## output is whether you want return spectral periodogram or the peak weeks
  ## "periodogram" or "weeks"

# Define function
periodogram <- function(x, 
                        counts, 
                        start_week = c(2002, 1), 
                        period = 52, 
                        output = "weeks") {
  

    ## make sure is not a tsibble, filter to project and only keep columns of interest
    prepare_data <- dplyr::as_tibble(x)
    
    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
    prepare_data <- dplyr::select(prepare_data, {{counts}})
    
    ## create an intermediate "zoo" time series to be able to use with spec.pgram
    zoo_cases <- zoo::zooreg(prepare_data, 
                             start = start_week, frequency = period)
    
    ## get a spectral periodogram not using fast fourier transform 
    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)
    
    ## return the peak weeks 
    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period
    
    if (output == "weeks") {
      periodo_weeks
    } else {
      periodo
    }
    
}

## get spectral periodogram for extracting weeks with the highest frequencies 
## (checking of seasonality) 
periodo <- periodogram(counts, 
                       case_int, 
                       start_week = c(2002, 1),
                       output = "periodogram")

## pull spectrum and frequence in to a dataframe for plotting
periodo <- data.frame(periodo$freq, periodo$spec)

## plot a periodogram showing the most frequently occuring periodicity 
ggplot(data = periodo, 
                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + 
  geom_line() + 
  labs(x = "Period (Weeks)", y = "Log(density)")


## get a vector weeks in ascending order 
peak_weeks <- periodogram(counts, 
                          case_int, 
                          start_week = c(2002, 1), 
                          output = "weeks")

```

<span style="color: black;">**_NOTE:_** It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) </span>

<!-- ======================================================= -->
### Decomposition {.unnumbered}

Classical decomposition is used to break a time series down several parts, which
when taken together make up for the pattern you see. 
These different parts are:  

* The trend-cycle (the long-term direction of the data)  
* The seasonality (repeating patterns)  
* The random (what is left after removing trend and season)  


```{r decomposition, warning=F, message=F}

## decompose the counts dataset 
counts %>% 
  # using an additive classical decomposition model
  model(classical_decomposition(case_int, type = "additive")) %>% 
  ## extract the important information from the model
  components() %>% 
  ## generate a plot 
  autoplot()

```

<!-- ======================================================= -->
### Autocorrelation {.unnumbered}

Autocorrelation tells you about the relation between the counts of each week 
and the weeks before it (called lags).  

Using the `ACF()` function, we can produce a plot which shows us a number of lines 
for the relation at different lags. Where the lag is 0 (x = 0), this line would 
always be 1 as it shows the relation between an observation and itself (not shown here). 
The first line shown here (x = 1) shows the relation between each observation 
and the observation before it (lag of 1), the second shows the relation between 
each observation and the observation before last (lag of 2) and so on until lag of
52 which shows the relation between each observation and the observation from 1 
year (52 weeks before).  

Using the `PACF()` function (for partial autocorrelation) shows the same type of relation 
but adjusted for all other weeks between. This is less informative for determining
periodicity. 

```{r autocorrelation}

## using the counts dataset
counts %>% 
  ## calculate autocorrelation using a full years worth of lags
  ACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

## using the counts data set 
counts %>% 
  ## calculate the partial autocorrelation using a full years worth of lags
  PACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

```

You can formally test the null hypothesis of independence in a time series (i.e. 
that it is not autocorrelated) using the Ljung-Box test (in the **stats** package). 
A significant p-value suggests that there is autocorrelation in the data.

```{r ljung_box}

## test for independance 
Box.test(counts$case_int, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Fitting regressions {  }

It is possible to fit a large number of different regressions to a time series, 
however, here we will demonstrate how to fit a negative binomial regression - as 
this is often the most appropriate for counts data in infectious diseases. 

<!-- ======================================================= -->
### Fourier terms {.unnumbered}

Fourier terms are the equivalent of sin and cosin curves. The difference is that 
these are fit based on finding the most appropriate combination of curves to explain
your data.  

If only fitting one fourier term, this would be the equivalent of fitting a sin 
and a cosin for your most frequently occurring lag seen in your periodogram (in our 
case 52 weeks). We use the `fourier()` function from the **forecast** package.  

In the below code we assign using the `$`, as `fourier()` returns two columns (one 
for sin one for cosin) and so these are added to the dataset as a list, called 
"fourier" - but this list can then be used as a normal variable in regression. 

```{r fourier}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  fourier(K = 1)
```

<!-- ======================================================= -->
### Negative binomial {.unnumbered}

It is possible to fit regressions using base **stats** or **MASS**
functions (e.g. `lm()`, `glm()` and `glm.nb()`). However we will be using those from 
the **trending** package, as this allows for calculating appropriate confidence
and prediction intervals (which are otherwise not available). 
The syntax is the same, and you specify an outcome variable then a tilde (~) 
and then add your various exposure variables of interest separated by a plus (+). 

The other difference is that we first define the model and then `fit()` it to the 
data. This is useful because it allows for comparing multiple different models 
with the same syntax. 

<span style="color: darkgreen;">**_TIP:_** If you wanted to use rates, rather than 
counts you could include the population variable as a logarithmic offset term, by adding 
`offset(log(population)`. You would then need to set population to be 1, before 
using `predict()` in order to produce a rate. </span>

<span style="color: darkgreen;">**_TIP:_** For fitting more complex models such 
as ARIMA or prophet, see the [**fable**](https://fable.tidyverts.org/index.html) package.</span>

```{r nb_reg, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier)

## fit your model using the counts dataset
fitted_model <- trending::fit(model, data.frame(counts))

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```

<!-- ======================================================= -->
### Residuals {.unnumbered}

To see how well our model fits the observed data we need to look at the residuals. 
The residuals are the difference between the observed counts and the counts 
estimated from the model. We could calculate this simply by using `case_int - estimate`, 
but the `residuals()` function extracts this directly from the regression for us.

What we see from the below, is that we are not explaining all of the variation 
that we could with the model. It might be that we should fit more fourier terms, 
and address the amplitude. However for this example we will leave it as is. 
The plots show that our model does worse in the peaks and troughs (when counts are
at their highest and lowest) and that it might be more likely to underestimate 
the observed counts. 

```{r, warning=F, message=F}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = residuals(fitted_model$fitted_model, type = "response"))

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Relation of two time series {  }

Here we look at using weather data (specifically the temperature) to explain 
campylobacter case counts. 

<!-- ======================================================= -->
### Merging datasets {.unnumbered}

We can join our datasets using the week variable. For more on merging see the 
handbook section on [joining](https://epirhandbook.com/joining-data.html).

```{r join}

## left join so that we only have the rows already existing in counts
## drop the date variable from temp_data (otherwise is duplicated)
counts <- left_join(counts, 
                    select(temp_data, -date),
                    by = "epiweek")

```

<!-- ======================================================= -->
### Descriptive analysis {.unnumbered}

First plot your data to see if there is any obvious relation. 
The plot below shows that there is a clear relation in the seasonality of the two
variables, and that temperature might peak a few weeks before the case number.
For more on pivoting data, see the handbook section on [pivoting data](https://epirhandbook.com/pivoting-data.html). 

```{r basic_plot_bivar}

counts %>% 
  ## keep the variables we are interested 
  select(epiweek, case_int, t2m) %>% 
  ## change your data in to long format
  pivot_longer(
    ## use epiweek as your key
    !epiweek,
    ## move column names to the new "measure" column
    names_to = "measure", 
    ## move cell values to the new "values" column
    values_to = "value") %>% 
  ## create a plot with the dataset above
  ## plot epiweek on the x axis and values (counts/celsius) on the y 
  ggplot(aes(x = epiweek, y = value)) + 
    ## create a separate plot for temperate and case counts 
    ## let them set their own y-axes
    facet_grid(measure ~ ., scales = "free_y") +
    ## plot both as a line
    geom_line()

```

<!-- ======================================================= -->
### Lags and cross-correlation {.unnumbered}

To formally test which weeks are most highly related between cases and temperature. 
We can use the cross-correlation function (`CCF()`) from the **feasts** package. 
You could also visualise (rather than using `arrange`) using the `autoplot()` function. 

```{r cross_correlation, warning=FALSE}

counts %>% 
  ## calculate cross-correlation between interpolated counts and temperature
  CCF(case_int, t2m,
      ## set the maximum lag to be 52 weeks
      lag_max = 52, 
      ## return the correlation coefficient 
      type = "correlation") %>% 
  ## arange in decending order of the correlation coefficient 
  ## show the most associated lags
  arrange(-ccf) %>% 
  ## only show the top ten 
  slice_head(n = 10)

```

We see from this that a lag of 4 weeks is most highly correlated, 
so we make a lagged temperature variable to include in our regression. 

<span style="color: red;">**_DANGER:_** Note that the first four weeks of our data
in the lagged temperature variable are missing (`NA`) - as there are not four 
weeks prior to get data from. In order to use this dataset with the **trending** 
`predict()` function, we need to use the the `simulate_pi = FALSE` argument within
`predict()` further down. If we did want to use the simulate option, then 
we have to drop these missings and store as a new data set by adding `drop_na(t2m_lag4)` 
to the code chunk below.</span>  
 

```{r lag_tempvar}

counts <- counts %>% 
  ## create a new variable for temperature lagged by four weeks
  mutate(t2m_lag4 = lag(t2m, n = 4))

```


<!-- ======================================================= -->
### Negative binomial with two variables {.unnumbered}

We fit a negative binomial regression as done previously. This time we add the 
temperature variable lagged by four weeks. 

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r nb_reg_bivar, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier + 
    ## use the temperature lagged by four weeks 
    t2m_lag4
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, data.frame(counts))

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)

```


To investigate the individual terms, we can pull the original negative binomial
regression out of the **trending** format using `get_model()` and pass this to the
**broom** package `tidy()` function to retrieve exponentiated estimates and associated
confidence intervals.  

What this shows us is that lagged temperature, after controlling for trend and seasonality, 
is similar to the case counts (estimate ~ 1) and significantly associated. 
This suggests that it might be a good variable for use in predicting future case
numbers (as climate forecasts are readily available). 

```{r results_nb_reg_bivar}

fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE)
```

A quick visual inspection of the model shows that it might do a better job of 
estimating the observed case counts. 

```{r plot_nb_reg_bivar, warning=F, message=F}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```


#### Residuals {.unnumbered}

We investigate the residuals again to see how well our model fits the observed data. 
The results and interpretation here are similar to those of the previous regression, 
so it may be more feasible to stick with the simpler model without temperature. 

```{r}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = case_int - estimate)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Outbreak detection {  }

We will demonstrate two (similar) methods of detecting outbreaks here. 
The first builds on the sections above. 
We use the **trending** package to fit regressions to previous years, and then
predict what we expect to see in the following year. If observed counts are above
what we expect, then it could suggest there is an outbreak. 
The second method is based on similar principles but uses the **surveillance** package,
which has a number of different algorithms for aberration detection.

<span style="color: orange;">**_CAUTION:_** Normally, you are interested in the current year (where you only know counts up to the present week). So in this example we are pretending to be in week 39 of 2011.</span>

<!-- ======================================================= -->
### **trending** package {.unnumbered}

For this method we define a baseline (which should usually be about 5 years of data). 
We fit a regression to the baseline data, and then use that to predict the estimates
for the next year. 

<!-- ======================================================= -->
#### Cut-off date { -}

It is easier to define your dates in one place and then use these throughout the
rest of your code.  

Here we define a start date (when our observations started) and a cut-off date 
(the end of our baseline period - and when the period we want to predict for starts). 
~We also define how many weeks are in our year of interest (the one we are going to
be predicting)~.
We also define how many weeks are between our baseline cut-off and the end date 
that we are interested in predicting for. 


<span style="color: black;">**_NOTE:_** In this example we pretend to currently be at the end of September 2011 ("2011 W39").</span>  

```{r cut_off}

## define start date (when observations began)
start_date <- min(counts$epiweek)

## define a cut-off week (end of baseline, start of prediction period)
cut_off <- yearweek("2010-12-31")

## define the last date interested in (i.e. end of prediction)
end_date <- yearweek("2011-12-31")

## find how many weeks in period (year) of interest
num_weeks <- as.numeric(end_date - cut_off)

```


<!-- ======================================================= -->
#### Add rows {.unnumbered}

To be able to forecast in a tidyverse format, we need to have the right number 
of rows in our dataset, i.e. one row for each week up to the `end_date`defined above. 
The code below allows you to add these rows for by a grouping variable - for example
if we had multiple countries in one dataset, we could group by country and then 
add rows appropriately for each. 
The `group_by_key()` function from **tsibble** allows us to do this grouping 
and then pass the grouped data to **dplyr** functions, `group_modify()` and 
`add_row()`. Then we specify the sequence of weeks between one after the maximum week 
currently available in the data and the end week. 

```{r add_rows}

## add in missing weeks till end of year 
counts <- counts %>%
  ## group by the region
  group_by_key() %>%
  ## for each group add rows from the highest epiweek to the end of year
  group_modify(~add_row(.,
                        epiweek = seq(max(.$epiweek) + 1, 
                                      end_date,
                                      by = 1)))

```



<!-- ======================================================= -->
#### Fourier terms {.unnumbered}

We need to redefine our fourier terms - as we want to fit them to the baseline 
date only and then predict (extrapolate) those terms for the next year. 
To do this we need to combine two output lists from the `fourier()` function together; 
the first one is for the baseline data, and the second one predicts for the 
year of interest (by defining the `h` argument).  

*N.b.* to bind rows we have to use `rbind()` (rather than tidyverse `bind_rows`) as
the fourier columns are a list (so not named individually). 

```{r fourier_terms_pred}


## define fourier terms (sincos) 
counts <- counts %>% 
  mutate(
    ## combine fourier terms for weeks prior to  and after 2010 cut-off date
    ## (nb. 2011 fourier terms are predicted)
    fourier = rbind(
      ## get fourier terms for previous years
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for 2011 (using baseline data)
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = num_weeks
        )
      )
    )

```

<!-- ======================================================= -->
#### Split data and fit regression {.unnumbered}

We now have to split our dataset in to the baseline period and the prediction 
period. This is done using the **dplyr** `group_split()` function after `group_by()`, 
and will create a list with two data frames, one for before your cut-off and one 
for after.  

We then use the **purrr** package `pluck()` function to pull the datasets out of the
list (equivalent of using square brackets, e.g. `dat[[1]]`), and can then fit 
our model to the baseline data, and then use the `predict()` function for our data
of interest after the cut-off.  

See the page on [Iteration, loops, and lists] to learn more about **purrr**.  

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r forecast_regression, warning = FALSE}
# split data for fitting and prediction
dat <- counts %>% 
  group_by(epiweek <= cut_off) %>%
  group_split()

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier
)

# define which data to use for fitting and which for predicting
fitting_data <- pluck(dat, 2)
pred_data <- pluck(dat, 1) %>% 
  select(case_int, epiweek, fourier)

# fit model 
fitted_model <- trending::fit(model, data.frame(fitting_data))

# get confint and estimates for fitted data
observed <- fitted_model %>% 
  predict(simulate_pi = FALSE)

# forecast with data want to predict with 
forecasts <- fitted_model %>% 
  predict(data.frame(pred_data), simulate_pi = FALSE)

## combine baseline and predicted datasets
observed <- bind_rows(observed, forecasts)

```

As previously, we can visualise our model with **ggplot**. We highlight alerts with
red dots for observed counts above the 95% prediction interval. 
This time we also add a vertical line to label when the forecast starts. 

```{r forecast_plot}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "grey") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## plot in points for the observed counts above expected
  geom_point(
    data = filter(observed, case_int > upper_pi), 
    aes(y = case_int), 
    colour = "red", 
    size = 2) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(cut_off), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Forecast", 
           x = cut_off, 
           y = max(observed$upper_pi) - 250, 
           angle = 90, 
           vjust = 1
           ) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```



<!-- ======================================================= -->
#### Prediction validation {.unnumbered}

Beyond inspecting residuals, it is important to investigate how good your model is
at predicting cases in the future. This gives you an idea of how reliable your 
threshold alerts are.  

The traditional way of validating is to see how well you can predict the latest 
year before the present one (because you don't yet know the counts for the "current year"). 
For example in our data set we would use the data from 2002 to 2009 to predict 2010, 
and then see how accurate those predictions are. Then refit the model to include
2010 data and use that to predict 2011 counts.  

As can be seen in the figure below by *Hyndman et al* in ["Forecasting principles 
and practice"](https://otexts.com/fpp3/). 

![](`r "https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png"`)
*figure reproduced with permission from the authors* 

The downside of this is that you are not using all the data available to you, and 
it is not the final model that you are using for prediction. 

An alternative is to use a method called cross-validation. In this scenario you 
roll over all of the data available to fit multiple models to predict one year ahead. 
You use more and more data in each model, as seen in the figure below from the 
same [*Hyndman et al* text]((https://otexts.com/fpp3/). 
For example, the first model uses 2002 to predict 2003, the second uses 2002 and 
2003 to predict 2004, and so on. 
![](`r "https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png"`)
*figure reproduced with permission from the authors*

In the below we use **purrr** package `map()` function to loop over each dataset. 
We then put estimates in one data set and merge with the original case counts, 
to use the **yardstick** package to compute measures of accuracy. 
We compute four measures including: Root mean squared error (RMSE), Mean absolute error	
(MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r cross_validation, warning = FALSE}

## Cross validation: predicting week(s) ahead based on sliding window

## expand your data by rolling over in 52 week windows (before + after) 
## to predict 52 week ahead
## (creates longer and longer chains of observations - keeps older data)

## define window want to roll over
roll_window <- 52

## define weeks ahead want to predict 
weeks_ahead <- 52

## create a data set of repeating, increasingly long data
## label each data set with a unique id
## only use cases before year of interest (i.e. 2011)
case_roll <- counts %>% 
  filter(epiweek < cut_off) %>% 
  ## only keep the week and case counts variables
  select(epiweek, case_int) %>% 
    ## drop the last x observations 
    ## depending on how many weeks ahead forecasting 
    ## (otherwise will be an actual forecast to "unknown")
    slice(1:(n() - weeks_ahead)) %>%
    as_tsibble(index = epiweek) %>% 
    ## roll over each week in x after windows to create grouping ID 
    ## depending on what rolling window specify
    stretch_tsibble(.init = roll_window, .step = 1) %>% 
  ## drop the first couple - as have no "before" cases
  filter(.id > roll_window)


## for each of the unique data sets run the code below
forecasts <- purrr::map(unique(case_roll$.id), 
                        function(i) {
  
  ## only keep the current fold being fit 
  mini_data <- filter(case_roll, .id == i) %>% 
    as_tibble()
  
  ## create an empty data set for forecasting on 
  forecast_data <- tibble(
    epiweek = seq(max(mini_data$epiweek) + 1,
                  max(mini_data$epiweek) + weeks_ahead,
                  by = 1),
    case_int = rep.int(NA, weeks_ahead),
    .id = rep.int(i, weeks_ahead)
  )
  
  ## add the forecast data to the original 
  mini_data <- bind_rows(mini_data, forecast_data)
  
  ## define the cut off based on latest non missing count data 
  cv_cut_off <- mini_data %>% 
    ## only keep non-missing rows
    drop_na(case_int) %>% 
    ## get the latest week
    summarise(max(epiweek)) %>% 
    ## extract so is not in a dataframe
    pull()
  
  ## make mini_data back in to a tsibble
  mini_data <- tsibble(mini_data, index = epiweek)
  
  ## define fourier terms (sincos) 
  mini_data <- mini_data %>% 
    mutate(
    ## combine fourier terms for weeks prior to  and after cut-off date
    fourier = rbind(
      ## get fourier terms for previous years
      forecast::fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for following year (using baseline data)
      fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = weeks_ahead
        )
      )
    )
  
  
  # split data for fitting and prediction
  dat <- mini_data %>% 
    group_by(epiweek <= cv_cut_off) %>%
    group_split()

  ## define the model you want to fit (negative binomial) 
  model <- glm_nb_model(
    ## set number of cases as outcome of interest
    case_int ~
      ## use epiweek to account for the trend
      epiweek +
      ## use the furier terms to account for seasonality
      fourier
  )

  # define which data to use for fitting and which for predicting
  fitting_data <- pluck(dat, 2)
  pred_data <- pluck(dat, 1)
  
  # fit model 
  fitted_model <- trending::fit(model, fitting_data)
  
  # forecast with data want to predict with 
  forecasts <- fitted_model %>% 
    predict(data.frame(pred_data), simulate_pi = FALSE) %>% 
    ## only keep the week and the forecast estimate
    select(epiweek, estimate)
    
  }
  )

## make the list in to a data frame with all the forecasts
forecasts <- bind_rows(forecasts)

## join the forecasts with the observed
forecasts <- left_join(forecasts, 
                       select(counts, epiweek, case_int),
                       by = "epiweek")

## using {yardstick} compute metrics
  ## RMSE: Root mean squared error
  ## MAE:  Mean absolute error	
  ## MASE: Mean absolute scaled error
  ## MAPE: Mean absolute percent error
model_metrics <- bind_rows(
  ## in your forcasted dataset compare the observed to the predicted
  rmse(forecasts, case_int, estimate), 
  mae( forecasts, case_int, estimate),
  mase(forecasts, case_int, estimate),
  mape(forecasts, case_int, estimate),
  ) %>% 
  ## only keep the metric type and its output
  select(Metric  = .metric, 
         Measure = .estimate) %>% 
  ## make in to wide format so can bind rows after
  pivot_wider(names_from = Metric, values_from = Measure)

## return model metrics 
model_metrics

```


<!-- ======================================================= -->
### **surveillance** package {.unnumbered}

In this section we use the **surveillance** package to create alert thresholds 
based on outbreak detection algorithms. There are several different methods 
available in the package, however we will focus on two options here. 
For details, see these papers on the [application](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf)
and [theory](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf)
of the alogirthms used. 

The first option uses the improved Farrington method. This fits a negative 
binomial glm (including trend) and down-weights past outbreaks (outliers) to 
create a threshold level. 

The second option use the glrnb method. This also fits a negative binomial glm 
but includes trend and fourier terms (so is favoured here). The regression is used
to calculate the "control mean" (~fitted values) - it then uses a computed 
generalized likelihood ratio statistic to assess if there is shift in the mean 
for each week. Note that the threshold for each week takes in to account previous
weeks so if there is a sustained shift an alarm will be triggered. 
(Also note that after each alarm the algorithm is reset)

In order to work with the **surveillance** package, we first need to define a 
"surveillance time series" object (using the `sts()` function) to fit within the 
framework. 

```{r surveillance_obj}

## define surveillance time series object
## nb. you can include a denominator with the population object (see ?sts)
counts_sts <- sts(observed = counts$case_int[!is.na(counts$case_int)],
                  start = c(
                    ## subset to only keep the year from start_date 
                    as.numeric(str_sub(start_date, 1, 4)), 
                    ## subset to only keep the week from start_date
                    as.numeric(str_sub(start_date, 7, 8))), 
                  ## define the type of data (in this case weekly)
                  freq = 52)

## define the week range that you want to include (ie. prediction period)
## nb. the sts object only counts observations without assigning a week or 
## year identifier to them - so we use our data to define the appropriate observations
weekrange <- cut_off - start_date

```

<!-- ======================================================= -->
#### Farrington method {.unnumbered}

We then define each of our parameters for the Farrington method in a `list`. 
Then we run the algorithm using `farringtonFlexible()` and then we can extract the 
threshold for an alert using `farringtonmethod@upperbound`to include this in our 
dataset. It is also possible to extract a TRUE/FALSE for each week if it triggered 
an alert (was above the threshold) using `farringtonmethod@alarm`. 

```{r farrington}

## define control
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  b = 9, ## how many years backwards for baseline
  w = 2, ## rolling window size in weeks
  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)
  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)
  trend = TRUE,
  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)
  thresholdMethod = "nbPlugin",
  populationOffset = TRUE
  )

## apply farrington flexible method
farringtonmethod <- farringtonFlexible(counts_sts, ctrl)

## create a new variable in the original dataset called threshold
## containing the upper bound from farrington 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold"] <- farringtonmethod@upperbound
```

We can then visualise the results in ggplot as done previously. 

```{r plot_farrington, warning=F, message=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
#### GLRNB method {.unnumbered}

Similarly for the GLRNB method we define each of our parameters for the in a `list`, 
then fit the algorithm and extract the upper bounds.

<span style="color: orange;">**_CAUTION:_** This method uses "brute force" (similar to bootstrapping) for calculating thresholds, so can take a long time!</span>

See the [GLRNB vignette](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) 
for details. 

```{r glrnb, warning = FALSE, message = FALSE}

## define control options
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include
  trend = TRUE,   ## whether to include trend or not
  refit = FALSE), ## whether to refit model after each alarm
  ## cARL = threshold for GLR statistic (arbitrary)
     ## 3 ~ middle ground for minimising false positives
     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)
   c.ARL = 2,
   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak
   ret = "cases"     ## return threshold upperbound as case counts
  )

## apply the glrnb method
glrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)

## create a new variable in the original dataset called threshold
## containing the upper bound from glrnb 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold_glrnb"] <- glrnbmethod@upperbound

```

Visualise the outputs as previously. 

```{r plot_glrnb, message=F, warning=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold_glrnb, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
## Interrupted timeseries {  }

Interrupted timeseries (also called segmented regression or intervention analysis), 
is often used in assessing the impact of vaccines on the incidence of disease. 
But it can be used for assessing impact of a wide range of interventions or introductions. 
For example changes in hospital procedures or the introduction of a new disease 
strain to a population. 
In this example we will pretend that a new strain of Campylobacter was introduced
to Germany at the end of 2008, and see if that affects the number of cases. 
We will use negative binomial regression again. The regression this time will be 
split in to two parts, one before the intervention (or introduction of new strain here) 
and one after (the pre and post-periods). This allows us to calculate an incidence rate ratio comparing the
two time periods. Explaining the equation might make this clearer (if not then just
ignore!). 

The negative binomial regression can be defined as follows: 

$$\log(Y_t)= β_0 + β_1 \times t+ β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+ + log(pop_t) + e_t$$

Where:
$Y_t$is the number of cases observed at time $t$  
$pop_t$ is the population size in 100,000s at time $t$ (not used here)  
$t_0$ is the last year of the of the pre-period (including transition time if any)  
$δ(x$ is the indicator function (it is 0 if x≤0 and 1 if x>0)  
$(x)^+$ is the cut off operator (it is x if x>0 and 0 otherwise)  
$e_t$ denotes the residual 
Additional terms trend and season can be added as needed. 

$β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+$ is the generalised linear 
part of the post-period and is zero in the pre-period. 
This means that the $β_2$ and $β_3$ estimates are the effects of the intervention. 

We need to re-calculate the fourier terms without forecasting here, as we will use
all the data available to us (i.e. retrospectively). Additionally we need to calculate
the extra terms needed for the regression. 

```{r define_terms_interrupted}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  as_tsibble(index = epiweek) %>% 
  fourier(K = 1)

## define intervention week 
intervention_week <- yearweek("2008-12-31")

## define variables for regression 
counts <- counts %>% 
  mutate(
    ## corresponds to t in the formula
      ## count of weeks (could probably also just use straight epiweeks var)
    # linear = row_number(epiweek), 
    ## corresponds to delta(t-t0) in the formula
      ## pre or post intervention period
    intervention = as.numeric(epiweek >= intervention_week), 
    ## corresponds to (t-t0)^+ in the formula
      ## count of weeks post intervention
      ## (choose the larger number between 0 and whatever comes from calculation)
    time_post = pmax(0, epiweek - intervention_week + 1))

```

We then use these terms to fit a negative binomial regression, and produce a 
table with percentage change. What this example shows is that there was no 
significant change. 

<span style="color: orange;">**_CAUTION:_** Note the use of `simulate_pi = FALSE`
within the `predict()` argument. This is because the default behaviour of **trending** 
is to use the **ciTools** package to estimate a prediction interval. This does not 
work if there are `NA` counts, and also produces more granular intervals. 
See `?trending::predict.trending_model_fit` for details. </span>  

```{r interrupted_regression, warning = FALSE}


## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier + 
    ## add in whether in the pre- or post-period 
    intervention + 
    ## add in the time post intervention 
    time_post
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)



## show estimates and percentage change in a table
fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE) %>% 
  ## only keep the intervention value 
  filter(term == "intervention") %>% 
  ## change the IRR to percentage change for estimate and CIs 
  mutate(
    ## for each of the columns of interest - create a new column
    across(
      all_of(c("estimate", "conf.low", "conf.high")), 
      ## apply the formula to calculate percentage change
            .f = function(i) 100 * (i - 1), 
      ## add a suffix to new column names with "_perc"
      .names = "{.col}_perc")
    ) %>% 
  ## only keep (and rename) certain columns 
  select("IRR" = estimate, 
         "95%CI low" = conf.low, 
         "95%CI high" = conf.high,
         "Percentage change" = estimate_perc, 
         "95%CI low (perc)" = conf.low_perc, 
         "95%CI high (perc)" = conf.high_perc,
         "p-value" = p.value)
```

As previously we can visualise the outputs of the regression. 

```{r plot_interrupted}

ggplot(observed, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate, col = "Estimate")) + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(intervention_week), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Intervention", 
           x = intervention_week, 
           y = max(observed$upper_pi), 
           angle = 90, 
           vjust = 1
           ) + 
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Estimate" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```


<!-- ======================================================= -->
## Resources {  }

[forecasting: principles and practice textbook](https://otexts.com/fpp3/)  
[EPIET timeseries analysis case studies](https://github.com/EPIET/TimeSeriesAnalysis)  
[Penn State course](https://online.stat.psu.edu/stat510/lesson/1) 
[Surveillance package manuscript](https://www.jstatsoft.org/article/view/v070i10)





```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/time_series.Rmd-->


# Epidemic modeling { }  


<!-- ======================================================= -->
## Overview {  }

There exists a growing body of tools for epidemic modelling that lets us conduct
fairly complex analyses with minimal effort. This section will provide an
overview on how to use these tools to:

* estimate the effective reproduction number R<sub>t</sub> and related statistics
  such as the doubling time
* produce short-term projections of future incidence

It is *not* intended as an overview of the methodologies and statistical methods
underlying these tools, so please refer to the Resources tab for links to some
papers covering this. Make sure you have an understanding of
the methods before using these tools; this will ensure you can accurately
interpret their results.

Below is an example of one of the outputs we'll be producing in this section.

```{r out.width=c('100%', '100%'), fig.show='hold', echo=F, fig.width = 12, fig.height = 9, message=F, warning=F}

## install and load packages
pacman::p_load(tidyverse, EpiNow2, EpiEstim, here, incidence2, epicontacts, rio, projections)

## load linelist
linelist <- import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## ## estimate gamma generation time
## generation_time <- bootstrapped_dist_fit(
##   get_pairwise(epic, "date_infection"),
##   dist = "gamma",
##   max_value = 20,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   generation_time,
##   here("data/cache/epidemic_models/generation_time.rds")
## )

## import cached generation time
generation_time <- import(here("data/cache/epidemic_models/generation_time.rds"))

## ## estimate incubation period
## incubation_period <- bootstrapped_dist_fit(
##   linelist$date_onset - linelist$date_infection,
##   dist = "lognormal",
##   max_value = 100,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   incubation_period,
##   here("data/cache/epidemic_models/incubation_period.rds")
## )

## import cached incubation period
incubation_period <- import(here("data/cache/epidemic_models/incubation_period.rds"))

## get incidence from onset date
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())

## ## run epinow
## epinow_res <- epinow(
##   reported_cases = cases,
##   generation_time = generation_time,
##   delays = delay_opts(incubation_period),
##   target_folder = here("data/cache/epidemic_models"),
##   return_output = TRUE,
##   output = "samples",
##   verbose = TRUE,
##   stan = stan_opts(samples = 750, chains = 4),
##   horizon = 21
## )

## ## export for caching
## export(
##   epinow_res,
##   here("data/cache/epidemic_models/epinow_res.rds")
## )

## import cached epinow results
epinow_res <- import(here("data/cache/epidemic_models/epinow_res.rds"))

## plot summary figure
plot(epinow_res)

```

<!-- ======================================================= -->
## Preparation {  }

We will use two different methods and packages for R<sub>t</sub> estimation,
namely **EpiNow** and **EpiEstim**, as well as the **projections** package for
forecasting case incidence.  

This code chunk shows the loading of packages required for the analyses. 
In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. 
You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

	
```{r epidemic_models_packages, }
pacman::p_load(
   rio,          # File import
   here,         # File locator
   tidyverse,    # Data management + ggplot2 graphics
   epicontacts,  # Analysing transmission networks
   EpiNow2,      # Rt estimation
   EpiEstim,     # Rt estimation
   projections,  # Incidence projections
   incidence2,   # Handling incidence data
   epitrix,      # Useful epi functions
   distcrete     # Discrete delay distributions
)
```
	
We will use the cleaned case linelist for all analyses in this section. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). See the [Download handbook and data] page to download all example data used in this handbook.  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# import the cleaned linelist
linelist <- import("linelist_cleaned.rds")
```


<!-- ======================================================= -->
## Estimating R<sub>t</sub> {  }

### EpiNow2 vs. EpiEstim {.unnumbered}

The reproduction number R is a measure of the transmissibility of a disease and
is defined as the expected number of secondary cases per infected case. In a
fully susceptible population, this value represents the basic reproduction
number R<sub>0</sub>. However, as the number of susceptible individuals in a
population changes over the course of an outbreak or pandemic, and as various
response measures are implemented, the most commonly used measure of
transmissibility is the effective reproduction number R<sub>t</sub>; this is
defined as the expected number of secondary cases per infected case at a given
time _t_.

The **EpiNow2** package provides the most sophisticated framework for estimating
R<sub>t</sub>. It has two key advantages over the other commonly used package,
**EpiEstim**:

* It accounts for delays in reporting and can therefore estimate R<sub>t</sub>
  even when recent data is incomplete.
* It estimates R<sub>t</sub> on _dates of infection_ rather than the dates of
  onset of reporting, which means that the effect of an intervention will
  be immediately reflected in a change in R<sub>t</sub>, rather than with a
  delay.

However, it also has two key disadvantages:

* It requires knowledge of the generation time distribution (i.e. distribution
  of delays between infection of a primary and secondary cases), incubation
  period distribution (i.e. distribution of delays between infection and symptom
  onset) and any further delay distribution relevant to your data (e.g. if you
  have dates of reporting, you require the distribution of delays from symptom
  onset to reporting). While this will allow more accurate estimation of
  R<sub>t</sub>, **EpiEstim** only requires the serial interval distribution
  (i.e. the distribution of delays between symptom onset of a primary and a
  secondary case), which may be the only distribution available to you.
* **EpiNow2** is significantly slower than **EpiEstim**, anecdotally by a factor
  of about 100-1000! For example, estimating R<sub>t</sub> for the sample outbreak
  considered in this section takes about four hours (this was run for a large
  number of iterations to ensure high accuracy and could probably be reduced if
  necessary, however the points stands that the algorithm is slow in
  general). This may be unfeasible if you are regularly updating your
  R<sub>t</sub> estimates.
  
Which package you choose to use will therefore depend on the data, time and
computational resources available to you.

### EpiNow2 {.unnumbered}

#### Estimating delay distributions {.unnumbered}

The delay distributions required to run **EpiNow2** depend on the data you
have. Essentially, you need to be able to describe the delay from the date of
infection to the date of the event you want to use to estimate R<sub>t</sub>. If
you are using dates of onset, this would simply be the incubation period
distribution. If you are using dates of reporting, you require the
delay from infection to reporting. As this distribution is unlikely to be known
directly, **EpiNow2** lets you chain multiple delay distributions together; in
this case, the delay from infection to symptom onset (e.g. the incubation
period, which is likely known) and from symptom onset to reporting (which you
can often estimate from the data).

As we have the dates of onset for all our cases in the example linelist, we will
only require the incubation period distribution to link our data (e.g. dates of
symptom onset) to the date of infection. We can either estimate this distribution
from the data or use values from the literature.

A literature estimate of the incubation period of Ebola (taken
from [this paper](https://www.nejm.org/doi/full/10.1056/nejmoa1411100)) with a
mean of 9.1, standard deviation of 7.3 and maximum value of 30 would be
specified as follows:

```{r epidemic_models_incubation_literature, eval=F}
incubation_period_lit <- list(
  mean = log(9.1),
  mean_sd = log(0.1),
  sd = log(7.3),
  sd_sd = log(0.1),
  max = 30
)
```
Note that **EpiNow2** requires these delay distributions to be provided on a **log**
scale, hence the `log` call around each value (except the `max` parameter which,
confusingly, has to be provided on a natural scale). The `mean_sd` and `sd_sd`
define the standard deviation of the mean and standard deviation estimates. As
these are not known in this case, we choose the fairly arbitrary value of 0.1.

In this analysis, we instead estimate the incubation period distribution
from the linelist itself using the function `bootstrapped_dist_fit`, which will
fit a lognormal distribution to the observed delays between infection and onset
in the linelist.

```{r epidemic_models_incubation_estimate, eval=F}
## estimate incubation period
incubation_period <- bootstrapped_dist_fit(
  linelist$date_onset - linelist$date_infection,
  dist = "lognormal",
  max_value = 100,
  bootstraps = 1
)
```

The other distribution we require is the generation time. As we have data on
infection times __and__ transmission links, we can estimate this
distribution from the linelist by calculating the delay between infection times
of infector-infectee pairs. To do this, we use the handy `get_pairwise` function
from the package **epicontacts**, which allows us to calculate pairwise
differences of linelist properties between transmission pairs. We first create an
epicontacts object (see [Transmission chains] page for further
details):

```{r epidemic_models_epicontacts, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in infection times between transmission pairs,
calculated using `get_pairwise`, to a gamma distribution:

```{r epidemic_models_generation_estimate, eval=F}
## estimate gamma generation time
generation_time <- bootstrapped_dist_fit(
  get_pairwise(epic, "date_infection"),
  dist = "gamma",
  max_value = 20,
  bootstraps = 1
)
```

#### Running **EpiNow2** {.unnumbered}

Now we just need to calculate daily incidence from the linelist, which we can do
easily with the **dplyr** functions `group_by()` and `n()`. Note
that **EpiNow2** requires the column names to  be `date` and `confirm`.

```{r epidemic_models_cases, eval=F}
## get incidence from onset dates
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())
```

We can then estimate R<sub>t</sub> using the `epinow` function. Some notes on
the inputs:

* We can provide any number of 'chained' delay distributions to the `delays`
  argument; we would simply insert them alongside the `incubation_period` object
  within the `delay_opts` function.
* `return_output` ensures the output is returned within R and not just saved to
  a file.
* `verbose` specifies that we want a readout of the progress.
* `horizon` indicates how many days we want to project future incidence for.
* We pass additional options to the `stan` argument to specify how long
  we want to run the inference for. Increasing `samples` and `chains` will give
  you a more accurate estimate that better characterises uncertainty, however
  will take longer to run.

```{r epidemic_models_run_epinow, eval=F}
## run epinow
epinow_res <- epinow(
  reported_cases = cases,
  generation_time = generation_time,
  delays = delay_opts(incubation_period),
  return_output = TRUE,
  verbose = TRUE,
  horizon = 21,
  stan = stan_opts(samples = 750, chains = 4)
)
```

#### Analysing outputs {.unnumbered}

Once the code has finished running, we can plot a summary very easily as follows. Scroll the image to see the full extent.  


```{r out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F }
## plot summary figure
plot(epinow_res)
```

We can also look at various summary statistics:

```{r epidemic_models_epinow_summary,}
## summary table
epinow_res$summary
```

For further analyses and custom plotting, you can access the summarised daily
estimates via `$estimates$summarised`. We will convert this from the default
`data.table` to a `tibble` for ease of use with **dplyr**.

```{r epidemic_models_to_tibble, eval=F}
## extract summary and convert to tibble
estimates <- as_tibble(epinow_res$estimates$summarised)
estimates
```

```{r epidemic_models_tibble_show,  echo = F}
## show outputs
estimates <- as_tibble(epinow_res$estimates$summarised)
DT::datatable(
  estimates,
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap'
)
```

As an example, let's make a plot of the doubling time and R<sub>t</sub>. We will
only look at the first few months of the outbreak when R<sub>t</sub> is well
above one, to avoid plotting extremely high doublings times.

We use the formula `log(2)/growth_rate` to calculate the doubling time from the
estimated growth rate.

```{r epidemic_models_plot_epinow_cusotom, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## make wide df for median plotting
df_wide <- estimates %>%
  filter(
    variable %in% c("growth_rate", "R"),
    date < as.Date("2014-09-01")
  ) %>%
  ## convert growth rates to doubling times
  mutate(
    across(
      c(median, lower_90:upper_90),
      ~ case_when(
        variable == "growth_rate" ~ log(2)/.x,
        TRUE ~ .x
      )
    ),
    ## rename variable to reflect transformation
    variable = replace(variable, variable == "growth_rate", "doubling_time")
  )

## make long df for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(R = "R[t]", doubling_time = "Doubling~time"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credibel\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )

```

<!-- ======================================================= -->
### EpiEstim {.unnumbered}

To run **EpiEstim**, we need to provide data on daily incidence and specify the
serial interval (i.e. the distribution of delays between symptom onset of
primary and secondary cases). 

Incidence data can be provided to **EpiEstim** as a vector, a data frame, or an `incidence`
object from the original **incidence** package. You can even distinguish between imports
and locally acquired infections; see the documentation at `?estimate_R` for
further details.  

We will create the input using **incidence2**. See the page on [Epidemic curves] for more examples with the **incidence2** package. Since there have been updates to the **incidence2** package that don't completely align with `estimateR()`'s expected input, there are some minor additional steps needed. The incidence object consists of a tibble with dates and their respective case counts. We use `complete()` from **tidyr** to ensure all dates are included (even those with no cases), and then `rename()` the columns to align with what is expected by `estimate_R()` in a later step.  

```{r epidemic_models_epiestim_incidence,}
## get incidence from onset date
cases <- incidence2::incidence(linelist, date_index = date_onset) %>% # get case counts by day
  tidyr::complete(date_index = seq.Date(                              # ensure all dates are represented
    from = min(date_index, na.rm = T),
    to = max(date_index, na.rm=T),
    by = "day"),
    fill = list(count = 0)) %>%                                       # convert NA counts to 0
  rename(I = count,                                                   # rename to names expected by estimateR
         dates = date_index)
```

The package provides several options for specifying the serial interval, the
details of which are provided in the documentation at `?estimate_R`. We will
cover two of them here.

#### Using serial interval estimates from the literature {.unnumbered}

Using the option `method = "parametric_si"`, we can manually specify the mean and
standard deviation of the serial interval in a `config` object created using the
function `make_config`. We use a mean and standard deviation of 12.0 and 5.2, respectively, defined in
[this paper](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0):

```{r epidemic_models_epiestim_config,}
## make config
config_lit <- make_config(
  mean_si = 12.0,
  std_si = 5.2
)
```

We can then estimate R<sub>t</sub> with the `estimate_R` function:

```{r epidemic_models_epiestim_lit,  warning = FALSE}
epiestim_res_lit <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_lit
)
```

and plot a summary of the outputs:

```{r epidemic_models_epiestim_lit_plot,  warning = FALSE}
plot(epiestim_res_lit)
```

#### Using serial interval estimates from the data {.unnumbered}

As we have data on dates of symptom onset _and_ transmission links, we can
also estimate the serial interval from the linelist by calculating the delay
between onset dates of infector-infectee pairs. As we did in the **EpiNow2**
section, we will use the `get_pairwise` function from the **epicontacts**
package, which allows us to calculate pairwise differences of linelist
properties between transmission pairs. We first create an epicontacts object
(see [Transmission chains] page for further details):

```{r epidemic_models_epicontacts_epiestim, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in onset dates between transmission pairs, calculated
using `get_pairwise`, to a gamma distribution. We use the handy `fit_disc_gamma`
from the **epitrix** package for this fitting procedure, as we require a
_discretised_ distribution.

```{r epidemic_models_incubation_estimate_epiestim,  warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))
```

We then pass this information to the `config` object, run **EpiEstim**
again and plot the results:

```{r epidemic_models_epiestim_emp,  warning = FALSE}
## make config
config_emp <- make_config(
  mean_si = serial_interval$mu,
  std_si = serial_interval$sd
)

## run epiestim
epiestim_res_emp <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_emp
)

## plot outputs
plot(epiestim_res_emp)
```

#### Specifying estimation time windows {.unnumbered}

These default options will provide a weekly sliding estimate and might act as a
warning that you are estimating R<sub>t</sub> too early in the outbreak for a
precise estimate. You can change this by setting a later start date for the
estimation as shown below. Unfortunately, **EpiEstim** only provides a very
clunky way of specifying these estimations times, in that you have to provide a
vector of __integers__ referring to the start and end dates for each time
window.

```{r epidemic_models_epiestim_config_late,}

## define a vector of dates starting on June 1st
start_dates <- seq.Date(
  as.Date("2014-06-01"),
  max(cases$dates) - 7,
  by = 1
) %>%
  ## subtract the starting date to convert to numeric
  `-`(min(cases$dates)) %>%
  ## convert to integer
  as.integer()

## add six days for a one week sliding window
end_dates <- start_dates + 6
  
## make config
config_partial <- make_config(
  mean_si = 12.0,
  std_si = 5.2,
  t_start = start_dates,
  t_end = end_dates
)
```
Now we re-run **EpiEstim** and can see that the estimates only start from June:

```{r epidemic_models_epiestim_config_late_run,}

## run epiestim
epiestim_res_partial <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_partial
)

## plot outputs
plot(epiestim_res_partial)

```

#### Analysing outputs {.unnumbered}

The main outputs can be accessed via `$R`. As an example, we will create a plot of
R<sub>t</sub> and a measure of "transmission potential" given by the product of
R<sub>t</sub> and the number of cases reported on that day; this represents the
expected number of cases in the next generation of infection.

```{r epidemic_models_epiestim_plot_full, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## make wide dataframe for median
df_wide <- epiestim_res_lit$R %>%
  rename_all(clean_labels) %>%
  rename(
    lower_95_r = quantile_0_025_r,
    lower_90_r = quantile_0_05_r,
    lower_50_r = quantile_0_25_r,
    upper_50_r = quantile_0_75_r,
    upper_90_r = quantile_0_95_r,
    upper_95_r = quantile_0_975_r,
    ) %>%
  mutate(
    ## extract the median date from t_start and t_end
    dates = epiestim_res_emp$dates[round(map2_dbl(t_start, t_end, median))],
    var = "R[t]"
  ) %>%
  ## merge in daily incidence data
  left_join(cases, "dates") %>%
  ## calculate risk across all r estimates
  mutate(
    across(
      lower_95_r:upper_95_r,
      ~ .x*I,
      .names = "{str_replace(.col, '_r', '_risk')}"
    )
  ) %>%
  ## seperate r estimates and risk estimates
  pivot_longer(
    contains("median"),
    names_to = c(".value", "variable"),
    names_pattern = "(.+)_(.+)"
  ) %>%
  ## assign factor levels
  mutate(variable = factor(variable, c("risk", "r")))

## make long dataframe from quantiles
df_long <- df_wide %>%
  select(-variable, -median) %>%
  ## seperate r/risk estimates and quantile levels
  pivot_longer(
    contains(c("lower", "upper")),
    names_to = c(".value", "quantile", "variable"),
    names_pattern = "(.+)_(.+)_(.+)"
  ) %>%
  mutate(variable = factor(variable, c("risk", "r")))

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = dates, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = dates, y = median),
    alpha = 0.2
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(r = "R[t]", risk = "Transmission~potential"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`50` = 0.7, `90` = 0.4, `95` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )
  
```

<!-- ======================================================= -->
## Projecting incidence {  }

### EpiNow2 {.unnumbered}

Besides estimating R<sub>t</sub>, **EpiNow2** also supports forecasting of
R<sub>t</sub> and projections of case numbers by integration with the
**EpiSoon** package under the hood. All you need to do is specify the `horizon`
argument in your `epinow` function call, indicating how many days you want to
project into the future; see the **EpiNow2** section under the "Estimating
R<sub>t</sub>" for details on how to get **EpiNow2** up and running. In this
section, we will just plot the outputs from that analysis, stored in the
`epinow_res` object.

```{r epidemic_models_episoon, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## define minimum date for plot
min_date <- as.Date("2015-03-01")

## extract summarised estimates
estimates <-  as_tibble(epinow_res$estimates$summarised)

## extract raw data on case incidence
observations <- as_tibble(epinow_res$estimates$observations) %>%
  filter(date > min_date)

## extract forecasted estimates of case numbers
df_wide <- estimates %>%
  filter(
    variable == "reported_cases",
    type == "forecast",
    date > min_date
  )

## convert to even longer format for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_histogram(
    data = observations,
    aes(x = date, y = confirm),
    stat = 'identity',
    binwidth = 1
  ) +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  geom_vline(xintercept = min(df_long$date), linetype = 2) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = "Daily reported cases",
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14)

```

### projections {.unnumbered}

The **projections** package developed by RECON makes it very easy to make short
term incidence forecasts, requiring only knowledge of the effective reproduction
number R<sub>t</sub> and the serial interval. Here we will cover how to use
serial interval estimates from the literature and how to use our own estimates
from the linelist.

#### Using serial interval estimates from the literature {.unnumbered}

**projections** requires a discretised serial interval distribution of the class
`distcrete` from the package **distcrete**. We will use a gamma distribution
with a mean of 12.0 and and standard deviation of 5.2 defined in
[this paper](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0). To
convert these values into the shape and scale parameters required for a gamma
distribution, we will use the function `gamma_mucv2shapescale` from the
**epitrix** package.

```{r epidemic_models_projections_distcrete,}

## get shape and scale parameters from the mean mu and the coefficient of
## variation (e.g. the ratio of the standard deviation to the mean)
shapescale <- epitrix::gamma_mucv2shapescale(mu = 12.0, cv = 5.2/12)

## make distcrete object
serial_interval_lit <- distcrete::distcrete(
  name = "gamma",
  interval = 1,
  shape = shapescale$shape,
  scale = shapescale$scale
)

```

Here is a quick check to make sure the serial interval looks correct. We
access the density of the gamma distribution we have just defined by `$d`, which
is equivalent to calling `dgamma`:

```{r epidemic_models_projections_distcrete_plot,}

## check to make sure the serial interval looks correct
qplot(
  x = 0:50, y = serial_interval_lit$d(0:50), geom = "area",
  xlab = "Serial interval", ylab = "Density"
)

```

#### Using serial interval estimates from the data {.unnumbered}

As we have data on dates of symptom onset _and_ transmission links, we can
also estimate the serial interval from the linelist by calculating the delay
between onset dates of infector-infectee pairs. As we did in the **EpiNow2**
section, we will use the `get_pairwise` function from the **epicontacts**
package, which allows us to calculate pairwise differences of linelist
properties between transmission pairs. We first create an epicontacts object
(see [Transmission chains] page for further details):

```{r epidemic_models_epicontacts_projections, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

We then fit the difference in onset dates between transmission pairs, calculated
using `get_pairwise`, to a gamma distribution. We use the handy `fit_disc_gamma`
from the **epitrix** package for this fitting procedure, as we require a
_discretised_ distribution.

```{r epidemic_models_incubation_estimate_projections,  warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))

## inspect estimate
serial_interval[c("mu", "sd")]
```

#### Projecting incidence {.unnumbered}

To project future incidence, we still need to provide historical incidence in
the form of an `incidence` object, as well as a sample of plausible
R<sub>t</sub> values. We will generate these values using the R<sub>t</sub>
estimates generated by **EpiEstim** in the previous section (under "Estimating
R<sub>t</sub>") and stored in the `epiestim_res_emp` object. In the code below,
we extract the mean and standard deviation estimates of R<sub>t</sub> for the
last time window of the outbreak (using the `tail` function to access the last
element in a vector), and simulate 1000 values from a gamma distribution using
`rgamma`. You can also provide your own vector of R<sub>t</sub> values that you
want to use for forward projections.

```{r epidemic_models_projection_setup,  warning = FALSE}

## create incidence object from dates of onset
inc <- incidence::incidence(linelist$date_onset)

## extract plausible r values from most recent estimate
mean_r <- tail(epiestim_res_emp$R$`Mean(R)`, 1)
sd_r <- tail(epiestim_res_emp$R$`Std(R)`, 1)
shapescale <- gamma_mucv2shapescale(mu = mean_r, cv = sd_r/mean_r)
plausible_r <- rgamma(1000, shape = shapescale$shape, scale = shapescale$scale)

## check distribution
qplot(x = plausible_r, geom = "histogram", xlab = expression(R[t]), ylab = "Counts")

```

We then use the `project()` function to make the actual forecast. We specify how
many days we want to project for via the `n_days` arguments, and specify the
number of simulations using the `n_sim` argument.

```{r epidemic_models_make_projection,}

## make projection
proj <- project(
  x = inc,
  R = plausible_r,
  si = serial_interval$distribution,
  n_days = 21,
  n_sim = 1000
)

```

We can then handily plot the incidence and projections using the `plot()` and
`add_projections()` functions. We can easily subset the `incidence` object to only
show the most recent cases by using the square bracket operator.

```{r epidemic_models_plot_projection, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## plot incidence and projections
plot(inc[inc$dates > as.Date("2015-03-01")]) %>%
  add_projections(proj)

```

You can also easily extract the raw estimates of daily case numbers by
converting the output to a dataframe.

```{r epidemic_models_projection_df, eval=F, warning = FALSE}
## convert to data frame for raw data
proj_df <- as.data.frame(proj)
proj_df
```

```{r epidemic_models_projection_dt,  echo = F}

## convert to data frame for raw data
proj_df <- as.data.frame(proj)

## data table output
DT::datatable(
  proj_df[1:11],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap'
)

```


<!-- ======================================================= -->
## Resources {  }

* [Here is the paper](https://www.sciencedirect.com/science/article/pii/S1755436519300350) describing
  the methodology implemented in **EpiEstim**.
* [Here is the paper](https://wellcomeopenresearch.org/articles/5-112/v1) describing
  the methodology implemented in **EpiNow2**.
* [Here is a paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008409) describing
  various methodological and practical considerations for estimating R<sub>t</sub>.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epidemic_models.Rmd-->


# Contact tracing { }


This page demonstrates descriptive analysis of contact tracing data, addessing some key considerations and approaches unique to these kinds of data.  

This page references many of the core R data management and visualisation competencies covered in other pages (e.g. data cleaning, pivoting, tables, time-series analyses), but we will highlight examples specific to contact tracing that have been useful for operational decision making. For example, this includes visualizing contact tracing follow-up data over time or across geographic areas, or producing clean Key Performance Indicator (KPI) tables for contact tracing supervisors.

For demonstration purposes we will use sample contact tracing data from the [Go.Data](https://www.who.int/tools/godata) platform. The principles covered here will apply for contact tracing data from other platforms - you may just need to undergo different data pre-processing steps depending on the structure of your data.  

You can read more about the Go.Data project on the [Github Documentation site](https://worldhealthorganization.github.io/godata/) or [Community of Practice](https://community-godata.who.int/). 

## Preparation


### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, message = F}
pacman::p_load(
  rio,          # importing data  
  here,         # relative file pathways  
  janitor,      # data cleaning and tables
  lubridate,    # working with dates
  epikit,       # age_categories() function
  apyramid,     # age pyramids
  tidyverse,    # data manipulation and visualization
  RColorBrewer, # color palettes
  formattable,  # fancy tables
  kableExtra    # table formatting
)
```


### Import data {.unnumbered}

We will import sample datasets of contacts, and of their "follow-up". These data have been retrieved and un-nested from the Go.Data API and stored as ".rds" files.  

You can download all the example data for this handbook from the [Download handbook and data] page. 

If you want to download the example contact tracing data specific to this page, use the three download links below:  

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the case investigation data (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the contact registration data (.rds file)</span>
</a>

<a href='https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true' class='download-button'>
	Click to download
	<span>the contact follow-up data (.rds file)</span>
</a>

<!-- ```{r out.width = "100%", fig.align = "center", echo=F} -->
<!-- knitr::include_graphics(here::here("images", "godata_api_github.png")) -->
<!-- ``` -->


In their original form in the downloadable files, the data reflect data as provided by the Go.Data API (learn about [APIs here](#import_api)). For example purposes here, we will clean the data to make it easier to read on this page. If you are using a Go.Data instance, you can view complete instructions on how to retrieve your data [here](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting).  

Below, the datasets are imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data. We use `here()` to specify the file path - you should provide the file path specific to your computer. We then use `select()` to select only certain columns of the data, to simplify for purposes of demonstration.  

#### Case data {.unnumbered}  

These data are a table of the cases, and information about them.  

```{r}
cases <- import(here("data", "godata", "cases_clean.rds")) %>% 
  select(case_id, firstName, lastName, gender, age, age_class,
         occupation, classification, was_contact, hospitalization_typeid)
```

Here are the ` nrow(cases)` cases:  

```{r, message=FALSE, echo=F}
DT::datatable(cases, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Contacts data {.unnumbered}  

These data are a table of all the contacts and information about them. Again, provide your own file path. After importing we perform a few preliminary data cleaning steps including:  

* Set age_class as a factor and reverse the level order so that younger ages are first  
* Select only certain column, while re-naming a one of them  
* Artificially assign rows with missing admin level 2 to "Djembe", to improve clarity of some example visualisations  


```{r}
contacts <- import(here("data", "godata", "contacts_clean.rds")) %>% 
  mutate(age_class = forcats::fct_rev(age_class)) %>% 
  select(contact_id, contact_status, firstName, lastName, gender, age,
         age_class, occupation, date_of_reporting, date_of_data_entry,
         date_of_last_exposure = date_of_last_contact,
         date_of_followup_start, date_of_followup_end, risk_level, was_case, admin_2_name) %>% 
  mutate(admin_2_name = replace_na(admin_2_name, "Djembe"))
```

Here are the ` nrow(contacts)` rows of the `contacts` dataset:  

```{r, message=FALSE, echo=F}
DT::datatable(contacts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Follow-up data {.unnumbered}  

These data are records of the "follow-up" interactions with the contacts. Each contact is supposed to have an encounter each day for 14 days after their exposure.  

We import and perform a few cleaning steps. We select certain columns, and also convert a character column to all lowercase values.  

```{r}
followups <- rio::import(here::here("data", "godata", "followups_clean.rds")) %>% 
  select(contact_id, followup_status, followup_number,
         date_of_followup, admin_2_name, admin_1_name) %>% 
  mutate(followup_status = str_to_lower(followup_status))
```

Here are the first 50 rows of the ` nrow(followups)`-row `followups` dataset (each row is a follow-up interaction, with outcome status in the `followup_status` column):  

```{r, message=FALSE, echo=F}
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Relationships data {.unnumbered}  

Here we import data showing the relationship between cases and contacts. We select certain column to show.  

```{r}
relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_visualid, source_gender, source_age, date_of_last_contact,
         date_of_data_entry, target_visualid, target_gender,
         target_age, exposure_type)
```

Below are the first 50 rows of the `relationships` dataset, which records all relationships between cases and contacts.  

```{r, message=FALSE, echo=F}
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```








## Descriptive analyses  

You can use the techniques covered in other pages of this handbook to conduct descriptive analyses of your cases, contacts, and their relationships. Below are some examples.  


### Demographics {.unnumbered}  

As demonstrated in the page covering [Demographic pyramids][Demographic pyramids and Likert-scales], you can visualise the age and gender distribution (here we use the **apyramid** package).  


#### Age and Gender of contacts {.unnumbered}  

The pyramid below compares the age distribution of contacts, by gender. Note that contacts missing age are included in their own bar at the top. You can change this default behavior, but then consider listing the number missing in a caption.  

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = contacts,                                   # use contacts dataset
  age_group = "age_class",                           # categorical age column
  split_by = "gender") +                             # gender for halfs of pyramid
  labs(
    fill = "Gender",                                 # title of legend
    title = "Age/Sex Pyramid of COVID-19 contacts")+ # title of the plot
  theme_minimal()                                    # simple background
```


With the Go.Data data structure, the `relationships` dataset contains the ages of both cases and contacts, so you could use that dataset and create an age pyramid showing the differences between these two groups of people. The `relationships` data frame will be mutated to transform the numberic age columns into categories (see the [Cleaning data and core functions] page). We also pivot the dataframe longer to facilitate easy plotting with **ggplot2** (see [Pivoting data]).  

```{r}
relation_age <- relationships %>% 
  select(source_age, target_age) %>% 
  transmute(                              # transmute is like mutate() but removes all other columns not mentioned
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5)),
    ) %>% 
  pivot_longer(cols = contains("class"), names_to = "category", values_to = "age_class")  # pivot longer


relation_age
```


Now we can plot this transformed dataset with `age_pyramid()` as before, but replacing `gender` with `category` (contact, or case).  

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = relation_age,                               # use modified relationship dataset
  age_group = "age_class",                           # categorical age column
  split_by = "category") +                           # by cases and contacts
  scale_fill_manual(
    values = c("orange", "purple"),                  # to specify colors AND labels
    labels = c("Case", "Contact"))+
  labs(
    fill = "Legend",                                           # title of legend
    title = "Age/Sex Pyramid of COVID-19 contacts and cases")+ # title of the plot
  theme_minimal()                                              # simple background
```

We can also view other characteristics such as occupational breakdown (e.g. in form of a pie chart).

```{r, warning=F, message=F}
# Clean dataset and get counts by occupation
occ_plot_data <- cases %>% 
  mutate(occupation = forcats::fct_explicit_na(occupation),  # make NA missing values a category
         occupation = forcats::fct_infreq(occupation)) %>%   # order factor levels in order of frequency
  count(occupation)                                          # get counts by occupation
  
# Make pie chart
ggplot(data = occ_plot_data, mapping = aes(x = "", y = n, fill = occupation))+
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  labs(
    fill = "Occupation",
    title = "Known occupations of COVID-19 cases")+
  theme_minimal() +                    
  theme(axis.line = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank())
```


### Contacts per case {.unnumbered}  

The number of contacts per case can be an important metric to assess quality of contact enumeration and the compliance of the population toward public health response. 

Depending on your data structure, this can be assessed with a dataset that contains all cases and contacts. In the Go.Data datasets, the links between cases ("sources") and contacts ("targets") is stored in the `relationships` dataset.  

In this dataset, each row is a contact, and the source case is listed in the row. There are no contacts who have relationships with multiple cases, but if this exists you may need to account for those before plotting (and explore them too!).  

We begin by counting the number of rows (contacts) per source case. This is saved as a data frame.  

```{r}
contacts_per_case <- relationships %>% 
  count(source_visualid)

contacts_per_case
```

We use `geom_histogram()` to plot these data as a histogram.  

```{r, warning=F, message=F}
ggplot(data = contacts_per_case)+        # begin with count data frame created above
  geom_histogram(mapping = aes(x = n))+  # print histogram of number of contacts per case
  scale_y_continuous(expand = c(0,0))+   # remove excess space below 0 on y-axis
  theme_light()+                         # simplify background
  labs(
    title = "Number of contacts per case",
    y = "Cases",
    x = "Contacts per case"
  )
  

```



## Contact Follow Up  


Contact tracing data often contain "follow-up" data, which record outcomes of daily symptom checks of persons in quarantine. Analysis of this data can inform response strategy, identify contacts at-risk of loss-to-follow-up or at-risk of developing disease.  




### Data cleaning {.unnumbered}  

These data can exist in a variety of formats. They may exist as a "wide" format Excel sheet with one row per contact, and one column per follow-up "day". See [Pivoting data] for descriptions of "long" and "wide" data and how to pivot data wider or longer.  

In our Go.Data example, these data are stored in the `followups` data frame, which is in a "long" format  with one row per follow-up interaction. The first 50 rows look like this:   

```{r, message=FALSE, echo=FALSE}
# display the first 50 rows of contact linelist data as a table
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<span style="color: orange;">**_CAUTION:_** Beware of duplicates when dealing with followup data; as there could be several erroneous followups on the same day for a given contact. Perhaps it seems to be an error but reflects reality - e.g. a contact tracer could submit a follow-up form early in the day when they could not reach the contact, and submit a second form when they were later reached. It will depend on the operational context for how you want to handle duplicates - just make sure to document your approach clearly. </span>

Let's *see* how many instances of "duplicate" rows we have:  

```{r}
followups %>% 
  count(contact_id, date_of_followup) %>%   # get unique contact_days
  filter(n > 1)                             # view records where count is more than 1  
```

In our example data, the only records that this applies to are ones missing an ID! We can remove those. But, for purposes of demonstration we will go show the steps for de-duplication so there is only one follow-up encoutner per person per day. See the page on [De-duplication] for more detail. We will assume that the most recent encounter record is the correct one. We also take the opportunity to clean the `followup_number` column (the "day" of follow-up which should range 1 - 14).  

```{r, warning=F, message=F}
followups_clean <- followups %>%
  
  # De-duplicate
  group_by(contact_id, date_of_followup) %>%        # group rows per contact-day
  arrange(contact_id, desc(date_of_followup)) %>%   # arrange rows, per contact-day, by date of follow-up (most recent at top)
  slice_head() %>%                                  # keep only the first row per unique contact id  
  ungroup() %>% 
  
  # Other cleaning
  mutate(followup_number = replace(followup_number, followup_number > 14, NA)) %>% # clean erroneous data
  drop_na(contact_id)                               # remove rows with missing contact_id
```

For each follow-up encounter, we have a follow-up status (such as whether the encounter occurred and if so, did the contact have symptoms or not). To see all the values we can run a quick `tabyl()` (from **janitor**) or `table()` (from **base** R) (see [Descriptive tables]) by `followup_status` to see the frequency of each of the outcomes.  

In this dataset, "seen_not_ok" means "seen with symptoms", and "seen_ok" means "seen without symptoms".  

```{r}
followups_clean %>% 
  tabyl(followup_status)
```


### Plot over time {.unnumbered}  

As the dates data are continuous, we will use a histogram to plot them with `date_of_followup` assigned to the x-axis. We can achieve a "stacked" histogram by specifying a `fill = ` argument within `aes()`, which we assign to the column `followup_status`. Consequently, you can set the legend title using the `fill = ` argument of `labs()`.  

We can see that the contacts were identified in waves (presumably corresponding with epidemic waves of cases), and that follow-up completion did not seemingly improve over the course of the epidemic.  

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_histogram(mapping = aes(x = date_of_followup, fill = followup_status)) +
  scale_fill_discrete(drop = FALSE)+   # show all factor levels (followup_status) in the legend, even those not used
  theme_classic() +
  labs(
    x = "",
    y = "Number of contacts",
    title = "Daily Contact Followup Status",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups$date_of_followup, na.rm=T)}"))   # dynamic subtitle
  
```


<span style="color: orange;">**_CAUTION:_** If you are preparing many plots (e.g. for multiple jurisdictions) you will want the legends to appear identically even with varying levels of data completion or data composition. There may be plots for which not all follow-up statuses are present in the data, but you still want those categories to appear the legends. In ggplots (like above), you can specify the `drop = FALSE` argument of the `scale_fill_discrete()`. In tables, use `tabyl()` which shows counts for all factor levels, or if using `count()` from **dplyr** add the argument `.drop = FALSE` to include counts for all factor levels.</span>  


### Daily individual tracking  {.unnumbered}  

If your outbreak is small enough, you may want to look at each contact individually and see their status over the course of their follow-up. Fortunately, this `followups` dataset already contains a column with the day "number" of follow-up (1-14). If this does not exist in your data, you could create it by calculating the difference between the encounter date and the date follow-up was intended to begin for the contact.  

A convenient visualisation mechanism (if the number of cases is not too large) can be a heat plot, made with `geom_tile()`. See more details in the [heat plot] page.  

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_tile(mapping = aes(x = followup_number, y = contact_id, fill = followup_status),
            color = "grey")+       # grey gridlines
  scale_fill_manual( values = c("yellow", "grey", "orange", "darkred", "darkgreen"))+
  theme_minimal()+
  scale_x_continuous(breaks = seq(from = 1, to = 14, by = 1))
```


### Analyse by group {.unnumbered}  

Perhaps these follow-up data are being viewed on a daily or weekly basis for operational decision-making. You may want more meaningful disaggregations by geographic area or by contact-tracing team. We can do this by adjusting the columns provided to `group_by()`.  

```{r, warning=F, message=F}

plot_by_region <- followups_clean %>%                                        # begin with follow-up dataset
  count(admin_1_name, admin_2_name, followup_status) %>%   # get counts by unique region-status (creates column 'n' with counts)
  
  # begin ggplot()
  ggplot(                                         # begin ggplot
    mapping = aes(x = reorder(admin_2_name, n),     # reorder admin factor levels by the numeric values in column 'n'
                  y = n,                            # heights of bar from column 'n'
                  fill = followup_status,           # color stacked bars by their status
                  label = n))+                      # to pass to geom_label()              
  geom_col()+                                     # stacked bars, mapping inherited from above 
  geom_text(                                      # add text, mapping inherited from above
    size = 3,                                         
    position = position_stack(vjust = 0.5), 
    color = "white",           
    check_overlap = TRUE,
    fontface = "bold")+
  coord_flip()+
  labs(
    x = "",
    y = "Number of contacts",
    title = "Contact Followup Status, by Region",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups_clean$date_of_followup, na.rm=T)}")) +
  theme_classic()+                                                                      # Simplify background
  facet_wrap(~admin_1_name, strip.position = "right", scales = "free_y", ncol = 1)      # introduce facets 

plot_by_region
```

<!-- If this was disaggregated by contact tracer, perhaps we would want to add a threshold line to display total # contacts that normally one person or area/team can handle, and how the current workload compares. We just do this by using `geom_hline()` function. -->

<!-- ```{r, warning=F, message=F} -->

<!-- plot_by_region +  -->
<!--      geom_hline(aes(yintercept=25), color="#C70039", linetype = "dashed") # fictitious threshold at 25 contacts -->

<!-- ``` -->



## KPI Tables  

There are a number of different Key Performance Indicators (KPIs) that can be calculated and tracked at varying levels of disaggregations and across different time periods to monitor contact tracing performance. Once you have the calculations down and the basic table format; it is fairly easy to swap in and out different KPIs. 

There are numerous sources of contact tracing KPIs, such as this one from [ResolveToSaveLives.org](https://contacttracingplaybook.resolvetosavelives.org/checklists/metrics). The majority of the work will be walking through your data structure and thinking through all of the inclusion/exclusion criteria. We show a few examples below; using Go.Data metadata structure:

Category          | Indicator                | Go.Data Numerator         | Go.Data Denominator
------------------|--------------------------|---------------------------|--------------------
Process Indicator - Speed of Contact Tracing|% cases interviewed and isolated within 24h of case report |COUNT OF `case_id` WHERE (`date_of_reporting` - `date_of_data_entry`) < 1 day AND (`isolation_startdate` - `date_of_data_entry`) < 1 day|COUNT OF  `case_id`
Process Indicator - Speed of Contact Tracing|% contacts notified and quarantined within 24h of elicitation|COUNT OF `contact_id` WHERE `followup_status` == "SEEN_NOT_OK" OR "SEEN_OK" AND `date_of_followup` -  `date_of_reporting` < 1 day|COUNT OF `contact_id`
Process Indicator - Completeness of Testing|% new symptomatic cases tested and interviewed within 3 days of onset of symptoms|COUNT OF `case_id` WHERE (`date_of_reporting` - `date_of_onset`) < =3 days|COUNT OF  `case_id`
Outcome Indicator - Overall|% new cases among existing contact list|COUNT OF `case_id` WHERE `was_contact` == "TRUE"|COUNT OF  `case_id`

Below we will walk through a sample exercise of creating a nice table visual to show contact follow-up across admin areas. At the end, we will make it fit for presentation with the **formattable** package (but you could use other packages like **flextable** - see [Tables for presentation]).  

How you create a table like this will depend on the structure of your contact tracing data. Use the [Descriptive tables] page to learn how to summarise data using **dplyr** functions. 

We will create a table that will be dynamic and change as the data change. To make the results interesting, we will set a `report_date` to allow us to simulate running the table on a certain day (we pick 10th June 2020). The data are filtered to that date.  

```{r, warning=F, message=F}
# Set "Report date" to simulate running the report with data "as of" this date
report_date <- as.Date("2020-06-10")

# Create follow-up data to reflect the report date.
table_data <- followups_clean %>% 
  filter(date_of_followup <= report_date)
```


Now, based on our data structure, we will do the following:  

1) Begin with the `followups` data and summarise it to contain, for each unique contact:  
  * The date of latest record (no matter the status of the encounter)  
  * The date of latest encounter where the contact was "seen"  
  * The encounter status at that final "seen" encounter (e.g. with symptoms, without symptoms)  
2) Join these data to the contacts data, which contains other information such as the overall contact status, date of last exposure to a case, etc. Also we will calculate metrics of interest for each contact such as days since last exposure  
3) We group the enhanced contact data by geographic region (`admin_2_name`) and calculate summary statistics per region  
4) Finally, we format the table nicely for presentation  


First we summarise the follow-up data to get the information of interest:  

```{r, warning=F, message=F}
followup_info <- table_data %>% 
  group_by(contact_id) %>% 
  summarise(
    date_last_record   = max(date_of_followup, na.rm=T),
    date_last_seen     = max(date_of_followup[followup_status %in% c("seen_ok", "seen_not_ok")], na.rm=T),
    status_last_record = followup_status[which(date_of_followup == date_last_record)]) %>% 
  ungroup()
```

Here is how these data look:  

```{r, echo=F}
DT::datatable(followup_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```


Now we will add this information to the `contacts` dataset, and calculate some additional columns.  

```{r}
contacts_info <- followup_info %>% 
  right_join(contacts, by = "contact_id") %>% 
  mutate(
    database_date       = max(date_last_record, na.rm=T),
    days_since_seen     = database_date - date_last_seen,
    days_since_exposure = database_date - date_of_last_exposure
    )
```

Here is how these data look. Note `contacts` column to the right, and new calculated column at the far right.  

```{r, echo=F}
DT::datatable(contacts_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```


Next we summarise the contacts data by region, to achieve a concise data frame of summary statistic columns.    

```{r}
contacts_table <- contacts_info %>% 
  
  group_by(`Admin 2` = admin_2_name) %>%
  
  summarise(
    `Registered contacts` = n(),
    `Active contacts`     = sum(contact_status == "UNDER_FOLLOW_UP", na.rm=T),
    `In first week`       = sum(days_since_exposure < 8, na.rm=T),
    `In second week`      = sum(days_since_exposure >= 8 & days_since_exposure < 15, na.rm=T),
    `Became case`         = sum(contact_status == "BECAME_CASE", na.rm=T),
    `Lost to follow up`   = sum(days_since_seen >= 3, na.rm=T),
    `Never seen`          = sum(is.na(date_last_seen)),
    `Followed up - signs` = sum(status_last_record == "Seen_not_ok" & date_last_record == database_date, na.rm=T),
    `Followed up - no signs` = sum(status_last_record == "Seen_ok" & date_last_record == database_date, na.rm=T),
    `Not Followed up`     = sum(
      (status_last_record == "NOT_ATTEMPTED" | status_last_record == "NOT_PERFORMED") &
        date_last_record == database_date, na.rm=T)) %>% 
    
  arrange(desc(`Registered contacts`))

```


```{r, echo=F}
DT::datatable(contacts_table, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

And now we apply styling from the **formattable** and **knitr** packages, including a footnote that shows the "as of" date.  


```{r}
contacts_table %>%
  mutate(
    `Admin 2` = formatter("span", style = ~ formattable::style(
      color = ifelse(`Admin 2` == NA, "red", "grey"),
      font.weight = "bold",font.style = "italic"))(`Admin 2`),
    `Followed up - signs`= color_tile("white", "orange")(`Followed up - signs`),
    `Followed up - no signs`= color_tile("white", "#A0E2BD")(`Followed up - no signs`),
    `Became case`= color_tile("white", "grey")(`Became case`),
    `Lost to follow up`= color_tile("white", "grey")(`Lost to follow up`), 
    `Never seen`= color_tile("white", "red")(`Never seen`),
    `Active contacts` = color_tile("white", "#81A4CE")(`Active contacts`)
  ) %>%
  kable("html", escape = F, align =c("l","c","c","c","c","c","c","c","c","c","c")) %>%
  kable_styling("hover", full_width = FALSE) %>%
  add_header_above(c(" " = 3, 
                     "Of contacts currently under follow up" = 5,
                     "Status of last visit" = 3)) %>% 
  kableExtra::footnote(general = str_glue("Data are current to {format(report_date, '%b %d %Y')}"))

```


## Transmission Matrices  

As discussed in the [Heat plots] page, you can create a matrix of "who infected whom" using `geom_tile()`.

When new contacts are created, Go.Data stores this relationship information in the `relationships` API endpoint; and we can see the first 50 rows of this dataset below. This means that we can create a heat plot with relatively few steps given each contact is already joined to it's source case.

```{r, warning=F, message=F, echo=F}
# display the first 50 rows of relationships data as a table
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

As done above for the age pyramid comparing cases and contacts, we can select the few variables we need and create columns with categorical age groupings for both sources (cases) and targets (contacts).

```{r}
heatmap_ages <- relationships %>% 
  select(source_age, target_age) %>% 
  mutate(                              # transmute is like mutate() but removes all other columns
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5))) 
```

As described previously, we create cross-tabulation; 

```{r, warning=F, message=FALSE}

cross_tab <- table(
  source_cases = heatmap_ages$source_age_class,
  target_cases = heatmap_ages$target_age_class)

cross_tab
```

convert into long format with proportions;

```{r, warning=FALSE, message=FALSE}

long_prop <- data.frame(prop.table(cross_tab))

```

and create a heat-map for age.


```{r, warning=F, message=F}

ggplot(data = long_prop)+       # use long data, with proportions as Freq
  geom_tile(                    # visualize it in tiles
    aes(
      x = target_cases,         # x-axis is case age
      y = source_cases,     # y-axis is infector age
      fill = Freq))+            # color of the tile is the Freq column in the data
  scale_fill_gradient(          # adjust the fill color of the tiles
    low = "blue",
    high = "orange")+
  theme(axis.text.x = element_text(angle = 90))+
  labs(                         # labels
    x = "Target case age",
    y = "Source case age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )

```


## Resources  

https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting

https://worldhealthorganization.github.io/godata/

https://community-godata.who.int/
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/contact_tracing.Rmd-->


# Survey analysis { }  

<!-- ======================================================= -->
## Overview {  }

This page demonstrates the use of several packages for survey analysis. 


Most survey R packages rely on the [**survey** package](https://cran.r-project.org/web/packages/survey/index.html) 
for doing weighted analysis. 
We will use **survey** as well as [**srvyr**](https://cran.r-project.org/web/packages/srvyr/index.html) 
(a wrapper for **survey** allowing for tidyverse-style coding) and
[**gtsummary**](https://cran.r-project.org/web/packages/gtsummary/index.html) 
(a wrapper for **survey** allowing for publication ready tables). 
While the original **survey** package does not allow for tidyverse-style coding, 
it does have the added benefit of allowing for survey-weighted generalised linear 
models (which will be added to this page at a later date). 
We will also demonstrate using a function from the [**sitrep**](https://github.com/R4EPI/sitrep)
package to create sampling weights (*n.b* this package is currently not yet on CRAN, 
but can be installed from github).

Most of this page is based off work done for the ["R4Epis" project](https://r4epis.netlify.app/); 
for detailed code and R-markdown templates see the ["R4Epis" github page](https://github.com/R4EPI/sitrep). 
Some of the **survey** package based code is based off early versions of 
[EPIET case studies](https://github.com/EPIET/RapidAssessmentSurveys).

At current this page does not address sample size calculations or sampling. 
For a simple to use sample size calculator see [OpenEpi](https://www.openepi.com/Menu/OE_Menu.htm). 
The [GIS basics](https://epirhandbook.com/gis-basics.html) page of the handbook 
will eventually have a section on spatial random sampling, and this page will 
eventually have a section on sampling frames as well as sample size calculations. 



1.  Survey data 
2.  Observation time 
3.  Weighting 
4.  Survey design objects
5.  Descriptive analysis 
6.  Weighted proportions
7.  Weighted rates 


<!-- ======================================================= -->
## Preparation {  }

### Packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  
Here we also demonstrate using the `p_load_gh()` function from **pacman** to install a load a package from github which has not yet been published on CRAN. 

```{r}

## load packages from CRAN
pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets
               survey,       # for survey functions
               srvyr,        # dplyr wrapper for survey package
               gtsummary,    # wrapper for survey package to produce tables
               apyramid,     # a package dedicated to creating age pyramids
               patchwork,    # for combining ggplots
               ggforce       # for alluvial/sankey plots
               ) 

## load packages from github
pacman::p_load_gh(
     "R4EPI/sitrep"          # for observation time / weighting functions
)

``` 

### Load data {.unnumbered}

The example dataset used in this section:

-   fictional mortality survey data.
-   fictional population counts for the survey area. 
-   data dictionary for the fictional mortality survey data. 

This is based off the MSF OCA ethical review board pre-approved survey. The 
fictional dataset was produced as part of the ["R4Epis" project](https://r4epis.netlify.app/). 
This is all based off data collected using [KoboToolbox](https://www.kobotoolbox.org/), 
which is a data collection software based off [Open Data Kit](https://opendatakit.org/).

Kobo allows you to export both the collected data, as well as the data dictionary 
for that dataset. We strongly recommend doing this as it simplifies data cleaning 
and is useful for looking up variables/questions. 


<span style="color: darkgreen;">**_TIP:_** The Kobo data dictionary has variable
names in the "name" column of the survey sheet. 
Possible values for each variable are specified in choices sheet. 
In the choices tab, "name" has the shortened value and the "label::english" and 
"label::french" columns have the appropriate long versions. 
Using the **epidict** package `msf_dict_survey()` function to import a Kobo 
dictionary excel file will re-format this for you so it can be used easily to recode.  </span>

<span style="color: orange;">**_CAUTION:_** The example dataset is not the same 
as an export (as in Kobo you export different questionnaire levels individually) 
- see the survey data section below to merge the different levels.</span>


The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export](https://epirhandbook.com/import-and-export.html) for various ways to import data.

```{r echo = FALSE}
# import the survey into R
survey_data <- rio::import(here::here("data", "surveys", "survey_data.xlsx"))

# import the dictionary into R
survey_dict <- rio::import(here::here("data", "surveys", "survey_dict.xlsx")) 

# import the population in to R 
population <- rio::import(here::here("data", "surveys", "population.xlsx"))
```

```{r eval = FALSE}
# import the survey data
survey_data <- rio::import("survey_data.xlsx")

# import the dictionary into R
survey_dict <- rio::import("survey_dict.xlsx") 
```

The first 10 rows of the survey are displayed below.

```{r, message = FALSE, echo = FALSE}
# display the survey data as a table
DT::datatable(head(survey_data, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

We also want to import the data on sampling population so that we can produce 
appropriate weights. This data can be in different formats, however we would 
suggest to have it as seen below (this can just be typed in to an excel). 


```{r read_data_pop_show, eval = FALSE}
# import the population data
population <- rio::import("population.xlsx")
```

The first 10 rows of the survey are displayed below.

```{r message=FALSE, echo=F}
# display the survey data as a table
DT::datatable(head(population, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

For cluster surveys you may want to add survey weights at the cluster level. 
You could read this data in as above. 
Alternatively if there are only a few counts, these could be entered as below
in to a tibble. 
In any case you will need to have one column with a cluster identifier which 
matches your survey data, and another column with the number of households in 
each cluster. 

```{r cluster_counts}

## define the number of households in each cluster
cluster_counts <- tibble(cluster = c("village_1", "village_2", "village_3", "village_4", 
                                     "village_5", "village_6", "village_7", "village_8",
                                     "village_9", "village_10"), 
                         households = c(700, 400, 600, 500, 300, 
                                        800, 700, 400, 500, 500))

```

### Clean data {.unnumbered}

The below makes sure that the date column is in the appropriate format. 
There are several other ways of doing this (see the [Working with dates](https://epirhandbook.com/working-with-dates.html)
page for details), however using the dictionary to define dates is quick and easy. 

We also create an age group variable using the `age_categories()` function from 
**epikit** - see [cleaning data](https://epirhandbook.com/cleaning-data-and-core-functions.html#num_cats)
handbook section for details. 
In addition, we create a character variable defining which district the various clusters
are in. 

Finally, we recode all of the yes/no variables to TRUE/FALSE variables - otherwise
these cant be used by the **survey** proportion functions. 

```{r cleaning}

## select the date variable names from the dictionary 
DATEVARS <- survey_dict %>% 
  filter(type == "date") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filter to match the column names of your data
  pull(name) # select date vars
  
## change to dates 
survey_data <- survey_data %>%
  mutate(across(all_of(DATEVARS), as.Date))


## add those with only age in months to the year variable (divide by twelve)
survey_data <- survey_data %>% 
  mutate(age_years = if_else(is.na(age_years), 
                             age_months / 12, 
                             age_years))

## define age group variable
survey_data <- survey_data %>% 
     mutate(age_group = age_categories(age_years, 
                                    breakers = c(0, 3, 15, 30, 45)
                                    ))


## create a character variable based off groups of a different variable 
survey_data <- survey_data %>% 
  mutate(health_district = case_when(
    cluster_number %in% c(1:5) ~ "district_a", 
    TRUE ~ "district_b"
  ))


## select the yes/no variable names from the dictionary 
YNVARS <- survey_dict %>% 
  filter(type == "yn") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filter to match the column names of your data
  pull(name) # select yn vars
  
## change to dates 
survey_data <- survey_data %>%
  mutate(across(all_of(YNVARS), 
                str_detect, 
                pattern = "yes"))

```



<!-- ======================================================= -->
## Survey data {  }

There numerous different sampling designs that can be used for surveys. Here 
we will demonstrate code for: 
- Stratified 
- Cluster 
- Stratified and cluster 

As described above (depending on how you design your questionnaire) the data for 
each level would be exported as a separate dataset from Kobo. In our example there
is one level for households and one level for individuals within those households. 

These two levels are linked by a unique identifier. 
For a Kobo dataset this variable is "_index" at the household level, which 
matches the "_parent_index" at the individual level.
This will create new rows for household with each matching individual, 
see the handbook section on [joining](https://epirhandbook.com/joining-data.html)
for details. 

```{r merge_data_levels, eval = FALSE}

## join the individual and household data to form a complete data set
survey_data <- left_join(survey_data_hh, 
                         survey_data_indiv,
                         by = c("_index" = "_parent_index"))


## create a unique identifier by combining indeces of the two levels 
survey_data <- survey_data %>% 
     mutate(uid = str_glue("{index}_{index_y}"))

```

<!-- ======================================================= -->
## Observation time {  }

For mortality surveys we want to now how long each individual was present for in 
the location to be able to calculate an appropriate mortality rate for our period 
of interest. This is not relevant to all surveys, but particularly for mortality
surveys this is important as they are conducted frequently among mobile or displaced
populations. 

To do this we first define our time period of interest, also known as a recall 
period (i.e. the time that participants are asked to report on when answering 
questions). 
We can then use this period to set inappropriate dates to missing, i.e. if deaths
are reported from outside the period of interest. 

```{r recall_period}

## set the start/end of recall period
## can be changed to date variables from dataset 
## (e.g. arrival date & date questionnaire)
survey_data <- survey_data %>% 
  mutate(recall_start = as.Date("2018-01-01"), 
         recall_end   = as.Date("2018-05-01")
  )


# set inappropriate dates to NA based on rules 
## e.g. arrivals before start, departures departures after end
survey_data <- survey_data %>%
      mutate(
           arrived_date = if_else(arrived_date < recall_start, 
                                 as.Date(NA),
                                  arrived_date),
           birthday_date = if_else(birthday_date < recall_start,
                                  as.Date(NA),
                                  birthday_date),
           left_date = if_else(left_date > recall_end,
                              as.Date(NA),
                               left_date),
           death_date = if_else(death_date > recall_end,
                               as.Date(NA),
                               death_date)
           )

```


We can then use our date variables to define start and end dates for each individual. 
We can use the `find_start_date()` function from **sitrep** to fine the causes for 
the dates and then use that to calculate the difference between days (person-time). 

start date: 
Earliest appropriate arrival event within your recall period
Either the beginning of your recall period (which you define in advance), or a 
date after the start of recall if applicable (e.g. arrivals or births)

end date: 
Earliest appropriate departure event within your recall period
Either the end of your recall period, or a date before the end of recall 
if applicable (e.g. departures, deaths)

```{r observation_time}

## create new variables for start and end dates/causes
survey_data <- survey_data %>% 
     ## choose earliest date entered in survey
     ## from births, household arrivals, and camp arrivals 
     find_start_date("birthday_date",
                  "arrived_date",
                  period_start = "recall_start",
                  period_end   = "recall_end",
                  datecol      = "startdate",
                  datereason   = "startcause" 
                 ) %>%
     ## choose earliest date entered in survey
     ## from camp departures, death and end of the study
     find_end_date("left_date",
                "death_date",
                period_start = "recall_start",
                period_end   = "recall_end",
                datecol      = "enddate",
                datereason   = "endcause" 
               )


## label those that were present at the start/end (except births/deaths)
survey_data <- survey_data %>% 
     mutate(
       ## fill in start date to be the beginning of recall period (for those empty) 
       startdate = if_else(is.na(startdate), recall_start, startdate), 
       ## set the start cause to present at start if equal to recall period 
       ## unless it is equal to the birth date 
       startcause = if_else(startdate == recall_start & startcause != "birthday_date",
                              "Present at start", startcause), 
       ## fill in end date to be end of recall period (for those empty) 
       enddate = if_else(is.na(enddate), recall_end, enddate), 
       ## set the end cause to present at end if equall to recall end 
       ## unless it is equal to the death date
       endcause = if_else(enddate == recall_end & endcause != "death_date", 
                            "Present at end", endcause))


## Define observation time in days
survey_data <- survey_data %>% 
  mutate(obstime = as.numeric(enddate - startdate))

```


<!-- ======================================================= -->
## Weighting {  }

It is important that you drop erroneous observations before adding survey weights. 
For example if you have observations with negative observation time, you will need
to check those (you can do this with the `assert_positive_timespan()` function 
from **sitrep**. 
Another thing is if you want to drop empty rows (e.g. with `drop_na(uid)`)
or remove duplicates (see handbook section on [De-duplication] 
for details). 
Those without consent need to be dropped too. 

In this example we filter for the cases we want to drop and store them in a separate
data frame - this way we can describe those that were excluded from the survey. 
We then use the `anti_join()` function from **dplyr** to remove these dropped cases
from our survey data. 

<span style="color: red;">**_DANGER:_** You cant have missing values in your weight variable, or any of the variables relevant to your survey design (e.g. age, sex, strata or cluster variables).</span>  

```{r remove_unused_data}

## store the cases that you drop so you can describe them (e.g. non-consenting 
## or wrong village/cluster)
dropped <- survey_data %>% 
  filter(!consent | is.na(startdate) | is.na(enddate) | village_name == "other")

## use the dropped cases to remove the unused rows from the survey data set  
survey_data <- anti_join(survey_data, dropped, by = names(dropped))

```

As mentioned above we demonstrate how to add weights for three different study 
designs (stratified, cluster and stratified cluster). These require information 
on the source population and/or the clusters surveyed. 
We will use the stratified cluster code for this example, but use whichever is
most appropriate for your study design. 

```{r survey_weights}

# stratified ------------------------------------------------------------------
# create a variable called "surv_weight_strata"
# contains weights for each individual - by age group, sex and health district
survey_data <- add_weights_strata(x = survey_data,
                                         p = population,
                                         surv_weight = "surv_weight_strata",
                                         surv_weight_ID = "surv_weight_ID_strata",
                                         age_group, sex, health_district)

## cluster ---------------------------------------------------------------------

# get the number of people of individuals interviewed per household
# adds a variable with counts of the household (parent) index variable
survey_data <- survey_data %>%
  add_count(index, name = "interviewed")


## create cluster weights
survey_data <- add_weights_cluster(x = survey_data,
                                          cl = cluster_counts,
                                          eligible = member_number,
                                          interviewed = interviewed,
                                          cluster_x = village_name,
                                          cluster_cl = cluster,
                                          household_x = index,
                                          household_cl = households,
                                          surv_weight = "surv_weight_cluster",
                                          surv_weight_ID = "surv_weight_ID_cluster",
                                          ignore_cluster = FALSE,
                                          ignore_household = FALSE)


# stratified and cluster ------------------------------------------------------
# create a survey weight for cluster and strata
survey_data <- survey_data %>%
  mutate(surv_weight_cluster_strata = surv_weight_strata * surv_weight_cluster)

```


<!-- ======================================================= -->
## Survey design objects {  }

Create survey object according to your study design. 
Used the same way as data frames to calculate weight proportions etc. 
Make sure that all necessary variables are created before this. 

There are four options, comment out those you do not use: 
- Simple random 
- Stratified 
- Cluster 
- Stratified cluster

For this template - we will pretend that we cluster surveys in two separate 
strata (health districts A and B). 
So to get overall estimates we need have combined cluster and strata weights. 

As mentioned previously, there are two packages available for doing this. The 
classic one is **survey** and then there is a wrapper package called **srvyr** 
that makes tidyverse-friendly objects and functions. We will demonstrate both, 
but note that most of the code in this chapter will use **srvyr** based objects. 
The one exception is that the **gtsummary** package only accepts **survey** objects. 

### **Survey** package  

The **survey** package effectively uses **base** *R* coding, and so it is not 
possible to use pipes (`%>%`) or other **dplyr** syntax. 
With the **survey** package we use the `svydesign()` function to define a survey
object with appropriate clusters, weights and strata. 

<span style="color: black;">**_NOTE:_** we need to use the tilde (`~`) in front of variables, this is because the package uses the **base** *R* syntax of assigning variables based on formulae. </span>

```{r survey_design}

# simple random ---------------------------------------------------------------
base_survey_design_simple <- svydesign(ids = ~1, # 1 for no cluster ids
                   weights = NULL,               # No weight added
                   strata = NULL,                # sampling was simple (no strata)
                   data = survey_data            # have to specify the dataset
                  )

## stratified ------------------------------------------------------------------
base_survey_design_strata <- svydesign(ids = ~1,  # 1 for no cluster ids
                   weights = ~surv_weight_strata, # weight variable created above
                   strata = ~health_district,     # sampling was stratified by district
                   data = survey_data             # have to specify the dataset
                  )

# cluster ---------------------------------------------------------------------
base_survey_design_cluster <- svydesign(ids = ~village_name, # cluster ids
                   weights = ~surv_weight_cluster, # weight variable created above
                   strata = NULL,                 # sampling was simple (no strata)
                   data = survey_data              # have to specify the dataset
                  )

# stratified cluster ----------------------------------------------------------
base_survey_design <- svydesign(ids = ~village_name,      # cluster ids
                   weights = ~surv_weight_cluster_strata, # weight variable created above
                   strata = ~health_district,             # sampling was stratified by district
                   data = survey_data                     # have to specify the dataset
                  )
```



### **Srvyr** package  

With the **srvyr** package we can use the `as_survey_design()` function, which 
has all the same arguments as above but allows pipes (`%>%`), and so we do not 
need to use the tilde (`~`). 

```{r survey_design_srvyr}
## simple random ---------------------------------------------------------------
survey_design_simple <- survey_data %>% 
  as_survey_design(ids = 1, # 1 for no cluster ids 
                   weights = NULL, # No weight added
                   strata = NULL # sampling was simple (no strata)
                  )
## stratified ------------------------------------------------------------------
survey_design_strata <- survey_data %>%
  as_survey_design(ids = 1, # 1 for no cluster ids
                   weights = surv_weight_strata, # weight variable created above
                   strata = health_district # sampling was stratified by district
                  )
## cluster ---------------------------------------------------------------------
survey_design_cluster <- survey_data %>%
  as_survey_design(ids = village_name, # cluster ids
                   weights = surv_weight_cluster, # weight variable created above
                   strata = NULL # sampling was simple (no strata)
                  )

## stratified cluster ----------------------------------------------------------
survey_design <- survey_data %>%
  as_survey_design(ids = village_name, # cluster ids
                   weights = surv_weight_cluster_strata, # weight variable created above
                   strata = health_district # sampling was stratified by district
                  )
```

<!-- ======================================================= -->
## Descriptive analysis {  }

Basic descriptive analysis and visualisation is covered extensively in other 
chapters of the handbook, so we will not dwell on it here. 
For details see the chapters on [descriptive tables](https://epirhandbook.com/descriptive-tables.html), 
[statistical tests](https://epirhandbook.com/simple-statistical-tests.html), 
[tables for presentation](https://epirhandbook.com/tables-for-presentation.html), 
[ggplot basics](https://epirhandbook.com/ggplot-basics.html) and 
[R markdown reports](https://epirhandbook.com/r-markdown-reports.html). 

In this section we will focus on how to investigate bias in your sample and visualise this. 
We will also look at visualising population flow in a survey setting using 
alluvial/sankey diagrams. 

In general, you should consider including the following descriptive analyses:  

- Final number of clusters, households and individuals included  
- Number of excluded individuals and the reasons for exclusion 
- Median (range) number of households per cluster and individuals per household 


### Sampling bias 

Compare the proportions in each age group between your sample and 
the source population. 
This is important to be able to highlight potential sampling bias. 
You could similarly repeat this looking at distributions by sex. 

Note that these p-values are just indicative, and a descriptive discussion (or
visualisation with age-pyramids below) of the distributions in your study sample 
compared to the source population is more important than the binomial test itself.
This is because increasing sample size will more often than not lead to 
differences that may be irrelevant after weighting your data.

```{r descriptive_sampling_bias, warning = FALSE}

## counts and props of the study population
ag <- survey_data %>% 
  group_by(age_group) %>% 
  drop_na(age_group) %>% 
  tally() %>% 
  mutate(proportion = n / sum(n), 
         n_total = sum(n))

## counts and props of the source population
propcount <- population %>% 
  group_by(age_group) %>%
    tally(population) %>%
    mutate(proportion = n / sum(n))

## bind together the columns of two tables, group by age, and perform a 
## binomial test to see if n/total is significantly different from population
## proportion.
  ## suffix here adds to text to the end of columns in each of the two datasets
left_join(ag, propcount, by = "age_group", suffix = c("", "_pop")) %>%
  group_by(age_group) %>%
  ## broom::tidy(binom.test()) makes a data frame out of the binomial test and
  ## will add the variables p.value, parameter, conf.low, conf.high, method, and
  ## alternative. We will only use p.value here. You can include other
  ## columns if you want to report confidence intervals
  mutate(binom = list(broom::tidy(binom.test(n, n_total, proportion_pop)))) %>%
  unnest(cols = c(binom)) %>% # important for expanding the binom.test data frame
  mutate(proportion_pop = proportion_pop * 100) %>%
  ## Adjusting the p-values to correct for false positives 
  ## (because testing multiple age groups). This will only make 
  ## a difference if you have many age categories
  mutate(p.value = p.adjust(p.value, method = "holm")) %>%
                      
  ## Only show p-values over 0.001 (those under report as <0.001)
  mutate(p.value = ifelse(p.value < 0.001, 
                          "<0.001", 
                          as.character(round(p.value, 3)))) %>% 
  
  ## rename the columns appropriately
  select(
    "Age group" = age_group,
    "Study population (n)" = n,
    "Study population (%)" = proportion,
    "Source population (n)" = n_pop,
    "Source population (%)" = proportion_pop,
    "P-value" = p.value
  )
```



### Demographic pyramids 

Demographic (or age-sex) pyramids are an easy way of visualising the distribution
in your survey population. It is also worth considering creating 
[descriptive tables](https://epirhandbook.com/descriptive-tables.html) of age 
and sex by survey strata. 
We will demonstrate using the **apyramid** package as it allows for weighted 
proportions using our survey design object created above. Other options for creating
[demographic pyramids](https://epirhandbook.com/demographic-pyramids-and-likert-scales.html)
are covered extensively in that chapter of the handbook. We will also use a 
wrapper function from **apyramid** called `age_pyramid()` which saves a few lines
of coding for producing a plot with proportions. 

As with the formal binomial test of difference, seen above in the sampling bias 
section, we are interested here in visualising whether our sampled population 
is substantially different from the source population and whether weighting corrects
this difference. To do this we will use the **patchwork** package to show our 
**ggplot** visualisations side-by-side; for details see the section on 
combining plots in [ggplot tips](https://epirhandbook.com/ggplot-tips.html?q=patch#combine-plots)
chapter of the handbook. 
We will visualise our source population, our un-weighted survey population and 
our weighted survey population.
You may also consider visualising by each strata of your survey - in our example 
here that would be by using the argument `stack_by  = "health_district"` 
(see `?plot_age_pyramid` for details). 

<span style="color: black;">**_NOTE:_** The x and y axes are flipped in pyramids </span>

```{r weighted_age_pyramid, warning = FALSE, message = FALSE, fig.show = "hold", fig.width = 15}

## define x-axis limits and labels ---------------------------------------------
## (update these numbers to be the values for your graph)
max_prop <- 35      # choose the highest proportion you want to show 
step <- 5           # choose the space you want beween labels 

## this part defines vector using the above numbers with axis breaks
breaks <- c(
    seq(max_prop/100 * -1, 0 - step/100, step/100), 
    0, 
    seq(0 + step / 100, max_prop/100, step/100)
    )

## this part defines vector using the above numbers with axis limits
limits <- c(max_prop/100 * -1, max_prop/100)

## this part defines vector using the above numbers with axis labels
labels <-  c(
      seq(max_prop, step, -step), 
      0, 
      seq(step, max_prop, step)
    )


## create plots individually  --------------------------------------------------

## plot the source population 
## nb: this needs to be collapsed for the overall population (i.e. removing health districts)
source_population <- population %>%
  ## ensure that age and sex are factors
  mutate(age_group = factor(age_group, 
                            levels = c("0-2", 
                                       "3-14", 
                                       "15-29",
                                       "30-44", 
                                       "45+")), 
         sex = factor(sex)) %>% 
  group_by(age_group, sex) %>% 
  ## add the counts for each health district together 
  summarise(population = sum(population)) %>% 
  ## remove the grouping so can calculate overall proportion
  ungroup() %>% 
  mutate(proportion = population / sum(population)) %>% 
  ## plot pyramid 
  age_pyramid(
            age_group = age_group, 
            split_by = sex, 
            count = proportion, 
            proportional = TRUE) +
  ## only show the y axis label (otherwise repeated in all three plots)
  labs(title = "Source population", 
       y = "", 
       x = "Age group (years)") + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)
  
  
## plot the unweighted sample population 
sample_population <- age_pyramid(survey_data, 
                 age_group = "age_group", 
                 split_by = "sex",
                 proportion = TRUE) + 
  ## only show the x axis label (otherwise repeated in all three plots)
  labs(title = "Unweighted sample population", 
       y = "Proportion (%)", 
       x = "") + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)


## plot the weighted sample population 
weighted_population <- survey_design %>% 
  ## make sure the variables are factors
  mutate(age_group = factor(age_group), 
         sex = factor(sex)) %>%
  age_pyramid(
    age_group = "age_group",
    split_by = "sex", 
    proportion = TRUE) +
  ## only show the x axis label (otherwise repeated in all three plots)
  labs(title = "Weighted sample population", 
       y = "", 
       x = "")  + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)

## combine all three plots  ----------------------------------------------------
## combine three plots next to eachother using + 
source_population + sample_population + weighted_population + 
  ## only show one legend and define theme 
  ## note the use of & for combining theme with plot_layout()
  plot_layout(guides = "collect") & 
  theme(legend.position = "bottom",                    # move legend to bottom
        legend.title = element_blank(),                # remove title
        text = element_text(size = 18),                # change text size
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1) # turn x-axis text
       )
```


### Alluvial/sankey diagram

Visualising starting points and outcomes for individuals can be very helpful to 
get an overview. There is quite an obvious application for mobile populations, 
however there are numerous other applications such as cohorts or any other situation
where there are transitions in states for individuals. These diagrams have several
different names including alluvial, sankey and parallel sets - the details are 
in the handbook chapter on [diagrams and charts](https://epirhandbook.com/diagrams-and-charts.html#alluvialsankey-diagrams). 


```{r visualise_population_flow}

## summarize data
flow_table <- survey_data %>%
  count(startcause, endcause, sex) %>%  # get counts 
  gather_set_data(x = c("startcause", "endcause"))     # change format for plotting


## plot your dataset 
  ## on the x axis is the start and end causes
  ## gather_set_data generates an ID for each possible combination
  ## splitting by y gives the possible start/end combos
  ## value as n gives it as counts (could also be changed to proportion)
ggplot(flow_table, aes(x, id = id, split = y, value = n)) +
  ## colour lines by sex 
  geom_parallel_sets(aes(fill = sex), alpha = 0.5, axis.width = 0.2) +
  ## fill in the label boxes grey
  geom_parallel_sets_axes(axis.width = 0.15, fill = "grey80", color = "grey80") +
  ## change text colour and angle (needs to be adjusted)
  geom_parallel_sets_labels(color = "black", angle = 0, size = 5) +
  ## remove axis labels
  theme_void()+
  ## move legend to bottom
  theme(legend.position = "bottom")               

```


<!-- ======================================================= -->
## Weighted proportions {  }

This section will detail how to produce tables for weighted counts and proportions,
with associated confidence intervals and design effect. 
There are four different options using functions from the following packages: 
**survey**, **srvyr**, **sitrep** and **gtsummary**. 
For minimal coding to produce a standard epidemiology style table, we would 
recommend the **sitrep** function - which is a wrapper for **srvyr** code; note 
however that this is not yet on CRAN and may change in the future. 
Otherwise, the **survey** code is likely to be the most stable long-term, whereas 
**srvyr** will fit most nicely within tidyverse work-flows. While **gtsummary** 
functions hold a lot of potential, they appear to be experimental and incomplete
at the time of writing. 


### **Survey** package 

We can use the `svyciprop()` function from **survey** to get weighted proportions 
and accompanying 95% confidence intervals. An appropriate design effect can be 
extracted using the `svymean()` rather than `svyprop()` function. 
It is worth noting that `svyprop()` only appears to accept variables between 0 and
1 (or TRUE/FALSE), so categorical variables will not work.

<span style="color: black;">**_NOTE:_** Functions from **survey** also accept **srvyr** design objects, but here we have used the **survey** design object just for consistency </span>


```{r survey_props}

## produce weighted counts 
svytable(~died, base_survey_design)

## produce weighted proportions
svyciprop(~died, base_survey_design, na.rm = T)

## get the design effect 
svymean(~died, base_survey_design, na.rm = T, deff = T) %>% 
  deff()

```

We can combine the functions from **survey** shown above in to a function which 
we define ourselves below, called `svy_prop`; and we can then use that function 
together with `map()` from the purrr package to iterate over several variables 
and create a table. See the handbook [iteration](https://epirhandbook.com/iteration-loops-and-lists.html) 
chapter for details on **purrr**. 

```{r survey_prop_fun}
# Define function to calculate weighted counts, proportions, CI and design effect
# x is the variable in quotation marks 
# design is your survey design object

svy_prop <- function(design, x) {
  
  ## put the variable of interest in a formula 
  form <- as.formula(paste0( "~" , x))
  ## only keep the TRUE column of counts from svytable
  weighted_counts <- svytable(form, design)[[2]]
  ## calculate proportions (multiply by 100 to get percentages)
  weighted_props <- svyciprop(form, design, na.rm = TRUE) * 100
  ## extract the confidence intervals and multiply to get percentages
  weighted_confint <- confint(weighted_props) * 100
  ## use svymean to calculate design effect and only keep the TRUE column
  design_eff <- deff(svymean(form, design, na.rm = TRUE, deff = TRUE))[[TRUE]]
  
  ## combine in to one data frame
  full_table <- cbind(
    "Variable"        = x,
    "Count"           = weighted_counts,
    "Proportion"      = weighted_props,
    weighted_confint, 
    "Design effect"   = design_eff
    )
  
  ## return table as a dataframe
  full_table <- data.frame(full_table, 
             ## remove the variable names from rows (is a separate column now)
             row.names = NULL)
  
  ## change numerics back to numeric
  full_table[ , 2:6] <- as.numeric(full_table[, 2:6])
  
  ## return dataframe
  full_table
}

## iterate over several variables to create a table 
purrr::map(
  ## define variables of interest
  c("left", "died", "arrived"), 
  ## state function using and arguments for that function (design)
  svy_prop, design = base_survey_design) %>% 
  ## collapse list in to a single data frame
  bind_rows() %>% 
  ## round 
  mutate(across(where(is.numeric), round, digits = 1))

```



### **Srvyr** package 

With **srvyr** we can use **dplyr** syntax to create a table. Note that the 
`survey_mean()` function is used and the proportion argument is specified, and 
also that the same function is used to calculate design effect. This is because 
**srvyr** wraps around both of the **survey** package functions `svyciprop()` and 
`svymean()`, which are used in the above section. 

<span style="color: black;">**_NOTE:_** It does not seem to be possible to get proportions from categorical variables using **srvyr** either, if you need this then check out the section below using **sitrep** </span>

```{r srvyr_prop}

## use the srvyr design object
survey_design %>% 
  summarise(
    ## produce the weighted counts 
    counts = survey_total(died), 
    ## produce weighted proportions and confidence intervals 
    ## multiply by 100 to get a percentage 
    props = survey_mean(died, 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produce the design effect 
    deff = survey_mean(died, deff = TRUE)) %>% 
  ## only keep the rows of interest
  ## (drop standard errors and repeat proportion calculation)
  select(counts, props, props_low, props_upp, deff_deff)

```

Here too we could write a function to then iterate over multiple variables using
the **purrr** package. 
See the handbook [iteration](https://epirhandbook.com/iteration-loops-and-lists.html) 
chapter for details on **purrr**. 

```{r srvyr_prop_fun}

# Define function to calculate weighted counts, proportions, CI and design effect
# design is your survey design object
# x is the variable in quotation marks 


srvyr_prop <- function(design, x) {
  
  summarise(
    ## using the survey design object
    design, 
    ## produce the weighted counts 
    counts = survey_total(.data[[x]]), 
    ## produce weighted proportions and confidence intervals 
    ## multiply by 100 to get a percentage 
    props = survey_mean(.data[[x]], 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produce the design effect 
    deff = survey_mean(.data[[x]], deff = TRUE)) %>% 
  ## add in the variable name
  mutate(variable = x) %>% 
  ## only keep the rows of interest
  ## (drop standard errors and repeat proportion calculation)
  select(variable, counts, props, props_low, props_upp, deff_deff)
  
}
  

## iterate over several variables to create a table 
purrr::map(
  ## define variables of interest
  c("left", "died", "arrived"), 
  ## state function using and arguments for that function (design)
  ~srvyr_prop(.x, design = survey_design)) %>% 
  ## collapse list in to a single data frame
  bind_rows()
  

```



### **Sitrep** package 

The `tab_survey()` function from **sitrep** is a wrapper for **srvyr**, allowing 
you to create weighted tables with minimal coding. It also allows you to calculate
weighted proportions for categorical variables. 

```{r sitrep_props}

## using the survey design object
survey_design %>% 
  ## pass the names of variables of interest unquoted
  tab_survey(arrived, left, died, education_level,
             deff = TRUE,   # calculate the design effect
             pretty = TRUE  # merge the proportion and 95%CI
             )

```



### **Gtsummary** package

With **gtsummary** there does not seem to be inbuilt functions yet to add confidence
intervals or design effect. 
Here we show how to define a function for adding confidence intervals and then 
add confidence intervals to a **gtsummary** table created using the `tbl_svysummary()` 
function. 


```{r gtsummary_table}


confidence_intervals <- function(data, variable, by, ...) {
  
  ## extract the confidence intervals and multiply to get percentages
  props <- svyciprop(as.formula(paste0( "~" , variable)),
              data, na.rm = TRUE)
  
  ## extract the confidence intervals 
  as.numeric(confint(props) * 100) %>% ## make numeric and multiply for percentage
    round(., digits = 1) %>%           ## round to one digit
    c(.) %>%                           ## extract the numbers from matrix
    paste0(., collapse = "-")          ## combine to single character
}

## using the survey package design object
tbl_svysummary(base_survey_design, 
               include = c(arrived, left, died),   ## define variables want to include
               statistic = list(everything() ~ c("{n} ({p}%)"))) %>% ## define stats of interest
  add_n() %>%  ## add the weighted total 
  add_stat(fns = everything() ~ confidence_intervals) %>% ## add CIs
  ## modify the column headers
  modify_header(
    list(
      n ~ "**Weighted total (N)**",
      stat_0 ~ "**Weighted Count**",
      add_stat_1 ~ "**95%CI**"
    )
    )

```



<!-- ======================================================= -->
## Weighted ratios {  }

Similarly for weighted ratios (such as for mortality ratios) you can use the 
**survey** or the **srvyr** package. 
You could similarly write functions (similar to those above) to iterate over 
several variables. You could also create a function for **gtsummary** as above
but currently it does not have inbuilt functionality. 


### **Survey** package 

```{r survey_ratio}

ratio <- svyratio(~died, 
         denominator = ~obstime, 
         design = base_survey_design)

ci <- confint(ratio)

cbind(
  ratio$ratio * 10000, 
  ci * 10000
)

```


### **Srvyr** package 

```{r srvyr_ratio}

survey_design %>% 
  ## survey ratio used to account for observation time 
  summarise(
    mortality = survey_ratio(
      as.numeric(died) * 10000, 
      obstime, 
      vartype = "ci")
    )

```




<!-- ======================================================= -->
## Resources {  }

[UCLA stats page](https://stats.idre.ucla.edu/r/seminars/survey-data-analysis-with-r/)  

[Analyze survey data free](http://asdfree.com/)  

[srvyr packge](http://gdfe.co/srvyr/)  

[gtsummary package](http://www.danieldsjoberg.com/gtsummary/reference/index.html) 

[EPIET survey case studies](https://github.com/EPIET/RapidAssessmentSurveys)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survey_analysis.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Survival analysis { }  


```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "survival_analysis.png"))
```

<!-- ======================================================= -->
## Overview {}


*Survival analysis* focuses on describing for a given individual or group of individuals, a defined point of event called **_the failure_** (occurrence of a disease, cure from a disease, death, relapse after response to treatment...) that occurs after a period of time called **_failure time_** (or  **_follow-up time_** in cohort/population-based studies) during which individuals are observed. To determine the failure time, it is then necessary to define a time of origin (that can be the inclusion date, the date of diagnosis...). 

The target of inference for survival analysis is then the time between an origin and an event.
In current medical research, it is widely used in clinical studies to assess the effect of a treatment for instance, or in cancer epidemiology to assess a large variety of cancer survival measures. 


It is usually expressed through the **_survival probability_** which is the probability that the event of interest has not occurred by a duration t.


**_Censoring_**: Censoring occurs when at the end of follow-up, some of the individuals have not had the event of interest, and thus their true time to event is unknown. We will mostly focus on right censoring here but for more details on censoring and survival analysis in general, you can see references. 


```{r echo=F, eval=F, out.width = "80%", out.height="80%", fig.align = "center"}
 
#Add a figure from the following chunks for the last version of the page
#do not forget to save the output figure in "images"
# knitr::include_graphics(here::here("images", "survanalysis.png"))

```  

<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}  

To run survival analyses in R, one the most widely used package is the **survival** package. We first install it and then load it as well as the other packages that will be used in this section:

In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, echo=F, message=FALSE, warning=FALSE}

# install/load the different packages needed for this page
pacman::p_load(
  survival,      # survival analysis 
  survminer,     # survival analysis
  rio,           # importing data  
  here,          # relative file pathways  
  janitor,       # tabulations
  SemiCompRisks, # dataset examples and advanced tools for working with Semi-Competing Risks data
  tidyverse,     # data manipulation and visualization
  Epi,           # stat analyses in Epi
  survival,      # survival analysis
  survminer      # survival analysis: advanced KM curves
)


```


This page explores survival analyses using the linelist used in most of the previous pages and on which we apply some changes to have a proper survival data.


### Import dataset {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r echo=F}
# import linelist
linelist_case_data <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# import linelist
linelist_case_data <- rio::import("linelist_cleaned.rds")
```

### Data management and transformation {.unnumbered}

In short, survival data can be described as having the following three characteristics:

1) the dependent variable or response is the waiting time until the occurrence of a well-defined event,
2) observations are censored, in the sense that for some units the event of interest has not occurred at the time the data are analyzed, and 
3) there are predictors or explanatory variables whose effect on the waiting time we wish to assess or control. 

Thus, we will create different variables needed to respect that structure and run the survival analysis.

We define:

- a new data frame `linelist_surv` for this analysis  
- our event of interest as being "death" (hence our survival probability will be the probability of being alive after a certain time after the time of origin),
- the follow-up time (`futime`) as the time between the time of onset and the time of outcome *in days*,
- censored patients as those who recovered or for whom the final outcome is not known ie the event "death" was not observed (`event=0`).

<span style="color: orange;">**_CAUTION:_** Since in a real cohort study, the information on the time of origin and the end of the follow-up is known given individuals are observed, we will remove observations where the date of onset or the date of outcome is unknown. Also the cases where the date of onset is later than the date of outcome will be removed since they are considered as wrong.</span>

<span style="color: darkgreen;">**_TIP:_** Given that filtering to greater than (>) or less than (<) a date can remove rows with missing values, applying the filter on the wrong dates will also remove the rows with missing dates.</span>

We then use `case_when()` to create a column `age_cat_small` in which there are only 3 age categories.

```{r }
#create a new data called linelist_surv from the linelist_case_data

linelist_surv <-  linelist_case_data %>% 
     
  dplyr::filter(
       # remove observations with wrong or missing dates of onset or date of outcome
       date_outcome > date_onset) %>% 
  
  dplyr::mutate(
       # create the event var which is 1 if the patient died and 0 if he was right censored
       event = ifelse(is.na(outcome) | outcome == "Recover", 0, 1), 
    
       # create the var on the follow-up time in days
       futime = as.double(date_outcome - date_onset), 
    
       # create a new age category variable with only 3 strata levels
       age_cat_small = dplyr::case_when( 
            age_years < 5  ~ "0-4",
            age_years >= 5 & age_years < 20 ~ "5-19",
            age_years >= 20   ~ "20+"),
       
       # previous step created age_cat_small var as character.
       # now convert it to factor and specify the levels.
       # Note that the NA values remain NA's and are not put in a level "unknown" for example,
       # since in the next analyses they have to be removed.
       age_cat_small = fct_relevel(age_cat_small, "0-4", "5-19", "20+")
       )
```


<span style="color: darkgreen;">**_TIP:_** We can verify the new columns we have created by doing a summary on the `futime` and a cross-tabulation between `event` and `outcome` from which it was created. Besides this verification it is a good habit to communicate the median follow-up time when interpreting survival analysis results.</span>

```{r }

summary(linelist_surv$futime)

# cross tabulate the new event var and the outcome var from which it was created
# to make sure the code did what it was intended to
linelist_surv %>% 
  tabyl(outcome, event)
```

Now we cross-tabulate the new age_cat_small var and the old age_cat col to ensure correct assingments  

```{r}
linelist_surv %>% 
  tabyl(age_cat_small, age_cat)
```

Now we review the 10 first observations of the `linelist_surv` data looking at specific variables (including those newly created).  


```{r}
linelist_surv %>% 
  select(case_id, age_cat_small, date_onset, date_outcome, outcome, event, futime) %>% 
  head(10)
```

We can also cross-tabulate the columns `age_cat_small` and `gender` to have more details on the distribution of this new column by gender. We use `tabyl()` and the *adorn* functions from **janitor** as described in the [Descriptive tables] page. 

<!-- For this we use the `stat.table()` function of the **Epi** package. -->

```{r}

linelist_surv %>% 
  tabyl(gender, age_cat_small, show_na = F) %>% 
  adorn_totals(where = "both") %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front")

```

<!-- Epi::stat.table(  -->
<!--   #give variables for the cross tabulation -->
<!--   list( -->
<!--     gender,  -->
<!--     age_cat_small -->
<!--     ), -->

<!--   #precise the function you want to call (mean,count..) -->
<!--   list(  -->
<!--     count(), -->
<!--     percent(age_cat_small) -->
<!--     ),  -->

<!--   #add margins -->
<!--   margins=T,  -->

<!--   #data used -->
<!--   data = linelist_surv  -->
<!--   ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Basics of survival analysis {}


### Building a surv-type object {.unnumbered}

We will first use `Surv()` from **survival** to build a survival object from the follow-up time and event columns.  

The result of such a step is to produce an object of type *Surv* that condenses the time information and whether the event of interest (death) was observed. This object will ultimately be used in the right-hand side of subsequent model formulae (see [documentation](https://cran.r-project.org/web/packages/survival/vignettes/survival.pdf)).  


```{r survobj }
# Use Suv() syntax for right-censored data
survobj <- Surv(time = linelist_surv$futime,
                event = linelist_surv$event)
```

<!-- ```{r} -->
<!-- survobj <- with(linelist_surv, -->

<!--                 survival::Surv(futime, event) -->

<!--                 ) -->
<!-- ``` -->


To review, here are the first 10 rows of the `linelist_surv` data, viewing only some important columns.  

```{r}
linelist_surv %>% 
  select(case_id, date_onset, date_outcome, futime, outcome, event) %>% 
  head(10)
```

And here are the first 10 elements of `survobj`. It prints as essentially a vector of follow-up time, with "+" to represent if an observation was right-censored. See how the numbers align above and below.  

```{r}
#print the 50 first elements of the vector to see how it presents
head(survobj, 10)
```


### Running initial analyses {.unnumbered}

We then start our analysis using the `survfit()` function to produce a *survfit object*, which fits the default calculations for **_Kaplan Meier_** (KM) estimates of the overall (marginal) survival curve, which are in fact a step function with jumps at observed event times. The final *survfit object*  contains one or more survival curves and is created using the *Surv* object as a response variable in the model formula.  

<span style="color: black;">**_NOTE:_** The Kaplan-Meier estimate is a nonparametric maximum likelihood estimate (MLE) of the survival function. . (see resources for more information).</span>

The summary of this *survfit object* will give what is called a *life table*. For each time step of the follow-up (`time`) where an event happened (in ascending order):  

* the number of people who were at risk of developing the event (people who did not have the event yet nor were censored: `n.risk`)  
* those who did develop the event  (`n.event`)  
* and from the above: the probability of *not* developing the event (probability of not dying, or of surviving past that specific time)  
* finally, the standard error and the confidence interval for that probability are derived and displayed  

We fit the KM estimates using the formula where the previously Surv object "survobj" is the response variable. "~ 1" precises we run the model for the overall survival.  

```{r fit}
# fit the KM estimates using a formula where the Surv object "survobj" is the response variable.
# "~ 1" signifies that we run the model for the overall survival  
linelistsurv_fit <-  survival::survfit(survobj ~ 1)

#print its summary for more details
summary(linelistsurv_fit)

```


While using `summary()` we can add the option `times` and  specify certain times at which we want to see the survival information 

```{r print_spec_times}

#print its summary at specific times
summary(linelistsurv_fit, times = c(5,10,20,30,60))

```


We can also use the `print()` function. The `print.rmean = TRUE` argument is used to obtain the mean survival time and its standard error (se).

<span style="color: black;">**_NOTE:_** The restricted mean survival time (RMST) is a specific survival measure more and more used in cancer survival analysis and which is often defined as the area under the survival curve, given we observe patients up to restricted time T (more details in Resources section).


```{r, mean_survtime}
# print linelistsurv_fit object with mean survival time and its se. 
print(linelistsurv_fit, print.rmean = TRUE)

```


<span style="color: darkgreen;">**_TIP:_** We can create the *surv object* directly in the `survfit()` function and save a line of code. This will then look like: `linelistsurv_quick <-  survfit(Surv(futime, event) ~ 1, data=linelist_surv)`.</span>


### Cumulative hazard {.unnumbered}  

Besides the `summary()` function, we can also use the `str()` function that gives more details on the structure of the `survfit()` object. It is a list of 16 elements.  

Among these elements is an important one: `cumhaz`, which is a numeric vector. This could be plotted to allow show the **_cumulative hazard_**, with the **_hazard_** being the **_instantaneous rate of event occurrence_** (see references).

```{r fit_struct}

str(linelistsurv_fit)

```

<!-- ======================================================= -->
### Plotting Kaplan-Meir curves  {.unnumbered}

Once the KM estimates are fitted, we can visualize the probability of being alive through a given time using the basic `plot()` function that draws the "Kaplan-Meier curve". In other words, the curve below is a conventional illustration of the survival experience in the whole patient group.

We can quickly verify the follow-up time min and max on the curve.  

An easy way to interpret is to say that at time zero, all the participants are still alive and survival probability is then 100%. This probability decreases over time as patients die. The proportion of participants surviving past 60 days of follow-up is around 40%.

```{r }

plot(linelistsurv_fit, 
     xlab = "Days of follow-up",    # x-axis label
     ylab="Survival Probability",   # y-axis label
     main= "Overall survival curve" # figure title
     )

```

The confidence interval of the KM survival estimates are also plotted by default and can be dismissed by adding the option `conf.int = FALSE` to the `plot()` command.

Since the event of interest is "death", drawing a curve describing the complements of the survival proportions will lead to drawing the cumulative mortality proportions. This can be done with `lines()`, which adds information to an existing plot.  


```{r}

# original plot
plot(
  linelistsurv_fit,
  xlab = "Days of follow-up",       
  ylab = "Survival Probability",       
  mark.time = TRUE,              # mark events on the curve: a "+" is printed at every event
  conf.int = FALSE,              # do not plot the confidence interval
  main = "Overall survival curve and cumulative mortality"
  )

# draw an additional curve to the previous plot
lines(
  linelistsurv_fit,
  lty = 3,             # use different line type for clarity
  fun = "event",       # draw the cumulative events instead of the survival 
  mark.time = FALSE,
  conf.int = FALSE
  )

# add a legend to the plot
legend(
  "topright",                               # position of legend
  legend = c("Survival", "Cum. Mortality"), # legend text 
  lty = c(1, 3),                            # line types to use in the legend
  cex = .85,                                # parametes that defines size of legend text
  bty = "n"                                 # no box type to be drawn for the legend
  )

```

<!-- ======================================================= -->
## Comparison of survival curves 

To compare the survival within different groups of our observed participants or patients, we might need to first look at their respective survival curves and then run tests to evaluate the difference between independent groups. This comparison can concern groups based on gender, age, treatment, comorbidity...

### Log rank test {.unnumbered}

The log rank test is a popular test that compares the entire survival experience between two or more *independent* groups and can be thought of as a test of whether the survival curves are identical (overlapping) or not (null hypothesis of no difference in survival between the groups). The `survdiff()` function of the **survival package** allows running the log-rank test when we specify `rho = 0` (which is the default). The test results gives a chi-square statistic along with a p-value since the log rank statistic is approximately distributed as a chi-square test statistic.

We first try to compare the survival curves by gender group. For this, we first try to visualize it (check whether the two survival curves are overlapping). A new *survfit object*  will be created with a slightly different formula. Then the *survdiff object* will be created.

By supplying ` ~ gender` as the right side of the formula, we no longer plot the overall survival but instead by gender.  


```{r comp_surv, warning=FALSE}

# create the new survfit object based on gender
linelistsurv_fit_sex <-  survfit(Surv(futime, event) ~ gender, data = linelist_surv)
```

Now we can plot the survival curves by gender. Have a look at the *order* of the strata levels in the gender column before defining your colors and legend.  

```{r}
# set colors
col_sex <- c("lightgreen", "darkgreen")

# create plot
plot(
  linelistsurv_fit_sex,
  col = col_sex,
  xlab = "Days of follow-up",
  ylab = "Survival Probability")

# add legend
legend(
  "topright",
  legend = c("Female","Male"),
  col = col_sex,
  lty = 1,
  cex = .9,
  bty = "n")
```

And now we can compute the test of the difference between the survival curves using `survdiff()`

```{r}
#compute the test of the difference between the survival curves
survival::survdiff(
  Surv(futime, event) ~ gender, 
  data = linelist_surv
  )

```

We see that the survival curve for women and the one for men overlap and the log-rank test does not give evidence of a survival difference between women and men.

Some other R packages allow illustrating survival curves for different groups and testing the difference all at once. Using the `ggsurvplot()` function from the **survminer** package, we can also include in our curve the printed risk tables for each group, as well the p-value from the log-rank test. 

<span style="color: orange;">**_CAUTION:_** **survminer** functions require that you specify the survival object *and* again specify the data used to fit the survival object. Remember to do this to avoid non-specific error messages. </span>

```{r, warning=F, message=F}

survminer::ggsurvplot(
    linelistsurv_fit_sex, 
    data = linelist_surv,          # again specify the data used to fit linelistsurv_fit_sex 
    conf.int = FALSE,              # do not show confidence interval of KM estimates
    surv.scale = "percent",        # present probabilities in the y axis in %
    break.time.by = 10,            # present the time axis with an increment of 10 days
    xlab = "Follow-up days",
    ylab = "Survival Probability",
    pval = T,                      # print p-value of Log-rank test 
    pval.coord = c(40,.91),        # print p-value at these plot coordinates
    risk.table = T,                # print the risk table at bottom 
    legend.title = "Gender",       # legend characteristics
    legend.labs = c("Female","Male"),
    font.legend = 10, 
    palette = "Dark2",             # specify color palette 
    surv.median.line = "hv",       # draw horizontal and vertical lines to the median survivals
    ggtheme = theme_light()        # simplify plot background
)

```


We may also want to test for differences in survival by the source of infection (source of contamination).  

In this case, the Log rank test gives enough evidence of a difference in the survival probabilities at `alpha= 0.005`. The survival probabilities for patients that were infected at funerals are higher than the survival probabilities for patients that got infected in other places, suggesting a survival benefit.

```{r}

linelistsurv_fit_source <-  survfit(
  Surv(futime, event) ~ source,
  data = linelist_surv
  )

# plot
ggsurvplot( 
  linelistsurv_fit_source,
  data = linelist_surv,
  size = 1, linetype = "strata",   # line types
  conf.int = T,
  surv.scale = "percent",  
  break.time.by = 10, 
  xlab = "Follow-up days",
  ylab= "Survival Probability",
  pval = T,
  pval.coord = c(40,.91),
  risk.table = T,
  legend.title = "Source of \ninfection",
  legend.labs = c("Funeral", "Other"),
  font.legend = 10,
  palette = c("#E7B800","#3E606F"),
  surv.median.line = "hv", 
  ggtheme = theme_light()
)

```

<!-- ======================================================= -->
## Cox regression analysis {}

Cox proportional hazards regression is one of the most popular regression techniques for survival analysis. Other models  can also be used since the Cox model requires *important assumptions* that need to be verified for an appropriate use such as the proportional hazards assumption: see references. 

In a Cox proportional hazards regression model, the measure of effect is the **_hazard rate_** (HR), which is the risk of failure (or the risk of death in our example), given that the participant has survived up to a specific time.  Usually, we are interested in comparing *independent* groups with respect to their hazards, and we use a hazard ratio, which is analogous to an odds ratio in the setting of multiple logistic regression analysis. The `cox.ph()` function from the **survival** package is used to fit the model. The function `cox.zph()` from **survival** package may be used to test the proportional hazards assumption for a Cox regression model fit. 

<span style="color: black;">**_NOTE:_** A probability must lie in the range 0 to 1. However, the hazard represents the expected number of events per one unit of time. 

* If the hazard ratio for a predictor is close to 1 then that predictor does not affect survival,
* if the HR is less than 1, then the predictor is protective (i.e., associated with improved survival),
* and if the HR is greater than 1, then the predictor is associated with increased risk (or decreased survival).</span> 

### Fitting a Cox model {.unnumbered}

We can first fit a model to assess the effect of age and gender on the survival. By just printing the model, we have the information on:

  + the estimated regression coefficients `coef` which quantifies the association between the predictors and the outcome,
  + their exponential (for interpretability, `exp(coef)`) which produces the *hazard ratio*,
  + their standard error `se(coef)`,
  + the z-score: how many standard errors is the estimated coefficient away from  0,
  + and the p-value:  the probability that the estimated coefficient could be 0.
  
The `summary()` function applied to the cox model object gives more information, such as the confidence interval of the estimated HR and the different test scores.

The effect of the first covariate `gender`  is presented in the first row. `genderm` (male) is printed, implying that the first strata level ("f"), i.e the female group, is the reference group for the gender. Thus the interpretation of the test parameter is that of men compared to women. The p-value indicates there was not enough evidence of an effect of the gender on the expected hazard or of an association between gender and all-cause mortality.

The same lack of evidence is noted regarding age-group.

```{r coxmodel_agesex}

#fitting the cox model
linelistsurv_cox_sexage <-  survival::coxph(
              Surv(futime, event) ~ gender + age_cat_small, 
              data = linelist_surv
              )


#printing the model fitted
linelistsurv_cox_sexage


#summary of the model
summary(linelistsurv_cox_sexage)

```


It was interesting to run the model and look at the results but a first look to verify whether the proportional hazards assumptions is respected could help saving time.

```{r test_assumption}

test_ph_sexage <- survival::cox.zph(linelistsurv_cox_sexage)
test_ph_sexage

```


<span style="color: black;">**_NOTE:_** A second argument called *method* can be specified when computing the cox model, that determines how ties are handled. The *default* is "efron", and the other options are "breslow" and "exact".</span>

In another model we add more risk factors such as the source of infection and the number of days between date of onset and admission. This time, we first  verify the proportional hazards assumption before going forward.

In this model, we have included a continuous predictor (`days_onset_hosp`). In this case we interpret the parameter estimates as the increase in the expected log of the relative hazard for each one unit increase in the predictor, holding other predictors constant. We first verify the proportional hazards assumption.  

```{r coxmodel_fit_ph,  message=FALSE}

#fit the model
linelistsurv_cox <-  coxph(
                        Surv(futime, event) ~ gender + age_years+ source + days_onset_hosp,
                        data = linelist_surv
                        )


#test the proportional hazard model
linelistsurv_ph_test <- cox.zph(linelistsurv_cox)
linelistsurv_ph_test
```

The graphical verification of this assumption may be performed with the function `ggcoxzph()` from the **survminer** package. 

```{r}
survminer::ggcoxzph(linelistsurv_ph_test)

```


The model results indicate there is a negative association between onset to admission duration and all-cause mortality. The expected hazard is 0.9 times lower in a person who who is one day later admitted than another, holding gender constant. Or in a more straightforward explanation, a one unit increase in the duration of onset to admission is associated with a 10.7% (`coef *100`) decrease in the risk of death.

Results show also a positive association between the source of infection and the all-cause mortality. Which is to say there is an increased risk of death (1.21x) for patients that got a source of infection other than funerals.


```{r coxmodel_summary,  message=FALSE}

#print the summary of the model
summary(linelistsurv_cox)

```


We can verify this relationship with a table:  


```{r}
linelist_case_data %>% 
  tabyl(days_onset_hosp, outcome) %>% 
  adorn_percentages() %>%  
  adorn_pct_formatting()

```


We would need to consider and investigate why this association exists in the data. One possible explanation could be that patients who live long enough to be admitted later had less severe disease to begin with. Another perhaps more likely explanation is that since we used a simulated fake dataset, this pattern does not reflect reality!  


<!-- ======================================================= -->

### Forest plots {.unnumbered}

We can then visualize the results of the cox model using the practical forest plots with the `ggforest()` function of the **survminer package**.

```{r forestp}

ggforest(linelistsurv_cox, data = linelist_surv)

```

<!-- ======================================================= -->
## Time-dependent covariates in survival models {}

Some of the following sections have been adapted with permission from an excellent [introduction to survival analysis in R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html) by [Dr. Emily Zabor](https://www.emilyzabor.com/) 

In the last section we covered using Cox regression to examine associations between covariates of interest and survival outcomes.But these analyses rely on the covariate being measured at baseline, that is, before follow-up time for the event begins.

What happens if you are interested in a covariate that is measured **after** follow-up time begins? Or, what if you have a covariate that can change over time?

For example, maybe you are working with clinical data where you repeated measures of hospital laboratory values that can change over time. This is an example of a **Time Dependent Covariate**. In order to address this you need a special setup, but fortunately the cox model is very flexible and this type of data can also be modeled with tools from the **survival** package. 

### Time-dependent covariate setup {.unnumbered} 

Analysis of time-dependent covariates in R requires setup of a special dataset. If interested, see the more detailed paper on this by the author of the **survival** package [Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf).

For this, we'll use a new dataset from the `SemiCompRisks` package named `BMT`, which includes data on 137 bone marrow transplant patients. The variables we'll focus on are:  

* `T1`  - time (in days) to death or last follow-up  
* `delta1` - death indicator; 1-Dead, 0-Alive  
* `TA` -  time (in days) to acute graft-versus-host disease  
* `deltaA` -  acute graft-versus-host disease indicator;  
  * 1 - Developed acute graft-versus-host disease  
  * 0 - Never developed acute graft-versus-host disease

We'll load this dataset from the **survival** package using the **base** R command `data()`, which can be used for loading data that is already included in a R package that is loaded. The data frame `BMT` will appear in your R environment.  

```{r}
data(BMT, package = "SemiCompRisks")
```

#### Add unique patient identifier {.unnumbered}  

There is no unique ID column in the `BMT` data, which is needed to create the type of dataset we want. So we use the function `rowid_to_column()` from the **tidyverse** package **tibble** to create a new id column called `my_id` (adds column at start of data frame with sequential row ids, starting at 1). We name the data frame `bmt`.  

```{r}
bmt <- rowid_to_column(BMT, "my_id")
```

The dataset now looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(bmt, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Expand patient rows {.unnumbered}  

Next, we'll use the `tmerge()` function with the `event()` and `tdc()` helper functions to create the restructured dataset. Our goal is to restructure the dataset to create a separate row for each patient for each time interval where they have a different value for `deltaA`. In this case, each patient can have at most two rows depending on whether they developed acute graft-versus-host disease during the data collection period. We'll call our new indicator for the development of acute graft-versus-host disease `agvhd`.

- `tmerge()` creates a long dataset with multiple time intervals for the different covariate values for each patient
- `event()` creates the new event indicator to go with the newly-created time intervals
- `tdc()` creates the time-dependent covariate column, `agvhd`, to go with the newly created time intervals

```{r}
td_dat <- 
  tmerge(
    data1 = bmt %>% select(my_id, T1, delta1), 
    data2 = bmt %>% select(my_id, T1, delta1, TA, deltaA), 
    id = my_id, 
    death = event(T1, delta1),
    agvhd = tdc(TA)
    )
```

To see what this does, let's look at the data for the first 5 individual patients.

The variables of interest in the original data looked like this:

```{r}
bmt %>% 
  select(my_id, T1, delta1, TA, deltaA) %>% 
  filter(my_id %in% seq(1, 5))
```

The new dataset for these same patients looks like this:

```{r}
td_dat %>% 
  filter(my_id %in% seq(1, 5))
```

Now some of our patients have two rows in the dataset corresponding to intervals where they have a different value of our new variable, `agvhd`. For example, Patient 1 now has two rows with a `agvhd` value of zero from time 0 to time 67, and a value of 1 from time 67 to time 2081. 

### Cox regression with time-dependent covariates {.unnumbered} 

Now that we've reshaped our data and added the new time-dependent `aghvd` variable, let's fit a simple single variable cox regression model. We can use the same `coxph()` function as before, we just need to change our `Surv()` function to specify both the start and stop time for each interval using the `time1 = ` and `time2 = ` arguments. 


```{r}
bmt_td_model = coxph(
  Surv(time = tstart, time2 = tstop, event = death) ~ agvhd, 
  data = td_dat
  )

summary(bmt_td_model)
```

Again, we'll visualize our cox model results using the `ggforest()` function from the **survminer package**.:

```{r}

ggforest(bmt_td_model, data = td_dat)

```

As you can see from the forest plot, confidence interval, and p-value, there does not appear to be a strong association between death and acute graft-versus-host disease in the context of our simple model. 

<!-- ======================================================= -->
## Resources {  }

[Survival Analysis Part I: Basic concepts and first analyses](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/)

[Survival Analysis in R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html)

[Survival analysis in infectious disease research: Describing events in time](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2954271/)

[Chapter on advanced survival models Princeton](https://data.princeton.edu/wws509/notes/c7.pdf)

[Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf)

[Survival analysis cheatsheet R](https://publicifsv.sund.ku.dk/~ts/survival/survival-cheat.pdf)

[Survminer cheatsheet](https://paulvanderlaken.files.wordpress.com/2017/08/survminer_cheatsheet.pdf)

[Paper on different survival measures for cancer registry data with Rcode provided as supplementary materials](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6322561/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survival_analysis.Rmd-->

# GIS basics { }  

```{r, out.width=c('100%', '100%'), echo=F}
knitr::include_graphics(here::here("images", "gis_head_image.png"))
```

<!-- ======================================================= -->
## Overview {  }

Spatial aspects of your data can provide a lot of insights into the situation of the outbreak, and to answer questions such as: 

* Where are the current disease hotspots?
* How have the hotspots have changed over time?
* How is the access to health facilities? Are any improvements needed?

The current focus of this GIS page to address the needs of applied epidemiologists in outbreak response. We will explore basic spatial data visualization methods using **tmap** and **ggplot2** packages. We will also walk through some of the basic spatial data management and querying methods with the **sf** package. Lastly, we will briefly touch upon concepts of *spatial statistics* such as spatial relationships, spatial autocorrelation, and spatial regression using the **spdep** package.  



## Key terms {}  

Below we introduce some key terminology. For a thorough introduction to GIS and spatial analysis, we suggest that you review one of the longer tutorials or courses listed in the References section.  

**Geographic Information System (GIS)** - A GIS is a framework or environment for gathering, managing, analyzing, and visualizing spatial data.

### GIS software {.unnumbered}

Some popular GIS software allow point-and-click interaction for map development and spatial analysis. These tools comes with advantages such as not needing to learn code and the ease of manually selecting and placing icons and features on a map. Here are two popular ones:  

**ArcGIS** - A commercial GIS software developed by the company ESRI, which is very popular but quite expensive  

**QGIS** - A free open-source GIS software that can do almost anything that ArcGIS can do. You can [download QGIS here](https://qgis.org/en/site/forusers/download.html)  

Using R as a GIS can seem more intimidating at first because instead of "point-and-click", it has a "command-line interface" (you must code to acquire the desired outcome). However, this is a major advantage if you need to repetitively produce maps or create an analysis that is reproducible.  

### Spatial data {.unnumbered}

The two primary forms of spatial data used in GIS are vector and raster data:

**Vector Data** - The most common format of spatial data used in GIS, vector data are comprised of geometric features of vertices and paths. Vector spatial data can be further divided into three widely-used types:

  * *Points* - A point consists of a coordinate pair (x,y)  representing a specific location in a coordinate system. Points are the most basic form of spatial data, and may be used to denote a case (i.e. patient home) or a location (i.e. hospital) on a map.

  * *Lines* - A line is composed of two connected points. Lines have a length, and may be used to denote things like roads or rivers.

  * *Polygons* - A polygon is composed of at least three line segments connected by points. Polygon features have a length (i.e. the perimeter of the area) as well as an area measurement. Polygons may be used to note an area (i.e. a village) or a structure (i.e. the actual area of a hospital). 

**Raster Data** - An alternative format for spatial data, raster data is a matrix of cells (e.g. pixels) with each cell containing information such as height, temperature, slope, forest cover, etc. These are often aerial photographs, satellite imagery, etc. Rasters can also be used as “base maps” below vector data.

### Visualizing spatial data {.unnumbered}

To visually represent spatial data on a map, GIS software requires you to provide sufficient information about where different features should be, in relation to one another. If you are using vector data, which will be true for most use cases, this information will typically be stored in a shapefile:

**Shapefiles** - A shapefile is a common data format for storing "vector" spatial data consisting or lines, points, or polygons. A single shapefile is actually a collection of at least three files - .shp, .shx, and .dbf. All of these sub-component files must be present in a given directory (folder) for the shapefile to be readable. These associated files can be compressed into a ZIP folder to be sent via email or download from a website.  

The shapefile will contain information about the features themselves, as well as where to locate them on the Earth’s surface. This is important because while the Earth is a globe, maps are typically two-dimensional; choices about how to “flatten” spatial data can have a big impact on the look and interpretation of the resulting map.

**Coordinate Reference Systems (CRS)** - A CRS is a coordinate-based system used to locate geographical features on the Earth's surface. It has a few key components:  

  * *Coordinate System* - There are many many different coordinate systems, so make sure you know which system your coordinates are from. Degrees of latitude/longitude are common, but you could also see [UTM](https://www.maptools.com/tutorials/utm/quick_guide) coordinates.  
  
  * *Units* - Know what the units are for your coordinate system (e.g. decimal degrees, meters)  

  * *Datum* - A particular modeled version of the Earth. These have been revised over the years, so ensure that your map layers are using the same datum.  

  * *Projection* - A reference to the mathematical equation that was used to project the truly round earth onto a flat surface (map).  

Remember that you can summarise spatial data without using the mapping tools shown below. Sometimes a simple table by geography (e.g. district, country, etc.) is all that is needed!  

## Getting started with GIS  


There are a couple of key items you will need to have and to think about to make a map. These include:

  * A **dataset** -- this can be in a spatial data format (such as shapefiles, as noted above) or it may not be in a spatial format (for instance just as a csv).
  
  * If your dataset is not in a spatial format you will also need a **reference dataset**. Reference data consists of the spatial representation of the data and the related **attributes**, which would include material containing the location and address information of specific features.
  
    + If you are working with pre-defined geographic boundaries (for example, administrative regions), reference shapefiles are often freely available to download from a government agency or data sharing organization. When in doubt, a good place to start is to Google “[regions] shapefile”
    
    + If you have address information, but no latitude and longitude, you may need to use a **geocoding engine** to get the spatial reference data for your records. 
    
  * An idea about **how you want to present** the information in your datasets to your target audience. There are many different types of maps, and it is important to think about which type of map best fits your needs.

### Types of maps for visualizing your data {.unnumbered}

**Choropleth map** - a type of thematic map where colors, shading, or patterns are used to represent geographic regions in relation to their value of an attribute. For instance a larger value could be indicated by a darker colour than a smaller value. This type of map is particularly useful when visualizing a variable and how it changes across defined regions or geopolitical areas.

```{r, out.width = '50%', fig.align = "center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "gis_choropleth.png"))
```

**Case density heatmap** - a type of thematic map where colours are used to represent intensity of a value, however, it does not use defined regions or geopolitical boundaries to group data. This type of map is typically used for showing ‘hot spots’ or areas with a high density or concentration of points. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_heatmap.png"))
```

**Dot density map** - a thematic map type that uses dots to represent attribute values in your data. This type of map is best used to visualize the scatter of your data and visually scan for clusters.

```{r, fig.align = "center", echo=F}
# dot density img here
```

**Proportional symbols map (graduated symbols map)** - a thematic map similar to a choropleth map, but instead of using colour to indicate the value of an attribute it uses a symbol (usually a circle) in relation to the value. For instance a larger value could be indicated by a larger symbol than a smaller value. This type of map is best used when you want to visualize the size or quantity of your data across geographic regions. 

```{r, fig.align = "center", echo=F}
# proportional symbols img here
```

You can also combine several different types of visualizations to show complex geographic patterns. For example, the cases (dots) in the map below are colored according to their closest health facility (see legend). The large red circles show *health facility catchment areas* of a certain radius, and the bright red case-dots those that were outside any catchment range:

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_hf_catchment.png"))
```

Note: The primary focus of this GIS page is based on the context of field outbreak response. Therefore the contents of the page will cover the basic spatial data manipulations, visualizations, and analyses.


<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,           # to import data
  here,          # to locate files
  tidyverse,     # to clean, handle, and plot the data (includes ggplot2 package)
  sf,            # to manage spatial data using a Simple Feature format
  tmap,          # to produce simple maps, works for both interactive and static maps
  janitor,       # to clean column names
  OpenStreetMap, # to add OSM basemap in ggplot map
  spdep          # spatial statistics
  ) 
                  
```

You can see an overview of all the R packages that deal with spatial data at the [CRAN "Spatial Task View"](https://cran.r-project.org/web/views/Spatial.html).  


### Sample case data {.unnumbered}

For demonstration purposes, we will work with a random sample of 1000 cases from the simulated Ebola epidemic `linelist` dataframe (computationally, working with fewer cases is easier to display in this handbook). If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file).  

Since we are taking a random sample of the cases, your results may look slightly different from what is demonstrated here when you run the codes on your own.

Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import clean case linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))  
```

```{r, eval=F}
# import clean case linelist
linelist <- import("linelist_cleaned.rds")  
```

Next we select a random sample of 1000 rows using `sample()` from **base** R.   

```{r}
# generate 1000 random row numbers, from the number of rows in linelist
sample_rows <- sample(nrow(linelist), 1000)

# subset linelist to keep only the sample rows, and all columns
linelist <- linelist[sample_rows,]
```

Now we want to convert this `linelist` which is class dataframe, to an object of class "sf" (spatial features). Given that the linelist has two columns "lon" and "lat" representing the longitude and latitude of each case's residence, this will be easy.  

We use the package **sf** (spatial features) and its function `st_as_sf()` to create the new object we call `linelist_sf`. This new object looks essentially the same as the linelist, but the columns `lon` and `lat` have been designated as coordinate columns, and a coordinate reference system (CRS) has been assigned for when the points are displayed. 4326 identifies our coordinates as based on the [World Geodetic System 1984 (WGS84)](https://gisgeography.com/wgs84-world-geodetic-system/) - which is standard for GPS coordinates.  

```{r}
# Create sf object
linelist_sf <- linelist %>%
     sf::st_as_sf(coords = c("lon", "lat"), crs = 4326)
```

This is how the original `linelist` dataframe looks like. In this demonstration, we will only use the column `date_onset` and `geometry` (which was constructed from the longitude and latitude fields above and is the last column in the data frame).  

```{r}
DT::datatable(head(linelist_sf, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Admin boundary shapefiles {.unnumbered}  

**Sierra Leone: Admin boundary shapefiles**  

In advance, we have downloaded all administrative boundaries for Sierra Leone from the Humanitarian Data Exchange (HDX) [website here](https://data.humdata.org/dataset/sierra-leone-all-ad-min-level-boundaries). Alternatively, you can download these and all other example data for this handbook via our R package, as explained in the [Download handbook and data] page.  

Now we are going to do the following to save the Admin Level 3 shapefile in R:  

1) Import the shapefile  
2) Clean the column names  
3) Filter rows to keep only areas of interest  

To import a shapefile we use the `read_sf()` function from **sf**. It is provided the filepath via `here()`. - in our case the file is within our R project in the "data", "gis", and "shp" subfolders, with filename "sle_adm3.shp" (see pages on [Import and export] and [R projects] for more information). You will need to provide your own file path.  

```{r, echo=F}
sle_adm3_raw <- sf::read_sf(here("data", "gis", "shp", "sle_adm3.shp"))
```


Next we use `clean_names()` from the **janitor** package to standardize the column names of the shapefile. We also use `filter()` to keep only the rows with admin2name of "Western Area Urban" or "Western Area Rural".    

```{r}
# ADM3 level clean
sle_adm3 <- sle_adm3_raw %>%
  janitor::clean_names() %>% # standardize column names
  filter(admin2name %in% c("Western Area Urban", "Western Area Rural")) # filter to keep certain areas
```

Below you can see the how the shapefile looks after import and cleaning. *Scroll to the right* to see how there are columns with admin level 0 (country), admin level 1, admin level 2, and finally admin level 3. Each level has a character name and a unique identifier "pcode". The pcode expands with each increasing admin level e.g. SL (Sierra Leone) -> SL04 (Western) -> SL0410 (Western Area Rural) -> SL040101 (Koya Rural).  

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(sle_adm3, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Population data {.unnumbered}  

**Sierra Leone: Population by ADM3**  

These data can again be downloaded from HDX (link [here](https://data.humdata.org/dataset/sierra-leone-population)) or via our **epirhandbook** R package as explained [in this page][Download handbook and data]. We use `import()` to load the .csv file. We also pass the imported file to `clean_names()` to standardize the column name syntax.   

```{r}
# Population by ADM3
sle_adm3_pop <- import(here("data", "gis", "population", "sle_admpop_adm3_2020.csv")) %>%
  janitor::clean_names()
```

Here is what the population file looks like. Scroll to the right to see how each jurisdiction has columns with `male` population, `female` populaton, `total` population, and the population break-down in columns by age group.  

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_adm3_pop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Health Facilities {.unnumbered}

**Sierra Leone: Health facility data from OpenStreetMap**  

Again we have downloaded the locations of health facilities from HDX [here](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities) or via instructions in the [Download handbook and data] page.   

We import the facility points shapefile with `read_sf()`, again clean the column names, and then filter to keep only the points tagged as either "hospital", "clinic", or "doctors".  


```{r}
# OSM health facility shapefile
sle_hf <- sf::read_sf(here("data", "gis", "shp", "sle_hf.shp")) %>% 
  janitor::clean_names() %>%
  filter(amenity %in% c("hospital", "clinic", "doctors"))
```

Here is the resulting dataframe - *scroll right* to see the facility name and `geometry` coordinates.  

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Plotting coordinates {  }

The easiest way to plot X-Y coordinates (longitude/latitude, points), in this case of cases, is to draw them as points directly from the `linelist_sf` object which we created in the preparation section.

The package **tmap** offers simple mapping capabilities for both static ("plot" mode) and interactive ("view" mode) with just a few lines of code. The **tmap** syntax is similar to that of **ggplot2**, such that commands are added to each other with `+`. Read more detail in this [vignette](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html). 


1) Set the **tmap** mode. In this case we will use "plot" mode, which produces static outputs.  

```{r, warning = F, message=F}
tmap_mode("plot") # choose either "view" or "plot"
```

Below, the points are plotted alone.`tm_shape()` is provided with the `linelist_sf` objects. We then add points via `tm_dots()`, specifying the size and color. Because `linelist_sf` is an sf object, we have already designated the two columns that contain the lat/long coordinates and the coordinate reference system (CRS): 


```{r, warning = F, message=F}
# Just the cases (points)
tm_shape(linelist_sf) + tm_dots(size=0.08, col='blue')
```

Alone, the points do not tell us much. So we should also map the administrative boundaries:  

Again we use `tm_shape()` (see [documentation](https://www.rdocumentation.org/packages/tmap/versions/3.3/topics/tm_shape)) but instead of providing the case points shapefile, we provide the administrative boundary shapefile (polygons).  

With the `bbox = ` argument (bbox stands for "bounding box") we can specify the coordinate boundaries. First we show the map display without `bbox`, and then with it.  

```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Just the administrative boundaries (polygons)
tm_shape(sle_adm3) +               # admin boundaries shapefile
  tm_polygons(col = "#F7F7F7")+    # show polygons in light grey
  tm_borders(col = "#000000",      # show borders with color and line weight
             lwd = 2) +
  tm_text("admin3name")            # column text to display for each polygon


# Same as above, but with zoom from bounding box
tm_shape(sle_adm3,
         bbox = c(-13.3, 8.43,    # corner
                  -13.2, 8.5)) +  # corner
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")

```


And now both points and polygons together:  

```{r, warning=F, message=FALSE}
# All together
tm_shape(sle_adm3, bbox = c(-13.3, 8.43, -13.2, 8.5)) +     #
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")+
tm_shape(linelist_sf) +
  tm_dots(size=0.08, col='blue', alpha = 0.5) +
  tm_layout(title = "Distribution of Ebola cases")   # give title to map

```


To read a good comparison of mapping options in R, see this [blog post](https://rstudio-pubs-static.s3.amazonaws.com/324400_69a673183ba449e9af4011b1eeb456b9.html).  




<!-- ======================================================= -->
## Spatial joins {}

You may be familiar with *joining* data from one dataset to another one. Several methods are discussed in the [Joining data] page of this handbook. A spatial join serves a similar purpose but leverages spatial relationships. Instead of relying on common values in columns to correctly match observations, you can utilize their spatial relationships, such as one feature being *within* another, or *the nearest neighbor* to another, or within a *buffer* of a certain radius from another, etc.  


The **sf** package offers various methods for spatial joins. See more documentation about the st_join method and spatial join types in this [reference](https://r-spatial.github.io/sf/reference/geos_binary_pred.html).  


### Points in polygon {.unnumbered}
**Spatial assign administrative units to cases**

Here is an interesting conundrum: the case linelist does not contain any information about the administrative units of the cases. Although it is ideal to collect such information during the initial data collection phase, we can also assign administrative units to individual cases based on their spatial relationships (i.e. point intersects with a polygon).  

Below, we will spatially intersect our case locations (points) with the ADM3 boundaries (polygons):  

1) Begin with the linelist (points)  
2) Spatial join to the boundaries, setting the type of join at "st_intersects"  
3) Use `select()` to keep only certain of the new administrative boundary columns  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3, join = st_intersects)
```

All the columns from `sle_adms` have been added to the linelist! Each case now has columns detailing the administrative levels that it falls within. In this example, we only want to keep two of the new columns (admin level 3), so we `select()` the old column names and just the two additional of interest:  

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3, join = st_intersects) %>% 
  
  # Keep the old column names and two new admin ones of interest
  select(names(linelist_sf), admin3name, admin3pcod)
```

Below, just for display purposes you can see the first ten cases and that their admin level 3 (ADM3) jurisdictions that have been attached, based on where the point spatially intersected with the polygon shapes.    

```{r, warning=F, message=F}
# Now you will see the ADM3 names attached to each case
linelist_adm %>% select(case_id, admin3name, admin3pcod)
```

Now we can describe our cases by administrative unit - something we were not able to do before the spatial join!  

```{r, warning=F, message=F}
# Make new dataframe containing counts of cases by administrative unit
case_adm3 <- linelist_adm %>%          # begin with linelist with new admin cols
  as_tibble() %>%                      # convert to tibble for better display
  group_by(admin3pcod, admin3name) %>% # group by admin unit, both by name and pcode 
  summarise(cases = n()) %>%           # summarize and count rows
  arrange(desc(cases))                     # arrange in descending order

case_adm3
```

We can also create a bar plot of case counts by administrative unit.  

In this example, we begin the `ggplot()` with the `linelist_adm`, so that we can apply factor functions like `fct_infreq()` which orders the bars by frequency (see page on [Factors] for tips).  

```{r, warning=F, message=F}
ggplot(
    data = linelist_adm,                       # begin with linelist containing admin unit info
    mapping = aes(
      x = fct_rev(fct_infreq(admin3name))))+ # x-axis is admin units, ordered by frequency (reversed)
  geom_bar()+                                # create bars, height is number of rows
  coord_flip()+                              # flip X and Y axes for easier reading of adm units
  theme_classic()+                           # simplify background
  labs(                                      # titles and labels
    x = "Admin level 3",
    y = "Number of cases",
    title = "Number of cases, by adminstative unit",
    caption = "As determined by a spatial join, from 1000 randomly sampled cases from linelist"
  )
```


<!-- ======================================================= -->
### Nearest neighbor {.unnumbered}

**Finding the nearest health facility / catchment area**  

It might be useful to know where the health facilities are located in relation to the disease hot spots.

We can use the *st_nearest_feature* join method from the `st_join()` function (**sf** package) to visualize the closest health facility to individual cases.  

1) We begin with the shapefile linelist `linelist_sf`  
2) We spatially join with `sle_hf`, which is the locations of health facilities and clinics (points)  

```{r, warning=F, message=F}
# Closest health facility to each case
linelist_sf_hf <- linelist_sf %>%                  # begin with linelist shapefile  
  st_join(sle_hf, join = st_nearest_feature) %>%   # data from nearest clinic joined to case data 
  select(case_id, osm_id, name, amenity) %>%       # keep columns of interest, including id, name, type, and geometry of healthcare facility
  rename("nearest_clinic" = "name")                # re-name for clarity
```

We can see below (first 50 rows) that the each case now has data on the nearest clinic/hospital  

```{r message=FALSE, echo=F}
DT::datatable(head(linelist_sf_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


We can see that "Den Clinic" is the closest health facility for about ~30% of the cases.

```{r}
# Count cases by health facility
hf_catchment <- linelist_sf_hf %>%   # begin with linelist including nearest clinic data
  as.data.frame() %>%                # convert from shapefile to dataframe
  count(nearest_clinic,              # count rows by "name" (of clinic)
        name = "case_n") %>%         # assign new counts column as "case_n"
  arrange(desc(case_n))              # arrange in descending order

hf_catchment                         # print to console
```

To visualize the results, we can use **tmap** - this time interactive mode for easier viewing  

```{r, warning=F, message=F}
tmap_mode("view")   # set tmap mode to interactive  

# plot the cases and clinic points 
tm_shape(linelist_sf_hf) +            # plot cases
  tm_dots(size=0.08,                  # cases colored by nearest clinic
          col='nearest_clinic') +    
tm_shape(sle_hf) +                    # plot clinic facilities in large black dots
  tm_dots(size=0.3, col='black', alpha = 0.4) +      
  tm_text("name") +                   # overlay with name of facility
tm_view(set.view = c(-13.2284, 8.4699, 13), # adjust zoom (center coords, zoom)
        set.zoom.limits = c(13,14))+
tm_layout(title = "Cases, colored by nearest clinic")
```


### Buffers {.unnumbered} 

We can also explore how many cases are located within 2.5km (~30 mins) walking distance from the closest health facility.

*Note: For more accurate distance calculations, it is better to re-project your sf object to the respective local map projection system such as UTM (Earth projected onto a planar surface). In this example, for simplicity we will stick to the World Geodetic System (WGS84) Geograhpic coordinate system (Earth represented in a spherical / round surface, therefore the units are in decimal degrees). We will use a general conversion of: 1 decimal degree = ~111km.*  

See more information about map projections and coordinate systems at this [esri article](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/). This [blog](http://www.geo.hunter.cuny.edu/~jochen/gtech201/lectures/lec6concepts/map%20coordinate%20systems/how%20to%20choose%20a%20projection.htm) talks about different types of map projection and how one can choose a suitable projection depending on the area of interest and the context of your map / analysis.


**First**, create a circular buffer with a radius of ~2.5km around each health facility. This is done with the function `st_buffer()` from **tmap**. Because the unit of the map is in lat/long decimal degrees, that is how "0.02" is interpreted. If your map coordinate system is in meters, the number must be provided in meters.  

```{r, warning=F, message=F}
sle_hf_2k <- sle_hf %>%
  st_buffer(dist=0.02)       # decimal degrees translating to approximately 2.5km 
```

Below we plot the buffer zones themselves, with the :  

```{r, warning=F, message=F}
tmap_mode("plot")
# Create circular buffers
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2)+
tm_shape(sle_hf) +                    # plot clinic facilities in large red dots
  tm_dots(size=0.3, col='black')      
```


**Second*, we intersect these buffers with the cases (points) using `st_join()` and the join type of *st_intersects*. That is, the data from the buffers are joined to the points that they intersect with. 

```{r, warning=F, message=F}
# Intersect the cases with the buffers
linelist_sf_hf_2k <- linelist_sf_hf %>%
  st_join(sle_hf_2k, join = st_intersects, left = TRUE) %>%
  filter(osm_id.x==osm_id.y | is.na(osm_id.y)) %>%
  select(case_id, osm_id.x, nearest_clinic, amenity.x, osm_id.y)
```

Now we can count the results: ` nrow(linelist_sf_hf_2k[is.na(linelist_sf_hf_2k$osm_id.y),])` out of 1000 cases did not intersect with any buffer (that value is missing), and so live more than 30 mins walk from the nearest health facility.

```{r}
# Cases which did not get intersected with any of the health facility buffers
linelist_sf_hf_2k %>% 
  filter(is.na(osm_id.y)) %>%
  nrow()
```

We can visualize the results such that cases that did not intersect with any buffer appear in red.  

```{r, out.width = '100%', warning=F, message=F}
tmap_mode("view")

# First display the cases in points
tm_shape(linelist_sf_hf) +
  tm_dots(size=0.08, col='nearest_clinic') +

# plot clinic facilities in large black dots
tm_shape(sle_hf) +                    
  tm_dots(size=0.3, col='black')+   

# Then overlay the health facility buffers in polylines
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2) +

# Highlight cases that are not part of any health facility buffers
# in red dots  
tm_shape(linelist_sf_hf_2k %>%  filter(is.na(osm_id.y))) +
  tm_dots(size=0.1, col='red') +
tm_view(set.view = c(-13.2284,8.4699, 13), set.zoom.limits = c(13,14))+

# add title  
tm_layout(title = "Cases by clinic catchment area")

```


### Other spatial joins {.unnumbered}  

Alternative values for argument `join` include (from the [documentation](https://r-spatial.github.io/sf/reference/st_join.html))

* st_contains_properly  
* st_contains  
* st_covered_by  
* st_covers  
* st_crosses  
* st_disjoint  
* st_equals_exact  
* st_equals  
* st_is_within_distance  
* st_nearest_feature  
* st_overlaps  
* st_touches  
* st_within  





## Choropleth maps {}  


Choropleth maps can be useful to visualize your data by pre-defined area, usually administrative unit or health area. In outbreak response this can help to target resource allocation for specific areas with high incidence rates, for example.

Now that we have the administrative unit names assigned to all cases (see section on spatial joins, above), we can start mapping the case counts by area (choropleth maps).

Since we also have population data by ADM3, we can add this information to the *case_adm3* table created previously.

We begin with the dataframe created in the previous step `case_adm3`, which is a summary table of each administrative unit and its number of cases.  

1) The population data `sle_adm3_pop` are joined using a `left_join()` from **dplyr** on the basis of common values across column `admin3pcod` in the `case_adm3` dataframe, and column `adm_pcode` in the `sle_adm3_pop` dataframe. See page on [Joining data]).  
2) `select()` is applied to the new dataframe, to keep only the useful columns - `total` is total population  
3) Cases per 10,000 populaton is calculated as a new column with `mutate()`  


```{r}
# Add population data and calculate cases per 10K population
case_adm3 <- case_adm3 %>% 
     left_join(sle_adm3_pop,                             # add columns from pop dataset
               by = c("admin3pcod" = "adm3_pcode")) %>%  # join based on common values across these two columns
     select(names(case_adm3), total) %>%                 # keep only important columns, including total population
     mutate(case_10kpop = round(cases/total * 10000, 3)) # make new column with case rate per 10000, rounded to 3 decimals

case_adm3                                                # print to console for viewing
```

Join this table with the ADM3 polygons shapefile for mapping

```{r, warning=F, message=F}
case_adm3_sf <- case_adm3 %>%                 # begin with cases & rate by admin unit
  left_join(sle_adm3, by="admin3pcod") %>%    # join to shapefile data by common column
  select(objectid, admin3pcod,                # keep only certain columns of interest
         admin3name = admin3name.x,           # clean name of one column
         admin2name, admin1name,
         cases, total, case_10kpop,
         geometry) %>%                        # keep geometry so polygons can be plotted
  drop_na(objectid) %>%                       # drop any empty rows
  st_as_sf()                                  # convert to shapefile

```


Mapping the results

```{r, message=F, warning=F}
# tmap mode
tmap_mode("plot")               # view static map

# plot polygons
tm_shape(case_adm3_sf) + 
        tm_polygons("cases") +  # color by number of cases column
        tm_text("admin3name")   # name display
```

We can also map the incidence rates  


```{r, warning=F, message=F}
# Cases per 10K population
tmap_mode("plot")             # static viewing mode

# plot
tm_shape(case_adm3_sf) +                # plot polygons
  tm_polygons("case_10kpop",            # color by column containing case rate
              breaks=c(0, 10, 50, 100), # define break points for colors
              palette = "Purples"       # use a purple color palette
              ) +
  tm_text("admin3name")                 # display text

```

## Mapping with ggplot2
If you are already familiar with using **ggplot2**, you can use that package instead to create static maps of your data. The `geom_sf()` function will draw different objects based on which features (points, lines, or polygons) are in your data. For example, you can use `geom_sf()` in a `ggplot()` using `sf` data with polygon geometry to create a choropleth map.

To illustrate how this works, we can start with the ADM3 polygons shapefile that we used earlier. Recall that these are Admin Level 3 regions in Sierra Leone:

```{r}
sle_adm3
```
We can use the `left_join()` function from **dplyr** to add the data we would like to map to the shapefile object. In this case, we are going to use the `case_adm3` data frame that we created earlier to summarize case counts by administrative region; however, we can use this same approach to map any data stored in a data frame.
```{r}
sle_adm3_dat <- sle_adm3 %>% 
  inner_join(case_adm3, by = "admin3pcod") # inner join = retain only if in both data objects

select(sle_adm3_dat, admin3name.x, cases) # print selected variables to console
```

To make a column chart of case counts by region, using **ggplot2**, we could then call `geom_col()` as follows:

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +
  geom_col(aes(x=fct_reorder(admin3name.x, cases, .desc=T),   # reorder x axis by descending 'cases'
               y=cases)) +                                  # y axis is number of cases by region
  theme_bw() +
  labs(                                                     # set figure text
    title="Number of cases, by administrative unit",
    x="Admin level 3",
    y="Number of cases"
  ) + 
  guides(x=guide_axis(angle=45))                            # angle x-axis labels 45 degrees to fit better

```

If we want to use **ggplot2** to instead make a choropleth map of case counts, we can use similar syntax to call the `geom_sf()` function:

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) + 
  geom_sf(aes(fill=cases))    # set fill to vary by case count variable

```

We can then customize the appearance of our map using grammar that is consistent across **ggplot2**, for example:
```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +                           
  geom_sf(aes(fill=cases)) +						
  scale_fill_continuous(high="#54278f", low="#f2f0f7") +    # change color gradient
  theme_bw() +
  labs(title = "Number of cases, by administrative unit",   # set figure text
       subtitle = "Admin level 3"
  )
```

For R users who are comfortable working with **ggplot2**, `geom_sf()` offers a simple and direct implementation that is suitable for basic map visualizations. To learn more, read the [geom_sf() vignette](https://ggplot2.tidyverse.org/reference/ggsf.html) or the [ggplot2 book](https://ggplot2-book.org/maps.html). 





<!-- ======================================================= -->
## Basemaps { }

### OpenStreetMap {.unnumbered} 

Below we describe how to achieve a basemap for a **ggplot2** map using OpenStreetMap features. Alternative methods include using **ggmap** which requires free registration with Google ([details](https://www.earthdatascience.org/courses/earth-analytics/lidar-raster-data-r/ggmap-basemap/)).  

[**OpenStreetMap**](https://en.wikipedia.org/wiki/OpenStreetMap) is a collaborative project to create a free editable map of the world. The underlying geolocation data (e.g. locations of cities, roads, natural features, airports, schools, hospitals, roads etc) are considered the primary output of the project.

First we load the **OpenStreetMap** package, from which we will get our basemap.  

Then, we create the object `map`, which we define using the function `openmap()` from **OpenStreetMap** package ([documentation](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/openmap)). We provide the following:  

* `upperLeft` and `lowerRight` Two coordinate pairs specifying the limits of the basemap tile  
  * In this case we've put in the max and min from the linelist rows, so the map will respond dynamically to the data  
* `zoom = ` (if null it is determined automatically)  
* `type =` which type of basemap - we have listed several possibilities here and the code is currently using the first one (`[1]`) "osm"  
* `mergeTiles = ` we chose TRUE so the basetiles are all merged into one


```{r, message=FALSE, warning=FALSE}
# load package
pacman::p_load(OpenStreetMap)

# Fit basemap by range of lat/long coordinates. Choose tile type
map <- OpenStreetMap::openmap(
  upperLeft = c(max(linelist$lat, na.rm=T), max(linelist$lon, na.rm=T)),   # limits of basemap tile
  lowerRight = c(min(linelist$lat, na.rm=T), min(linelist$lon, na.rm=T)),
  zoom = NULL,
  type = c("osm", "stamen-toner", "stamen-terrain", "stamen-watercolor", "esri","esri-topo")[1])
```

If we plot this basemap right now, using `autoplot.OpenStreetMap()` from **OpenStreetMap** package, you see that the units on the axes are not latitude/longitude coordinates. It is using a different coordinate system. To correctly display the case residences (which are stored in lat/long), this must be changed.  

```{r, warning=F, message=F}
autoplot.OpenStreetMap(map)
```
Thus, we want to convert the map to latitude/longitude with the `openproj()` function from **OpenStreetMap** package. We provide the basemap `map` and also provide the Coordinate Reference System (CRS) we want. We do this by providing the "proj.4" character string for the WGS 1984 projection, but you can provide the CRS in other ways as well. (see [this page](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/understand-epsg-wkt-and-other-crs-definition-file-types/) to better understand what a proj.4 string is)  

```{r, warning=F, message=F}
# Projection WGS84
map_latlon <- openproj(map, projection = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
```

Now when we create the plot we see that along the axes are latitude and longitude coordinate. The coordinate system has been converted. Now our cases will plot correctly if overlaid!  

```{r, warning=F, message=F}
# Plot map. Must use "autoplot" in order to work with ggplot
autoplot.OpenStreetMap(map_latlon)
```

See the tutorials [here](http://data-analytics.net/cep/Schedule_files/geospatial.html) and [here](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/autoplot.OpenStreetMap) for more info.  





## Contoured density heatmaps {}

Below we describe how to achieve a contoured density heatmap of cases, over a basemap, beginning with a linelist (one row per case).  

1) Create basemap tile from OpenStreetMap, as described above  
2) Plot the cases from `linelist` using the latitude and longitude columns  
3) Convert the points to a density heatmap with `stat_density_2d()` from **ggplot2**, 


When we have a basemap with lat/long coordinates, we can plot our cases on top using the lat/long coordinates of their residence. 

Building on the function `autoplot.OpenStreetMap()` to create the basemap, **ggplot2** functions will easily add on top, as shown with `geom_point()` below:  

```{r, warning=F, message=F}
# Plot map. Must be autoplotted to work with ggplot
autoplot.OpenStreetMap(map_latlon)+                 # begin with the basemap
  geom_point(                                       # add xy points from linelist lon and lat columns 
    data = linelist,                                
    aes(x = lon, y = lat),
    size = 1, 
    alpha = 0.5,
    show.legend = FALSE) +                          # drop legend entirely
  labs(x = "Longitude",                             # titles & labels
       y = "Latitude",
       title = "Cumulative cases")

```
The map above might be difficult to interpret, especially with the points overlapping. So you can instead plot a 2d density map using the **ggplot2** function `stat_density_2d()`. You are still using the linelist lat/lon coordinates, but a 2D kernel density estimation is performed and the results are displayed with contour lines - like a topographical map. Read the full [documentation here](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).  


```{r, warning=F, message=F}
# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases")

```





<!-- ======================================================= -->
### Time series heatmap {.unnumbered}

The density heatmap above shows *cumulative cases*. We can examine the outbreak over time and space by faceting the heatmap based on the *month of symptom onset*, as derived from the linelist.  

We begin in the `linelist`, creating a new column with the Year and Month of onset. The `format()` function from **base** R changes how a date is displayed. In this case we want "YYYY-MM".  

```{r, warning=F, message=F}
# Extract month of onset
linelist <- linelist %>% 
  mutate(date_onset_ym = format(date_onset, "%Y-%m"))

# Examine the values 
table(linelist$date_onset_ym, useNA = "always")
```

Now, we simply introduce facetting via **ggplot2** to the density heatmap. `facet_wrap()` is applied, using the new column as rows. We set the number of facet columns to 3 for clarity.  


```{r, warning=F, message=F}
# packages
pacman::p_load(OpenStreetMap, tidyverse)

# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases over time")+
  
  # facet the plot by month-year of onset
  facet_wrap(~ date_onset_ym, ncol = 4)               

```



<!-- SPATIAL STATISTICS SECTION IS UNDER DEVELOPMENT --> 
## Spatial statistics
Most of our discussion so far has focused on visualization of spatial data. In some cases, you may also be interested in using *spatial statistics* to quantify the spatial relationships of attributes in your data. This section will provide a very brief overview of some key concepts in spatial statistics, and suggest some resources that will be helpful to explore if you wish to do more comprehensive spatial analyses. 

### Spatial relationships {.unnumbered}  

Before we can calculate any spatial statistics, we need to specify the relationships between features in our data. There are many ways to conceptualize spatial relationships, but a simple and commonly-applicable model to use is that of *adjacency* - specifically, that we expect a geographic relationship between areas that share a border or “neighbour” one another. 

We can quantify adjacency relationships between administrative region polygons in the `sle_adm3` data we have been using with the **spdep** package. We will specify *queen* contiguity, which means that regions will be neighbors if they share at least one point along their borders. The alternative would be *rook* contiguity, which requires that regions share an edge - in our case, with irregular polygons, the distinction is trivial, but in some cases the choice between queen and rook can be influential.  
 
```{r}
sle_nb <- spdep::poly2nb(sle_adm3_dat, queen=T) # create neighbors 
sle_adjmat <- spdep::nb2mat(sle_nb)    # create matrix summarizing neighbor relationships
sle_listw <- spdep::nb2listw(sle_nb)   # create listw (list of weights) object -- we will need this later

sle_nb
round(sle_adjmat, digits = 2)
```

The matrix printed above shows the relationships between the 9 regions in our `sle_adm3` data. A score of 0 indicates two regions are not neighbors, while any value other than 0 indicates a neighbor relationship. The values in the matrix are scaled so that each region has a total row weight of 1.

A better way to visualize these neighbor relationships is by plotting them:
```{r, fig.align='center', results='hide'}
plot(sle_adm3_dat$geometry) +                                           # plot region boundaries
  spdep::plot.nb(sle_nb,as(sle_adm3_dat, 'Spatial'), col='grey', add=T) # add neighbor relationships
```

We have used an adjacency approach to identify neighboring polygons; the neighbors we identified are also sometimes called **contiguity-based neighbors**. But this is just one way of choosing which regions are expected to have a geographic relationship. The most common alternative approaches for identifying geographic relationships generate **distance-based neighbors**; briefly, these are:
  
  * **K-nearest neighbors** - Based on the distance between centroids (the geographically-weighted center of each polygon region), select the *n* closest regions as neighbors. A maximum-distance proximity threshold may also be specified. In **spdep**, you can use `knearneigh()` (see [documentation](https://r-spatial.github.io/spdep/reference/knearneigh.html)).
  
  * **Distance threshold neighbors** - Select all neighbors within a distance threshold. In **spdep**, these neighbor relationships can be identified using `dnearneigh()` (see [documentation](https://www.rdocumentation.org/packages/spdep/versions/1.1-7/topics/dnearneigh)).

### Spatial autocorrelation {.unnumbered}  

Tobler's oft-cited first law of geography states that "everything is related to everything else, but near things are more related than distant things." In epidemiology, this often means that risk of a particular health outcome in a given region is more similar to its neighboring regions than to those far away. This concept has been formalized as **spatial autocorrelation** - the statistical property that geographic features with similar values are clustered together in space. Statistical measures of spatial autocorrelation can be used to *quantify the extent of spatial clustering* in your data, *locate where clustering occurs*, and *identify shared patterns of spatial autocorrelation* between distinct variables in your data. This section gives an overview of some common measures of spatial autocorrelation and how to calculate them in R.

**Moran's I** - This is a global summary statistic of the correlation between the value of a variable in one region, and the values of the same variable in neighboring regions. The Moran's I statistic typically ranges from -1 to 1. A value of 0 indicates no pattern of spatial correlation, while values closer to 1 or -1 indicate stronger spatial autocorrelation (similar values close together) or spatial dispersion (dissimilar values close together), respectively.

For an example, we will calculate a Moran's I statistic to quantify the spatial autocorrelation in Ebola cases we mapped earlier (remember, this is a subset of cases from the simulated epidemic `linelist` dataframe). The **spdep** package has a function, `moran.test`, that can do this calculation for us:

```{r}
moran_i <-spdep::moran.test(sle_adm3_dat$cases,    # numeric vector with variable of interest
                            listw=sle_listw)       # listw object summarizing neighbor relationships

moran_i                                            # print results of Moran's I test
```
The output from the `moran.test()` function shows us a Moran I statistic of ` round(moran_i$estimate[1],2)`. This indicates the presence of spatial autocorrelation in our data - specifically, that regions with similar numbers of Ebola cases are likely to be close together. The p-value provided by `moran.test()` is generated by comparison to the expectation under null hypothesis of no spatial autocorrelation, and can be used if you need to report the results of a formal hypothesis test.

**Local Moran's I** - We can decompose the (global) Moran's I statistic calculated above to identify *localized* spatial autocorrelation; that is, to identify specific clusters in our data. This statistic, which is sometimes called a **Local Indicator of Spatial Association (LISA)** statistic, summarizes the extent of spatial autocorrelation around each individual region. It can be useful for finding "hot" and "cold" spots on the map.

To show an example, we can calculate and map Local Moran's I for the Ebola case counts used above, with the `local_moran()` function from **spdep**:
```{r, fig.align='center'}
# calculate local Moran's I
local_moran <- spdep::localmoran(                  
  sle_adm3_dat$cases,                              # variable of interest
  listw=sle_listw                                  # listw object with neighbor weights
)

# join results to sf data
sle_adm3_dat<- cbind(sle_adm3_dat, local_moran)    

# plot map
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=Ii)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Local Moran's I") +
  labs(title="Local Moran's I statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```

**Getis-Ord Gi\*** - This is another statistic that is commonly used for hotspot analysis; in large part, the popularity of this statistic relates to its use in the Hot Spot Analysis tool in ArcGIS. It is based on the assumption that typically, the difference in a variable's value between neighboring regions should follow a normal distribution. It uses a z-score approach to identify regions that have significantly higher (hot spot) or significantly lower (cold spot) values of a specified variable, compared to their neighbors. 

We can calculate and map the Gi* statistic using the `localG()` function from **spdep**:  

```{r}
# Perform local G analysis
getis_ord <- spdep::localG(
  sle_adm3_dat$cases,
  sle_listw
)

# join results to sf data
sle_adm3_dat$getis_ord <- getis_ord

# plot map
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=getis_ord)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Gi*") +
  labs(title="Getis-Ord Gi* statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```


As you can see, the map of Getis-Ord Gi* looks slightly different from the map of Local Moran's I produced earlier. This reflects that the method used to calculate these two statistics are slightly different; which one you should use depends on your specific use case and the research question of interest.

**Lee's L test** - This is a statistical test for bivariate spatial correlation. It allows you to test whether the spatial pattern for a given variable *x* is similar to the spatial pattern of another variable, *y*, that is hypothesized to be related spatially to *x*. 

To give an example, let's test whether the spatial pattern of Ebola cases from the simulated epidemic is correlated with the spatial pattern of population. To start, we need to have a `population` variable in our `sle_adm3` data. We can use the `total` variable from the `sle_adm3_pop` dataframe that we loaded earlier.

```{r}
sle_adm3_dat <- sle_adm3_dat %>% 
  rename(population = total)                          # rename 'total' to 'population'
```

We can quickly visualize the spatial patterns of the two variables side by side, to see whether they look similar:
```{r, fig.align='center', warning=F, message=F}
tmap_mode("plot")

cases_map <- tm_shape(sle_adm3_dat) + tm_polygons("cases") + tm_layout(main.title="Cases")
pop_map <- tm_shape(sle_adm3_dat) + tm_polygons("population") + tm_layout(main.title="Population")

tmap_arrange(cases_map, pop_map, ncol=2)   # arrange into 2x1 facets
```

Visually, the patterns seem dissimilar. We can use the `lee.test()` function in **spdep** to test statistically whether the pattern of spatial autocorrelation in the two variables is related. The L statistic will be close to 0 if there is no correlation between the patterns, close to 1 if there is a strong positive correlation (i.e. the patterns are similar), and close to -1 if there is a strong negative correlation (i.e. the patterns are inverse). 

```{r, warning=F, message=F}
lee_test <- spdep::lee.test(
  x=sle_adm3_dat$cases,          # variable 1 to compare
  y=sle_adm3_dat$population,     # variable 2 to compare
  listw=sle_listw                # listw object with neighbor weights
)

lee_test
```

The output above shows that the Lee's L statistic for our two variables was ` round(lee_test$estimate[1],2)`, which indicates weak negative correlation. This confirms our visual assessment that the pattern of cases and population are not related to one another, and provides evidence that the spatial pattern of cases is not strictly a result of population density in high-risk areas.

The Lee L statistic can be useful for making these kinds of inferences about the relationship between spatially distributed variables; however, to describe the nature of the relationship between two variables in more detail, or adjust for confounding, *spatial regression* techniques will be needed. These are described briefly in the following section.

### Spatial regression {.unnumbered}  

You may wish to make statistical inferences about the relationships between variables in your spatial data. In these cases, it is useful to consider *spatial regression* techniques - that is, approaches to regression that explicitly consider the spatial organization of units in your data. Some reasons that you may need to consider spatial regression models, rather than standard regression models such as GLMs, include:

  * Standard regression models assume that residuals are independent from one another. In the presence of strong *spatial autocorrelation*, the residuals of a standard regression model are likely to be spatially autocorrelated as well, thus violating this assumption. This can lead to problems with interpreting the model results, in which case a spatial model would be preferred.
  
  * Regression models also typically assume that the effect of a variable *x* is constant over all observations. In the case of *spatial heterogeneity*, the effects we wish to estimate may vary over space, and we may be interested in quantifying those differences. In this case, spatial regression models offer more flexibility for estimating and interpreting effects.
  
The details of spatial regression approaches are beyond the scope of this handbook. This section will instead provide an overview of the most common spatial regression models and their uses, and refer you to references that may of use if you wish to explore this area further.

**Spatial error models** - These models assume that the error terms across spatial units are correlated, in which case the data would violate the assumptions of a standard OLS model. Spatial error models are also sometimes referred to as **simultaneous autoregressive (SAR) models**. They can be fit using the `errorsarlm()` function in the **spatialreg** package (spatial regression functions which used to be a part of **spdep**). 

**Spatial lag models** - These models assume that the dependent variable for a region *i* is influenced not only by value of independent variables in *i*, but also by the values of those variables in regions neighboring *i*. Like spatial error models, spatial lag models are also sometimes described as **simultaneous autoregressive (SAR) models**.  They can be fit using the `lagsarlm()` function in the **spatialreg** package.

The **spdep** package contains several useful diagnostic tests for deciding between standard OLS, spatial lag, and spatial error models. These tests, called *Lagrange Multiplier diagnostics*, can be used to identify the type of spatial dependence in your data and choose which model is most appropriate. The function `lm.LMtests()` can be used to calculate all of the Lagrange Multiplier tests. Anselin (1988) also provides a useful flow chart tool to decide which spatial regression model to use based on the results of the Lagrange Multiplier tests:

```{r, fig.align='center', echo=F}
knitr::include_graphics(here::here("images", "gis_lmflowchart.jpg"))
```

**Bayesian hierarchical models** - Bayesian approaches are commonly used for some applications in spatial analysis, most commonly for [disease mapping](https://pubmed.ncbi.nlm.nih.gov/15690999/). They are preferred in cases where case data are sparsely distributed (for example, in the case of a rare outcome) or statistically "noisy", as they can be used to generate "smoothed" estimates of disease risk by accounting for the underlying latent spatial process. This may improve the quality of estimates. They also allow investigator pre-specification (via choice of prior) of complex spatial correlation patterns that may exist in the data, which can account for spatially-dependent and -independent variation in both independent and dependent variables. In R, Bayesian hierarchical models can be fit using the **CARbayes** package (see [vignette](https://cran.r-project.org/web/packages/CARBayes/vignettes/CARBayes.pdf)) or R-INLA (see [website](https://www.r-inla.org/home) and [textbook](https://becarioprecario.bitbucket.io/inla-gitbook/)). R can also be used to call external software that does Bayesian estimation, such as JAGS or WinBUGS.

<!-- ======================================================= -->
## Resources {  }

* R Simple Features and sf package [vignette](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)


* R tmap package [vignette](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html)


* ggmap: [Spatial Visualization with ggplot2](https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf)


* [Intro to making maps with R, overview of different packages](https://bookdown.org/nicohahn/making_maps_with_r5/docs/introduction.html)  

* Spatial Data in R [(EarthLab course)](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/)

* Applied Spatial Data Analysis in R [textbook](https://link.springer.com/book/10.1007/978-1-4614-7618-4)

* **SpatialEpiApp** - a [Shiny app that is downloadable as an R package](https://github.com/Paula-Moraga/SpatialEpiApp), allowing you to provide your own data and conduct mapping, cluster analysis, and spatial statistics.  


* An Introduction to Spatial Econometrics in R [workshop](http://www.econ.uiuc.edu/~lab/workshop/Spatial_in_R.html)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/gis.Rmd-->

# (PART) Data Visualization {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_viz.Rmd-->


# Tables for presentation { }  


```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  #bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```


This page demonstrates how to convert summary data frames into presentation-ready tables with the **flextable** package. These tables can be inserted into powerpoint slides, HTML pages, PDF or Word documents, etc.  

Understand that *before* using **flextable**, you must create the summary table as a data frame. Use methods from the [Descriptive tables] and [Pivoting data] pages such as tabulations, cross-tabulations, pivoting, and calculating descriptive statistics. The resulting data frame can then be passed to **flextable** for display formatting.  


There are many other R packages that can be used to craft tables for presentation - we chose to highlight **flextable** in this page. An example using the **knitr** package and its `kable()` function can be found in the [Contact Tracing] page. Likewise, the **DT** package is highlighted in the page [Dashboards with Shiny]. Others such as **GT** and **huxtable** are mentione in the [Suggested packages] page.  



<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered} 

Install and load **flextable**. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

```

### Import data {.unnumbered}  

To begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details). 


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Prepare table {.unnumbered}  

*Before* beginning to use **flextable** you will need to *create* your table as a data frame. See the page on [Descriptive tables] and [Pivoting data] to learn how to create a data frame using packages such as **janitor** and **dplyr**. You must arrange the content in rows and columns as you want it displayed. Then, the data frame will be passed to **flextable** to display it with colors, headers, fonts, etc. 
  
Below is an example from the [Descriptive tables] page of converting the case `linelist` into a data frame that summarises patient outcomes and CT values by hospital, with a Totals row at the bottom. The output is saved as `table`.  

```{r message=FALSE, warning=FALSE}
table <- linelist %>% 
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known)                                    # Arrange rows from lowest to highest (Total row at bottom)

table  # print

```




<!-- ======================================================= -->
## Basic flextable {  }

### Create a flextable {.unnumbered}  

To create and manage **flextable** objects, we first pass the data frame through the `flextable()` function. We save the result as `my_table`.  

```{r}

my_table <- flextable(table) 
my_table

```

After doing this, we can progressively pipe the `my_table` object through more **flextable** formatting functions.  

In this page for sake of clarity we will save the table at intermediate steps as `my_table`, adding **flextable** functions bit-by-bit. If you want to see *all* the code from beginning to end written in one chunk, visit the [All code together](#tbl_pres_all) section below.  

The general syntax of each line of **flextable** code is as follows:

* `function(table, i = X, j = X, part = "X")`, where:
  * The 'function' can be one of many different functions, such as `width()` to determine column widths, `bg()` to set background colours, `align()` to set whether text is centre/right/left aligned, and so on. 
  * `table = ` is the name of the data frame, although does not need to be stated if the data frame is piped into the function.
  * `part = ` refers to which part of the table the function is being applied to. E.g. "header", "body" or "all". 
  * `i = ` specifies the *row* to apply the function to, where 'X' is the row number. If multiple rows, e.g. the first to third rows, one can specify: `i = c(1:3)`. Note if 'body' is selected, the first row starts from underneath the header section.
  * `j = ` specifies the *column* to apply the function to, where 'x' is the column number or name. If multiple columns, e.g. the fifth and sixth, one can specify: `j = c(5,6)`. 
  
You can find the complete list of **flextable** formatting function [here](https://davidgohel.github.io/flextable/reference/index.html) or review the documentation by entering `?flextable`.  


### Column width {.unnumbered}

We can use the `autofit()` function, which nicely stretches out the table so that each cell only has one row of text. The function `qflextable()` is a convenient shorthand for `flextable()` and `autofit()`.  

```{r}

my_table %>% autofit()

```

However, this might not always be appropriate, especially if there are very long values within cells, meaning the table might not fit on the page. 

Instead, we can specify widths with the `width()` function. It can take some playing around to know what width value to put. In the example below, we specify different widths for column 1, column 2, and columns 4 to 8. 

```{r}

my_table <- my_table %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1)

my_table
  
```

### Column headers {.unnumbered}

We want more clearer headers for easier interpretation of the table contents.

For this table, we will want to add a second header layer so that columns covering the same subgroups can be grouped together. We do this with the `add_header_row()` function with `top = TRUE`. We provide the new name of each column to `values = `, leaving empty values `""` for columns we know we will merge together later.  

We also rename the header names in the now-second header in a separate `set_header_labels()` command.  

Finally, to "combine" certain column headers in the top header we use `merge_at()` to merge the column headers in the top header row.  

```{r}
my_table <- my_table %>% 
  
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    
  set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header")     # Horizontally merge columns 6 to 8 in new header row

my_table  # print

```

### Borders and background {.unnumbered}  

You can adjust the borders, internal lines, etc. with various **flextable** functions. It is often easier to start by removing all existing borders with `border_remove()`.  

Then, you can apply default border themes by passing the table to `theme_box()`, `theme_booktabs()`, or `theme_alafoli()`.  

You can add vertical and horizontal lines with a variety of functions. `hline()` and `vline()` add lines to a specified row or column, respectively. Within each, you must specify the `part = ` as either "all", "body", or "header". For vertical lines, specify the column to `j = `, and for horizontal lines the row to `i = `. Other functions like `vline_right()`, `vline_left()`, `hline_top()`, and `hline_bottom()` add lines to the outsides only.  

In all of these functions, the actual line style itself must be specified to `border = ` and must be the output of a separate command using the `fp_border()` function from the **officer** package. This function helps you define the width and color of the line. You can define this above the table commands, as shown below.  

```{r}
# define style for border line
border_style = officer::fp_border(color="black", width=1)

# add border lines to table
my_table <- my_table %>% 

  # Remove all existing borders
  border_remove() %>%  
  
  # add horizontal lines via a pre-determined theme setting
  theme_booktabs() %>% 
  
  # add vertical lines to separate Recovered and Died sections
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style)       # at column 5

my_table
```

### Font and alignment {.unnumbered}

We centre-align all columns aside from the left-most column with the hospital names, using the `align()` function from **flextable**.

```{r}
my_table <- my_table %>% 
   flextable::align(align = "center", j = c(2:8), part = "all") 
my_table
```

Additionally, we can increase the header font size and change then to bold. We can also change the total row to bold.  

```{r}

my_table <-  my_table %>%  
  fontsize(i = 1, size = 12, part = "header") %>%   # adjust font size of header
  bold(i = 1, bold = TRUE, part = "header") %>%     # adjust bold face of header
  bold(i = 7, bold = TRUE, part = "body")           # adjust bold face of total row (row 7 of body)

my_table

```


We can ensure that the proportion columns display only one decimal place using the function `colformat_num()`. Note this could also have been done at data management stage with the `round()` function. 

```{r}
my_table <- colformat_num(my_table, j = c(4,7), digits = 1)
my_table
```

### Merge cells {.unnumbered}  

Just as we merge cells horizontally in the header row, we can also merge cells vertically using `merge_at()` and specifying the rows (`i`) and column (`j`). Here we merge the "Hospital" and "Total cases with known outcome" values vertically to give them more space.   

```{r}
my_table <- my_table %>% 
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header")

my_table
```

### Background color {.unnumbered}

To distinguish the content of the table from the headers, we may want to add additional formatting. e.g. changing the background color. In this example we change the table body to gray.

```{r}
my_table <- my_table %>% 
    bg(part = "body", bg = "gray95")  

my_table 
```


<!-- ======================================================= -->
## Conditional formatting {  }

We can highlight all values in a column that meet a certain rule, e.g. where more than 55% of cases died. Simply put the criteria to the `i = ` or `j = ` argument, preceded by a tilde `~`. Reference the column in the data frame, not the display heading values.  

```{r}

my_table %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") 

```



Or, we can highlight the entire row meeting a certain criterion, such as a hospital of interest. To do this we just remove the column (`j`) specification so the criteria apply to all columns.


```{r}

my_table %>% 
  bg(., i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") 

```

## All code together {#tbl_pres_all}  


Below we show all the code from the above sections together.  

```{r}  

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 

  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>%              # table is piped in from above
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```


<!-- ======================================================= -->
## Saving your table {  }

There are different ways the table can be integrated into your output. 

### Save single table {.unnumbered}

You can export the tables to Word, PowerPoint or HTML or as an image (PNG) files. To do this, use one of the following functions:

* `save_as_docx()`  
* `save_as_pptx()`  
* `save_as_image()`  
* `save_as_html()`  

For instance below we save our table as a word document. Note the syntax of the first argument - you can just provide the name of your flextable object e.g. `my_table`, or you can give is a "name" as shown below (the name is "my table"). If name, this will appear as the title of the table in Word. We also demonstrate code to save as PNG image.  

```{r message=FALSE, warning=FALSE, eval=F}
# Edit the 'my table' as needed for the title of table.  
save_as_docx("my table" = my_table, path = "file.docx")

save_as_image(my_table, path = "file.png")
```

Note the packages `webshot` or `webshot2` are required to save a flextable as an image. Images may come out with transparent backgrounds.

If you want to view a 'live' version of the **flextable** output in the intended document format, use `print()` and specify one of the below to `preview = `. The document will "pop-up" open on your computer in the specified software program, but will not be saved. This can be useful to check if the table fits in one page/slide or so you can quickly copy it into another document, you can use the print method with the argument preview set to “pptx” or “docx”.  

```{r, eval=F}
print(my_table, preview = "docx") # Word document example
print(my_table, preview = "pptx") # Powerpoint example
```

### Print table in R markdown {.unnumbered}  

This table can be integrated into your an automated document, an R markdown output, if the table object is called within the R markdown chunk. This means the table can be updated as part of a report where the data might change, so the numbers can be refreshed.

See detail in the [Reports with R Markdown] page of this handbook. 

<!-- ======================================================= -->
## Resources {  }

The full **flextable** book is here: https://ardata-fr.github.io/flextable-book/
The Github site is [here](https://davidgohel.github.io/flextable/)  
A manual of all the **flextable** functions can be found [here](https://davidgohel.github.io/flextable/reference/index.html)

A gallery of beautiful example **flextable** tables with code can be accessed [here](https://ardata-fr.github.io/flextable-gallery/gallery/)  
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_presentation.Rmd-->


# ggplot basics {}

```{r, out.width=c('100%', '100%'), fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "ggplot_basics_top.png"))
```


**ggplot2** is the most popular data visualisation R package. Its `ggplot()` function is at the core of this package, and this whole approach is colloquially known as *"ggplot"* with the resulting figures sometimes affectionately called "ggplots". The "gg" in these names reflects the "**g**rammar of **g**raphics" used to construct the figures. **ggplot2** benefits from a wide variety of supplementary R packages that further enhance its functionality.  

The syntax is significantly different from **base** `R` plotting, and has a learning curve associated with it. Using **ggplot2** generally requires the user to format their data in a way that is highly **tidyverse** compatible, which ultimately makes using these packages together very effective.

In this page we will cover the fundamentals of plotting with **ggplot2**. See the page [ggplot tips] for suggestions and advanced techniques to make your plots really look nice.  

There are several extensive **ggplot2** tutorials linked in the resources section. You can also download this [data visualization with ggplot cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf) from the RStudio website. If you want inspiration for ways to creatively visualise your data, we suggest reviewing websites like the [R graph gallery](https://www.r-graph-gallery.com/) and [Data-to-viz](https://www.data-to-viz.com/caveats.html). 



<!-- ======================================================= -->
## Preparation {}

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,      # includes ggplot2 and other data management tools
  rio,            # import/export
  here,           # file locator
  stringr         # working with characters   
)
```

### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below. We will focus on the continuous variables `age`, `wt_kg` (weight in kilos), `ct_blood` (CT values), and `days_onset_hosp` (difference between onset date and hospitalisation).  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### General cleaning {.unnumbered}

When preparing data to plot, it is best to make the data adhere to ["tidy" data standards](https://r4ds.had.co.nz/tidy-data.html) as much as possible. How to achieve this is expanded on in the data management pages of this handbook, such as [Cleaning data and core functions]. 

Some simple ways we can prepare our data to make it better for plotting can include making the contents of the data better for display - which does not necessarily equate to better for data manipulation. For example:  

* Replace `NA` values in a character column with the character string "Unknown"  
* Consider converting column to class *factor* so their values have prescribed ordinal levels  
* Clean some columns so that their "data friendly" values with underscores etc are changed to normal text or title case (see [Characters and strings])  

Here are some examples of this in action:

```{r, }
# make display version of columns with more friendly names
linelist <- linelist %>%
  mutate(
    gender_disp = case_when(gender == "m" ~ "Male",        # m to Male 
                            gender == "f" ~ "Female",      # f to Female,
                            is.na(gender) ~ "Unknown"),    # NA to Unknown
    
    outcome_disp = replace_na(outcome, "Unknown")          # replace NA outcome with "unknown"
  )
```

### Pivoting longer {.unnumbered}

As a matter of data structure, for **ggplot2** we often also want to pivot our data into *longer* formats. Read more about this is the page on [Pivoting data].  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```


For example, say that we want to plot data that are in a "wide" format, such as for each case in the `linelist` and their symptoms. Below we create a mini-linelist called `symptoms_data` that contains only the `case_id` and symptoms columns.  

```{r}
symptoms_data <- linelist %>% 
  select(c(case_id, fever, chills, cough, aches, vomit))
```

Here is how the first 50 rows of this mini-linelist look - see how they are formatted "wide" with each symptom as a column: 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(symptoms_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

If we wanted to plot the number of cases with specific symptoms, we are limited by the fact that each symptom is a specific column. However, we can *pivot* the symptoms columns to a longer format like this:

```{r, }
symptoms_data_long <- symptoms_data %>%    # begin with "mini" linelist called symptoms_data
  
  pivot_longer(
    cols = -case_id,                       # pivot all columns except case_id (all the symptoms columns)
    names_to = "symptom_name",             # assign name for new column that holds the symptoms
    values_to = "symptom_is_present") %>%  # assign name for new column that holds the values (yes/no)
  
  mutate(symptom_is_present = replace_na(symptom_is_present, "unknown")) # convert NA to "unknown"

```


Here are the first 50 rows. Note that case has 5 rows - one for each possible symptom. The new columns `symptom_name` and `symptom_is_present` are the result of the pivot. Note that this format may not be very useful for other operations, but is useful for plotting.

```{r, message=FALSE, echo=F}
DT::datatable(head(symptoms_data_long, 50), rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```






<!-- ======================================================= -->
## Basics of ggplot {}

**"Grammar of graphics" - ggplot2**  

Plotting with **ggplot2** is based on "adding" plot layers and design elements on top of one another, with each command added to the previous ones with a plus symbol (`+`). The result is a multi-layer plot object that can be saved, modified, printed, exported, etc.  

ggplot objects can be highly complex, but the basic order of layers will usually look like this:  

1. Begin with the baseline `ggplot()` command - this "opens" the ggplot and allow subsequent functions to be added with `+`. Typically the dataset is also specified in this command  
2. Add "geom" layers - these functions visualize the data as *geometries* (*shapes*), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with `geom_` as a prefix.  
3. Add design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation  

A simple example of skeleton code is as follows. We will explain each component in the sections below.  

```{r, eval=F}
# plot data from my_data columns as red points
ggplot(data = my_data)+                   # use the dataset "my_data"
  geom_point(                             # add a layer of points (dots)
    mapping = aes(x = col1, y = col2),    # "map" data column to axes
    color = "red")+                       # other specification for the geom
  labs()+                                 # here you add titles, axes labels, etc.
  theme()                                 # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.) 
```

 


## `ggplot()`  

The opening command of any ggplot2 plot is `ggplot()`. This command simply creates a blank canvas upon which to add layers. It "opens" the way for further layers to be added with a `+` symbol.

Typically, the command `ggplot()` includes the `data = ` argument for the plot. This sets the default dataset to be used for subsequent layers of the plot.  

This command will end with a `+` after its closing parentheses. This leaves the command "open". The ggplot will only execute/appear when the full command includes a final layer *without* a `+` at the end.  

```{r, eval=F}
# This will create plot that is a blank canvas
ggplot(data = linelist)
```


## Geoms  

A blank canvas is certainly not sufficient - we need to create geometries (shapes) from our data (e.g. bar plots, histograms, scatter plots, box plots).  

This is done by adding layers "geoms" to the initial `ggplot()` command. There are many **ggplot2** functions that create "geoms". Each of these functions begins with "geom_", so we will refer to them generically as `geom_XXXX()`. There are over 40 geoms in **ggplot2** and many others created by fans. View them at the [ggplot2 gallery](https://exts.ggplot2.tidyverse.org/gallery/). Some common geoms are listed below:  

* Histograms - `geom_histogram()`  
* Bar charts - `geom_bar()` or `geom_col()` (see ["Bar plot" section](#ggplot_basics_bars))  
* Box plots - `geom_boxplot()`  
* Points (e.g. scatter plots) - `geom_point()`  
* Line graphs - `geom_line()` or `geom_path()`  
* Trend lines - `geom_smooth()`  

In one plot you can display one or multiple geoms. Each is added to previous **ggplot2** commands with a `+`, and they are plotted sequentially such that later geoms are plotted on top of previous ones.  



## Mapping data to the plot {#ggplot_basics_mapping}  

Most geom functions must be told *what to use* to create their shapes - so you must tell them how they should *map (assign) columns in your data* to components of the plot like the axes, shape colors, shape sizes, etc. For most geoms, the *essential* components that must be mapped to columns in the data are the x-axis, and (if necessary) the y-axis.  

This "mapping" occurs with the `mapping = ` argument. The mappings you provide to `mapping` must be wrapped in the `aes()` function, so you would write something like `mapping = aes(x = col1, y = col2)`, as shown below.

Below, in the `ggplot()` command the data are set as the case `linelist`. In the `mapping = aes()` argument the column `age` is mapped to the x-axis, and the column `wt_kg` is mapped to the y-axis.  

After a `+`, the plotting commands continue. A shape is created with the "geom" function `geom_point()`. This geom *inherits* the mappings from the `ggplot()` command above - it knows the axis-column assignments and proceeds to visualize those relationships as *points* on the canvas.  

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+
  geom_point()
```

As another example, the following commands utilize the same data, a slightly different mapping, and a different geom. The `geom_histogram()` function only requires a column mapped to the x-axis, as the counts y-axis is generated automatically.  

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()
```


### Plot aesthetics {.unnumbered}  

In ggplot terminology a plot "aesthetic" has a specific meaning. It refers to a visual property of *plotted data*. Note that "aesthetic" here refers to the *data being plotted in geoms/shapes* - not the surrounding display such as titles, axis labels, background color, that you might associate with the word "aesthetics" in common English. In ggplot those details are called "themes" and are adjusted within a `theme()` command (see [this section](#ggplot_basics_themes)).  

Therefore, plot object *aesthetics* can be colors, sizes, transparencies, placement, etc. *of the plotted data*. Not all geoms will have the same aesthetic options, but many can be used by most geoms. Here are some examples:  

* `shape =` Display a point with `geom_point()` as a dot, star, triangle, or square...  
* `fill = ` The interior color (e.g. of a bar or boxplot)  
* `color =` The exterior line of a bar, boxplot, etc., or the point color if using `geom_point()`  
* `size = ` Size (e.g. line thickness, point size)  
* `alpha = ` Transparency (1 = opaque, 0 = invisible)  
* `binwidth = ` Width of histogram bins  
* `width = ` Width of "bar plot" columns  
* `linetype =` Line type (e.g. solid, dashed, dotted) 

These plot object aesthetics can be assigned values in two ways:  

1) Assigned a static value (e.g. `color = "blue"`) to apply across all plotted observations  
2) Assigned to a column of the data (e.g. `color = hospital`) such that display of each observation depends on its value in that column  

<!-- *These non-axis aesthetics can be assigned static values (e.g. `size = 1`) or can be mapped to a column (e.g. `size = age`).* If you want the aesthetic to be assigned a static value, the assignment is placed *outside* the `mapping = aes()`. If you want the aesthetic to be scaled/depend on the value in each row of data, the assignment is made *inside* the `mapping = aes()`.   -->

### Set to a static value {.unnumbered}  

If you want the plot object aesthetic to be static, that is - to be the same for every observation in the data, you write its assignment within the geom but *outside* of any `mapping = aes()` statement. These assignments could look like `size = 1` or `color = "blue"`. Here are two examples:  

* In the first example, the `mapping = aes()` is in the `ggplot()` command and the axes are mapped to age and weight columns in the data. The plot aesthetics `color = `, `size = `, and `alpha = ` (transparency) are assigned to static values. For clarity, this is done in the `geom_point()` function, as you may add other geoms afterward that would take different values for their plot aesthetics.  
* In the second example, the histogram requires only the x-axis mapped to a column. The histogram `binwidth = `, `color = `, `fill = ` (internal color), and `alpha = ` are again set within the geom to static values.  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# scatterplot
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  # set data and axes mapping
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)         # set static point aesthetics

# histogram
ggplot(data = linelist, mapping = aes(x = age))+       # set data and axes
  geom_histogram(              # display histogram
    binwidth = 7,                # width of bins
    color = "red",               # bin line color
    fill = "blue",               # bin interior color
    alpha = 0.1)                 # bin transparency
```


### Scaled to column values {.unnumbered}  

The alternative is to scale the plot object aesthetic by the values in a column. In this approach, the display of this aesthetic will depend on that observation's value in that column of the data. If the column values are continuous, the display scale (legend) for that aesthetic will be continuous. If the column values are discrete, the legend will display each value and the plotted data will appear as distinctly "grouped" (read more in the [grouping](#ggplotgroups) section of this page).  

To achieve this, you map that plot aesthetic to a *column name* (not in quotes). This must be done *within a `mapping = aes()` function* (note: there are several places in the code you can make these mapping assignments, as discussed [below](##ggplot_basics_map_loc)).  

Two examples are below.  

* In the first example, the `color = ` aesthetic (of each point) is mapped to the column `age` - and a scale has appeared in a legend! For now just note that the scale exists - we will show how to modify it in later sections.  
* In the second example two new plot aesthetics are also mapped to columns (`color = ` and `size = `), while the plot aesthetics `shape = ` and `alpha = ` are mapped to static values outside of any `mapping = aes()` function.  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# scatterplot
ggplot(data = linelist,   # set data
       mapping = aes(     # map aesthetics to column values
         x = age,           # map x-axis to age            
         y = wt_kg,         # map y-axis to weight
         color = age)
       )+     # map color to age
  geom_point()         # display data as points 

# scatterplot
ggplot(data = linelist,   # set data
       mapping = aes(     # map aesthetics to column values
         x = age,           # map x-axis to age            
         y = wt_kg,         # map y-axis to weight
         color = age,       # map color to age
         size = age))+      # map size to age
  geom_point(             # display data as points
    shape = "diamond",      # points display as diamonds
    alpha = 0.3)            # point transparency at 30%


```



Note: Axes assignments are always assigned to columns in the data (not to static values), and this is always done within `mapping = aes()`.  


It becomes important to keep track of your plot layers and aesthetics when making more complex plots - for example plots with multiple geoms. In the example below, the `size = ` aesthetic is assigned twice - once for `geom_point()` and once for `geom_smooth()` - both times as a static value.  

```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(           # map aesthetics to columns
         x = age,
         y = wt_kg,
         color = age_years)
       ) + 
  geom_point(                   # add points for each row of data
    size = 1,
    alpha = 0.5) +  
  geom_smooth(                  # add a trend line 
    method = "lm",              # with linear method
    size = 2)                   # size (width of line) of 2
```






### Where to make mapping assignments {#ggplot_basics_map_loc .unnumbered}


Aesthetic mapping within `mapping = aes()` can be written in several places in your plotting commands and can even be written more than once. This can be written in the top `ggplot()` command, and/or for each individual geom beneath. The nuances include:  

* Mapping assignments made in the top `ggplot()` command will be inherited as defaults across any geom below, like how `x = ` and `y = ` are inherited 
* Mapping assignments made within one geom apply only to that geom  

Likewise, `data = ` specified in the top `ggplot()` will apply by default to any geom below, but you could also specify data for each geom (but this is more difficult).  

Thus, each of the following commands will create the same plot:  

```{r, eval=F, warning=F, message=F}
# These commands will produce the exact same plot
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()

ggplot(data = linelist)+
  geom_histogram(mapping = aes(x = age))

ggplot()+
  geom_histogram(data = linelist, mapping = aes(x = age))
```




### Groups {#ggplotgroups .unnumbered}  

You can easily group the data and "plot by group". In fact, you have already done this!  

Assign the "grouping" column to the appropriate plot aesthetic, within a `mapping = aes()`. Above, we demonstrated this using continuous values when we assigned point `size = ` to the column `age`. However this works the same way for discrete/categorical columns.  

For example, if you want points to be displayed by gender, you would set `mapping = aes(color = gender)`. A legend automatically appears. This assignment can be made within the `mapping = aes()` in the top `ggplot()` command (and be inherited by the geom), or it could be set in a separate `mapping = aes()` within the geom. Both approaches are shown below:  


```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg, color = gender))+
  geom_point(alpha = 0.5)
```


```{r, eval=F}
# This alternative code produces the same plot
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg))+
  geom_point(
    mapping = aes(color = gender),
    alpha = 0.5)

```


Note that depending on the geom, you will need to use different arguments to group the data. For `geom_point()` you will most likely use `color =`, `shape = ` or `size = `. Whereas for `geom_bar()` you are more likely to use `fill = `. This just depends on the geom and what plot aesthetic you want to reflect the groupings.  

For your information - the most basic way of grouping the data is by using only the `group = ` argument within `mapping = aes()`. However, this by itself will not change the colors, fill, or shapes. Nor will it create a legend. Yet the data are grouped, so statistical displays may be affected.  

To adjust the order of groups in a plot, see the [ggplot tips] page or the page on [Factors]. There are many examples of grouped plots in the sections below on plotting continuous and categorical data.   



## Facets / Small-multiples {#ggplot_basics_facet}  

Facets, or "small-multiples", are used to split one plot into a multi-panel figure, with one panel ("facet") per group of data. The same type of plot is created multiple times, each one using a sub-group of the same dataset.  

Faceting is a functionality that comes with **ggplot2**, so the legends and axes of the facet "panels" are automatically aligned. There are other packages discussed in the [ggplot tips] page that are used to combine completely different plots (**cowplot** and **patchwork**) into one figure.  

Faceting is done with one of the following **ggplot2** functions:

  1. `facet_wrap()` To show a different panel for each level of a *single* variable. One example of this could be showing a different epidemic curve for each hospital in a region. Facets are ordered alphabetically, unless the variable is a factor with other ordering defined.  
  + You can invoke certain options to determine the layout of the facets, e.g. `nrow = 1` or `ncol = 1` to control the number of rows or columns that the faceted plots are arranged within.  
  
  2. `facet_grid()` This is used when you want to bring a second variable into the faceting arrangement. Here each panel of a grid shows the intersection between values in *two columns*. For example, epidemic curves for each hospital-age group combination with hospitals along the top (columns) and age groups along the sides (rows).  
  + `nrow` and `ncol` are not relevant, as the subgroups are presented in a grid  

Each of these functions accept a formula syntax to specify the column(s) for faceting. Both accept up to two columns, one on each side of a tilde `~`.  

* For `facet_wrap()` most often you will write only one column preceded by a tilde `~` like `facet_wrap(~hospital)`. However you can write two columns `facet_wrap(outcome ~ hospital)` - each unique combination will display in a separate panel, but they will not be arranged in a grid. The headings will show combined terms and these won't be specific logic to the columns vs. rows.  If you are providing only one faceting variable, a period `.` is used as a placeholder on the other side of the formula - see the code examples.  

* For `facet_grid()` you can also specify one or two columns to the formula (grid `rows ~ columns`). If you only want to specify one, you can place a period `.` on the other side of the tilde like `facet_grid(. ~ hospital)` or `facet_grid(hospital ~ .)`.  

Facets can quickly contain an overwhelming amount of information - its good to ensure you don't have too many levels of each variable that you choose to facet by. Here are some quick examples with the malaria dataset (see [Download handbook and data]) which consists of daily case counts of malaria for facilities, by age group.  

Below we import and do some quick modifications for simplicity:  

```{r, , warning=F, message=F}
# These data are daily counts of malaria cases, by facility-day
malaria_data <- import(here("data", "malaria_facility_count_data.rds")) %>%  # import
  select(-submitted_date, -Province, -newid)                                 # remove unneeded columns

```

The first 50 rows of the malaria data are below. Note there is a column `malaria_tot`, but also columns for counts by age group (these will be used in the second, `facet_grid()` example).  

```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### `facet_wrap()` {.unnumbered}

For the moment, let's focus on the columns `malaria_tot` and `District`. Ignore the age-specific count columns for now. We will plot epidemic curves with `geom_col()`, which produces a column for each day at the specified y-axis height given in column `malaria_tot` (the data are already daily counts, so we use `geom_col()` - see [the "Bar plot" section below](#ggplot_basics_bars)).  

When we add the command `facet_wrap()`, we specify a tilde and then the column to facet on (`District` in this case). You can place another column on the left side of the tilde, - this will create one facet for each combination - but we recommend you do this with `facet_grid()` instead. In this use case, one facet is created for each unique value of `District`.  

```{r, warning=F, message=F}
# A plot with facets by district
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # plot the count data as columns
  theme_minimal()+                              # simplify the background panels
  labs(                                         # add plot labels, title, etc.
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district") +
  facet_wrap(~District)                       # the facets are created
```

### `facet_grid()` {.unnumbered}  

We can use a `facet_grid()` approach to cross two variables. Let's say we want to cross `District` and age. Well, we need to do some data transformations on the age columns to get these data into ggplot-preferred "long" format. The age groups all have their own columns - we want them in a single column called `age_group` and another called `num_cases`. See the page on [Pivoting data] for more information on this process.  

```{r, message=F, warning=F}
malaria_age <- malaria_data %>%
  select(-malaria_tot) %>% 
  pivot_longer(
    cols = c(starts_with("malaria_rdt_")),  # choose columns to pivot longer
    names_to = "age_group",      # column names become age group
    values_to = "num_cases"      # values to a single column (num_cases)
  ) %>%
  mutate(
    age_group = str_replace(age_group, "malaria_rdt_", ""),
    age_group = forcats::fct_relevel(age_group, "5-14", after = 1))
```

Now the first 50 rows of data look like this:  

```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_age, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


When you pass the two variables to `facet_grid()`, easiest is to use formula notation (e.g. `x ~ y`) where x is rows and y is columns. Here is the plot, using `facet_grid()` to show the plots for each combination of the columns `age_group` and `District`.

```{r, message=F, warning=F}
ggplot(malaria_age, aes(x = data_date, y = num_cases)) +
  geom_col(fill = "darkred", width = 1) +
  theme_minimal()+
  labs(
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district and age group"
  ) +
  facet_grid(District ~ age_group)
```

### Free or fixed axes {.unnumbered}  

The axes scales displayed when faceting are by default the same (fixed) across all the facets. This is helpful for cross-comparison, but not always appropriate.  

When using `facet_wrap()` or `facet_grid()`, we can add `scales = "free_y"` to "free" or release the y-axes of the panels to scale appropriately to their data subset. This is particularly useful if the actual counts are small for one of the subcategories and trends are otherwise hard to see. Instead of "free_y" we can also write "free_x" to do the same for the x-axis (e.g. for dates) or "free" for both axes. Note that in `facet_grid`, the y scales will be the same for facets in the same row, and the x scales will be the same for facets in the same column.

When using `facet_grid` only, we can add `space = "free_y"` or `space = "free_x"` so that the actual height or width of the facet is weighted to the values of the figure within. This only works if `scales = "free"` (y or x) is already applied. 

```{r, message=FALSE, warning=FALSE}

# Free y-axis
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # plot the count data as columns
  theme_minimal()+                              # simplify the background panels
  labs(                                         # add plot labels, title, etc.
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district - 'free' x and y axes") +
  facet_wrap(~District, scales = "free")        # the facets are created
```


<!-- ```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')} -->
<!-- # A) Facet hospitalsation date by hospital, free y axis -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "A) Histogram with free y axis scales")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y") # Free the y scale of each facet -->

<!-- # B) Facet hospitalisation date by hospital, free y axis and vertical spacing -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "B) Histogram with free y axis scales and spacing")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y", # Free the y scale of each facet -->
<!--              space = "free_y") # Free the vertical spacing of each facet to optimise space -->

<!-- ``` -->

### Factor level order in facets {.unnumbered}  

See this [post](https://juliasilge.com/blog/reorder-within/) on how to re-order factor levels *within* facets.  


## Storing plots  

### Saving plots {.unnumbered}

By default when you run a `ggplot()` command, the plot will be printed to the Plots RStudio pane. However, you can also save the plot as an object by using the assignment operator `<-` and giving it a name. Then it will not print unless the object name itself is run. You can also print it by wrapping the plot name with `print()`, but this is only necessary in certain circumstances such as if the plot is created inside a *for loop* used to print multiple plots at once (see [Iteration, loops, and lists] page).  

```{r, warning=F, message=F}
# define plot
age_by_wt <- ggplot(data = linelist, mapping = aes(x = age_years, y = wt_kg, color = age_years))+
  geom_point(alpha = 0.1)

# print
age_by_wt    
```


### Modifying saved plots {.unnumbered}  

One nice thing about **ggplot2** is that you can define a plot (as above), and then add layers to it starting with its name. You do not have to repeat all the commands that created the original plot! 

For example, to modify the plot `age_by_wt` that was defined above, to include a vertical line at age 50, we would just add a `+` and begin adding additional layers to the plot.  

```{r, warning=F, message=F}
age_by_wt+
  geom_vline(xintercept = 50)
```


### Exporting plots {.unnumbered}   

Exporting ggplots is made easy with the `ggsave()` function from **ggplot2**. It can work in two ways, either:  

* Specify the name of the plot object, then the file path and name with extension  
  * For example: `ggsave(my_plot, here("plots", "my_plot.png"))`  
* Run the command with only a file path, to save the last plot that was printed  
  * For example: `ggsave(here("plots", "my_plot.png"))`  
  
You can export as png, pdf, jpeg, tiff, bmp, svg, or several other file types, by specifying the file extension in the file path.  

You can also specify the arguments `width = `, `height = `, and `units = ` (either "in", "cm", or "mm"). You can also specify `dpi = ` with a number for plot resolution (e.g. 300). See the function details by entering `?ggsave` or reading the [documentation online](https://ggplot2.tidyverse.org/reference/ggsave.html). 

Remember that you can use `here()` syntax to provide the desired file path. See the [Import and export] page for more information.  


## Labels 

Surely you will want to add or adjust the plot's labels. These are most easily done within the `labs()` function which is added to the plot with `+` just as the geoms were.  

Within `labs()` you can provide character strings to these arguements:  

* `x = ` and `y = ` The x-axis and y-axis title (labels)  
* `title = ` The main plot title  
* `subtitle = ` The subtitle of the plot, in smaller text below the title  
* `caption = ` The caption of the plot, in bottom-right by default  

Here is a plot we made earlier, but with nicer labels:  

```{r, warning=F, message=F}
age_by_wt <- ggplot(
  data = linelist,   # set data
  mapping = aes(     # map aesthetics to column values
         x = age,           # map x-axis to age            
         y = wt_kg,         # map y-axis to weight
         color = age))+     # map color to age
  geom_point()+           # display data as points
  labs(
    title = "Age and weight distribution",
    subtitle = "Fictional Ebola outbreak, 2014",
    x = "Age in years",
    y = "Weight in kilos",
    color = "Age",
    caption = stringr::str_glue("Data as of {max(linelist$date_hospitalisation, na.rm=T)}"))

age_by_wt
```

Note how in the caption assignment we used `str_glue()` from the **stringr** package to implant dynamic R code within the string text. The caption will show the "Data as of: " date that reflects the maximum hospitalization date in the linelist. Read more about this in the page on [Characters and strings].  

A note on specifying the *legend* title: There is no one "legend title" argument, as you could have multiple scales in your legend. Within `labs()`, you can write the argument for the plot aesthetic used to create the legend, and provide the title this way. For example, above we assigned `color = age` to create the legend. Therefore, we provide `color = ` to `labs()` and assign the legend title desired ("Age" with capital A). If you create the legend with `aes(fill = COLUMN)`, then in `labs()` you would write `fill = ` to adjust the title of that legend. The section on color scales in the [ggplot tips] page provides more details on editing legends, and an alternative approach using `scales_()` functions.  



## Themes {#ggplot_basics_themes} 

One of the best parts of **ggplot2** is the amount of control you have over the plot - you can define anything! As mentioned above, the design of the plot that is *not* related to the data shapes/geometries are adjusted within the `theme()` function. For example, the plot background color, presence/absence of gridlines, and the font/size/color/alignment of text (titles, subtitles, captions, axis text...). These adjustments can be done in one of two ways:  

* Add a [*complete theme*](https://ggplot2.tidyverse.org/reference/ggtheme.html) `theme_()` function to make sweeping adjustments - these include `theme_classic()`, `theme_minimal()`, `theme_dark()`, `theme_light()` `theme_grey()`, `theme_bw()` among others  
* Adjust each tiny aspect of the plot individually within `theme()`  


### Complete themes {.unnumbered}  

As they are quite straight-forward, we will demonstrate the complete theme functions below and will not describe them further here. Note that any micro-adjustments with `theme()` should be made *after* use of a complete theme.  

Write them with empty parentheses.  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme classic")+
  theme_classic()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme bw")+
  theme_bw()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme minimal")+
  theme_minimal()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme gray")+
  theme_gray()
  


```

### Modify theme {.unnumbered}  

The `theme()` function can take a large number of arguments, each of which edits a very specific aspect of the plot. There is no way we could cover all of the arguments, but we will describe the general pattern for them and show you how to find the argument name that you need. The basic syntax is this:

1. Within `theme()` write the argument name for the plot element you want to edit, like `plot.title = `  
3. Provide an `element_()` function to the argument  
  + Most often, use `element_text()`, but others include `element_rect()` for canvas background colors, or `element_blank()` to remove plot elements  
4. Within the `element_()` function, write argument assignments to make the fine adjustments you desire  

So, that description was quite abstract, so here are some examples.  

The below plot looks quite silly, but it serves to show you a variety of the ways you can adjust your plot.  

* We begin with the plot `age_by_wt` defined just above and add `theme_classic()`  
* For finer adjustments we add `theme()` and include one argument for each plot element to adjust  

It can be nice to organize the arguments in logical sections. To describe just some of those used below:  

* `legend.position = ` is unique in that it accepts simple values like "bottom", "top", "left", and "right". But generally, text-related arguments require that you place the details *within* `element_text()`.  
* Title size with `element_text(size = 30)`  
* The caption horizontal alignment with `element_text(hjust = 0)` (from right to left)  
* The subtitle is italicized with `element_text(face = "italic")`  

```{r, , warning=F, message=F}
age_by_wt + 
  theme_classic()+                                 # pre-defined theme adjustments
  theme(
    legend.position = "bottom",                    # move legend to bottom
    
    plot.title = element_text(size = 30),          # size of title to 30
    plot.caption = element_text(hjust = 0),        # left-align caption
    plot.subtitle = element_text(face = "italic"), # italicize subtitle
    
    axis.text.x = element_text(color = "red", size = 15, angle = 90), # adjusts only x-axis text
    axis.text.y = element_text(size = 15),         # adjusts only y-axis text
    
    axis.title = element_text(size = 20)           # adjusts both axes titles
    )     
```

Here are some especially common `theme()` arguments. You will recognize some patterns, such as appending `.x` or `.y` to apply the change only to one axis.  


`theme()` argument                 |What it adjusts
-----------------------------------|----------------------------------
`plot.title = element_text()`      |The title
`plot.subtitle = element_text()`   |The subtitle
`plot.caption = element_text()`    |The caption (family, face, color, size, angle, vjust, hjust...) 
`axis.title = element_text()`      |Axis titles (both x and y) (size, face, angle, color...)
`axis.title.x = element_text()`    |Axis title x-axis only (use `.y` for y-axis only)
`axis.text = element_text()`       |Axis text (both x and y)
`axis.text.x = element_text()`     |Axis text x-axis only (use `.y` for y-axis only)  
`axis.ticks = element_blank()`     |Remove axis ticks
`axis.line = element_line()`       |Axis lines (colour, size, linetype: solid dashed dotted etc)
`strip.text = element_text()`      |Facet strip text (colour, face, size, angle...)
`strip.background = element_rect()`|facet strip (fill, colour, size...)  

But there are so many theme arguments! How could I remember them all? Do not worry - it is impossible to remember them all. Luckily there are a few tools to help you:  

The **tidyverse** documentation on [modifying theme](https://ggplot2.tidyverse.org/reference/theme.html), which has a complete list.  

<span style="color: darkgreen;">**_TIP:_** Run `theme_get()` from **ggplot2** to print a list of all 90+ `theme()` arguments to the console.</span>  

<span style="color: darkgreen;">**_TIP:_** If you ever want to remove an element of a plot, you can also do it through `theme()`. Just pass `element_blank()` to an argument to have it disappear completely. For legends, set `legend.position = "none".`</span>  




## Colors  


Please see this [section on color scales of the ggplot tips page](#ggplot_tips_colors).  



## Piping into **ggplot2**   

When using pipes to clean and transform your data, it is easy to pass the transformed data into `ggplot()`.  

The pipes that pass the dataset from function-to-function will transition to `+` once the `ggplot()` function is called. Note that in this case, there is no need to specify the `data = ` argument, as this is automatically defined as the piped-in dataset.  

This is how that might look:  

```{r, warning=F, message=F}
linelist %>%                                                     # begin with linelist
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # select columns
  pivot_longer(                                                  # pivot longer
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # replace missing values
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  
  ggplot(                                                        # begin ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )
```









## Plot continuous data

Throughout this page, you have already seen many examples of plotting continuous data. Here we briefly consolidate these and present a few variations.  
Visualisations covered here include:

* Plots for one continuous variable:  
  * **Histogram**, a classic graph to present the distribution of a continuous variable. 
  * **Box plot** (also called box and whisker), to show the 25th, 50th, and 75th percentiles, tail ends of the distribution, and outliers ([important limitations](https://www.data-to-viz.com/caveat/boxplot.html)).  
  * **Jitter plot**, to show all values as points that are 'jittered' so they can (mostly) all be seen, even where two have the same value.  
  * **Violin plot**, show the distribution of a continuous variable based on the symmetrical width of the 'violin'. 
  * **Sina plot**, are a combination of jitter and violin plots, where individual points are shown but in the symmetrical shape of the distribution (via **ggforce** package).  
* **Scatter plot** for two continuous variables.  
* **Heat plots** for three continuous variables (linked to [Heat plots] page)  



### Histograms {.unnumbered}

Histograms may look like bar charts, but are distinct because they measure the distribution of a *continuous* variable. There are no spaces between the "bars", and only one column is provided to `geom_histogram()`.

Below is code for generating **histograms**, which group continuous data into ranges and display in adjacent bars of varying height. This is done using `geom_histogram()`. See the ["Bar plot" section](#ggplot_basics_bars) of the ggplot basics page to understand difference between `geom_histogram()`, `geom_bar()`, and `geom_col()`.  

We will show the distribution of ages of cases. Within `mapping = aes()` specify which column you want to see the distribution of. You can assign this column to either the x or the y axis. 

The rows will be assigned to "bins" based on their numeric age, and these bins will be graphically represented by bars. If you specify a number of bins with the `bins = ` plot aesthetic, the break points are evenly spaced between the minimum and maximum values of the histogram. If `bins = ` is unspecified, an appropriate number of bins will be guessed and this message displayed after the plot:  

```
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
``` 

If you do not want to specify a number of bins to `bins = `, you could alternatively specify `binwidth = ` in the units of the axis. We give a few examples showing different bins and bin widths:  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Regular histogram
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram()+
  labs(title = "A) Default histogram (30 bins)")

# B) More bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(bins = 50)+
  labs(title = "B) Set to 50 bins")

# C) Fewer bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(bins = 5)+
  labs(title = "C) Set to 5 bins")


# D) More bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(binwidth = 1)+
  labs(title = "D) binwidth of 1")

```



To get smoothed proportions, you can use `geom_density()`:  

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional density")

# Stacked frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_density(size = 2, alpha = 0.2, position = "stack")+
  labs(title = "'Stacked' proportional densities")
```


To get a "stacked" histogram (of a continuous column of data), you can do one of the following:  

1) Use `geom_histogram()` with the `fill = ` argument within `aes()` and assigned to the grouping column, or  
2) Use `geom_freqpoly()`, which is likely easier to read (you can still set `binwidth = `)  
3) To see proportions of all values, set the `y = after_stat(density)` (use this syntax exactly - not changed for your data). Note: these proportions will show *per group*.  

Each is shown below (*note use of `color = ` vs. `fill = ` in each):  

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# "Stacked" histogram
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_histogram(binwidth = 2)+
  labs(title = "'Stacked' histogram")

# Frequency 
ggplot(data = linelist, mapping = aes(x = age, color = gender)) +
  geom_freqpoly(binwidth = 2, size = 2)+
  labs(title = "Freqpoly")

# Frequency with proportion axis
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +
  geom_freqpoly(binwidth = 5, size = 2)+
  labs(title = "Proportional freqpoly")

# Frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional, smoothed with geom_density()")
```

If you want to have some fun, try `geom_density_ridges` from the **ggridges** package ([vignette here](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).  

Read more in detail about histograms at the **tidyverse** [page on geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html).  



### Box plots {.unnumbered}

Box plots are common, but have important limitations. They can obscure the actual distribution - e.g. a bi-modal distribution. See this [R graph gallery](https://www.r-graph-gallery.com/boxplot.html) and this [data-to-viz article](https://www.data-to-viz.com/caveat/boxplot.html) for more details. However, they do nicely display the inter-quartile range and outliers - so they can be overlaid on top of other types of plots that show the distribution in more detail.  

Below we remind you of the various components of a boxplot:  

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "boxplot.png"))
```

When using `geom_boxplot()` to create a box plot, you generally map only one axis (x or y) within `aes()`. The axis specified determines if the plots are horizontal or vertical. 

In most geoms, you create a plot per group by mapping an aesthetic like `color = ` or `fill = ` to a column within `aes()`. However, for box plots achieve this by assigning the grouping column to the un-assigned axis (x or y). Below is code for a boxplot of *all* age values in the dataset, and second is code to display one box plot for each (non-missing) gender in the dataset. Note that `NA` (missing) values will appear as a separate box plot unless removed. In this example we also set the `fill` to the column `outcome` so each plot is a different color - but this is not necessary.  

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Overall boxplot
ggplot(data = linelist)+  
  geom_boxplot(mapping = aes(y = age))+   # only y axis mapped (not x)
  labs(title = "A) Overall boxplot")

# B) Box plot by group
ggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + 
  geom_boxplot()+                     
  theme(legend.position = "none")+   # remove legend (redundant)
  labs(title = "B) Boxplot by gender")      
```

For code to add a box plot to the edges of a scatter plot ("marginal" plots) see the page [ggplot tips].  





### Violin, jitter, and sina plots {.unnumbered}

Below is code for creating **violin plots** (`geom_violin`) and **jitter plots** (`geom_jitter`) to show distributions. You can specify that the fill or color is also determined by the data, by inserting these options within `aes()`. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}


# A) Jitter plot by group
ggplot(data = linelist %>% drop_na(outcome),      # remove missing values
       mapping = aes(y = age,                     # Continuous variable
           x = outcome,                           # Grouping variable
           color = outcome))+                     # Color variable
  geom_jitter()+                                  # Create the violin plot
  labs(title = "A) jitter plot by gender")     



# B) Violin plot by group
ggplot(data = linelist %>% drop_na(outcome),       # remove missing values
       mapping = aes(y = age,                      # Continuous variable
           x = outcome,                            # Grouping variable
           fill = outcome))+                       # fill variable (color)
  geom_violin()+                                   # create the violin plot
  labs(title = "B) violin plot by gender")    
```


You can combine the two using the `geom_sina()` function from the **ggforce** package. The sina plots the jitter points in the shape of the violin plot. When overlaid on the violin plot (adjusting the transparencies) this can be easier to visually interpret.  

```{r, warning=F, message=F}

# A) Sina plot by group
ggplot(
  data = linelist %>% drop_na(outcome), 
  aes(y = age,           # numeric variable
      x = outcome)) +    # group variable
  geom_violin(
    aes(fill = outcome), # fill (color of violin background)
    color = "white",     # white outline
    alpha = 0.2)+        # transparency
  geom_sina(
    size=1,                # Change the size of the jitter
    aes(color = outcome))+ # color (color of dots)
  scale_fill_manual(       # Define fill for violin background by death/recover
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  scale_color_manual(      # Define colours for points by death/recover
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  theme_minimal() +                                # Remove the gray background
  theme(legend.position = "none") +                # Remove unnecessary legend
  labs(title = "B) violin and sina plot by gender, with extra formatting")      


```



### Two continuous variables  {.unnumbered}

Following similar syntax, `geom_point()` will allow you to plot two continuous variables against each other in a **scatter plot**. This is useful for showing actual values rather than their distributions. A basic scatter plot of age vs weight is shown in (A). In (B) we again use `facet_grid()` to show the relationship between two continuous variables in the linelist. 

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Basic scatter plot of weight and age
ggplot(data = linelist, 
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "A) Scatter plot of weight and age")

# Scatter plot of weight and age by gender and Ebola outcome
ggplot(data = linelist %>% drop_na(gender, outcome), # filter retains non-missing gender/outcome
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "B) Scatter plot of weight and age faceted by gender and outcome")+
  facet_grid(gender ~ outcome) 

```


### Three continuous variables {.unnumbered}  

You can display three continuous variables by utilizing the `fill = ` argument to create a *heat plot*. The color of each "cell" will reflect the value of the third continuous column of data. See the [ggplot tips] page and the page on on [Heat plots] for more details and several examples. 

There are ways to make 3D plots in R, but for applied epidemiology these are often difficult to interpret and therefore less useful for decision-making.  









## Plot categorical data  

Categorical data can be character values, could be logical (TRUE/FALSE), or factors (see the [Factors] page). 

### Preparation  {.unnumbered}

#### Data structure {.unnumbered}  

The first thing to understand about your categorical data is whether it exists as raw observations like a linelist of cases, or as a summary or aggregate data frame that holds counts or proportions. The state of your data will impact which plotting function you use:  

* If your data are raw observations with one row per observation, you will likely use `geom_bar()`  
* If your data are already aggregated into counts or proportions, you will likely use `geom_col()`  


#### Column class and value ordering {.unnumbered}  

Next, examine the class of the columns you want to plot. We look at `hospital`, first with `class()` from **base** R, and with `tabyl()` from **janitor**.  

```{r}
# View class of hospital column - we can see it is a character
class(linelist$hospital)

# Look at values and proportions within hospital column
linelist %>% 
  tabyl(hospital)
```

We can see the values within are characters, as they are hospital names, and by default they are ordered alphabetically. There are 'other' and 'missing' values, which we would prefer to be the last subcategories when presenting breakdowns. So we change this column into a factor and re-order it. This is covered in more detail in the [Factors] page.

```{r}
# Convert to factor and define level order so "Other" and "Missing" are last
linelist <- linelist %>% 
  mutate(
    hospital = fct_relevel(hospital, 
      "St. Mark's Maternity Hospital (SMMH)",
      "Port Hospital", 
      "Central Hospital",
      "Military Hospital",
      "Other",
      "Missing"))

```


```{r}
levels(linelist$hospital)
```

### `geom_bar()` {#ggplot_basics_bars .unnumbered}  

Use `geom_bar()` if you want bar height (or the height of stacked bar components) to reflect *the number of relevant rows in the data*. These bars will have gaps between them, unless the `width = ` plot aesthetic is adjusted.  

* Provide only one axis column assignment (typically x-axis). If you provide x and y, you will get `Error: stat_count() can only have an x or y aesthetic.`  
* You can create stacked bars by adding a `fill = ` column assignment within `mapping = aes()`  
* The opposite axis will be titled "count" by default, because it represents the number of rows  

Below, we have assigned outcome to the y-axis, but it could just as easily be on the x-axis. If you have longer character values, it can sometimes look better to flip the bars sideways and put the legend on the bottom. This may impact how your factor levels are ordered - in this case we reverse them with `fct_rev()` to put missing and other at the bottom.    

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Outcomes in all cases
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital)), width = 0.7) +
  theme_minimal()+
  labs(title = "A) Number of cases by hospital",
       y = "Hospital")


# B) Outcomes in all cases by hosptial
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital), fill = outcome), width = 0.7) +
  theme_minimal()+
  theme(legend.position = "bottom") +
  labs(title = "B) Number of recovered and dead Ebola cases, by hospital",
       y = "Hospital")

```





### `geom_col()` {.unnumbered}  

Use `geom_col()` if you want bar height (or height of stacked bar components) to reflect pre-calculated *values* that exists in the data. Often, these are summary or "aggregated" counts, or proportions.  

Provide column assignments for *both* axes to `geom_col()`. Typically your x-axis column is discrete and your y-axis column is numeric. 

Let's say we have this dataset `outcomes`:  

```{r, echo = F}
outcomes <- linelist %>% 
  drop_na() %>% 
  group_by(outcome) %>% 
  count %>% 
  ungroup() %>% # Ungroup so proportion is out of total
  mutate(proportion = n/sum(n)*100) # Caculate percentage
  
outcomes #View full table
```



Below is code using `geom_col` for creating  simple bar charts to show the distribution of Ebola patient outcomes. With geom_col, both x and y need to be specified. Here x is the categorical variable along the x axis, and y is the generated proportions column `proportion`. 

```{r, fig.height = 3, fig.width=4.5}
# Outcomes in all cases
ggplot(outcomes) + 
  geom_col(aes(x=outcome, y = proportion)) +
  labs(subtitle = "Number of recovered and dead Ebola cases")

```

To show breakdowns by hospital, we would need our table to contain more information, and to be in "long" format. We create this table with the frequencies of the combined categories `outcome` and `hospital` (see [Grouping data] page for grouping tips). 

```{r, fig.height = 4, fig.width=6}
outcomes2 <- linelist %>% 
  drop_na(outcome) %>% 
  count(hospital, outcome) %>%  # get counts by hospital and outcome
  group_by(hospital) %>%        # Group so proportions are out of hospital total
  mutate(proportion = n/sum(n)*100) # calculate proportions of hospital total

head(outcomes2) # Preview data
```

We then create the ggplot with some added formatting:

  * **Axis flip**: Swapped the axis around with `coord_flip()` so that we can read the hospital names.
  * **Columns side-by-side**: Added a `position = "dodge"` argument so that the bars for death and recover are presented side by side rather than stacked. Note stacked bars are the default.
  * **Column width**: Specified 'width', so the columns are half as thin as the full possible width.
  * **Column order**: Reversed the order of the categories on the y axis so that 'Other' and 'Missing' are at the bottom, with `scale_x_discrete(limits=rev)`. Note that we used that rather than `scale_y_discrete` because hospital is stated in the `x` argument of `aes()`, even if visually it is on the y axis. We do this because Ggplot seems to present categories backwards unless we tell it not to.  
  * **Other details**: Labels/titles and colours added within `labs` and `scale_fill_color` respectively.
  
```{r, fig.height = 4, fig.width=8}

# Outcomes in all cases by hospital
ggplot(outcomes2) +  
  geom_col(
    mapping = aes(
      x = proportion,                 # show pre-calculated proportion values
      y = fct_rev(hospital),          # reverse level order so missing/other at bottom
      fill = outcome),                # stacked by outcome
    width = 0.5)+                    # thinner bars (out of 1)
  theme_minimal() +                  # Minimal theme 
  theme(legend.position = "bottom")+
  labs(subtitle = "Number of recovered and dead Ebola cases, by hospital",
       fill = "Outcome",             # legend title
       y = "Count",                  # y axis title
       x = "Hospital of admission")+ # x axis title
  scale_fill_manual(                 # adding colors manually
    values = c("Death"= "#3B1c8C",
               "Recover" = "#21908D" )) 

```


Note that the proportions are binary, so we may prefer to drop 'recover' and just show the proportion who died. This is just for illustration purposes.  


If using `geom_col()` with dates data (e.g. an epicurve from aggregated data) - you will want to adjust the `width = ` argument to remove the "gap" lines between the bars. If using daily data set `width = 1`. If weekly, `width = 7`. Months are not possible because each month has a different number of days.  


### `geom_histogram()` {.unnumbered}  

Histograms may look like bar charts, but are distinct because they measure the distribution of a *continuous* variable. There are no spaces between the "bars", and only one column is provided to `geom_histogram()`. There are arguments specific to histograms such as `bin_width = ` and `breaks = ` to specify how the data should be binned. The section above on continuous data and the page on [Epidemic curves] provide additional detail.  



## Resources  

There is a huge amount of help online, especially with ggplot. See:

* [ggplot2 cheat sheet](http://r-statistics.co/ggplot2-cheatsheet.html)
* [another cheat sheet](https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/)
* [tidyverse ggplot basics page](https://ggplot2.tidyverse.org/reference/)  
* [plotting continuous variables](http://www.sthda.com/english/articles/32-r-graphics-essentials/131-plot-two-continuous-variables-scatter-graph-and-alternatives/)  
* R for Data Science pages on [data visualization](https://r4ds.had.co.nz/data-visualisation.html)
* [graphics for communicaton](https://r4ds.had.co.nz/graphics-for-communication.html)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_basics.Rmd-->


# ggplot tips {}

In this page we will cover tips and tricks to make your ggplots sharp and fancy. See the page on [ggplot basics] for the fundamentals.  

There a several extensive [**ggplot2** tutorials](https://ggplot2.tidyverse.org/) linked in the Resources section. You can also download this [data visualization with ggplot cheatsheet](https://rstudio.com/resources/cheatsheets/) from the RStudio website. We strongly recommend that you peruse for inspiration at the [R graph gallery](https://www.r-graph-gallery.com/) and [Data-to-viz](https://www.data-to-viz.com/caveats.html). 



<!-- ======================================================= -->
## Preparation {}

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,      # includes ggplot2 and other
  rio,            # import/export
  here,           # file locator
  stringr,        # working with characters   
  scales,         # transform numbers
  ggrepel,        # smartly-placed labels
  gghighlight,    # highlight one part of plot
  RColorBrewer    # color scales
)
```

### Import data {.unnumbered}  

For this page, we import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
## Scales for color, fill, axes, etc. {#ggplot_tips_colors}

In **ggplot2**, when aesthetics of plotted data (e.g. size, color, shape, fill, plot axis) are mapped to columns in the data, the exact display can be adjusted with the corresponding "scale" command. In this section we explain some common scale adjustments.  



### Color schemes

One thing that can initially be difficult to understand with **ggplot2** is control of color schemes. Note that this section discusses the color of *plot objects* (geoms/shapes) such as points, bars, lines, tiles, etc. To adjust color of accessory text, titles, or background color see the [Themes](#ggplot_basics_themes) section of the [ggplot basics] page. 

To control "color" of *plot objects* you will be adjusting either the `color = ` aesthetic (the *exterior* color) or the `fill = ` aesthetic (the *interior* color). One exception to this pattern is `geom_point()`, where you really only get to control `color = `, which controls the color of the point (interior and exterior).  

When setting colour or fill you can use colour names recognized by R like `"red"` (see [complete list](http://sape.inf.usi.ch/quick-reference/ggplot2/colour) or enter `?colors`), or a specific hex colour such as `"#ff0505"`.

```{r, warning=F, message=F}
# histogram - 
ggplot(data = linelist, mapping = aes(x = age))+       # set data and axes
  geom_histogram(              # display histogram
    binwidth = 7,                # width of bins
    color = "red",               # bin line color
    fill = "lightblue")          # bin interior color (fill) 
```



As explained the [ggplot basics] section on [mapping data to the plot](#ggplot_basics_mapping), aesthetics such as `fill = ` and `color = ` can be defined either *outside* of a `mapping = aes()` statement or *inside* of one. If *outside* the `aes()`, the assigned value should be static (e.g. `color = "blue"`) and will apply for *all* data plotted by the geom. If *inside*, the aesthetic should be mapped to a column, like `color = hospital`, and the expression will vary by the value for that row in the data. A few examples:  

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Static color for points and for line
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(color = "purple")+
  geom_vline(xintercept = 50, color = "orange")+
  labs(title = "Static color for points and line")

# Color mapped to continuous column
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = temp))+         
  labs(title = "Color mapped to continuous column")

# Color mapped to discrete column
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = gender))+         
  labs(title = "Color mapped to discrete column")

# bar plot, fill to discrete column, color to static value
ggplot(data = linelist, mapping = aes(x = hospital))+     
  geom_bar(mapping = aes(fill = gender), color = "yellow")+         
  labs(title = "Fill mapped to discrete column, static color")

```


### Scales {#ggplot_tips_scales .unnumbered}  

Once you map a column to a plot aesthetic (e.g. `x = `, `y = `, `fill = `, `color = `...), your plot will gain a scale/legend. See above how the scale can be continuous, discrete, date, etc. values depending on the class of the assigned column. If you have multiple aesthetics mapped to columns, your plot will have multiple scales.  

You can control the scales with the appropriate `scales_()` function. The scale functions of **ggplot()** have 3 parts that are written like this: `scale_AESTHETIC_METHOD()`.  

1) The first part, `scale_()`, is fixed.  
2) The second part, the AESTHETIC, should be the aesthetic that you want to adjust the scale for (`_fill_`, `_shape_`, `_color_`, `_size_`, `_alpha_`...) - the options here also include `_x_` and `_y_`.  
3) The third part, the METHOD, will be either `_discrete()`, `continuous()`, `_date()`, `_gradient()`, or `_manual()` depending on the class of the column and *how* you want to control it. There are others, but these are the most-often used.  

Be sure that you use the correct function for the scale! Otherwise your scale command will not appear to change anything. If you have multiple scales, you may use multiple scale functions to adjust them! For example:  

### Scale arguments {.unnumbered}  

Each kind of scale has its own arguments, though there is some overlap. Query the function like `?scale_color_discrete` in the R console to see the function argument documentation.  

For continuous scales, use `breaks = ` to provide a sequence of values with `seq()` (take `to = `, `from = `, and `by = ` as shown in the example below. Set `expand = c(0,0)` to eliminate padding space around the axes (this can be used on any `_x_` or `_y_` scale.  

For discrete scales, you can adjust the order of level appearance with `breaks = `, and how the values display with the `labels = ` argument. Provide a character vector to each of those (see example below). You can also drop `NA` easily by setting `na.translate = FALSE`.  

The nuances of date scales are covered more extensively in the [Epidemic curves] page.  


### Manual adjustments {.unnumbered}  

One of the most useful tricks is using "manual" scaling functions to explicitly assign colors as you desire. These are functions with the syntax `scale_xxx_manual()` (e.g. `scale_colour_manual()` or `scale_fill_manual()`). Each of the below arguments are demonstrated in the code example below.  

* Assign colors to data values with the `values = ` argument  
* Specify a color for `NA` with `na.value = `  
* Change how the values are *written* in the legend with the `labels = ` argument  
* Change the legend title with `name = `  


Below, we create a bar plot and show how it appears by default, and then with three scales adjusted - the continuous y-axis scale, the discrete x-axis scale, and manual adjustment of the fill (interior bar color).  


```{r, warning=F, message=F}
# BASELINE - no scale adjustment
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# SCALES ADJUSTED
ggplot(data = linelist)+
  
  geom_bar(mapping = aes(x = outcome, fill = gender), color = "black")+
  
  theme_minimal()+                   # simplify background
  
  scale_y_continuous(                # continuous scale for y-axis (counts)
    expand = c(0,0),                 # no padding
    breaks = seq(from = 0,
                 to = 3000,
                 by = 500))+
  
  scale_x_discrete(                   # discrete scale for x-axis (gender)
    expand = c(0,0),                  # no padding
    drop = FALSE,                     # show all factor levels (even if not in data)
    na.translate = FALSE,             # remove NA outcomes from plot
    labels = c("Died", "Recovered"))+ # Change display of values
    
  
  scale_fill_manual(                  # Manually specify fill (bar interior color)
    values = c("m" = "violetred",     # reference values in data to assign colors
               "f" = "aquamarine"),
    labels = c("m" = "Male",          # re-label the legend (use "=" assignment to avoid mistakes)
              "f" = "Female",
              "Missing"),
    name = "Gender",                  # title of legend
    na.value = "grey"                 # assign a color for missing values
  )+
  labs(title = "Adjustment of scales") # Adjust the title of the fill legend
```

### Continuous axes scales {.unnumbered}  

When data are mapping to the plot axes, these too can be adjusted with scales commands. A common example is adjusting the display of an axis (e.g. y-axis) that is mapped to a column with continuous data. 

We may want to adjust the breaks or display of the values in the ggplot using `scale_y_continuous()`. As noted above, use the argument `breaks = ` to provide a sequence of values that will serve as "breaks" along the scale. These are the values at which numbers will display. To this argument, you can provide a `c()` vector containing the desired break values, or you can provide a regular sequence of numbers using the **base** R function `seq()`. This `seq()` function accepts `to = `, `from = `, and `by = `.

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# BASELINE - no scale adjustment
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# 
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  scale_y_continuous(
    breaks = seq(
      from = 0,
      to = 3000,
      by = 100)
  )+
  labs(title = "Adjusted y-axis breaks")

```



#### Display percents {.unnumbered}  

If your original data values are proportions, you can easily display them as percents with "%" by providing `labels = scales::percent` in your scales command, as shown below.  

While an alternative would be to convert the values to character and add a "%" character to the end, this approach will cause complications because your data will no longer be continuous numeric values. 


```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Original y-axis proportions
#############################
linelist %>%                                   # start with linelist
  group_by(hospital) %>%                       # group data by hospital
  summarise(                                   # create summary columns
    n = n(),                                     # total number of rows in group
    deaths = sum(outcome == "Death", na.rm=T),   # number of deaths in group
    prop_death = deaths/n) %>%                   # proportion deaths in group
  ggplot(                                      # begin plotting
    mapping = aes(
      x = hospital,
      y = prop_death))+ 
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis original proportions")



# Display y-axis proportions as percents
########################################
linelist %>%         
  group_by(hospital) %>% 
  summarise(
    n = n(),
    deaths = sum(outcome == "Death", na.rm=T),
    prop_death = deaths/n) %>% 
  ggplot(
    mapping = aes(
      x = hospital,
      y = prop_death))+
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis as percents (%)")+
  scale_y_continuous(
    labels = scales::percent                    # display proportions as percents
  )

```

#### Log scale {.unnumbered}  

To transform a continuous axis to log scale, add `trans = "log2"` to the scale command. For purposes of example, we create a data frame of regions with their respective `preparedness_index` and cumulative cases values.  

```{r}
plot_data <- data.frame(
  region = c("A", "B", "C", "D", "E", "F", "G", "H", "I"),
  preparedness_index = c(8.8, 7.5, 3.4, 3.6, 2.1, 7.9, 7.0, 5.6, 1.0),
  cases_cumulative = c(15, 45, 80, 20, 21, 7, 51, 30, 1442)
)

plot_data
```

The cumulative cases for region "I" are dramatically greater than all the other regions. In circumstances like this, you may elect to display the y-axis using a log scale so the reader can see differences between the regions with fewer cumulative cases.  

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Original y-axis
preparedness_plot <- ggplot(data = plot_data,  
       mapping = aes(
         x = preparedness_index,
         y = cases_cumulative))+
  geom_point(size = 2)+            # points for each region 
  geom_text(
    mapping = aes(label = region),
    vjust = 1.5)+                  # add text labels
  theme_minimal()

preparedness_plot                  # print original plot


# print with y-axis transformed
preparedness_plot+                   # begin with plot saved above
  scale_y_continuous(trans = "log2") # add transformation for y-axis
```



### Gradient scales {.unnumbered}  

Fill gradient scales can involve additional nuance. The defaults are usually quite pleasing, but you may want to adjust the values, cutoffs, etc.  

To demonstrate how to adjust a continuous color scale, we'll use a data set from the [Contact tracing] page that contains the ages of cases and of their source cases.  


```{r, warning=F, message=F}
case_source_relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_age, target_age) 
```

Below, we produce a "raster" heat tile density plot. We won't elaborate how (see the link in paragraph above) but we will focus on how we can adjust the color scale. Read more about the `stat_density2d()` **ggplot2** function [here](https://ggplot2.tidyverse.org/reference/geom_density_2d.html). Note how the `fill` scale is *continuous*.  

```{r, warn=F, message=F}
trans_matrix <- ggplot(
    data = case_source_relationships,
    mapping = aes(x = source_age, y = target_age))+
  stat_density2d(
    geom = "raster",
    mapping = aes(fill = after_stat(density)),
    contour = FALSE)+
  theme_minimal()
```

Now we show some variations on the fill scale: 

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix
trans_matrix + scale_fill_viridis_c(option = "plasma")
```

Now we show some examples of actually adjusting the break points of the scale:  

* `scale_fill_gradient()` accepts two colors (high/low)  
* `scale_fill_gradientn()` accepts a vector of any length of colors to `values = ` (intermediate values will be interpolated)  
* Use [`scales::rescale()`](https://www.rdocumentation.org/packages/scales/versions/0.4.1/topics/rescale) to adjust how colors are positioned along the gradient; it rescales your vector of positions to be between 0 and 1.  


```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix + 
  scale_fill_gradient(     # 2-sided gradient scale
    low = "aquamarine",    # low value
    high = "purple",       # high value
    na.value = "grey",     # value for NA
    name = "Density")+     # Legend title
  labs(title = "Manually specify high/low colors")

# 3+ colors to scale
trans_matrix + 
  scale_fill_gradientn(    # 3-color scale (low/mid/high)
    colors = c("blue", "yellow","red") # provide colors in vector
  )+
  labs(title = "3-color scale")

# Use of rescale() to adjust placement of colors along scale
trans_matrix + 
  scale_fill_gradientn(    # provide any number of colors
    colors = c("blue", "yellow","red", "black"),
    values = scales::rescale(c(0, 0.05, 0.07, 0.10, 0.15, 0.20, 0.3, 0.5)) # positions for colors are rescaled between 0 and 1
    )+
  labs(title = "Colors not evenly positioned")

# use of limits to cut-off values that get fill color
trans_matrix + 
  scale_fill_gradientn(    
    colors = c("blue", "yellow","red"),
    limits = c(0, 0.0002))+
  labs(title = "Restrict value limits, resulting in grey space")

```


### Palettes {.unnumbered}  

#### Colorbrewer and Viridis {.unnumbered}
More generally, if you want predefined palettes, you can use the `scale_xxx_brewer` or `scale_xxx_viridis_y` functions.  

The 'brewer' functions can draw from [colorbrewer.org](colorbrewer.org) palettes.  

The 'viridis' functions draw from viridis (colourblind friendly!) palettes, which "provide colour maps that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness." (read more [here](https://ggplot2.tidyverse.org/reference/scale_viridis.html) and [here](https://bids.github.io/colormap/)). Define if the palette is discrete, continuous, or binned by specifying this at the end of the function (e.g. discrete is `scale_xxx_viridis_d`).

It is advised that you test your plot in this [color blindness simulator](https://www.color-blindness.com/coblis-color-blindness-simulator/). If you have a red/green color scheme, try a "hot-cold" (red-blue) scheme instead as described [here](https://www.visualisingdata.com/2019/08/five-ways-to-design-for-red-green-colour-blindness/#:~:text=The%20pink%2Dred%20through%20to,green%20hues%20used%20by%20default.)  

Here is an example from the [ggplot basics] page, using various color schemes. 

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F} 
symp_plot <- linelist %>%                                         # begin with linelist
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # select columns
  pivot_longer(                                                  # pivot longer
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # replace missing values
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  ggplot(                                                        # begin ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  theme(legend.position = "bottom")+
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )

symp_plot  # print with default colors

#################################
# print with manually-specified colors
symp_plot +
  scale_fill_manual(
    values = c("yes" = "black",         # explicitly define colours
               "no" = "white",
               "unknown" = "grey"),
    breaks = c("yes", "no", "unknown"), # order the factors correctly
    name = ""                           # set legend to no title

  ) 

#################################
# print with viridis discrete colors
symp_plot +
  scale_fill_viridis_d(
    breaks = c("yes", "no", "unknown"),
    name = ""
  )


```



<!-- ======================================================= -->
## Change order of discrete variables {}  

Changing the order that discrete variables appear in is often difficult to understand for people who are new to `ggplot2` graphs. It's easy to understand how to do this however once you understand how `ggplot2` handles discrete variables under the hood. Generally speaking, if a discrete varaible is used, it is automatically converted to a `factor` type - which orders factors by alphabetical order by default. To handle this, you simply have to reorder the factor levels to reflect the order you would like them to appear in the chart. For more detailed information on how to reorder `factor` objects, see the factor section of the guide. 

We can look at a common example using age groups - by default the 5-9 age group will be placed in the middle of the age groups (given alphanumeric order), but we can move it behind the 0-4 age group of the chart by releveling the factors.


```{r, , warning=F, message=F}
ggplot(
  data = linelist %>% drop_na(age_cat5),                         # remove rows where age_cat5 is missing
  mapping = aes(x = fct_relevel(age_cat5, "5-9", after = 1))) +  # relevel factor

  geom_bar() +
  
  labs(x = "Age group", y = "Number of hospitalisations",
       title = "Total hospitalisations by age group") +
  
  theme_minimal()


```

#### **ggthemr** {.unnnumbered}  

Also consider using the **ggthemr** package. You can download this package from Github using the instructions [here](https://github.com/Mikata-Project/ggthemr). It offers palettes that are very aesthetically pleasing, but be aware that these typically have a maximum number of values that can be limiting if you want more than 7 or 8 colors.  






## Contour lines  

Contour plots are helpful when you have many points that might cover each other ("overplotting"). The case-source data used above are again plotted, but more simply using `stat_density2d()` and `stat_density2d_filled()` to produce discrete contour levels - like a topographical map. Read more about the statistics [here](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).  


```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d()+
  geom_point()+
  theme_minimal()+
  labs(title = "stat_density2d() + geom_point()")


case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d_filled()+
  theme_minimal()+
  labs(title = "stat_density2d_filled()")

```



## Marginal distributions  

To show the distributions on the edges of a `geom_point()` scatterplot, you can use the **ggExtra** package and its function `ggMarginal()`. Save your original ggplot as an object, then pass it to `ggMarginal()` as shown below. Here are the key arguments:  

* You must specify the `type = ` as either "histogram", "density" "boxplot", "violin", or "densigram".  
* By default, marginal plots will appear for both axes. You can set `margins = ` to "x" or "y" if you only want one.  
* Other optional arguments include `fill = ` (bar color), `color = ` (line color), `size = ` (plot size relative to margin size, so larger number makes the marginal plot smaller).  
* You can provide other axis-specific arguments to `xparams = ` and `yparams = `. For example, to have different histogram bin sizes, as shown below.  

You can have the marginal plots reflect groups (columns that have been assigned to `color = ` in your `ggplot()` mapped aesthetics). If this is the case, set the `ggMarginal()` argument `groupColour = ` or `groupFill = ` to `TRUE`, as shown below.  

Read more at [this vignette](https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html), in the [R Graph Gallery](https://www.r-graph-gallery.com/277-marginal-histogram-for-ggplot2.html) or the function R documentation `?ggMarginal`.  

```{r, message=FALSE, warning=FALSE}
# Install/load ggExtra
pacman::p_load(ggExtra)

# Basic scatter plot of weight and age
scatter_plot <- ggplot(data = linelist)+
  geom_point(mapping = aes(y = wt_kg, x = age)) +
  labs(title = "Scatter plot of weight and age")
```

To add marginal histograms use `type = "histogram"`. You can optionally set `groupFill = TRUE` to get stacked histograms.     

```{r, message=FALSE, warning=FALSE}
# with histograms
ggMarginal(
  scatter_plot,                     # add marginal histograms
  type = "histogram",               # specify histograms
  fill = "lightblue",               # bar fill
  xparams = list(binwidth = 10),    # other parameters for x-axis marginal
  yparams = list(binwidth = 5))     # other parameters for y-axis marginal
```

Marginal density plot with grouped/colored values:  

```{r, message=FALSE, warning=FALSE}

# Scatter plot, colored by outcome
# Outcome column is assigned as color in ggplot. groupFill in ggMarginal set to TRUE
scatter_plot_color <- ggplot(data = linelist %>% drop_na(gender))+
  geom_point(mapping = aes(y = wt_kg, x = age, color = gender)) +
  labs(title = "Scatter plot of weight and age")+
  theme(legend.position = "bottom")

ggMarginal(scatter_plot_color, type = "density", groupFill = TRUE)
```

Set the `size = ` arguemnt to adjust the relative size of the marginal plot. Smaller number makes a larger marginal plot. You also set `color = `. Below are is a marginal boxplot, with demonstration of the `margins = ` argument so it appears on only one axis:  

```{r, message=FALSE, warning=FALSE}
# with boxplot 
ggMarginal(
  scatter_plot,
  margins = "x",      # only show x-axis marginal plot
  type = "boxplot")   
```



<!-- ======================================================= -->
## Smart Labeling {}  

In **ggplot2**, it is also possible to add text to plots. However, this comes with the notable limitation where text labels often clash with data points in a plot, making them look messy or hard to read. There is no ideal way to deal with this in the base package, but there is a **ggplot2** add-on, known as **ggrepel** that makes dealing with this very simple! 

The **ggrepel** package provides two new functions, `geom_label_repel()` and `geom_text_repel()`, which replace `geom_label()` and `geom_text()`. Simply use these functions instead of the base functions to produce neat labels. Within the function, map the aesthetics `aes()` as always, but include the argument `label = ` to which you provide a column name containing the values you want to display (e.g. patient id, or name, etc.). You can make more complex labels by combining columns and newlines (`\n`) within `str_glue()` as shown below.  

A few tips:  

* Use `min.segment.length = 0` to always draw line segments, or `min.segment.length = Inf` to never draw them  
* Use `size = ` outside of `aes()` to set text size  
* Use `force = ` to change the degree of repulsion between labels and their respective points (default is 1)  
* Include `fill = ` within `aes()` to have label colored by value  
  * A letter "a" may appear in the legend - add `guides(fill = guide_legend(override.aes = aes(color = NA)))+` to remove it  

See this is very in-depth [tutorial](https://ggrepel.slowkow.com/articles/examples.html) for more.  

```{r, , warning=F, message=F}
pacman::p_load(ggrepel)

linelist %>%                                               # start with linelist
  group_by(hospital) %>%                                   # group by hospital
  summarise(                                               # create new dataset with summary values per hospital
    n_cases = n(),                                           # number of cases per hospital
    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # mean delay per hospital
  ) %>% 
  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # send data frame to ggplot
  geom_point(size = 2)+                                    # add points
  geom_label_repel(                                        # add point labels
    mapping = aes(
      label = stringr::str_glue(
        "{hospital}\n{n_cases} cases, {delay_mean} days")  # how label displays
      ), 
    size = 3,                                              # text size in labels
    min.segment.length = 0)+                               # show all line segments                
  labs(                                                    # add axes labels
    title = "Mean delay to admission, by hospital",
    x = "Number of cases",
    y = "Mean delay (days)")
```

You can label only a subset of the data points - by using standard `ggplot()` syntax to provide different `data = ` for each `geom` layer of the plot. Below, All cases are plotted, but only a few are labeled.    

```{r, warning=F, message=FALSE}

ggplot()+
  # All points in grey
  geom_point(
    data = linelist,                                   # all data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    color = "grey",
    alpha = 0.5)+                                              # grey and semi-transparent
  
  # Few points in black
  geom_point(
    data = linelist %>% filter(days_onset_hosp > 15),  # filtered data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    alpha = 1)+                                                # default black and not transparent
  
  # point labels for few points
  geom_label_repel(
    data = linelist %>% filter(days_onset_hosp > 15),  # filter the data for the labels
    mapping = aes(
      x = ht_cm,
      y = wt_kg,
      fill = outcome,                                          # label color by outcome
      label = stringr::str_glue("Delay: {days_onset_hosp}d")), # label created with str_glue()
    min.segment.length = 0) +                                  # show line segments for all
  
  # remove letter "a" from inside legend boxes
  guides(fill = guide_legend(override.aes = aes(color = NA)))+
  
  # axis labels
  labs(
    title = "Cases with long delay to admission",
    y = "weight (kg)",
    x = "height(cm)")
```





<!-- ======================================================= -->
## Time axes {}

Working with time axes in ggplot can seem daunting, but is made very easy with a few key functions. Remember that when working with time or date that you should ensure that the correct variables are formatted as date or datetime class - see the [Working with dates] page for more information on this, or [Epidemic curves] page (ggplot section) for examples.

The single most useful set of functions for working with dates in `ggplot2` are the scale functions (`scale_x_date()`, `scale_x_datetime()`, and their cognate y-axis functions). These functions let you define how often you have axis labels, and how to format axis labels. To find out how to format dates, see the _working with dates_ section again! You can use the `date_breaks` and `date_labels` arguments to specify how dates should look:

  1. `date_breaks` allows you to specify how often axis breaks occur - you can pass a string here (e.g. `"3 months"`, or "`2 days"`)
  
  2. `date_labels` allows you to define the format dates are shown in. You can pass a date format string to these arguments (e.g. `"%b-%d-%Y"`):


```{r, , warning=F, message=F}
# make epi curve by date of onset when available
ggplot(linelist, aes(x = date_onset)) +
  geom_histogram(binwidth = 7) +
  scale_x_date(
    # 1 break every 1 month
    date_breaks = "1 months",
    # labels should show month then date
    date_labels = "%b %d"
  ) +
  theme_classic()

```


One easy solution to efficient date labels on the x-axis is to to assign the `labels = ` argument in `scale_x_date()` to the function `label_date_short()` from the package **scales**. This function will automatically construct efficient date labels (read more [here](https://scales.r-lib.org/reference/label_date.html)). An additional benefit of this function is that the labels will automatically adjust as your data expands over time, from days, to weeks, to months and years.

See a complete example in the Epicurves page section on [multi-level date labels](https://epirhandbook.com/en/epidemic-curves.html#multi-level-date-labels), but a quick example is shown below for reference:  

```{r, eval=T, warning=F}
ggplot(linelist, aes(x = date_onset)) +
  geom_histogram(binwidth = 7) +
  scale_x_date(
    labels = scales::label_date_short()  # automatically efficient date labels
  )+
  theme_classic()

```

<!-- ======================================================= -->
## Highlighting {}

Highlighting specific elements in a chart is a useful way to draw attention to a specific instance of a variable while also providing information on the dispersion of the full dataset. While this is not easily done in base **ggplot2**, there is an external package that can help to do this known as **gghighlight**. This is easy to use within the ggplot syntax.

The **gghighlight** package uses the `gghighlight()` function to achieve this effect. To use this function, supply a logical statement to the function - this can have quite flexible outcomes, but here we'll show an example of the age distribution of cases in our linelist, highlighting them by outcome.

```{r, , warning=F, message=F}
# load gghighlight
library(gghighlight)

# replace NA values with unknown in the outcome variable
linelist <- linelist %>%
  mutate(outcome = replace_na(outcome, "Unknown"))

# produce a histogram of all cases by age
ggplot(
  data = linelist,
  mapping = aes(x = age_years, fill = outcome)) +
  geom_histogram() + 
  gghighlight::gghighlight(outcome == "Death")     # highlight instances where the patient has died.

```

This also works well with faceting functions - it allows the user to produce facet plots with the background data highlighted that doesn't apply to the facet! Below we count cases by week and plot the epidemic curves by hospital (`color = ` and `facet_wrap()` set to `hospital` column).  

```{r, , warning=F, message=F}

# produce a histogram of all cases by age
linelist %>% 
  count(week = lubridate::floor_date(date_hospitalisation, "week"),
        hospital) %>% 
  ggplot()+
  geom_line(aes(x = week, y = n, color = hospital))+
  theme_minimal()+
  gghighlight::gghighlight() +                      # highlight instances where the patient has died
  facet_wrap(~hospital)                              # make facets by outcome

```





## Plotting multiple datasets  

Note that properly aligning axes to plot from multiple datasets in the same plot can be difficult. Consider one of the following strategies:  

* Merge the data prior to plotting, and convert to "long" format with a column reflecting the dataset  
* Use **cowplot** or a similar package to combine two plots (see below)  






<!-- ======================================================= -->
## Combine plots {}

Two packages that are very useful for combining plots are **cowplot** and **patchwork**. In this page we will mostly focus on **cowplot**, with occassional use of **patchwork**.  

Here is the online [introduction to cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html). You can read the more extensive documentation for each function online [here](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1). We will cover a few of the most common use cases and functions below.  

The **cowplot** package works in tandem with **ggplot2** - essentially, you use it to arrange and combine ggplots and their legends into compound figures. It can also accept **base** R graphics.  

```{r}
pacman::p_load(
  tidyverse,      # data manipulation and visualisation
  cowplot,        # combine plots
  patchwork       # combine plots
)
```


While faceting (described in the [ggplot basics] page) is a convenient approach to plotting, sometimes its not possible to get the results you want from its relatively restrictive approach. Here, you may choose to combine plots by sticking them together into a larger plot. There are three well known packages that are great for this - **cowplot**, **gridExtra**, and **patchwork**. However, these packages largely do the same things, so we'll focus on **cowplot** for this section. 

### `plot_grid()` {.unnumbered}

The **cowplot** package has a fairly wide range of functions, but the easiest use of it can be achieved through the use of `plot_grid()`. This is effectively a way to arrange predefined plots in a grid formation. We can work through another example with the malaria dataset - here we can plot the total cases by district, and also show the epidemic curve over time.


```{r, , warning=F, message=F}
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) 

# bar chart of total cases by district
p1 <- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +
  geom_bar(stat = "identity") +
  labs(
    x = "District",
    y = "Total number of cases",
    title = "Total malaria cases by district"
  ) +
  theme_minimal()

# epidemic curve over time
p2 <- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1) +
  labs(
    x = "Date of data submission",
    y =  "number of cases"
  ) +
  theme_minimal()

cowplot::plot_grid(p1, p2,
                  # 1 column and two rows - stacked on top of each other
                   ncol = 1,
                   nrow = 2,
                   # top plot is 2/3 as tall as second
                   rel_heights = c(2, 3))


```




### Combine legends {.unnumbered}  

If your plots have the same legend, combining them is relatively straight-forward. Simple use the **cowplot** approach above to combine the plots, but remove the legend from one of them (de-duplicate).  

If your plots have different legends, you must use an alternative approach:  

1) Create and save your plots *without legends* using `theme(legend.position = "none")`  
2) Extract the legends from each plot using `get_legend()` as shown below - *but extract legends from the plots modified to actually show the legend*  
3) Combine the legends into a legends panel  
4) Combine the plots and legends panel  


For demonstration we show the two plots separately, and then arranged in a grid with their own legends showing (ugly and inefficient use of space):  

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  labs(title = "Cases by outcome")


p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(axis.text.y = element_blank())+
  labs(title = "Cases by age")

```

Here is how the two plots look when combined using `plot_grid()` without combining their legends:  

```{r, warning=F, message=F}
cowplot::plot_grid(p1, p2, rel_widths = c(0.3))
```

And now we show how to combine the legends. Essentially what we do is to define each plot *without* its legend (`theme(legend.position = "none"`), and then we define each plot's legend *separately*, using the `get_legend()` function from **cowplot**. When we extract the legend from the saved plot, we need to add `+` the legend back in, including specifying the placement ("right") and smaller adjustments for alignment of the legends and their titles. Then, we combine the legends together vertically, and then combine the two plots with the newly-combined legends. Voila!  

```{r, warning=F, message=F}

# Define plot 1 without legend
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(legend.position = "none")+
  labs(title = "Cases by outcome")


# Define plot 2 without legend
p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  )+
  labs(title = "Cases by age")


# extract legend from p1 (from p1 + legend)
leg_p1 <- cowplot::get_legend(p1 +
                                theme(legend.position = "right",        # extract vertical legend
                                      legend.justification = c(0,0.5))+ # so legends  align
                                labs(fill = "Outcome"))                 # title of legend
# extract legend from p2 (from p2 + legend)
leg_p2 <- cowplot::get_legend(p2 + 
                                theme(legend.position = "right",         # extract vertical legend   
                                      legend.justification = c(0,0.5))+  # so legends align
                                labs(fill = "Age Category"))             # title of legend

# create a blank plot for legend alignment
#blank_p <- patchwork::plot_spacer() + theme_void()

# create legends panel, can be one on top of the other (or use spacer commented above)
legends <- cowplot::plot_grid(leg_p1, leg_p2, nrow = 2, rel_heights = c(.3, .7))

# combine two plots and the combined legends panel
combined <- cowplot::plot_grid(p1, p2, legends, ncol = 3, rel_widths = c(.4, .4, .2))

combined  # print


```

This solution was learned from [this post](https://stackoverflow.com/questions/52060601/ggplot-multiple-legends-arrangement) with a minor fix to align legends from [this post](https://github.com/wilkelab/cowplot/issues/33).  


<span style="color: darkgreen;">**_TIP:_** Fun note - the "cow" in **cowplot** comes from the creator's name - Claus O. Wilke.</span>  


### Inset plots {.unnumbered} 

You can inset one plot in another using **cowplot**. Here are things to be aware of:  

* Define the main plot with `theme_half_open()` from **cowplot**; it may be best to have the legend either on top or bottom  
* Define the inset plot. Best is to have a plot where you do not need a legend. You can remove plot theme elements with `element_blank()` as shown below.  
* Combine them by applying `ggdraw()` to the main plot, then adding `draw_plot()` on the inset plot and specifying the coordinates (x and y of lower left corner), height and width as proportion of the whole main plot.  


```{r, out.width=c('100%'), fig.show='hold', warning=F, message=F}

# Define main plot
main_plot <- ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset, fill = hospital))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+ 
  theme_half_open()+
  theme(legend.position = "bottom")+
  labs(title = "Epidemic curve and outcomes by hospital")


# Define inset plot
inset_plot <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
    geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
    scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
    coord_flip()+
    theme_minimal()+
    theme(legend.position = "none",
          axis.title.y = element_blank())+
    labs(title = "Cases by outcome") 


# Combine main with inset
cowplot::ggdraw(main_plot)+
     draw_plot(inset_plot,
               x = .6, y = .55,    #x = .07, y = .65,
               width = .4, height = .4)

```


This technique is explained more in these two vignettes:  

[Wilke lab](https://wilkelab.org/cowplot/articles/drawing_with_on_plots.html)  
[draw_plot() documentation](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1/topics/draw_plot)




<!-- ======================================================= -->
## Dual axes {}

A secondary y-axis is often a requested addition to a `ggplot2` graph. While there is a robust debate about the validity of such graphs in the data visualization community, and they are often not recommended, your manager may still want them. Below, we present one method to achieve them: using the **cowplot** package to combine two separate plots.  

This approach involves creating two separate plots - one with a y-axis on the left, and the other with y-axis on the right. Both will use a specific `theme_cowplot()` and must have the same x-axis. Then in a third command the two plots are aligned and overlaid on top of each other. The functionalities of **cowplot**, of which this is only one, are described in depth at this [site](https://wilkelab.org/cowplot/articles/aligning_plots.html).  

To demonstrate this technique we will overlay the epidemic curve with a line of the weekly percent of patients who died. We use this example because the alignment of dates on the x-axis is more complex than say, aligning a bar chart with another plot. Some things to note:  

* The epicurve and the line are aggregated into weeks prior to plotting *and* the `date_breaks` and `date_labels` are identical - we do this so that the x-axes of the two plots are the same when they are overlaid.  
* The y-axis is moved to the right-side for plot 2 with the `position = ` argument of `scale_y_continuous()`.  
* Both plots make use of `theme_cowplot()`  

Note there is another example of this technique in the [Epidemic curves] page - overlaying cumulative incidence on top of the epicurve.  

**Make plot 1**  
This is essentially the epicurve. We use `geom_area()` just to demonstrate its use (area under a line, by default)  

```{r, warning=F, message=F}
pacman::p_load(cowplot)            # load/install cowplot

p1 <- linelist %>%                 # save plot as object
     count(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     ggplot()+
          geom_area(aes(x = epiweek, y = n), fill = "grey")+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
     theme_cowplot()+
     labs(
       y = "Weekly cases"
     )

p1                                      # view plot 
```

**Make plot 2**  
Create the second plot showing a line of the weekly percent of cases who died.  

```{r, warning=F, message=F}

p2 <- linelist %>%         # save plot as object
     group_by(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     summarise(
       n = n(),
       pct_death = 100*sum(outcome == "Death", na.rm=T) / n) %>% 
     ggplot(aes(x = epiweek, y = pct_death))+
          geom_line()+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
          scale_y_continuous(
               position = "right")+
          theme_cowplot()+
          labs(
            x = "Epiweek of symptom onset",
            y = "Weekly percent of deaths",
            title = "Weekly case incidence and percent deaths"
          )

p2     # view plot
```

Now we align the plot using the function `align_plots()`, specifying horizontal and vertical alignment ("hv", could also be "h", "v", "none"). We specify alignment of all axes as well (top, bottom, left, and right) with "tblr". The output is of class list (2 elements).    

Then we draw the two plots together using `ggdraw()` (from **cowplot**) and referencing the two parts of the `aligned_plots` object.  

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(p1, p2, align="hv", axis="tblr")         # align the two plots and save them as list
aligned_plotted <- ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])  # overlay them and save the visual plot
aligned_plotted                                                                # print the overlayed plots

```



<!-- ### Statistical transformation {.unnumbered}   -->
<!-- Another way to do this to have the second axis be a direct transformation of the secondary axis.  -->

<!-- Differences in axis values will be purely cosmetic - if you want to show two different variables on one graph, with different y-axis scales for each variable, this will not work without some work behind the scenes. To obtain this effect, you will have to transform one of your variables in the data, and apply the same transformation *in reverse* when specifying the axis labels. Based on this, you can either specify the transformation explicitly (e.g. variable a is around 10x as large as variable b) or calculate it in the code (e.g. what is the ratio between the maximum values of each dataset). -->


<!-- The syntax for adding a secondary axis is very straightforward! When calling a `scale_xxx_xxx()` function (e.g. `scale_y_continuous()`), use the `sec.axis` argument to call the `sec_axis()` function. The `trans` argument in this function allows you to specify the label transformation for the axis - provide this in standard tidyverse syntax.  -->

<!-- For example, if we want to show the number of positive RDTs in the malaria dataset for facility 1, showing 0-4 year olds and all cases on chart: -->


<!-- ```{r, , warning=F, message=F} -->

<!-- # take malaria data from facility 1 -->
<!-- malaria_facility_1 <- malaria_data %>% -->
<!--   filter(location_name == "Facility 1") -->

<!-- # calculate the ratio between malaria_rdt_0-4 and malaria_tot  -->

<!-- tf_ratio <- max(malaria_facility_1$malaria_tot, na.rm = T) / max(malaria_facility_1$`malaria_rdt_0-4`, na.rm = T) -->

<!-- # transform the values in the dataset -->

<!-- malaria_facility_1 <- malaria_facility_1 %>% -->
<!--   mutate(malaria_rdt_0_4_tf = `malaria_rdt_0-4` * tf_ratio) -->


<!-- # plot the graph with dual axes -->

<!-- ggplot(malaria_facility_1, aes(x = data_date)) + -->
<!--   geom_line(aes(y = malaria_tot, col = "Total cases")) + -->
<!--   geom_line(aes(y = malaria_rdt_0_4_tf, col = "Cases: 0-4 years old")) + -->
<!--   scale_y_continuous( -->
<!--     name = "Total cases", -->
<!--     sec.axis = sec_axis(trans = ~ . / tf_ratio, name = "Cases: 0-4 years old") -->
<!--   ) + -->
<!--   labs(x = "date of data collection") + -->
<!--   theme_minimal() + -->
<!--   theme(legend.title = element_blank()) -->



<!-- ``` -->






<!-- ## Sparklines   -->

<!-- UNDER CONSTRUCTION   -->
<!-- (perhaps move to Tables for presentation page) -->




## Packages to help you  


There are some really neat R packages specifically designed to help you navigate **ggplot2**:  


### Point-and-click **ggplot2** with **equisse**  {.unnumbered}

"This addin allows you to interactively explore your data by visualizing it with the ggplot2 package. It allows you to draw bar plots, curves, scatter plots, histograms, boxplot and sf objects, then export the graph or retrieve the code to reproduce the graph."

Install and then launch the addin via the RStudio menu or with `esquisse::esquisser()`.

See the [Github page](https://github.com/dreamRs/esquisse)

[Documentation](https://dreamrs.github.io/esquisse/index.html)









## Miscellaneous  


### Numeric display {.unnumbered}  

You can disable scientific notation by running this command prior to plotting.  

```{r, eval=F}
options(scipen=999)
```

Or apply `number_format()` from the **scales** package to a specific value or column, as shown below.  

Use functions from the package **scales** to easily adjust how numbers are displayed. These can be applied to columns in your data frame, but are shown on individual numbers for purpose of example.  

```{r}
scales::number(6.2e5)
scales::number(1506800.62,  accuracy = 0.1,)
scales::comma(1506800.62, accuracy = 0.01)
scales::comma(1506800.62, accuracy = 0.01,  big.mark = "." , decimal.mark = ",")
scales::percent(0.1)
scales::dollar(56)
scales::scientific(100000)
```

## Resources

Inspiration
[ggplot graph gallery](https://www.tidyverse.org/blog/2018/07/ggplot2-3-0-0/)

Presentation of data
European Centre for Disease Prevention and Control [Guidelines of presentation of surveillance data](https://ecdc.europa.eu/sites/portal/files/documents/Guidelines%20for%20presentation%20of%20surveillance%20data-final-with-cover-for-we....pdf) 


Facets and labellers
[Using labellers for facet strips](http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#modifying-facet-label-text)
[Labellers](https://ggplot2.tidyverse.org/reference/labellers.html)

Adjusting order with factors
[fct_reorder](https://forcats.tidyverse.org/reference/fct_reorder.html)  
[fct_inorder](https://forcats.tidyverse.org/reference/fct_inorder.html)  
[How to reorder a boxplot](https://cmdlinetips.com/2019/02/how-to-reorder-a-boxplot-in-r/)  
[Reorder a variable in ggplot2](https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2.html)  
[R for Data Science - Factors](https://r4ds.had.co.nz/factors.html)  

Legends  
[Adjust legend order](https://stackoverflow.com/questions/38425908/reverse-stacking-order-without-affecting-legend-order-in-ggplot2-bar-charts)  

Captions
[Caption alignment](https://stackoverflow.com/questions/64701500/left-align-ggplot-caption)  

Labels  
[ggrepel](https://ggrepel.slowkow.com/articles/examples.html)  

Cheatsheets  
[Beautiful plotting with ggplot2](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/)  




<!-- TO DO - Under construction -->


<!-- * Straight horizontal, vertical, or other line -->

<!-- You can also add straight lines to your plot with `geom_hline()` (horizontal), `geom_vline()` (vertical) or `geom_abline()` (with a specified y intercept and slope) -->


<!-- Using option `label_wrap_gen` in facet_wrap to have multiple strip lines -->
<!-- labels and colors of strips -->

<!-- Axis text vertical adjustment -->
<!-- rotation -->
<!-- Labellers -->

<!-- limit range with limit() and coord_cartesian(), ylim(), or scale_x_continuous() -->
<!-- theme_classic() -->

<!-- expand = c(0,0) -->
<!-- coord_flip() -->
<!-- tick marks -->

<!-- ggrepel -->
<!-- animations -->

<!-- remove -->
<!-- remove title -->
<!-- using fill = or color = in labs() -->
<!-- flip order / don't flip order -->
<!-- move location -->
<!-- color?    theme(legend.title = element_text(colour="chocolate", size=16, face="bold"))+ scale_color_discrete(name="This color is\ncalled chocolate!?") -->
<!-- Color of boxes behind points in legend  -->
<!--      theme(legend.key=element_rect(fill='pink'))   or use fill = NA to remove them. http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/  -->
<!-- Change size of symbols in legend only guides(colour = guide_legend(override.aes = list(size=4))) -->


<!-- Turn off a layer in the legend -->
<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp)), size=4) -->
<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp), size=4), show_guide=FALSE) -->

<!-- Force a legend even if there is no aes().  -->
<!-- ggplot(nmmaps, aes(x=date, y=o3))+ -->
<!--      geom_line(aes(color="Important line"))+ -->
<!--      geom_point(aes(color="My points")) -->
<!-- Control the shape in the legend with guides - a list with linetype and shape -->
<!-- ggplot(nmmaps, aes(x=date, y=o3))+geom_line(aes(color="Important line"))+ -->
<!--    geom_point(aes(color="Point values"))+ -->
<!--   scale_colour_manual(name='', values=c('Important line'='grey', 'Point values'='red'), guide='legend') + -->
<!--   guides(colour = guide_legend(override.aes = list(linetype=c(1,0) -->
<!--                                                       , shape=c(NA, 16)))) -->
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_tips.Rmd-->


# Epidemic curves { }  

```{r, out.width=c('75%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "epicurve_top.png"))
```    


An epidemic curve (also known as an "epi curve") is a core epidemiological chart typically used to visualize the temporal pattern of illness onset among a cluster or epidemic of cases.  

Analysis of the epicurve can reveal temporal trends, outliers, the magnitude of the outbreak, the most likely time period of exposure, time intervals between case generations, and can even help identify the mode of transmission of an unidentified disease (e.g. point source, continuous common source, person-to-person propagation). One online lesson on interpretation of epi curves can be found at the website of the [US CDC](https://www.cdc.gov/training/quicklearns/epimode/index.html).    

In this page we demonstrate two approaches to producing epicurves in R:  

* The **incidence2** package, which can produce an epi curve with simple commands  
* The **ggplot2** package, which allows for advanced customizability via more complex commands  

Also addressed are specific use-cases such as:  

* Plotting aggregated count data  
* Faceting or producing small-multiples  
* Applying moving averages  
* Showing which data are "tentative" or subject to reporting delays  
* Overlaying cumulative case incidence using a second axis  

<!-- ======================================================= -->
## Preparation


### Packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r message=F, warning=F}
pacman::p_load(
  rio,          # file import/export
  here,         # relative filepaths 
  lubridate,    # working with dates/epiweeks
  aweek,        # alternative package for working with dates/epiweeks
  incidence2,   # epicurves of linelist data
  i2extras,     # supplement to incidence2
  stringr,      # search and manipulate character strings
  forcats,      # working with factors
  RColorBrewer, # Color palettes from colorbrewer2.org
  tidyverse     # data management + ggplot2 graphics
) 
```


### Import data {.unnumbered}

Two example datasets are used in this section:  

* Linelist of individual cases from a simulated epidemic  
* Aggregated counts by hospital from the same simulated epidemic  

The datasets are imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.  


```{r, echo=F, message=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```


**Case linelist**

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instruction in the [Download handbook and data] page. We assume the file is in the working directory so no sub-folders are specified in this file path.  

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")
```

The first 50 rows are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



**Case counts aggregated by hospital**  

For the purposes of the handbook, the dataset of weekly aggregated counts by hospital is created from the `linelist` with the following code. 

```{r, eval=F}
# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```

The first 50 rows are displayed below:  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




### Set parameters {.unnumbered}

For production of a report, you may want to set editable parameters such as the date for which the data is current (the "data date"). You can then reference the object `data_date` in your code when applying filters or in dynamic captions.

```{r set_parameters}
## set the report date for the report
## note: can be set to Sys.Date() for the current date
data_date <- as.Date("2015-05-15")
```



### Verify dates {.unnumbered}

Verify that each relevant date column is class Date and has an appropriate range of values. You can do this simply using `hist()` for histograms, or `range()` with `na.rm=TRUE`, or with `ggplot()` as below.  

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# check range of onset dates
ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset))
```



<!-- ======================================================= -->
## Epicurves with **incidence2** package { }

Below we demonstrate how to make epicurves using the **incidence2** package. The authors of this package have tried to allow the user to create and modify epicurves without needing to know **ggplot2** syntax. Much of this page is adapted from the package vignettes, which can be found at the **incidence2** [github page](https://github.com/reconhub/incidence2).   


<!-- ======================================================= -->
### Simple example {.unnumbered}

**2 steps are required to plot an epidemic curve with the *incidence2* package:**  

1) **Create** an *incidence object* (using the function `incidence()`)  
    + Provide the data  
    + Specify the date column to `date_index = `  
    + Specify the `interval = ` into which the cases should be aggregated (daily, weekly, monthly..)  
    + Specify any grouping columns (e.g. gender, hospital, outcome)  
2) **Plot** the incidence object  
    + Specify labels, colors, titles, etc.  


Below, we load the **incidence2** package, create the incidence object from the `linelist` on column `date_onset` and aggregated cases by day. We then print a summary of the incidence object. 

```{r, warning=F, message=F}
# load incidence2 package
pacman::p_load(incidence2)

# create the incidence object, aggregating cases by day
epi_day <- incidence(       # create incidence object
  x = linelist,             # dataset
  date_index = date_onset,  # date column
  interval = "day"          # date grouping interval
  )
```

The **incidence2** object itself looks like a tibble (like a data frame) and can be printed or further manipulated like a data frame.  

```{r}
class(epi_day)
```

Here is what it looks like when printed. It has a `date_index` column and a `count` column.  

```{r}
epi_day
```

You can also print a summary of the object:  

```{r}
# print summary of the incidence object
summary(epi_day)
```

To *plot* the *incidence* object, use `plot()` on the *name of the incidence object*. In the background, the function `plot.incidence2()` is called, so to read the **incidence2**-specific documentation you would run `?plot.incidence2`.  

```{r}
# plot the incidence object
plot(epi_day)
```

If you notice lots of tiny white vertical lines, try to adjust the size of your image. For example, if you export your plot with `ggsave()`, you can provide numbers to `width = ` and `height = `. If you widen the plot those lines may disappear.    



### Change time interval of case aggregation {.unnumbered}  
The `interval = ` argument of `incidence()` defines how the observations are grouped into vertical bars. 

**Specify interval**  

**incidence2** provides flexibility and understandable syntax for specifying how you want to aggregate your cases into epicurve bars. Provide a value like the ones below to the `interval = ` argument. You can write any of the below as plural (e.g. "week**s**"), and you can add numbers before (e.g. "3 months").  

Argument option | Further explanation 
------------------- | ------------------------------------ |
Number (1, 7, 13, 14, etc.) | Number of days per interval  
"week" | note: Monday start day is default
"2 weeks" | or 3, 4, 5...
"Sunday week" | weeks beginning on Sundays (could also use Thursday, etc.)
"2 Sunday weeks" | or 3, 4, 5...
"MMWRweek" | week starts on Sundays - see US CDC
"month" | 1st of month
"quarter" | 1st of month of quarter
"2 months" | or 3, 4, 5...
"year" | 1st day of calendar year


Below are examples of how different intervals look when applied to the linelist. Note how the default format and frequency of the date *labels* on the x-axis change as the date interval changes.  

```{r incidence, out.width=c('50%', '50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# Create the incidence objects (with different intervals)
##############################
# Weekly (Monday week by default)
epi_wk      <- incidence(linelist, date_onset, interval = "Monday week")

# Sunday week
epi_Sun_wk  <- incidence(linelist, date_onset, interval = "Sunday week")

# Three weeks (Monday weeks by default)
epi_2wk     <- incidence(linelist, date_onset, interval = "2 weeks")

# Monthly
epi_month   <- incidence(linelist, date_onset, interval = "month")

# Quarterly
epi_quarter   <- incidence(linelist, date_onset, interval = "quarter")

# Years
epi_year   <- incidence(linelist, date_onset, interval = "year")


# Plot the incidence objects (+ titles for clarity)
############################
plot(epi_wk)+      labs(title = "Monday weeks")
plot(epi_Sun_wk)+  labs(title = "Sunday weeks")
plot(epi_2wk)+     labs(title = "2 (Monday) weeks")
plot(epi_month)+   labs(title = "Months")
plot(epi_quarter)+ labs(title = "Quarters")
plot(epi_year)+    labs(title = "Years")

```


<!-- **Begin at first case**   -->

<!-- If you want the intervals to begin at the first case, you can add the argument `standard = TRUE` to the `incidence()` command. This only works if the interval is either "week", "month", "quarter" or "year".   -->

**First date**

You can optionally specify a value of class Date (e.g. `as.Date("2016-05-01")`) to `firstdate = ` in the `incidence()` command. If given, the data will be trimmed to this range and the intervals will begin on this date. 



### Groups {.unnumbered}

Groups are specified in the `incidence()` command, and can be used to color the bars or to facet the data. To specify groups in your data provide the column name(s) to the `groups =` argument in the `incidence()` command (no quotes around the column name). If specifying multiple columns, put their names within `c()`.

You can specify that cases with missing values in the grouping columns be listed as a distinct `NA` group by setting `na_as_group = TRUE`. Otherwise, they will be excluded from the plot.   

* To *color the bars by a grouping column*, you must again provide the column name to `fill = ` in the `plot()` command.  

* To *facet based on a grouping column*, see the section below on facets with **incidence2**.  

In the example below, the cases in the whole outbreak are grouped by their age category. Missing values are included as a group. The epicurve interval is weeks.  


```{r, message=F, warning=F}
# Create incidence object, with data grouped by age category
age_outbreak <- incidence(
  linelist,                # dataset
  date_index = date_onset, # date column
  interval = "week",       # Monday weekly aggregation of cases
  groups = age_cat,        # age_cat is set as a group
  na_as_group = TRUE)      # missing values assigned their own group

# plot the grouped incidence object
plot(
  age_outbreak,             # incidence object with age_cat as group
  fill = age_cat)+          # age_cat is used for bar fill color (must have been set as a groups column above)
labs(fill = "Age Category") # change legend title from default "age_cat" (this is a ggplot2 modification)
```
<span style="color: darkgreen;">**_TIP:_** Change the title of the legend by adding `+` the **ggplot2** command `labs(fill = "your title")` to your **incidence2** plot.</span>  

You can also have the grouped bars display side-by-side by setting `stack = FALSE` in `plot()`, as shown below:  

```{r, warning=F, message=F}
# Make incidence object of monthly counts. 
monthly_gender <- incidence(
 linelist,
 date_index = date_onset,
 interval = "month",
 groups = gender            # set gender as grouping column
)

plot(
  monthly_gender,   # incidence object
  fill = gender,    # display bars colored by gender
  stack = FALSE)    # side-by-side (not stacked)
``` 

You can set the `na_as_group = ` argument to FALSE in the `incidence()` command to remove rows with missing values from the plot.  




### Filtered data {.unnumbered}

To plot the epicurve of a subset of data:  

1) Filter the linelist data  
2) Provide the filtered data to the `incidence()` command  
3) Plot the incidence object

The example below uses data filtered to show only cases at Central Hospital.  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(central_data, date_index = date_onset, interval = "week")

# plot the incidence object
plot(central_outbreak, title = "Weekly case incidence at Central Hospital")
```




### Aggregated counts {.unnumbered}

If your original data are aggregated (counts), provide the name of the column that contains the case counts to the `count = ` argument when creating the incidence object with `incidence()`.  

For example, this data frame `count_data` is the linelist aggregated into daily counts by hospital. The first 50 rows look like this:  

```{r message=FALSE, echo=F}
DT::datatable(head(count_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

If you are beginning your analysis with daily count data like the dataset above, your `incidence()` command to convert this to a weekly epicurve by hospital would look like this:  

```{r}
epi_counts <- incidence(              # create weekly incidence object
  count_data,                         # dataset with counts aggregated by day
  date_index = date_hospitalisation,  # column with dates
  count = n_cases,                    # column with counts
  interval = "week",                  # aggregate daily counts up to weeks
  groups = hospital                   # group by hospital
  )

# plot the weekly incidence epi curve, with stacked bars by hospital
plot(epi_counts,                      # incidence object
     fill = hospital)                 # color the bars by hospital
```




### Facets/small multiples {.unnumbered}  

To facet the data by group (i.e. produce "small multiples"):  

1) Specify the faceting column to `groups = ` when you create the incidence object  
2) Use the `facet_plot()` command instead of `plot()`  
3) Specify which grouping columns to use as `fill = ` and which to use as `facets = `  

Below, we set both columns `hospital` and `outcome` as grouping columns in the `incidence()` command. Then, in `facet_plot()` we plot the epicurve, specifying that we want a different epicurve for each hospital and that within each epicurve the bars should be stacked and colored by outcome.  
 

```{r, warning=F, message=F}
epi_wks_hosp_out <- incidence(
  linelist,                      # dataset
  date_index = date_onset,       # date column
  interval = "month",            # monthly bars  
  groups = c(outcome, hospital)  # both outcome and hospital are given as grouping columns
  )

# plot
incidence2::facet_plot(
  epi_wks_hosp_out,      # incidence object
  facets = hospital,     # facet column
  fill = outcome)        # fill column

```

Note that the package **ggtree** (used for displaying phylogenetic trees) also has a function `facet_plot()` - this is why we specified `incidence2::facet_plot()` above.  



### Modifications with `plot()` {.unnumbered} 

An epicurve produced by **incidence2** can be modified via these arguments *within the `plot()` function*.  

**Here are `plot()` arguments that modify the appearance of the bars:**  

Argument | Description | Examples
------------------|---------------------------------------|-------------------
`fill = `|Bar color. Either a color name or a column name previously specified to `groups = ` in the `incidence()` command|`fill = "red"`, or `fill = gender`  
`color = ` |Color around each bar, or around each grouping within a bar|`border = "white"` 
`legend = `|Location of legend|One of "bottom", "top", "left", "right", or "none"  
`alpha = `|Transparency of bars/boxes|1 is fully opaque, 0 is fully transparent
`width = `|Value between 0 and 1 indicating the relative size of the bars to their time interval|`width = .7`  
`show_cases = `|Logical; if TRUE, each case shows as a box. Displays best on smaller outbreaks.|`show_cases = TRUE`  

**Here are `plot()` arguments that modify the date axis:**  

Argument(s)|Description
----------------------|----------------------------------------------------
`centre_dates = `|TRUE/FALSE as to whether date displays appear under center of bars, or at beginning of bars  
`date_format = `|Adjust the date display format using strptime ("%") syntax. Only works if `centre_dates = FALSE` (details below).  
`n.breaks = `|Approximate number of x-axis label breaks desired.  
`angle = `|Angle of x-axis date labels (number of degrees)  
`size = `|Size of text in points  

Note that the `date_breaks = ` argument only works if `centre_dates = FALSE`. Provide a character value in quotation marks using the strptime syntax below, as detailed in the [Working with dates] page. You can use `\n` for a "newline".  

%d = Day number of month (5, 17, 28, etc.)  
%j = Day number of the year (Julian day 001-366)  
%a = Abbreviated weekday (Mon, Tue, Wed, etc.)  
%A = Full weekday (Monday, Tuesday, etc.)  
%w = Weekday number (0-6, Sunday is 0)  
%u = Weekday number (1-7, Monday is 1)  
%W = Week number (00-53, Monday is week start)  
%U = Week number (01-53, Sunday is week start)  
%m = Month number (e.g. 01, 02, 03, 04)  
%b = Abbreviated month (Jan, Feb, etc.)  
%B = Full month (January, February, etc.)  
%y = 2-digit year  (e.g. 89)  
%Y = 4-digit year  (e.g. 1989)  
%h = hours (24-hr clock)  
%m = minutes  
%s = seconds  
%z = offset from GMT  
%Z = Time zone (character)


<!-- <span style="color: darkgreen;">**_TIP:_** For breaks every "nth" interval (e.g. every 4th), use `n.breaks = nrow(i)/n` (where “i” is your incidence object name and “n” is a number). If your data are grouped, you will need to multiply "n" by the number of unique groups.</span>   -->



**Here are `plot()` arguments that modify plot labels:**

Argument(s)|Description
----------------------|----------------------------------------------------
`title = `|Title of plot|`title = "Epidemic curve of Acute Jaundice Syndrome (AJS)"`
`xlab = `|Title of x-axis|`xlab = "Date of onset"`  
`ylab = `|Title of y-axis|`ylab = "Daily case"`  
`size = `|Size of x-axis text in pts (use ggplot's theme() to adjust other sizes)  


An example using many of the above arguments:  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = outcome)

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  angle = 30,                           # angle of date labels
  centre_dates = FALSE,                 # date labels at edge of bar
  date_format = "%a %d %b %Y\n(Week %W)" # adjust how dates are displayed
  )
```

To further adjust plot appearance, see the section below on modifications with `ggplot()`.  






### Modifications with ggplot2 {.unnumbered}

You can further modify an **incidence2** plot by adding **ggplot2** modifications with a `+` after the close of the incidence `plot()` function, as demonstrated below.  

Below, the **incidence2** plot finishes and then **ggplot2** commands are used to modify the axes, add a caption, and adjust the bold font and text size.  

Note that if you add `scale_x_date()`, most date formatting from `plot()` will be overwritten. See the `ggplot()` epicurves section and the Handbook page [ggplot tips] for more options.  

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = c(outcome))

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  centre_dates = FALSE,                   
  date_format = "%a %d %b\n%Y (Week %W)", 
  angle = 30                           # angle of date labels
  )+
  
  scale_y_continuous(
    breaks = seq(from = 0, to = 30, by = 5),  # specify y-axis increments by 5
    expand = c(0,0))+                         # remove excess space below 0 on y-axis
  
  # add dynamic caption
  labs(
    fill = "Patient outcome",                               # Legend title
    caption = stringr::str_glue(                            # dynamic caption - see page on characters and strings for details
      "n = {central_cases} from Central Hospital
      Case onsets range from {earliest_date} to {latest_date}. {missing_onset} cases are missing date of onset and not shown",
      central_cases = nrow(central_data),
      earliest_date = format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),
      latest_date = format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),      
      missing_onset = nrow(central_data %>% filter(is.na(date_onset)))))+
  
  # adjust bold face, and caption position
  theme(
    axis.title = element_text(size = 12, face = "bold"),    # axis titles larger and bold
    axis.text = element_text(size = 10, face = "bold"),     # axis text size and bold
    plot.caption = element_text(hjust = 0, face = "italic") # move caption to left
  )
  
```




### Change colors  {.unnumbered}  

#### Specify a palette {.unnumbered}  

Provide the name of a pre-defined palette to the `col_pal = ` argument in `plot()`. The **incidence2** package comes with 2 pre-defined paletted: "vibrant" and "muted". In "vibrant" the first 6 colors and distinct and in "muted" the first 9 colors are distinct. After these numbers, the colors are interpolations/intermediaries of other colors. These pre-defined palettes can be found at [this website](https://personal.sron.nl/~pault/#sec:qualitative). The palettes exclude grey, which is reserved for missing data (use `na_color = ` to change this default).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# Create incidence object, with data grouped by age category  
age_outbreak <- incidence(
  linelist,
  date_index = date_onset,   # date of onset for x-axis
  interval = "week",         # weekly aggregation of cases
  groups = age_cat)

# plot the epicurve with default palette
plot(age_outbreak, fill = age_cat, title = "'vibrant' default incidence2 palette")

# plot with different color palette
#plot(age_outbreak, fill = age_cat, col_pal = muted, title = "'muted' incidence2 palette")
```

You can also use one of the **base** R palettes (put the name of the palette *without* quotes).  

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = heat.colors, title = "base R heat.colors palette")

# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = rainbow, title = "base R rainbow palette")
```

You can also add a color palette from the **viridis** package or **RColorBrewer** package. First those packages must be loaded, then add their respective `scale_fill_*()` functions with a `+`, as shown below.

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
pacman::p_load(RColorBrewer, viridis)

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "Viridis palette")+
  scale_fill_viridis_d(
    option = "inferno",     # color scheme, try also "plasma" or the default
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "RColorBrewer palette")+
  scale_fill_brewer(
    palette = "Dark2",      # color palette, try also Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values
```


#### Specify manually {.unnumbered}  

To specify colors manually, add the **ggplot2** function `scale_fill_manual()` to the `plot()` with a `+` and provide the vector of colors names or HEX codes to the argument `values = `. The number of colors listed must equal the number of groups. Be aware of whether missing values are a group - they can be converted to a character value like "Missing" during your data preparation with the function `fct_explicit_na()` as explained in the page on [Factors].  

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# manual colors
plot(age_outbreak, fill = age_cat, title = "Manually-specified colors")+
  scale_fill_manual(
    values = c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange", "red", "lightblue"),  # colors
    name = "Age Category")      # Name for legend
```

As mentioned in the [ggplot tips] page, you can create your own palettes using `colorRampPalette()` on a vector of colors and specifying the number of colors you want in return. This is a good way to get many colors in a ramp by specifying a few.  

```{r}
my_cols <- c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange")
my_palette <- colorRampPalette(my_cols)(12)  # expand the 6 colors above to 12 colors
my_palette
```
          
          
### Adjust level order {.unnumbered}  

To adjust the order of group appearance (on plot and in legend), the grouping column must be class Factor. See the page on [Factors] for more information.  

First, let's see a weekly epicurve by hospital with the default ordering:  

```{r, message=F, warning=F}
# ORIGINAL - hospital NOT as factor
###################################

# create weekly incidence object, rows grouped by hospital and week
hospital_outbreak <- incidence(
  linelist,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak, fill = hospital, title = "ORIGINAL - hospital not a factor")
```

Now, to adjust the order so that "Missing" and "Other" are at the top of the epicurve we can do the following:  

* Load the package **forcats**, to work with factors  
* Adjust the dataset - in this case we'll define a new dataset (`plot_data`) in which:  
  * the `gender` column is defined as a factor the order of levels are set with `fct_relevel()` so that "Other" and "Missing" are first, so they appear at the top of the bars  
* The incidence object is created and plotted as before  
* We add **ggplot2** modifications  
  * `scale_fill_manual()` to manually assign colors so that "Missing" is grey and "Other" is beige  
 



```{r, message=F, warning=F}
# MODIFIED - hospital as factor
###############################

# load forcats package for working with factors
pacman::p_load(forcats)

# Convert hospital column to factor and adjust levels
plot_data <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Set "Missing" and "Other" as top levels


# Create weekly incidence object, grouped by hospital and week
hospital_outbreak_mod <- incidence(
  plot_data,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak_mod, fill = hospital)+
  
  # manual specify colors
  scale_fill_manual(values = c("grey", "beige", "darkgreen", "green2", "orange", "red", "pink"))+                      

  # labels added via ggplot
  labs(
      title = "MODIFIED - hospital as factor",   # plot title
      subtitle = "Other & Missing at top of epicurve",
      y = "Weekly case incidence",               # y axis title  
      x = "Week of symptom onset",               # x axis title
      fill = "Hospital")                         # title of legend     
```

<span style="color: darkgreen;">**_TIP:_** If you want to reverse the order of the legend only, add this **ggplot2** command `guides(fill = guide_legend(reverse = TRUE))`.</span>  



### Vertical gridlines {.unnumbered}  

If you plot with default **incidence2** settings, you may notice that the vertical gridlines appear at each date label and once between each date label. This can result in gridlines intersecting with the top of some bars.  

<!-- [TO DO Note this paragraph is not applicable with version 1.0.0 of incidence2). You can specify the interval for the gridlines by adding **ggplot2**'s `scale_x_date()` command to your **incidence2** plot. Within it, specify the intervals for `date_breaks = ` and `date_minor_breaks = ` (e.g. "weeks" or "3 weeks" or "months"). Note that use of `scale_x_date()` will over-ride any formatting of the date labels in `plot()`, so you will need to specify any string format to `date_labels = ` as below.   -->

You can remove all gridlines by adding the **ggplot2** command `theme_classic()`.  

```{r, warning=F, message=F, out.width = c('50%', '50%', '50%'), fig.show='hold'}
# make incidence object
a <- incidence(
  central_data,
  date_index = date_onset,
  interval = "Monday weeks"
)

# Default gridlines
plot(a, title = "Default lines")

# Specified gridline intervals
# NOT WORKING WITH INCIDENCE2 1.0.0
# plot(a, title = "Weekly lines")+
#   scale_x_date(
#     date_breaks = "4 weeks",      # major vertical lines align on weeks
#     date_minor_breaks = "weeks",  # minor vertical lines every week
#     date_labels = "%a\n%d\n%b")   # format of date labels

# No gridlines
plot(a, title = "No lines")+
  theme_classic()                 # remove all gridlines
```

Note however, that if using weeks, the `date_breaks` and `date_minor_breaks` arguments only work for *Monday* weeks. If your weeks are by another day of the week you will need to manually provide a vector of dates to the `breaks = ` and `minor_breaks = ` arguments instead. See the **ggplot2** section for examples of this using `seq.Date()`.

### Cumulative incidence {.unnumbered}  

You can easily produce a plot of cumulative incidence by passing the incidence object to the **incidence2** command `cumulate()` and then to `plot()`. This also works with `facet_plot()`.  

```{r}
# make weekly incidence object
wkly_inci <- incidence(
  linelist,
  date_index = date_onset,
  interval = "week"
)

# plot cumulative incidence
wkly_inci %>% 
  cumulate() %>% 
  plot()
```


See the section farther down on this page for alternative method to plot cumulative incidence with **ggplot2** - for example to overlay a cumulative incidence line over an epicurve.  

### Rolling average  {.unnumbered}

You can add a rolling average to an **incidence2** plot easily with `add_rolling_average()` from the **i2extras** package. Pass your incidence2 object to this function, and then to `plot()`. Set `before = ` as the number of previous days you want included in the rolling average (default is 2). If your data are grouped, the rolling average will be calculated per group. 

```{r, warning=F, message=F}
rolling_avg <- incidence(                    # make incidence object
  linelist,
  date_index = date_onset,
  interval = "week",
  groups = gender) %>% 
  
  i2extras::add_rolling_average(before = 6)  # add rolling averages (in this case, by gender)

# plot
plot(rolling_avg) # faceted automatically because rolling average on groups
```

To learn how to apply rolling averages more generally on data, see the Handbook page on [Moving averages].  


<!-- ======================================================= -->
## Epicurves with ggplot2 { }

Using `ggplot()` to build your epicurve allows for more flexibility and customization, but requires more effort and understanding of how `ggplot()` works.  

Unlike using the **incidence2** package, you must *manually* control the aggregation of the cases by time (into weeks, months, etc) *and* the intervals of the labels on the date axis. This must be carefully managed.  

These examples use a subset of the `linelist` dataset - only the cases from Central Hospital.  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r}
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")
```

```{r, eval=F, echo=F}
detach("package:tidyverse", unload=TRUE)
library(tidyverse)
```


To produce an epicurve with `ggplot()` there are three main elements:  

* A histogram, with linelist cases aggregated into "bins" distinguished by specific "break" points  
* Scales for the axes and their labels  
* Themes for the plot appearance, including titles, labels, captions, etc.


### Specify case bins {.unnumbered}  

Here we show how to specify how cases will be aggregated into histogram bins ("bars"). It is important to recognize that the aggregation of cases into histogram bins is **not** necessarily the same intervals as the dates that will appear on the x-axis. 

Below is perhaps the most simple code to produce daily and weekly epicurves.  

In the over-arching `ggplot()` command the dataset is provided to `data = `. Onto this foundation, the geometry of a histogram is added with a `+`. Within the `geom_histogram()`, we map the aesthetics such that the column `date_onset` is mapped to the x-axis. Also within the `geom_histogram()` but *not* within `aes()` we set the `binwidth =` of the histogram bins, in days. If this **ggplot2** syntax is confusing, review the page on [ggplot basics].  

<span style="color: orange;">**_CAUTION:_** Plotting weekly cases by using `binwidth = 7` starts the first 7-day bin at the first case, which could be any day of the week! To create specific weeks, see section below .</span>


``` {r ggplot_simple,  out.width = c('50%', '50%'), fig.show='hold', warning= F, message = F}
# daily 
ggplot(data = central_data) +          # set data
  geom_histogram(                      # add histogram
    mapping = aes(x = date_onset),     # map date column to x-axis
    binwidth = 1)+                     # cases binned by 1 day 
  labs(title = "Central Hospital - Daily")                # title

# weekly
ggplot(data = central_data) +          # set data 
  geom_histogram(                      # add histogram
      mapping = aes(x = date_onset),   # map date column to x-axis
      binwidth = 7)+                   # cases binned every 7 days, starting from first case (!) 
  labs(title = "Central Hospital - 7-day bins, starting at first case") # title
```

Let us note that the first case in this Central Hospital dataset had symptom onset on:  

```{r}
format(min(central_data$date_onset, na.rm=T), "%A %d %b, %Y")
```

**To manually specify the histogram bin breaks, do *not* use the `binwidth = ` argument, and instead supply a vector of dates to `breaks = `.**  

Create the vector of dates with the **base** R function `seq.Date()`. This function expects arguments `to = `, `from = `, and `by = `. For example, the command below returns monthly dates starting at Jan 15 and ending by June 28.

```{r}
monthly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                           to = as.Date("2015-07-15"),
                           by = "months")

monthly_breaks   # print
```

This vector can be provided to `geom_histogram()` as `breaks = `:  

```{r, warning=F, message=F}
# monthly 
ggplot(data = central_data) +  
  geom_histogram(
    mapping = aes(x = date_onset),
    breaks = monthly_breaks)+         # provide the pre-defined vector of breaks                    
  labs(title = "Monthly case bins")   # title
```

A simple weekly date sequence can be returned by setting `by = "week"`. For example: 

```{r}
weekly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                          to = as.Date("2015-07-15"),
                          by = "week")
```

 
An alternative to supplying specific start and end dates is to write *dynamic* code so that weekly bins begin *the Monday before the first case*. **We will use these date vectors throughout the examples below.**  
     
```{r}
# Sequence of weekly Monday dates for CENTRAL HOSPITAL
weekly_breaks_central <- seq.Date(
  from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1), # monday before first case
  to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # monday after last case
  by   = "week")
```  

Let's unpack the rather daunting code above:  

* The "from" value (earliest date of the sequence) is created as follows: the minimum date value (`min()` with `na.rm=TRUE`) in the column `date_onset` is fed to `floor_date()` from the **lubridate** package. `floor_date()` set to "week" returns the start date of that cases's "week", given that the start day of each week is a Monday (`week_start = 1`).  
* Likewise, the "to" value (end date of the sequence) is created using the inverse function `ceiling_date()` to return the Monday *after* the last case.  
* The "by" argument of `seq.Date()` can be set to any number of days, weeks, or months.   
* Use `week_start = 7` for Sunday weeks  

As we will use these date vectors throughout this page, we also define one for the whole outbreak (the above is for Central Hospital only).  

```{r}
# Sequence for the entire outbreak
weekly_breaks_all <- seq.Date(
  from = floor_date(min(linelist$date_onset, na.rm=T),   "week", week_start = 1), # monday before first case
  to   = ceiling_date(max(linelist$date_onset, na.rm=T), "week", week_start = 1), # monday after last case
  by   = "week")
```

These `seq.Date()` outputs can be used to create histogram bin breaks, but also the breaks for the date labels, which may be independent from the bins. Read more about the date labels in later sections.  

<span style="color: darkgreen;">**_TIP:_** For a more simple `ggplot()` command, save the bin breaks and date label breaks as named vectors in advance, and simply provide their names to `breaks = `.</span>  




### Weekly epicurve example {.unnumbered}  

**Below is detailed example code to produce weekly epicurves for Monday weeks, with aligned bars, date labels, and vertical gridlines.** This section is for the user who needs code quickly. To understand each aspect (themes, date labels, etc.) in-depth, continue to the subsequent sections. Of note:  

* The *histogram bin breaks* are defined with `seq.Date()` as explained above to begin the Monday before the earliest case and to end the Monday after the last case  
* The interval of *date labels* is specified by `date_breaks =` within `scale_x_date()`  
* The interval of minor vertical gridlines between date labels is specified to `date_minor_breaks = `  
* We use `closed = "left"` in the `geom_histogram()` to ensure the date are counted in the correct bins  
* `expand = c(0,0)` in the x and y scales removes excess space on each side of the axes, which also ensures the date labels begin from the first bar.  

```{r, warning=F, message=F}
# TOTAL MONDAY WEEK ALIGNMENT
#############################
# Define sequence of weekly breaks
weekly_breaks_central <- seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1), # Monday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # Monday after last case
      by   = "week")    # bins are 7-days 


ggplot(data = central_data) + 
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    
    # mapping aesthetics
    mapping = aes(x = date_onset),  # date column mapped to x-axis
    
    # histogram bin breaks
    breaks = weekly_breaks_central, # histogram bin breaks defined previously
      
    closed = "left",  # count cases from start of breakpoint
    
    # bars
    color = "darkblue",     # color of lines around bars
    fill = "lightblue"      # color of fill within bars
  )+ 
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),           # remove excess x-axis space before and after case bars
    date_breaks       = "4 weeks",        # date labels and major vertical gridlines appear every 3 Monday weeks
    date_minor_breaks = "week",           # minor vertical lines appear every Monday week
    date_labels       = "%a\n%d %b\n%Y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+             # remove excess y-axis space below 0 (align histogram flush with x-axis)
  
  # aesthetic themes
  theme_minimal()+                # simplify plot background
  
  theme(
    plot.caption = element_text(hjust = 0,        # caption on left side
                                face = "italic"), # caption in italics
    axis.title = element_text(face = "bold"))+    # axis titles in bold
  
  # labels including dynamic caption
  labs(
    title    = "Weekly incidence of cases (Monday weeks)",
    subtitle = "Note alignment of bars, vertical gridlines, and axis labels on Monday weeks",
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### Sunday weeks {.unnumbered}  

To achieve the above plot for Sunday weeks a few modifications are needed, because the `date_breaks = "weeks"` only work for Monday weeks.  

* The break points of the *histogram bins* must be set to Sundays (`week_start = 7`)  
* Within `scale_x_date()`, the similar date breaks should be provided to `breaks =` and `minor_breaks = ` to ensure the date labels and vertical gridlines align on Sundays.  

For example, the `scale_x_date()` command for Sunday weeks could look like this:  

```{r, eval=F}
scale_x_date(
    expand = c(0,0),
    
    # specify interval of date labels and major vertical gridlines
    breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7), # Sunday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # Sunday after last case
      by   = "4 weeks"),
    
    # specify interval of minor vertical gridline 
    minor_breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7), # Sunday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # Sunday after last case
      by   = "week"),
   
    # date label format
    date_labels = "%a\n%d %b\n%Y")+         # day, above month abbrev., above 2-digit year

```



### Group/color by value {.unnumbered}

The histogram bars can be colored by group and "stacked". To designate the grouping column, make the following changes. See the [ggplot basics] page for details.  

* Within the histogram aesthetic mapping `aes()`, map the column name to the `group = ` and `fill = ` arguments  
* Remove any `fill = ` argument *outside* of `aes()`, as it will override the one inside  
* Arguments *inside* `aes()` will apply *by group*, whereas any *outside* will apply to all bars (e.g. you may still want `color = ` outside, so each bar has the same border)  

Here is what the `aes()` command would look like to group and color the bars by gender:  

```{r, eval=F}
aes(x = date_onset, group = gender, fill = gender)
```

Here it is applied:  

```{r, warning=F, message=F}
ggplot(data = linelist) +     # begin with linelist (many hospitals)
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = hospital,       # set data to be grouped by hospital
      fill = hospital),       # bar fill (inside color) by hospital
    
    # bin breaks are Monday weeks
    breaks = weekly_breaks_all,   # sequence of weekly Monday bin breaks for whole outbreak, defined in previous code       
    
    closed = "left",          # count cases from start of breakpoint

    # Color around bars
    color = "black")
```


### Adjust colors {.unnumbered}  

* To *manually* set the fill for each group, use `scale_fill_manual()` (note: `scale_color_manual()` is different!).
  * Use the `values = ` argument to apply a vector of colors.  
  * Use `na.value = ` to specify a color for `NA` values.  
  * Use the `labels = ` argument to change the text of legend items. To be safe, provide as a named vector like `c("old" = "new", "old" = "new")` or adjust the values in the data itself.  
  * Use `name = ` to give a proper title to the legend  
* For more tips on color scales and palettes, see the page on [ggplot basics].  

```{r, warning=F, message=F}
ggplot(data = linelist)+           # begin with linelist (many hospitals)
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,          # cases grouped by hospital
        fill = hospital),          # bar fill by hospital
    
    # bin breaks
    breaks = weekly_breaks_all,    # sequence of weekly Monday bin breaks, defined in previous code
    
    closed = "left",               # count cases from start of breakpoint

    # Color around bars
    color = "black")+              # border color of each bar
  
  # manual specification of colors
  scale_fill_manual(
    values = c("black", "orange", "grey", "beige", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St. Mark's"),
    name = "Hospital") # specify fill colors ("values") - attention to order!
```




### Adjust level order {.unnumbered}  

The order in which grouped bars are stacked is best adjusted by classifying the grouping column as class Factor. You can then designate the factor level order (and their display labels). See the page on [Factors] or [ggplot tips] for details.  

Before making the plot, use the `fct_relevel()` function from **forcats** package to convert the grouping column to class factor and manually adjust the level order, as detailed in the page on [Factors].  

```{r}
# load forcats package for working with factors
pacman::p_load(forcats)

# Define new dataset with hospital as factor
plot_data <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Convert to factor and set "Missing" and "Other" as top levels to appear on epicurve top

levels(plot_data$hospital) # print levels in order
```

In the below plot, the only differences from previous is that column `hospital` has been consolidated as above, and we use `guides()` to reverse the legend order, so that "Missing" is on the bottom of the legend.  

```{r, warning=F, message=F}
ggplot(plot_data) +                     # Use NEW dataset with hospital as re-ordered factor
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,               # cases grouped by hospital
        fill = hospital),               # bar fill (color) by hospital
    
    breaks = weekly_breaks_all,         # sequence of weekly Monday bin breaks for whole outbreak, defined at top of ggplot section
    
    closed = "left",                    # count cases from start of breakpoint

    color = "black")+                   # border color around each bar
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space before and after case bars
    date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
    date_minor_breaks = "week",         # vertical lines appear every Monday week
    date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+                   # remove excess y-axis space below 0
  
  # manual specification of colors, ! attention to order
  scale_fill_manual(
    values = c("grey", "beige", "black", "orange", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St. Mark's"),
    name = "Hospital")+ 
  
  # aesthetic themes
  theme_minimal()+                      # simplify plot background
  
  theme(
    plot.caption = element_text(face = "italic", # caption on left side in italics
                                hjust = 0), 
    axis.title = element_text(face = "bold"))+   # axis titles in bold
  
  # labels
  labs(
    title    = "Weekly incidence of cases by hospital",
    subtitle = "Hospital as re-ordered factor",
    x        = "Week of symptom onset",
    y        = "Weekly cases")
```

<span style="color: darkgreen;">**_TIP:_** To reverse the order of the legend only, add this **ggplot2** command: `guides(fill = guide_legend(reverse = TRUE))`.</span>  





### Adjust legend {.unnumbered}

Read more about legends and scales in the [ggplot tips] page. Here are a few highlights:  

* Edit legend title either in the scale function or with `labs(fill = "Legend title")` (if your are using `color = ` aesthetic, then use `labs(color = "")`)  
* `theme(legend.title = element_blank())` to have no legend title  
* `theme(legend.position = "top")` ("bottom", "left", "right", or "none" to remove the legend)
* `theme(legend.direction = "horizontal")` horizontal legend 
* `guides(fill = guide_legend(reverse = TRUE))` to reverse order of the legend  







### Bars side-by-side {.unnumbered}  

Side-by-side display of group bars (as opposed to stacked) is specified within `geom_histogram()` with `position = "dodge"` placed outside of `aes()`.  

If there are more than two value groups, these can become difficult to read. Consider instead using a faceted plot (small multiples). To improve readability in this example, missing gender values are removed.  

```{r, warning=F, message=F}
ggplot(central_data %>% drop_na(gender))+   # begin with Central Hospital cases dropping missing gender
    geom_histogram(
        mapping = aes(
          x = date_onset,
          group = gender,         # cases grouped by gender
          fill = gender),         # bars filled by gender
        
        # histogram bin breaks
        breaks = weekly_breaks_central,   # sequence of weekly dates for Central outbreak - defined at top of ggplot section
        
        closed = "left",          # count cases from start of breakpoint
        
        color = "black",          # bar edge color
        
        position = "dodge")+      # SIDE-BY-SIDE bars
                      
  
  # The labels on the x-axis
  scale_x_date(expand            = c(0,0),         # remove excess x-axis space below and after case bars
               date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
               date_minor_breaks = "week",         # vertical lines appear every Monday week
               date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+             # removes excess y-axis space between bottom of bars and the labels
  
  #scale of colors and legend labels
  scale_fill_manual(values = c("brown", "orange"),  # specify fill colors ("values") - attention to order!
                    na.value = "grey" )+     

  # aesthetic themes
  theme_minimal()+                                               # a set of themes to simplify plot
  theme(plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
        axis.title = element_text(face = "bold"))+               # axis titles in bold
  
  # labels
  labs(title    = "Weekly incidence of cases, by gender",
       subtitle = "Subtitle",
       fill     = "Gender",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported")
```




### Axis limits {.unnumbered}  

There are two ways to limit the extent of axis values.  

Generally the preferred way is to use the command `coord_cartesian()`, which accepts `xlim = c(min, max)` and `ylim = c(min, max)` (where you provide the min and max values). This acts as a "zoom" without actually removing any data, which is important for statistics and summary measures.  

Alternatively, you can set maximum and minimum date values using `limits = c()` within `scale_x_date()`. For example:  

```{r eval=F}
scale_x_date(limits = c(as.Date("2014-04-01"), NA)) # sets a minimum date but leaves the maximum open.  
```

Likewise, if you want to the x-axis to extend to a specific date (e.g. current date), even if no new cases have been reported, you can use:  

```{r eval=F}
scale_x_date(limits = c(NA, Sys.Date()) # ensures date axis will extend until current date  
```

<span style="color: red;">**_DANGER:_** Be cautious setting the y-axis scale breaks or limits (e.g. 0 to 30 by 5: `seq(0, 30, 5)`). Such static numbers can cut-off your plot too short if the data changes to exceed the limit!.</span>



### Date-axis labels/gridlines {.unnumbered} 

<span style="color: darkgreen;">**_TIP:_** Remember that date-axis **labels** are independent from the aggregation of the data into bars, but visually it can be important to align bins, date labels, and vertical grid lines.</span>

To **modify the date labels and grid lines**, use `scale_x_date()` in one of these ways:  

* **If your histogram bins are days, Monday weeks, months, or years**:  
  * Use `date_breaks = ` to specify the interval of labels and major gridlines (e.g. "day", "week", "3 weeks", "month", or "year")
  * Use `date_minor_breaks = ` to specify interval of minor vertical gridlines (between date labels)  
  * Add `expand = c(0,0)` to begin the labels at the first bar  
  * Use `date_labels = ` to specify format of date labels - see the Dates page for tips (use `\n` for a new line)  
* **If your histogram bins are Sunday weeks**:  
  * Use `breaks = ` and `minor_breaks = ` by providing a sequence of date breaks for each
  * You can still use `date_labels = ` and `expand = ` for formatting as described above  

Some notes:  

* See the opening ggplot section for instructions on how to create a sequence of dates using `seq.Date()`.  
* See [this page](https://rdrr.io/r/base/strptime.html) or the [Working with dates] page for tips on creating date labels.  




#### Demonstrations {.unnumbered}

Below is a demonstration of plots where the bins and the plot labels/grid lines are aligned and not aligned:  

```{r fig.show='hold', class.source = 'fold-hide', warning=F, message=F}
# 7-day bins + Monday labels
#############################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,                 # 7-day bins with start at first case
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),               # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",       # Monday every 3 weeks
    date_minor_breaks = "week",    # Monday weeks
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  scale_y_continuous(
    expand = c(0,0))+              # remove excess space under x-axis, make flush
  
  labs(
    title = "MISALIGNED",
    subtitle = "! CAUTION: 7-day bars start Thursdays at first case\nDate labels and gridlines on Mondays\nNote how ticks don't align with bars")



# 7-day bins + Months
#####################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),                  # remove excess x-axis space below and after case bars
    date_breaks = "months",           # 1st of month
    date_minor_breaks = "week",       # Monday weeks
    date_labels = "%a\n%d %b\n%Y")+   # label format
  
  scale_y_continuous(
    expand = c(0,0))+                 # remove excess space under x-axis, make flush 
  
  labs(
    title = "MISALIGNED",
    subtitle = "! CAUTION: 7-day bars start Thursdays with first case\nMajor gridlines and date labels at 1st of each month\nMinor gridlines weekly on Mondays\nNote uneven spacing of some gridlines and ticks unaligned with bars")


# TOTAL MONDAY ALIGNMENT: specify manual bin breaks to be mondays
#################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,    # defined earlier in this page
    
    closed = "left",                   # count cases from start of breakpoint
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "4 weeks",           # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%a\n%d %b\n%Y")+    # label format
  
  scale_y_continuous(
    expand = c(0,0))+                  # remove excess space under x-axis, make flush 
  
  labs(
    title = "ALIGNED Mondays",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels and gridlines on Mondays as well")


# TOTAL MONDAY ALIGNMENT WITH MONTHS LABELS:
############################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,    # defined earlier in this page
    
    closed = "left",                   # count cases from start of breakpoint
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "months",            # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%b\n%Y")+           # label format
  
  scale_y_continuous(
    expand = c(0,0))+                  # remove excess space under x-axis, make flush 
  
  theme(panel.grid.major = element_blank())+  # Remove major gridlines (fall on 1st of month)
          
  labs(
    title = "ALIGNED Mondays with MONTHLY labels",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels on 1st of Month\nMonthly major gridlines removed")


# TOTAL SUNDAY ALIGNMENT: specify manual bin breaks AND labels to be Sundays
############################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Sunday before first case
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by   = "7 days"),
    
    closed = "left",                    # count cases from start of breakpoint

    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),
    # date label breaks and major gridlines set to every 3 weeks beginning Sunday before first case
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by   = "3 weeks"),
    
    # minor gridlines set to weekly beginning Sunday before first case
    minor_breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                            to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                            by   = "7 days"),
    
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  labs(title = "ALIGNED Sundays",
       subtitle = "7-day bins manually set to begin Sunday before first case (27 Apr)\nDate labels and gridlines manually set to Sundays as well")

```





### Aggregated data {.unnumbered} 

Often instead of a linelist, you begin with aggregated counts from facilities, districts, etc. You can make an epicurve with `ggplot()` but the code will be slightly different. This section will utilize the `count_data` dataset that was imported earlier, in the data preparation section. This dataset is the `linelist` aggregated to day-hospital counts. The first 50 rows are displayed below.  

```{r message=FALSE, warning=F, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


#### Plotting daily counts {.unnumbered}  

We can plot a daily epicurve from these *daily counts*. Here are the differences to the code:  

* Within the aesthetic mapping `aes()`, specify `y = ` as the counts column (in this case, the column name is `n_cases`)
* Add the argument `stat = "identity"` within `geom_histogram()`, which specifies that bar height should be the `y = ` value, not the number of rows as is the default  
* Add the argument `width = ` to avoid vertical white lines between the bars. For daily data set to 1. For weekly count data set to 7. For monthly count data, white lines are an issue (each month has different number of days) - consider transforming your x-axis to a categorical ordered factor (months) and using `geom_col()`.


```{r, message=FALSE, warning=F}
ggplot(data = count_data)+
  geom_histogram(
   mapping = aes(x = date_hospitalisation, y = n_cases),
   stat = "identity",
   width = 1)+                # for daily counts, set width = 1 to avoid white space between bars
  labs(
    x = "Date of report", 
    y = "Number of cases",
    title = "Daily case incidence, from daily count data")
```

#### Plotting weekly counts {.unnumbered}

If your data are already case counts by week, they might look like this dataset (called `count_data_weekly`):  

```{r, warning=F, message=F, echo=F}
# Create weekly dataset with epiweek column
count_data_weekly <- count_data %>%
  mutate(epiweek = lubridate::floor_date(date_hospitalisation, "week")) %>% 
  group_by(hospital, epiweek, .drop=F) %>% 
  summarize(n_cases_weekly = sum(n_cases, na.rm=T))   
```

The first 50 rows of `count_data_weekly` are displayed below. You can see that the counts have been aggregated into weeks. Each week is displayed by the first day of the week (Monday by default).  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(count_data_weekly, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now plot so that `x = ` the epiweek column. Remember to add `y = ` the counts column to the aesthetic mapping, and add `stat = "identity"` as explained above.  

```{r, warning=F, message=F}
ggplot(data = count_data_weekly)+
  
  geom_histogram(
    mapping = aes(
      x = epiweek,           # x-axis is epiweek (as class Date)
      y = n_cases_weekly,    # y-axis height in the weekly case counts
      group = hospital,      # we are grouping the bars and coloring by hospital
      fill = hospital),
    stat = "identity")+      # this is also required when plotting count data
     
  # labels for x-axis
  scale_x_date(
    date_breaks = "2 months",      # labels every 2 months 
    date_minor_breaks = "1 month", # gridlines every month
    date_labels = '%b\n%Y')+       #labeled by month with year below
     
  # Choose color palette (uses RColorBrewer package)
  scale_fill_brewer(palette = "Pastel2")+ 
  
  theme_minimal()+
  
  labs(
    x = "Week of onset", 
    y = "Weekly case incidence",
    fill = "Hospital",
    title = "Weekly case incidence, from aggregated count data by hospital")
```




### Moving averages {.unnumbered}

See the page on [Moving averages] for a detailed description and several options. Below is one option for calculating moving averages with the package **slider**. In this approach, *the moving average is calculated in the dataset prior to plotting*:  

1) Aggregate the data into counts as necessary (daily, weekly, etc.) (see [Grouping data] page)  
2) Create a new column to hold the moving average, created with `slide_index()` from **slider** package  
3) Plot the moving average as a `geom_line()` on top of (after) the epicurve histogram  

See the helpful online [vignette for the **slider** package](https://cran.r-project.org/web/packages/slider/vignettes/slider.html)  


```{r, warning=F, message=F}
# load package
pacman::p_load(slider)  # slider used to calculate rolling averages

# make dataset of daily counts and 7-day moving average
#######################################################
ll_counts_7day <- linelist %>%    # begin with linelist
  
  ## count cases by date
  count(date_onset, name = "new_cases") %>%   # name new column with counts as "new_cases"
  drop_na(date_onset) %>%                     # remove cases with missing date_onset
  
  ## calculate the average number of cases in 7-day window
  mutate(
    avg_7day = slider::slide_index(    # create new column
      new_cases,                       # calculate based on value in new_cases column
      .i = date_onset,                 # index is date_onset col, so non-present dates are included in window 
      .f = ~mean(.x, na.rm = TRUE),    # function is mean() with missing values removed
      .before = 6,                     # window is the day and 6-days before
      .complete = FALSE),              # must be FALSE for unlist() to work in next step
    avg_7day = unlist(avg_7day))       # convert class list to class numeric


# plot
######
ggplot(data = ll_counts_7day) +  # begin with new dataset defined above 
    geom_histogram(              # create epicurve histogram
      mapping = aes(
        x = date_onset,          # date column as x-axis
        y = new_cases),          # height is number of daily new cases
        stat = "identity",       # height is y value
        fill="#92a8d1",          # cool color for bars
        colour = "#92a8d1",      # same color for bar border
        )+ 
    geom_line(                   # make line for rolling average
      mapping = aes(
        x = date_onset,          # date column for x-axis
        y = avg_7day,            # y-value set to rolling average column
        lty = "7-day \nrolling avg"), # name of line in legend
      color="red",               # color of line
      size = 1) +                # width of line
    scale_x_date(                # date scale
      date_breaks = "1 month",
      date_labels = '%d/%m',
      expand = c(0,0)) +
    scale_y_continuous(          # y-axis scale
      expand = c(0,0),
      limits = c(0, NA)) +       
    labs(
      x="",
      y ="Number of confirmed cases",
      fill = "Legend")+ 
    theme_minimal()+
    theme(legend.title = element_blank())  # removes title of legend
```




### Faceting/small-multiples {.unnumbered}

As with other ggplots, you can create facetted plots ("small multiples"). As explained in the [ggplot tips] page of this handbook, you can use either `facet_wrap()` or `facet_grid()`. Here we demonstrate with `facet_wrap()`. For epicurves, `facet_wrap()` is typically easier as it is likely that you only need to facet on one column.  

The general syntax is `facet_wrap(rows ~ cols)`, where to the left of the tilde (~) is the name of a column to be spread across the "rows" of the facetted plot, and to the right of the tilde is the name of a column to be spread across the "columns" of the facetted plot. Most simply, just use one column name, to the right of the tilde: `facet_wrap(~age_cat)`.  


**Free axes**  
You will need to decide whether the scales of the axes for each facet are "fixed" to the same dimensions (default), or "free" (meaning they will change based on the data within the facet). Do this with the `scales = ` argument within `facet_wrap()` by specifying "free_x" or "free_y", or "free".  


**Number of cols and rows of facets**  
This can be specified with `ncol = ` and `nrow = ` within `facet_wrap()`. 


**Order of panels**  
To change the order of appearance, change the underlying order of the levels of the factor column used to create the facets.  


**Aesthetics**  
Font size and face, strip color, etc. can be modified through `theme()` with arguments like:  

* `strip.text = element_text()` (size, colour, face, angle...)
* `strip.background = element_rect()` (e.g. element_rect(fill="grey"))  
* `strip.position = ` (position of the strip "bottom", "top", "left", or "right")  


**Strip labels**  
Labels of the facet plots can be modified through the "labels" of the column as a factor, or by the use of a "labeller".  

Make a labeller like this, using the function `as_labeller()` from **ggplot2**. Then provide the labeller to the `labeller = ` argument of `facet_wrap()` as shown below.  

```{r, class.source = 'fold-show'}
my_labels <- as_labeller(c(
     "0-4"   = "Ages 0-4",
     "5-9"   = "Ages 5-9",
     "10-14" = "Ages 10-14",
     "15-19" = "Ages 15-19",
     "20-29" = "Ages 20-29",
     "30-49" = "Ages 30-49",
     "50-69" = "Ages 50-69",
     "70+"   = "Over age 70"))
```

**An example facetted plot** - facetted by column `age_cat`.


```{r, warning=F, message=F}
# make plot
###########
ggplot(central_data) + 
  
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),    # arguments inside aes() apply by group
      
    color = "black",      # arguments outside aes() apply to all data
        
    # histogram breaks
    breaks = weekly_breaks_central, # pre-defined date vector (see earlier in this page)
    closed = "left" # count cases from start of breakpoint
    )+  
                      
  # The labels on the x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+                       # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "grey"))+         # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,
    ncol = 4,
    strip.position = "top",
    labeller = my_labels)+             
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```

See this [link](https://ggplot2.tidyverse.org/reference/labellers.html) for more information on labellers.  




#### Total epidemic in facet background {.unnumbered}

To show the total epidemic in the background of each facet, add the function `gghighlight()` with empty parentheses to the ggplot. This is from the package **gghighlight**. Note that the y-axis maximum in all facets is now based on the peak of the entire epidemic. There are more examples of this package in the [ggplot tips] page.  

```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # epicurves by group
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),  # arguments inside aes() apply by group
    
    color = "black",    # arguments outside aes() apply to all data
    
    # histogram breaks
    breaks = weekly_breaks_central, # pre-defined date vector (see earlier in this page)
    
    closed = "left", # count cases from start of breakpoint
    )+     # pre-defined date vector (see top of ggplot section)                
  
  # add grey epidemic in background to each facet
  gghighlight::gghighlight()+
  
  # labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space below 0
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "white"))+        # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,                          # each plot is one value of age_cat
    ncol = 4,                          # number of columns
    strip.position = "top",            # position of the facet title/strip
    labeller = my_labels)+             # labeller defines above
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### One facet with data {.unnumbered}  

If you want to have one facet box that contains all the data, duplicate the entire dataset and treat the duplicates as one faceting value. A "helper" function `CreateAllFacet()` below can assist with this (thanks to this [blog post](https://stackoverflow.com/questions/18933575/easily-add-an-all-facet-to-facet-wrap-in-ggplot2)). When it is run, the number of rows doubles and there will be a new column called `facet` in which the duplicated rows will have the value "all", and the original rows have the their original value of the faceting colum. Now you just have to facet on the `facet` column.   

Here is the helper function. Run it so that it is available to you.  

```{r}
# Define helper function
CreateAllFacet <- function(df, col){
     df$facet <- df[[col]]
     temp <- df
     temp$facet <- "all"
     merged <-rbind(temp, df)
     
     # ensure the facet value is a factor
     merged[[col]] <- as.factor(merged[[col]])
     
     return(merged)
}
```

Now apply the helper function to the dataset, on column `age_cat`:  

```{r}
# Create dataset that is duplicated and with new column "facet" to show "all" age categories as another facet level
central_data2 <- CreateAllFacet(central_data, col = "age_cat") %>%
  
  # set factor levels
  mutate(facet = fct_relevel(facet, "all", "0-4", "5-9",
                             "10-14", "15-19", "20-29",
                             "30-49", "50-69", "70+"))

# check levels
table(central_data2$facet, useNA = "always")
```

Notable changes to the `ggplot()` command are:  

* The data used is now central_data2 (double the rows, with new column "facet")
* Labeller will need to be updated, if used  
* Optional: to achieve vertically stacked facets: the facet column is moved to rows side of equation and on right is replaced by "." (`facet_wrap(facet~.)`), and `ncol = 1`. You may also need to adjust the width and height of the saved png plot image (see `ggsave()` in [ggplot tips]).  

```{r, fig.height=12, fig.width=5, warning=F, message=F}
ggplot(central_data2) + 
  
  # actual epicurves by group
  geom_histogram(
        mapping = aes(
          x = date_onset,
          group = age_cat,
          fill = age_cat),  # arguments inside aes() apply by group
        color = "black",    # arguments outside aes() apply to all data
        
        # histogram breaks
        breaks = weekly_breaks_central, # pre-defined date vector (see earlier in this page)
        
        closed = "left", # count cases from start of breakpoint
        )+    # pre-defined date vector (see top of ggplot section)
                     
  # Labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom")+               
  
  # create facets
  facet_wrap(facet~. ,                            # each plot is one value of facet
             ncol = 1)+            

  # labels
  labs(title    = "Weekly incidence of cases, by age category",
       subtitle = "Subtitle",
       fill     = "Age category",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported",
       caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```








## Tentative data  


The most recent data shown in epicurves should often be marked as tentative, or subject to reporting delays. This can be done in by adding a vertical line and/or rectangle over a specified number of days. Here are two options:  

1) Use `annotate()`:  
    + For a line use `annotate(geom = "segment")`. Provide `x`, `xend`, `y`, and `yend`. Adjust size, linetype (`lty`), and color.  
    + For a rectangle use `annotate(geom = "rect")`. Provide xmin/xmax/ymin/ymax. Adjust color and alpha.  
2) Group the data by tentative status and color those bars differently  

<span style="color: orange;">**_CAUTION:_** You might try `geom_rect()` to draw a rectangle, but adjusting the transparency does not work in a linelist context. This function overlays one rectangle for each observation/row!. Use either a very low alpha (e.g. 0.01), or another approach. </span>

### Using `annotate()` {.unnumbered}

* Within `annotate(geom = "rect")`, the `xmin` and `xmax` arguments must be given inputs of class Date.  
* Note that because these data are aggregated into weekly bars, and the last bar extends to the Monday after the last data point, the shaded region may appear to cover 4 weeks  
* Here is an `annotate()` [online example](https://ggplot2.tidyverse.org/reference/annotate.html)


```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # histogram
  geom_histogram(
    mapping = aes(x = date_onset),
    
    breaks = weekly_breaks_central,   # pre-defined date vector - see top of ggplot section
    
    closed = "left", # count cases from start of breakpoint
    
    color = "darkblue",
    
    fill = "lightblue") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "1 month",           # 1st of month
    date_minor_breaks = "1 month",     # 1st of month
    date_labels = "%b\n'%y")+          # label format
  
  # labels and theme
  labs(
    title = "Using annotate()\nRectangle and line showing that data from last 21-days are tentative",
    x = "Week of symptom onset",
    y = "Weekly case indicence")+ 
  theme_minimal()+
  
  # add semi-transparent red rectangle to tentative data
  annotate(
    "rect",
    xmin  = as.Date(max(central_data$date_onset, na.rm = T) - 21), # note must be wrapped in as.Date()
    xmax  = as.Date(Inf),                                          # note must be wrapped in as.Date()
    ymin  = 0,
    ymax  = Inf,
    alpha = 0.2,          # alpha easy and intuitive to adjust using annotate()
    fill  = "red")+
  
  # add black vertical line on top of other layers
  annotate(
    "segment",
    x     = max(central_data$date_onset, na.rm = T) - 21, # 21 days before last data
    xend  = max(central_data$date_onset, na.rm = T) - 21, 
    y     = 0,         # line begins at y = 0
    yend  = Inf,       # line to top of plot
    size  = 2,         # line size
    color = "black",
    lty   = "solid")+   # linetype e.g. "solid", "dashed"

  # add text in rectangle
  annotate(
    "text",
    x = max(central_data$date_onset, na.rm = T) - 15,
    y = 15,
    label = "Subject to reporting delays",
    angle = 90)
```


The same black vertical line can be achieved with the code below, but using `geom_vline()` you lose the ability to control the height:  

```{r, eval=F}
geom_vline(xintercept = max(central_data$date_onset, na.rm = T) - 21,
           size = 2,
           color = "black")
```



### Bars color {.unnumbered}  

An alternative approach could be to adjust the color or display of the tentative bars of data themselves. You could create a new column in the data preparation stage and use it to group the data, such that the `aes(fill = )` of tentative data can be a different color or alpha than the other bars. 

```{r, message=F, warning=F}
# add column
############
plot_data <- central_data %>% 
  mutate(tentative = case_when(
    date_onset >= max(date_onset, na.rm=T) - 7 ~ "Tentative", # tenative if in last 7 days
    TRUE                                       ~ "Reliable")) # all else reliable

# plot
######
ggplot(plot_data, aes(x = date_onset, fill = tentative)) + 
  
  # histogram
  geom_histogram(
    breaks = weekly_breaks_central,   # pre-defined data vector, see top of ggplot page
    closed = "left", # count cases from start of breakpoint
    color = "black") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_fill_manual(values = c("lightblue", "grey"))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",           # Monday every 3 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%d\n%b\n'%y")+      # label format
  
  # labels and theme
  labs(title = "Show days that are tentative reporting",
    subtitle = "")+ 
  theme_minimal()+
  theme(legend.title = element_blank())                 # remove title of legend
  
```


## Multi-level date labels  

If you want multi-level date labels (e.g. month and year) *without duplicating the lower label levels*, consider one of the approaches below:  

Remember - you can can use tools like `\n` *within* the `date_labels` or `labels` arguments to put parts of each label on a new line below. However, the codes below help you take years or months (for example) on a lower line *and only once*.   

The easiest method is to assign the `labels = ` argument in `scale_x_date()` to the function `label_date_short()` from the package **scales** (note: don't forget to include empty parentheses (), as shown below). This function will automatically construct efficient date labels (read more [here](https://scales.r-lib.org/reference/label_date.html)). An additional benefit of this function is that the labels will automatically adjust as your data expands over time: from days, to weeks, to months and years.  

```{r, warning=F}
ggplot(central_data) + 
  
  # histogram
  geom_histogram(
    mapping = aes(x = date_onset),
    breaks = weekly_breaks_central,   # pre-defined date vector - see top of ggplot section
    closed = "left",                  # count cases from start of breakpoint
    color = "darkblue",
    fill = "lightblue") +

  # y-axis scale as before 
  scale_y_continuous(expand = c(0,0))+
  
  # x-axis scale sets efficient date labels
  scale_x_date(
    expand = c(0,0),                      # remove excess x-axis space below and after case bars
    labels = scales::label_date_short())+ # auto efficient date labels
  
  # labels and theme
  labs(
    title = "Using label_date_short()\nTo make automatic and efficient date labels",
    x = "Week of symptom onset",
    y = "Weekly case indicence")+ 
  theme_minimal()
```


A second option is to use faceting. Below:  

* Case counts are aggregated into weeks for aesthetic reasons. See Epicurves page (aggregated data tab) for details.  
* A `geom_area()` line is used instead of a histogram, as the faceting approach below does not work well with histograms.  


**Aggregate to weekly counts**

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}

# Create dataset of case counts by week
#######################################
central_weekly <- linelist %>%
  filter(hospital == "Central Hospital") %>%   # filter linelist
  mutate(week = lubridate::floor_date(date_onset, unit = "weeks")) %>%  
  count(week) %>%                              # summarize weekly case counts
  drop_na(week) %>%                            # remove cases with missing onset_date
  complete(                                    # fill-in all weeks with no cases reported
    week = seq.Date(
      from = min(week),   
      to   = max(week),
      by   = "week"),
    fill = list(n = 0))                        # convert new NA values to 0 counts
```

**Make plots**  

```{r, warning=F, message=F}
# plot with no facet box border
#################################
ggplot(central_weekly,
       aes(x = week, y = n)) +              # establish x and y for entire plot
  geom_line(stat = "identity",              # make line, line height is count number
            color = "#69b3a2") +            # line color
  geom_point(size=1, color="#69b3a2") +     # make points at the weekly data points
  geom_area(fill = "#69b3a2",               # fill area below line
            alpha = 0.4)+                   # fill transparency
  scale_x_date(date_labels="%b",            # date label format show month 
               date_breaks="month",         # date labels on 1st of each month
               expand=c(0,0)) +             # remove excess space
  scale_y_continuous(
    expand  = c(0,0))+                      # remove excess space below x-axis
  facet_grid(~lubridate::year(week),        # facet on year (of Date class column)
             space="free_x",                
             scales="free_x",               # x-axes adapt to data range (not "fixed")
             switch="x") +                  # facet labels (year) on bottom
  theme_bw() +
  theme(strip.placement = "outside",                  # facet label placement
          strip.background = element_blank(),         # no facet lable background
          panel.grid.minor.x = element_blank(),          
          panel.border = element_blank(),             # no border for facet panel
          panel.spacing=unit(0,"cm"))+                # No space between facet panels
  labs(title = "Nested year labels - points, shaded, no label border")
```

The above technique for faceting was adapted from [this](https://stackoverflow.com/questions/44616530/axis-labels-on-two-lines-with-nested-x-variables-year-below-months) and [this](https://stackoverflow.com/questions/20571306/multi-row-x-axis-labels-in-ggplot-line-chart) post on stackoverflow.com.  






<!-- ======================================================= -->
## Dual-axis { }  

Although there are fierce discussions about the validity of dual axes within the data visualization community, many epi supervisors still want to see an epicurve or similar chart with a percent overlaid with a second axis. This is discussed more extensively in the [ggplot tips] page, but one example using the **cowplot** method is shown below:  

* Two distinct plots are made, and then combined with **cowplot** package.  
* The plots must have the exact same x-axis (set limits) or else the data and labels will not align  
* Each uses `theme_cowplot()` and one has the y-axis moved to the right side of the plot  

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
#######################################
plot_cases <- linelist %>% 
  
  # plot cases per week
  ggplot()+
  
  # create histogram  
  geom_histogram(
    
    mapping = aes(x = date_onset),
    
    # bin breaks every week beginning monday before first case, going to monday after last case
    breaks = weekly_breaks_all)+  # pre-defined vector of weekly dates (see top of ggplot section)
        
  # specify beginning and end of date axis to align with other plot
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  # labels
  labs(
      y = "Daily cases",
      x = "Date of symptom onset"
    )+
  theme_cowplot()


# make second plot of percent died per week
###########################################
plot_deaths <- linelist %>%                        # begin with linelist
  group_by(week = floor_date(date_onset, "week")) %>%  # create week column
  
  # summarise to get weekly percent of cases who died
  summarise(n_cases = n(),
            died = sum(outcome == "Death", na.rm=T),
            pct_died = 100*died/n_cases) %>% 
  
  # begin plot
  ggplot()+
  
  # line of weekly percent who died
  geom_line(                                # create line of percent died
    mapping = aes(x = week, y = pct_died),  # specify y-height as pct_died column
    stat = "identity",                      # set line height to the value in pct_death column, not the number of rows (which is default)
    size = 2,
    color = "black")+
  
  # Same date-axis limits as the other plot - perfect alignment
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  
  # y-axis adjustments
  scale_y_continuous(                # adjust y-axis
    breaks = seq(0,100, 10),         # set break intervals of percent axis
    limits = c(0, 100),              # set extent of percent axis
    position = "right")+             # move percent axis to the right
  
  # Y-axis label, no x-axis label
  labs(x = "",
       y = "Percent deceased")+      # percent axis label
  
  theme_cowplot()                   # add this to make the two plots merge together nicely
```

Now use **cowplot** to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of `theme_cowplot()`.  

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(plot_cases, plot_deaths, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```




## Cumulative Incidence {}

Note: If using **incidence2**, see the section on how you can produce cumulative incidence with a simple function. This page will address how to calculate cumulative incidence and plot it with `ggplot()`.  

If beginning with a case linelist, create a new column containing the cumulative number of cases per day in an outbreak using `cumsum()` from **base** R:    

```{r}
cumulative_case_counts <- linelist %>% 
  count(date_onset) %>%                # count of rows per day (returned in column "n")   
  mutate(                         
    cumulative_cases = cumsum(n)       # new column of the cumulative number of rows at each date
    )
```

The first 10 rows are shown below:  

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(cumulative_case_counts, 10), rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```



This cumulative column can then be plotted against `date_onset`, using `geom_line()`:

```{r, warning=F, message=F}
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")

plot_cumulative
```


It can also be overlaid onto the epicurve, with dual-axis using the **cowplot** method described above and in the [ggplot tips] page:

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
plot_cases <- ggplot()+
  geom_histogram(          
    data = linelist,
    aes(x = date_onset),
    binwidth = 1)+
  labs(
    y = "Daily cases",
    x = "Date of symptom onset"
  )+
  theme_cowplot()

# make second plot of cumulative cases line
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")+
  scale_y_continuous(
    position = "right")+
  labs(x = "",
       y = "Cumulative cases")+
  theme_cowplot()+
  theme(
    axis.line.x = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank())
```

Now use **cowplot** to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of `theme_cowplot()`.  

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(plot_cases, plot_cumulative, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```


<!-- ======================================================= -->
## Resources { }








```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epicurves.Rmd-->


# Demographic pyramids and Likert-scales {}  



```{r, out.width = c('50%', '50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pop_pyramid_baseline.png"))

knitr::include_graphics(here::here("images", "likert.png"))
```


Demographic pyramids are useful to show distributions of age and gender. Similar code can be used to visualize the results of Likert-style survey questions (e.g. "Strongly agree", "Agree", "Neutral", "Disagree", "Strongly disagree"). In this page we cover the following:  

* Fast & easy pyramids using the **apyramid** package  
* More customizeable pyramids using `ggplot()`  
* Displaying "baseline" demographics in the background of the pyramid  
* Using pyramid-style plots to show other types of data (e.g responses to **Likert-style** survey questions)  





<!-- ======================================================= -->
## Preparation {}



### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(rio,       # to import data
               here,      # to locate files
               tidyverse, # to clean, handle, and plot the data (includes ggplot2 package)
               apyramid,  # a package dedicated to creating age pyramids
               janitor,   # tables and cleaning data
               stringr)   # working with strings for titles, captions, etc.
```




### Import data {.unnumbered}  

To begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Cleaning {.unnumbered}  

To make a traditional age/gender demographic pyramid, the data must first be cleaned in the following ways:  

* The gender column must be cleaned.  
* Depending on your method, age should be stored as either a numeric or in an *age category* column.  

If using age categories, the column values should be corrected ordered, either by default alpha-numeric or intentionally set by converting to class factor.  

Below we use `tabyl()` from **janitor** to inspect the columns `gender` and `age_cat5`.  

```{r}
linelist %>% 
  tabyl(age_cat5, gender)
```


We also run a quick histogram on the `age` column to ensure it is clean and correctly classified:  

```{r}
hist(linelist$age)
```


<!-- ======================================================= -->
## **apyramid** package {}

The package **apyramid** is a product of the [R4Epis](https://r4epis.netlify.com/) project. You can read more about this package [here](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html). It allows you to quickly make an age pyramid. For more nuanced situations, see the section below [using `ggplot()`](#demo_pyr_gg). You can read more about the **apyramid** package in its Help page by entering `?age_pyramid` in your R console. 

### Linelist data {.unnumbered}  


Using the cleaned `linelist` dataset, we can create an age pyramid with one simple `age_pyramid()` command. In this command:  

* The `data = ` argument is set as the `linelist` data frame  
* The `age_group = ` argument (for y-axis) is set to the name of the categorical age column (in quotes)  
* The `split_by = ` argument (for x-axis) is set to the gender column  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender")
```


The pyramid can be displayed with percent of all cases on the x-axis, instead of counts, by including `proportional = TRUE`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      proportional = TRUE)
```


When using **agepyramid** package, if the `split_by` column is binary (e.g. male/female, or yes/no), then the result will appear as a pyramid. However if there are more than two values in the `split_by` column (not including `NA`), the pyramid will appears as a faceted bar plot with grey bars in the "background" indicating the range of the un-faceted data for that age group. In this case, values of `split_by = ` will appear as labels at top of each facet panel. For example, below is what occurs if the `split_by = ` is assigned the column `hospital`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "hospital")  
```

#### Missing values {.unnumbered}  

Rows that have `NA` missing values in the `split_by = ` or `age_group = ` columns, if coded as `NA`, will not trigger the faceting shown above. By default these rows will not be shown. However you can specify that they appear, in an adjacent barplot and as a separate age group at the top, by specifying `na.rm = FALSE`.  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      na.rm = FALSE)         # show patients missing age or gender
```

#### Proportions, colors, & aesthetics {.unnumbered}  

By default, the bars display counts (not %), a dashed mid-line for each group is shown, and the colors are green/purple. Each of these parameters can be adjusted, as shown below:  

You can also add additional `ggplot()` commands to the plot using the standard `ggplot()` "+" syntax, such as aesthetic themes and label adjustments: 

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = linelist,
  age_group = "age_cat5",
  split_by = "gender",
  proportional = TRUE,              # show percents, not counts
  show_midpoint = FALSE,            # remove bar mid-point line
  #pal = c("orange", "purple")      # can specify alt. colors here (but not labels)
  )+                 
  
  # additional ggplot commands
  theme_minimal()+                               # simplfy background
  scale_fill_manual(                             # specify colors AND labels
    values = c("orange", "purple"),              
    labels = c("m" = "Male", "f" = "Female"))+
  labs(y = "Percent of all cases",              # note x and y labs are switched
       x = "Age categories",                          
       fill = "Gender", 
       caption = "My data source and caption here",
       title = "Title of my plot",
       subtitle = "Subtitle with \n a second line...")+
  theme(
    legend.position = "bottom",                          # legend to bottom
    axis.text = element_text(size = 10, face = "bold"),  # fonts/sizes
    axis.title = element_text(size = 12, face = "bold"))
```



### Aggregated data {.unnumbered}  

The examples above assume your data are in a linelist format, with one row per observation. If your data are already aggregated into counts by age category, you can still use the **apyramid** package, as shown below.  

For demonstration, we aggregate the linelist data into counts by age category and gender, into a "wide" format. This will simulate as if your data were in counts to begin with. Learn more about [Grouping data] and [Pivoting data] in their respective pages.  

```{r, warning=F, message=F}
demo_agg <- linelist %>% 
  count(age_cat5, gender, name = "cases") %>% 
  pivot_wider(
    id_cols = age_cat5,
    names_from = gender,
    values_from = cases) %>% 
  rename(`missing_gender` = `NA`)
```

...which makes the dataset looks like this: with columns for age category, and male counts, female counts, and missing counts.  

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

To set-up these data for the age pyramid, we will pivot the data to be "long" with the `pivot_longer()` function from **dplyr**. This is because `ggplot()` generally prefers "long" data, and **apyramid** is using `ggplot()`.  

```{r, warning=F, message=F}
# pivot the aggregated data into long format
demo_agg_long <- demo_agg %>% 
  pivot_longer(
    col = c(f, m, missing_gender),            # cols to elongate
    names_to = "gender",                # name for new col of categories
    values_to = "counts") %>%           # name for new col of counts
  mutate(
    gender = na_if(gender, "missing_gender")) # convert "missing_gender" to NA
``` 

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg_long, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Then use the `split_by = ` and `count = ` arguments of `age_pyramid()` to specify the respective columns in the data:  

```{r, warning=F, message=F}
apyramid::age_pyramid(data = demo_agg_long,
                      age_group = "age_cat5",# column name for age category
                      split_by = "gender",   # column name for gender
                      count = "counts")      # column name for case counts
```

Note in the above, that the factor order of "m" and "f" is different (pyramid reversed). To adjust the order you must re-define gender in the aggregated data as a Factor and order the levels as desired. See the [Factors] page.  




<!-- ======================================================= -->
## `ggplot()` {#demo_pyr_gg}


Using `ggplot()` to build your age pyramid allows for more flexibility, but requires more effort and understanding of how `ggplot()` works. It is also easier to accidentally make mistakes.  

To use `ggplot()` to make demographic pyramids, you create two bar plots (one for each gender), convert the values in one plot to negative, and finally flip the x and y axes to display the bar plots vertically, their bases meeting in the plot middle.  


### Preparation {.unnumbered}

This approach uses the *numeric* age column, not the *categorical* column of `age_cat5`. So we will check to ensure the class of this column is indeed numeric.  

```{r}
class(linelist$age)
```

You could use the same logic below to build a pyramid from categorical data using `geom_col()` instead of `geom_histogram()`.  

<!-- ======================================================= -->
### Constructing the plot {.unnumbered} 

First, understand that to make such a pyramid using `ggplot()` the approach is as follows:

* Within the `ggplot()`, create **two** histograms using the numeric age column. Create one for each of the two grouping values (in this case genders male and female). To do this, the data for each histogram are specified within their respective `geom_histogram()` commands, with the respective filters applied to `linelist`.    

* One graph will have positive count values, while the other will have its counts converted to negative values - this creates the "pyramid" with the `0` value in the middle of the plot. The negative values are created using a special **ggplot2** term `..count..` and multiplying by -1.  

* The command `coord_flip()` switches the X and Y axes, resulting in the graphs turning vertical and creating the pyramid.

* Lastly, the counts-axis value labels must be altered so they appear as "positive" counts on both sides of the pyramid (despite the underlying values on one side being negative). 

A **simple** version of this, using `geom_histogram()`, is below:

```{r, warning=F, message=F}
  # begin ggplot
  ggplot(mapping = aes(x = age, fill = gender)) +
  
  # female histogram
  geom_histogram(data = linelist %>% filter(gender == "f"),
                 breaks = seq(0,85,5),
                 colour = "white") +
  
  # male histogram (values converted to negative)
  geom_histogram(data = linelist %>% filter(gender == "m"),
                 breaks = seq(0,85,5),
                 mapping = aes(y = ..count..*(-1)),
                 colour = "white") +
  
  # flip the X and Y axes
  coord_flip() +
  
  # adjust counts-axis scale
  scale_y_continuous(limits = c(-600, 900),
                     breaks = seq(-600,900,100),
                     labels = abs(seq(-600, 900, 100)))
```

<span style="color: red;">**_DANGER:_** If the **limits** of your counts axis are set too low, and a counts bar exceeds them, the bar will disappear entirely or be artificially shortened! Watch for this if analyzing data which is routinely updated. Prevent it by having your count-axis limits auto-adjust to your data, as below.</span>  

There are many things you can change/add to this simple version, including:  

* Auto adjust counts-axis scale to your data (avoid errors discussed in warning below)  
* Manually specify colors and legend labels  

**Convert counts to percents**  

To convert counts to percents (of total), do this in your data prior to plotting. Below, we get the age-gender counts, then `ungroup()`, and then `mutate()` to create new percent columns. If you want percents by gender, skip the ungroup step.  


```{r, warning=F, message=F}
# create dataset with proportion of total
pyramid_data <- linelist %>%
  count(age_cat5,
        gender,
        name = "counts") %>% 
  ungroup() %>%                 # ungroup so percents are not by group
  mutate(percent = round(100*(counts / sum(counts, na.rm=T)), digits = 1), 
         percent = case_when(
            gender == "f" ~ percent,
            gender == "m" ~ -percent,     # convert male to negative
            TRUE          ~ NA_real_))    # NA val must by numeric as well
```

Importantly, we save the max and min values so we know what the limits of the scale should be. These will be used in the `ggplot()` command below.    

```{r}
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)

max_per
min_per
```

Finally we make the `ggplot()` on the percent data. We specify `scale_y_continuous()` to extend the pre-defined lengths in each direction (positive and "negative"). We use `floor()` and `ceiling()` to round decimals the appropriate direction (down or up) for the side of the axis.  

```{r, warning=F, message=F}
# begin ggplot
  ggplot()+  # default x-axis is age in years;

  # case data graph
  geom_col(data = pyramid_data,
           mapping = aes(
             x = age_cat5,
             y = percent,
             fill = gender),         
           colour = "white")+       # white around each bar
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  

  # adjust the axes scales
  # scale_x_continuous(breaks = seq(0,100,5), labels = seq(0,100,5)) +
  scale_y_continuous(
    limits = c(min_per, max_per),
    breaks = seq(from = floor(min_per),                # sequence of values, by 2s
                 to = ceiling(max_per),
                 by = 2),
    labels = paste0(abs(seq(from = floor(min_per),     # sequence of absolute values, by 2s, with "%"
                            to = ceiling(max_per),
                            by = 2)),
                    "%"))+  

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",
               "m" = "darkgreen"),
    labels = c("Female", "Male")) +
  
  # label values (remember X and Y flipped now)
  labs(
    title = "Age and gender of cases",
    x = "Age group",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Data are from linelist \nn = {nrow(linelist)} (age or sex missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases) \nData as of: {format(Sys.Date(), '%d %b %Y')}")) +
  
  # display themes
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0.5), 
    plot.caption = element_text(hjust=0, size=11, face = "italic")
    )

```



<!-- ======================================================= -->
### Compare to baseline  {.unnumbered} 

With the flexibility of `ggplot()`, you can have a second layer of bars in the background that represent the "true" or "baseline" population pyramid. This can provide a nice visualization to compare the observed with the baseline.  

Import and view the population data (see [Download handbook and data] page):

```{r echo=F}
# import the population demographics data
pop <- rio::import(here::here("data", "standardization", "country_demographics.csv"))
```

```{r eval=F}
# import the population demographics data
pop <- rio::import("country_demographics.csv")
```

```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```


First some data management steps:  

Here we record the order of age categories that we want to appear. Due to some quirks the way the `ggplot()` is implemented, in this specific scenario it is easiest to store these as a character vector and use them later in the plotting function.  

```{r}
# record correct age cat levels
age_levels <- c("0-4","5-9", "10-14", "15-19", "20-24",
                "25-29","30-34", "35-39", "40-44", "45-49",
                "50-54", "55-59", "60-64", "65-69", "70-74",
                "75-79", "80-84", "85+")
```

Combine the population and case data through the **dplyr** function `bind_rows()`:  

* First, ensure they have the *exact same* column names, age categories values, and gender values  
* Make them have the same data structure: columns of age category, gender, counts, and percent of total  
* Bind them together, one on-top of the other (`bind_rows()`)  



```{r, warning=F, message=F}
# create/transform populaton data, with percent of total
########################################################
pop_data <- pop %>% 
  pivot_longer(      # pivot gender columns longer
    cols = c(m, f),
    names_to = "gender",
    values_to = "counts") %>% 
  
  mutate(
    percent  = round(100*(counts / sum(counts, na.rm=T)),1),  # % of total
    percent  = case_when(                                                        
     gender == "f" ~ percent,
     gender == "m" ~ -percent,               # if male, convert % to negative
     TRUE          ~ NA_real_))
```

Review the changed population dataset

```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now implement the same for the case linelist.  Slightly different because it begins with case-rows, not counts.  

```{r, warning=F, message=F}
# create case data by age/gender, with percent of total
#######################################################
case_data <- linelist %>%
  count(age_cat5, gender, name = "counts") %>%  # counts by age-gender groups
  ungroup() %>% 
  mutate(
    percent = round(100*(counts / sum(counts, na.rm=T)),1),  # calculate % of total for age-gender groups
    percent = case_when(                                     # convert % to negative if male
      gender == "f" ~ percent,
      gender == "m" ~ -percent,
      TRUE          ~ NA_real_))
```

Review the changed case dataset  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(case_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Now the two data frames are combined, one on top of the other (they have the same column names). We can "name" each of the data frame, and use the `.id = ` argument to create a new column "data_source" that will indicate which data frame each row originated from. We can use this column to filter in the `ggplot()`.  



```{r, warning=F, message=F}
# combine case and population data (same column names, age_cat values, and gender values)
pyramid_data <- bind_rows("cases" = case_data, "population" = pop_data, .id = "data_source")
```

Store the maximum and minimum percent values, used in the plotting function to define the extent of the plot (and not cut short any bars!)  

```{r}
# Define extent of percent axis, used for plot limits
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)
```

Now the plot is made with `ggplot()`: 

* One bar graph of population data (wider, more transparent bars)
* One bar graph of case data (small, more solid bars)  


```{r, warning=F, message=F}

# begin ggplot
##############
ggplot()+  # default x-axis is age in years;

  # population data graph
  geom_col(
    data = pyramid_data %>% filter(data_source == "population"),
    mapping = aes(
      x = age_cat5,
      y = percent,
      fill = gender),
    colour = "black",                               # black color around bars
    alpha = 0.2,                                    # more transparent
    width = 1)+                                     # full width
  
  # case data graph
  geom_col(
    data = pyramid_data %>% filter(data_source == "cases"), 
    mapping = aes(
      x = age_cat5,                               # age categories as original X axis
      y = percent,                                # % as original Y-axis
      fill = gender),                             # fill of bars by gender
    colour = "black",                               # black color around bars
    alpha = 1,                                      # not transparent 
    width = 0.3)+                                   # half width
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  
  # manually ensure that age-axis is ordered correctly
  scale_x_discrete(limits = age_levels)+     # defined in chunk above
  
  # set percent-axis 
  scale_y_continuous(
    limits = c(min_per, max_per),                                          # min and max defined above
    breaks = seq(floor(min_per), ceiling(max_per), by = 2),                # from min% to max% by 2 
    labels = paste0(                                                       # for the labels, paste together... 
              abs(seq(floor(min_per), ceiling(max_per), by = 2)), "%"))+                                                  

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",         # assign colors to values in the data
               "m" = "darkgreen"),
    labels = c("f" = "Female",
               "m"= "Male"),      # change labels that appear in legend, note order
  ) +

  # plot labels, titles, caption    
  labs(
    title = "Case age and gender distribution,\nas compared to baseline population",
    subtitle = "",
    x = "Age category",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Cases shown on top of country demographic baseline\nCase data are from linelist, n = {nrow(linelist)}\nAge or gender missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases\nCase data as of: {format(max(linelist$date_onset, na.rm=T), '%d %b %Y')}")) +
  
  # optional aesthetic themes
  theme(
    legend.position = "bottom",                             # move legend to bottom
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0), 
    plot.caption = element_text(hjust=0, size=11, face = "italic"))

```


<!-- ======================================================= -->
## Likert scale {}

The techniques used to make a population pyramid with `ggplot()` can also be used to make plots of Likert-scale survey data.  

```{r, eval=F, echo=F}
data_raw <- import("P:/Shared/equateur_mve_2020/lessons learned/Ebola After-Action Survey - HQ epi team (form responses).csv")


likert_data <- data_raw %>% 
  select(2, 4:11) %>% 
  rename(status = 1,
         Q1 = 2,
         Q2 = 3,
            Q3 = 4,
            Q4 = 5,
            Q5 = 6,
            Q6 = 7,
            Q7 = 8,
            Q8 = 9) %>% 
  mutate(status = case_when(
           stringr::str_detect(status, "Mar") ~ "Senior",
           stringr::str_detect(status, "Jan") ~ "Intermediate",
           stringr::str_detect(status, "Feb") ~ "Junior",
           TRUE ~ "Senior")) %>% 
  mutate(Q4 = recode(Q4, "Not applicable" = "Very Poor"))

table(likert_data$status)

rio::export(likert_data, here::here("data", "likert_data.csv"))
```

Import the data (see [Download handbook and data] page if desired).  

```{r echo=F}
# import the likert survey response data
likert_data <- rio::import(here::here("data", "likert_data.csv"))
```

```{r, eval=F}
# import the likert survey response data
likert_data <- rio::import("likert_data.csv")
```

Start with data that looks like this, with a categorical classification of each respondent (`status`) and their answers to 8 questions on a 4-point Likert-type scale ("Very poor", "Poor", "Good", "Very good").  

```{r, echo=F, message=FALSE}
# display the linelist data as a table
DT::datatable(likert_data, rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```

First, some data management steps:  

* Pivot the data longer  
* Create new column `direction` depending on whether response was generally "positive" or "negative"  
* Set the Factor level order for the `status` column and the `Response` column  
* Store the max count value so limits of plot are appropriate  


```{r, warning=F, message=F}
melted <- likert_data %>% 
  pivot_longer(
    cols = Q1:Q8,
    names_to = "Question",
    values_to = "Response") %>% 
  mutate(
    
    direction = case_when(
      Response %in% c("Poor","Very Poor")  ~ "Negative",
      Response %in% c("Good", "Very Good") ~ "Positive",
      TRUE                                 ~ "Unknown"),
    
    status = fct_relevel(status, "Junior", "Intermediate", "Senior"),
    
    # must reverse 'Very Poor' and 'Poor' for ordering to work
    Response = fct_relevel(Response, "Very Good", "Good", "Very Poor", "Poor")) 

# get largest value for scale limits
melted_max <- melted %>% 
  count(status, Question) %>% # get counts
  pull(n) %>%                 # column 'n'
  max(na.rm=T)                # get max
```


Now make the plot. As in the age pyramids above, we are creating two bar plots and inverting the values of one of them to negative. 

We use `geom_bar()` because our data are one row per observation, not aggregated counts. We use the special **ggplot2** term `..count..` in one of the bar plots to invert the values negative (*-1), and we set `position = "stack"` so the values stack on top of each other.  

```{r, warning=F, message=F}
# make plot
ggplot()+
     
  # bar graph of the "negative" responses 
     geom_bar(
       data = melted %>% filter(direction == "Negative"),
       mapping = aes(
         x = status,
         y = ..count..*(-1),    # counts inverted to negative
         fill = Response),
       color = "black",
       closed = "left",
       position = "stack")+
     
     # bar graph of the "positive responses
     geom_bar(
       data = melted %>% filter(direction == "Positive"),
       mapping = aes(
         x = status,
         fill = Response),
       colour = "black",
       closed = "left",
       position = "stack")+
     
     # flip the X and Y axes
     coord_flip()+
  
     # Black vertical line at 0
     geom_hline(yintercept = 0, color = "black", size=1)+
     
    # convert labels to all positive numbers
    scale_y_continuous(
      
      # limits of the x-axis scale
      limits = c(-ceiling(melted_max/10)*11,    # seq from neg to pos by 10, edges rounded outward to nearest 5
                 ceiling(melted_max/10)*10),   
      
      # values of the x-axis scale
      breaks = seq(from = -ceiling(melted_max/10)*10,
                   to = ceiling(melted_max/10)*10,
                   by = 10),
      
      # labels of the x-axis scale
      labels = abs(unique(c(seq(-ceiling(melted_max/10)*10, 0, 10),
                            seq(0, ceiling(melted_max/10)*10, 10))))) +
     
    # color scales manually assigned 
    scale_fill_manual(
      values = c("Very Good"  = "green4", # assigns colors
                "Good"      = "green3",
                "Poor"      = "yellow",
                "Very Poor" = "red3"),
      breaks = c("Very Good", "Good", "Poor", "Very Poor"))+ # orders the legend
     
    
     
    # facet the entire plot so each question is a sub-plot
    facet_wrap( ~ Question, ncol = 3)+
     
    # labels, titles, caption
    labs(
      title = str_glue("Likert-style responses\nn = {nrow(likert_data)}"),
      x = "Respondent status",
      y = "Number of responses",
      fill = "")+

     # display adjustments 
     theme_minimal()+
     theme(axis.text = element_text(size = 12),
           axis.title = element_text(size = 14, face = "bold"),
           strip.text = element_text(size = 14, face = "bold"),  # facet sub-titles
           plot.title = element_text(size = 20, face = "bold"),
           panel.background = element_rect(fill = NA, color = "black")) # black box around each facet
```


<!-- ======================================================= -->
## Resources {}

[apyramid documentation](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html)



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/age_pyramid.Rmd-->


# Heat plots { }  


Heat plots, also known as "heat maps" or "heat tiles", can be useful visualizations when trying to display 3 variables (x-axis, y-axis, and fill). Below we demonstrate two examples:  

* A visual matrix of transmission events by age ("who infected whom")  
* Tracking reporting metrics across many facilities/jurisdictions over time  


```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "transmission_matrix.png"))

knitr::include_graphics(here::here("images", "heat_tile.png"))

```





<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  tidyverse,       # data manipulation and visualization
  rio,             # importing data 
  lubridate        # working with dates
  )
```

**Datasets**  

This page utilizes the case linelist of a simulated outbreak for the transmission matrix section, and a separate dataset of daily malaria case counts by facility for the metrics tracking section. They are loaded and cleaned in their individual sections.  







## Transmission matrix  

Heat tiles can be useful to visualize matrices. One example is to display "who-infected-whom" in an outbreak. This assumes that you have information on transmission events.  

Note that the [Contact tracing] page contains another example of making a heat tile contact matrix, using a different (perhaps more simple) dataset where the ages of cases and their sources are neatly aligned in the same row of the data frame. This same data is used to make a *density* map in the [ggplot tips] page. This example below begins from a case linelist and so involves considerable data manipulation prior to achieving a plotable data frame. So there are many scenarios to chose from...  


We begin from the case linelist of a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import your data with the `import()` function from the **rio** package (it accepts many file types like .xlsx, .rds, .csv - see the [Import and export] page for details).  


The first 50 rows of the linelist are shown below for demonstration:  


```{r, echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```


In this linelist:  

* There is one row per case, as identified by `case_id`  
* There is a later column `infector` that contains the `case_id` of the *infector*, who is also a case in the linelist  


```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Data preparation {.unnumbered}  

**Objective**: We need to achieve a "long"-style data frame that contains one row per possible age-to-age transmission route, with a numeric column containing that row's proportion of all observed transmission events in the linelist.  

This will take several data manuipulation steps to achieve:  


#### Make cases data frame {.unnumbered} 

To begin, we create a data frame of the cases, their ages, and their infectors - we call the data frame `case_ages`. The first 50 rows are displayed below.  

```{r}
case_ages <- linelist %>% 
  select(case_id, infector, age_cat) %>% 
  rename("case_age_cat" = "age_cat")
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(case_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Make infectors data frame {.unnumbered}  

Next, we create a data frame of the infectors - at the moment it consists of a single column. These are the infector IDs from the linelist. Not every case has a known infector, so we remove missing values. The first 50 rows are displayed below.  


```{r}
infectors <- linelist %>% 
  select(infector) %>% 
  drop_na(infector)
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infectors, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Next, we use joins to procure the ages of the infectors. This is not simple, because in the `linelist`, the infector's ages are not listed as such. We achieve this result by joining the case `linelist` to the infectors. We begin with the infectors, and `left_join()` (add) the case `linelist` such that the `infector` id column left-side "baseline" data frame joins to the `case_id` column in the right-side `linelist` data frame.  

Thus, the data from the infector's case record in the linelist (including age) is added to the infector row. The 50 first rows are displayed below.  

```{r}
infector_ages <- infectors %>%             # begin with infectors
  left_join(                               # add the linelist data to each infector  
    linelist,
    by = c("infector" = "case_id")) %>%    # match infector to their information as a case
  select(infector, age_cat) %>%            # keep only columns of interest
  rename("infector_age_cat" = "age_cat")   # rename for clarity
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infector_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Then, we combine the cases and their ages with the infectors and their ages. Each of these data frame has the column `infector`, so it is used for the join. The first rows are displayed below:    

```{r}
ages_complete <- case_ages %>%  
  left_join(
    infector_ages,
    by = "infector") %>%        # each has the column infector
  drop_na()                     # drop rows with any missing data
```


```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(ages_complete, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Below, a simple cross-tabulation of counts between the case and infector age groups. Labels added for clarity.  

```{r}
table(cases = ages_complete$case_age_cat,
      infectors = ages_complete$infector_age_cat)
```


We can convert this table to a data frame with `data.frame()` from **base** R, which also automatically converts it to "long" format, which is desired for the `ggplot()`. The first rows are shown below.  

```{r}
long_counts <- data.frame(table(
    cases     = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_counts, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Now we do the same, but apply `prop.table()` from **base** R to the table so instead of counts we get proportions of the total. The first 50 rows are shown below.    

```{r}
long_prop <- data.frame(prop.table(table(
    cases = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat)))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_prop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




### Create heat plot {.unnumbered}  

Now finally we can create the heat plot with **ggplot2** package, using the `geom_tile()` function. See the [ggplot tips] page to learn more extensively about color/fill scales, especially the `scale_fill_gradient()` function.  

* In the aesthetics `aes()` of `geom_tile()` set the x and y as the case age and infector age  
* Also in `aes()` set the argument `fill = ` to the `Freq` column - this is the value that will be converted to a tile color  
* Set a scale color with `scale_fill_gradient()` - you can specify the high/low colors  
  * Note that `scale_color_gradient()` is different! In this case you want the fill  
* Because the color is made via "fill", you can use the `fill = ` argument in `labs()` to change the legend title  

```{r}
ggplot(data = long_prop)+       # use long data, with proportions as Freq
  geom_tile(                    # visualize it in tiles
    aes(
      x = cases,         # x-axis is case age
      y = infectors,     # y-axis is infector age
      fill = Freq))+            # color of the tile is the Freq column in the data
  scale_fill_gradient(          # adjust the fill color of the tiles
    low = "blue",
    high = "orange")+
  labs(                         # labels
    x = "Case age",
    y = "Infector age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )
  
```



<!-- ======================================================= -->
## Reporting metrics over time { }

Often in public health, one objective is to assess trends over time for many entities (facilities, jurisdictions, etc.). One way to visualize such trends over time is a heat plot where the x-axis is time and on the y-axis are the many entities.  



### Data preparation {.unnumbered}

We begin by importing a dataset of daily malaria reports from many facilities. The reports contain a date, province, district, and malaria counts. See the page on [Download handbook and data] for information on how to download these data. Below are the first 30 rows:  

```{r, echo=F}
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  select(location_name, data_date, District, malaria_tot)
```

```{r, eval=F}
facility_count_data <- import("malaria_facility_count_data.rds")
```


```{r, echo=F}
DT::datatable(head(facility_count_data,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


#### Aggregate and summarize {.unnumbered}

**The objective in this example** is to transform the daily facility *total* malaria case counts (seen in previous tab) into *weekly* summary statistics of facility reporting performance - in this case *the proportion of days per week that the facility reported any data*. For this example we will show data only for **Spring District**.  

To achieve this we will do the following data management steps:  

1) Filter the data as appropriate (by place, date)  
2) Create a week column using `floor_date()` from package **lubridate**  
    + This function returns the start-date of a given date's week, using a specified start date of each week (e.g. "Mondays")  
3) The data are grouped by columns "location" and "week" to create analysis units of "facility-week"  
4) The function `summarise()` creates new columns to reflecting summary statistics per facility-week group:  
    + Number of days per week (7 - a static value)  
    + Number of reports received from the facility-week (could be more than 7!)  
    + Sum of malaria cases reported by the facility-week (just for interest)  
    + Number of *unique* days in the facility-week for which there is data reported  
    + **Percent of the 7 days per facility-week for which data was reported**  
5) The data frame is joined with `right_join()` to a comprehensive list of all possible facility-week combinations, to make the dataset complete. The matrix of all possible combinations is created by applying `expand()` to those two columns of the data frame as it is at that moment in the pipe chain (represented by `.`). Because a `right_join()` is used, all rows in the `expand()` data frame are kept, and added to `agg_weeks` if necessary. These new rows appear with `NA` (missing) summarized values.  


Below we demonstrate step-by-step:  

```{r, message=FALSE, warning=FALSE}
# Create weekly summary dataset
agg_weeks <- facility_count_data %>% 
  
  # filter the data as appropriate
  filter(
    District == "Spring",
    data_date < as.Date("2020-08-01")) 
```

Now the dataset has ` nrow(agg_weeks)` rows, when it previously had ` nrow(facility_count_data)`.  

Next we create a `week` column reflecting the start date of the week for each record. This is achieved with the **lubridate** package and the function `floor_date()`, which is set to "week" and for the weeks to begin on Mondays (day 1 of the week - Sundays would be 7). The top rows are shown below.  

```{r}
agg_weeks <- agg_weeks %>% 
  # Create week column from data_date
  mutate(
    week = lubridate::floor_date(                     # create new column of weeks
      data_date,                                      # date column
      unit = "week",                                  # give start of the week
      week_start = 1))                                # weeks to start on Mondays 
```

The new week column can be seen on the far right of the data frame  

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Now we group the data into facility-weeks and summarise them to produce statistics per facility-week. See the page on [Descriptive tables] for tips. The grouping itself doesn't change the data frame, but it impacts how the subsequent summary statistics are calculated.  

The top rows are shown below. Note how the columns have completely changed to reflect the desired summary statistics. Each row reflects one facility-week. 

```{r, warning=F, message=F}
agg_weeks <- agg_weeks %>%   

  # Group into facility-weeks
  group_by(location_name, week) %>%
  
  # Create summary statistics columns on the grouped data
  summarize(
    n_days          = 7,                                          # 7 days per week           
    n_reports       = dplyr::n(),                                 # number of reports received per week (could be >7)
    malaria_tot     = sum(malaria_tot, na.rm = T),                # total malaria cases reported
    n_days_reported = length(unique(data_date)),                  # number of unique days reporting per week
    p_days_reported = round(100*(n_days_reported / n_days))) %>%  # percent of days reporting

  ungroup(location_name, week)                                    # ungroup so expand() works in next step
```

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Finally, we run the command below to ensure that ALL possible facility-weeks are present in the data, even if they were missing before.  

We are using a `right_join()` on itself (the dataset is represented by ".") but having been expanded to include all possible combinations of the columns `week` and `location_name`. See documentation on the `expand()` function in the page on [Pivoting]. Before running this code the dataset contains ` nrow(agg_weeks)` rows.   

```{r, message=F, warning=F}
# Create data frame of every possible facility-week
expanded_weeks <- agg_weeks %>% 
  tidyr::expand(location_name, week)  # expand data frame to include all possible facility-week combinations
```

Here is `expanded_weeks`, with `r nrow(expanded_weeks)` rows:  

```{r, echo=F}
DT::datatable(expanded_weeks, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Before running this code, `agg_weeks` contains `r nrow(agg_weeks)` rows.   

```{r}
# Use a right-join with the expanded facility-week list to fill-in the missing gaps in the data
agg_weeks <- agg_weeks %>%      
  right_join(expanded_weeks) %>%                            # Ensure every possible facility-week combination appears in the data
  mutate(p_days_reported = replace_na(p_days_reported, 0))  # convert missing values to 0                           
```

After running this code, `agg_weeks` contains ` nrow(agg_weeks)` rows.   


<!-- ======================================================= -->
### Create heat plot {.unnumbered}


The `ggplot()` is made using `geom_tile()` from the **ggplot2** package:  

* Weeks on the x-axis is transformed to dates, allowing use of `scale_x_date()`  
* `location_name` on the y-axis will show all facility names  
* The `fill` is `p_days_reported`, the performance for that facility-week (numeric)  
* `scale_fill_gradient()` is used on the numeric fill, specifying colors for high, low, and `NA`  
* `scale_x_date()` is used on the x-axis specifying labels every 2 weeks and their format  
* Display themes and labels can be adjusted as necessary




<!-- ======================================================= -->
### Basic {.unnumbered}  

A basic heat plot is produced below, using the default colors, scales, etc. As explained above, within the `aes()` for `geom_tile()` you must provide an x-axis column, y-axis column, **and** a column for the the `fill = `. The fill is the numeric value that presents as tile color.  

```{r}
ggplot(data = agg_weeks)+
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported))
```

### Cleaned plot {.unnumbered}

We can make this plot look better by adding additional **ggplot2** functions, as shown below. See the page on [ggplot tips] for details.  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
### Ordered y-axis {.unnumbered}  

Currently, the facilities are ordered "alpha-numerically" from the bottom to the top. If you want to adjust the order the y-axis facilities, convert them to class factor and provide the order. See the page on [Factors] for tips.  

Since there are many facilities and we don't want to write them all out, we will try another approach - ordering the facilities in a data frame and using the resulting column of names as the factor level order. Below, the column `location_name` is converted to a factor, and the order of its levels is set based on the total number of reporting days filed by the facility across the whole time-span.  

To do this, we create a data frame which represents the total number of reports per facility, arranged in ascending order. We can use this vector to order the factor levels in the plot.   

```{r}
facility_order <- agg_weeks %>% 
  group_by(location_name) %>% 
  summarize(tot_reports = sum(n_days_reported, na.rm=T)) %>% 
  arrange(tot_reports) # ascending order
```

See the data frame below:  

```{r, echo=F}
DT::datatable(facility_order, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```




Now use a column from the above data frame (`facility_order$location_name`) to be the order of the factor levels of `location_name` in the data frame `agg_weeks`:  

```{r, warning=F, message=F}
# load package 
pacman::p_load(forcats)

# create factor and define levels manually
agg_weeks <- agg_weeks %>% 
  mutate(location_name = fct_relevel(
    location_name, facility_order$location_name)
    )
```

And now the data are re-plotted, with location_name being an ordered factor:  

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
### Display values {.unnumbered}  


You can add a `geom_text()` layer on top of the tiles, to display the actual numbers of each tile. Be aware this may not look pretty if you have many small tiles!  

The following code has been added: `geom_text(aes(label = p_days_reported))`. This adds text onto every tile. The text displayed is the value assigned to the argument `label = `, which in this case has been set to the same numeric column `p_days_reported` that is also used to create the color gradient.  



  
```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  # text
  geom_text(
    aes(
      x = week,
      y = location_name,
      label = p_days_reported))+      # add text on top of tile
  
  # fill scale
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                    # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```




<!-- ======================================================= -->
## Resources { }

[scale_fill_gradient()](https://ggplot2.tidyverse.org/reference/scale_gradient.html)  

[R graph gallery - heatmap](https://ggplot2.tidyverse.org/reference/scale_gradient.html)  




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/heatmaps.Rmd-->


# Diagrams and charts { }  



```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "flow_chart.png"))
knitr::include_graphics(here::here("images", "sankey_diagram.png"))
```


This page covers code to produce:  

* Flow diagrams using **DiagrammeR** and the DOT language  
* Alluvial/Sankey diagrams  
* Event timelines  

<!-- * DAGs (Directed Acyclic Graphs)   -->
<!-- * GANTT charts   -->


<!-- ======================================================= -->
## Preparation { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  DiagrammeR,     # for flow diagrams
  networkD3,      # For alluvial/Sankey diagrams
  tidyverse)      # data management and visualization
```

### Import data {.unnumbered}  

Most of the content in this page does not require a dataset. However, in the Sankey diagram section, we will use the case linelist from a simulated Ebola epidemic. If you want to follow along for this part, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Flow diagrams { }

One can use the R package **DiagrammeR** to create charts/flow charts. They can be static, or they can adjust somewhat dynamically based on changes in a dataset.  

**Tools**  

The function `grViz()` is used to create a "Graphviz" diagram. This function accepts a *character string input containing instructions* for making the diagram. Within that string, the instructions are written in a different language, called [DOT](https://graphviz.org/doc/info/lang.html) - it is quite easy to learn the basics.  

**Basic structure**  

1) Open the instructions `grViz("`  
2) Specify directionality and name of the graph, and open brackets, e.g. `digraph my_flow_chart {`
3) Graph statement (layout, rank direction)  
4) Nodes statements (create nodes)
5) Edges statements (gives links between nodes)  
6) Close the instructions `}")`  

### Simple examples {.unnumbered} 

Below are two simple examples  

A very minimal example:  

```{r out.width='50%'}
# A minimal plot
DiagrammeR::grViz("digraph {
  
graph[layout = dot, rankdir = LR]

a
b
c

a -> b -> c
}")
```

An example with perhaps a bit more applied public health context:  

```{r out.width='50%'}
grViz("                           # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,
         overlap = true,
         fontsize = 10]
  
  # nodes
  #######
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]               # width of circles
  
  Primary                         # names of nodes
  Secondary
  Tertiary

  # edges
  #######
  Primary   -> Secondary [label = ' case transfer']
  Secondary -> Tertiary [label = ' case transfer']
}
")
```

### Syntax  {.unnumbered}

**Basic syntax**  

Node names, or edge statements, can be separated with spaces, semicolons, or newlines.  

**Rank direction**  

A plot can be re-oriented to move left-to-right by adjusting the `rankdir` argument within the graph statement. The default is TB (top-to-bottom), but it can be LR (left-to-right), RL, or BT.  

**Node names**  

Node names can be single words, as in the simple example above. To use multi-word names or special characters (e.g. parentheses, dashes), put the node name within single quotes (' '). It may be easier to have a short node name, and assign a *label*, as shown below within brackets [ ]. If you want to have a newline within the node's name, you must do it via a label - use `\n` in the node label within single quotes, as shown below.  

**Subgroups**  
Within edge statements, subgroups can be created on either side of the edge with curly brackets ({ }). The edge then applies to all nodes in the bracket - it is a shorthand.  


**Layouts**  

* dot (set `rankdir` to either TB, LR, RL, BT, )
* neato  
* twopi  
* circo  


**Nodes - editable attributes**  

* `label` (text, in single quotes if multi-word)  
* `fillcolor` (many possible colors)  
* `fontcolor`  
* `alpha` (transparency 0-1)  
* `shape` (ellipse, oval, diamond, egg, plaintext, point, square, triangle)  
* `style`  
* `sides`  
* `peripheries`  
* `fixedsize` (h x w)  
* `height`  
* `width`  
* `distortion`  
* `penwidth` (width of shape border)  
* `x` (displacement left/right)  
* `y` (displacement up/down)  
* `fontname`  
* `fontsize`  
* `icon`  


**Edges - editable attributes**  

* `arrowsize`  
* `arrowhead` (normal, box, crow, curve, diamond, dot, inv, none, tee, vee)  
* `arrowtail`  
* `dir` (direction, )  
* `style` (dashed, ...)  
* `color`  
* `alpha`  
* `headport` (text in front of arrowhead)  
* `tailport` (text in behind arrowtail)  
* `fontname`  
* `fontsize`  
* `fontcolor`  
* `penwidth` (width of arrow)  
* `minlen` (minimum length)

**Color names**: hexadecimal values or 'X11' color names, see [here for X11 details](http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html) 


### Complex examples  {.unnumbered}

The example below expands on the surveillance_diagram, adding complex node names, grouped edges, colors and styling


```
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = ' case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = ' case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```


```{r out.width='50%', echo=F}
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```

**Sub-graph clusters**  

To group nodes into boxed clusters, put them within the same named subgraph (`subgraph name {}`). To have each subgraph identified within a bounding box, begin the name of the subgraph with "cluster", as shown with the 4 boxes below.  

```
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance'] 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)'] 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)']
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media' Radio} -> EBS
  
  RECOs -> CBS
}
")

```


```{r out.width='120%', echo=F}
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance'] 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)'] 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)']
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media' Radio} -> EBS
  
  RECOs -> CBS
}
")

```


**Node shapes**  

The example below, borrowed from [this tutorial](http://rich-iannone.github.io/DiagrammeR/), shows applied node shapes and a shorthand for serial edge connections  

```{r out.width='75%'}
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle, style = filled, fillcolor = Linen]

data1 [label = 'Dataset 1', shape = folder, fillcolor = Beige]
data2 [label = 'Dataset 2', shape = folder, fillcolor = Beige]
process [label =  'Process \n Data']
statistical [label = 'Statistical \n Analysis']
results [label= 'Results']

# edge definitions with the node IDs
{data1 data2}  -> process -> statistical -> results
}")
```


### Outputs  {.unnumbered}

How to handle and save outputs  

* Outputs will appear in RStudio's Viewer pane, by default in the lower-right alongside Files, Plots, Packages, and Help.  
* To export you can "Save as image" or "Copy to clipboard" from the Viewer. The graphic will adjust to the specified size.  




### Parameterized figures {.unnumbered} 

Here is a quote from this tutorial: https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/  

"Parameterized figures: A great benefit of designing figures within R is that we are able to connect the figures directly with our analysis by reading R values directly into our flowcharts. For example, suppose you have created a filtering process which removes values after each stage of a process, you can have a figure show the number of values left in the dataset after each stage of your process. To do this we, you can use the @@X symbol directly within the figure, then refer to this in the footer of the plot using [X]:, where X is the a unique numeric index."  

We encourage you to review this tutorial if parameterization is something you are interested in.  


<!-- And below is some example code from this tutorial. -->

<!-- ```{r, eval=F} -->
<!-- # Define some sample data -->
<!-- data <- list(a=1000, b=800, c=600, d=400) -->


<!-- DiagrammeR::grViz(" -->
<!-- digraph graph2 { -->

<!-- graph [layout = dot] -->

<!-- # node definitions with substituted label text -->
<!-- node [shape = rectangle, width = 4, fillcolor = Biege] -->
<!-- a [label = '@@1'] -->
<!-- b [label = '@@2'] -->
<!-- c [label = '@@3'] -->
<!-- d [label = '@@4'] -->

<!-- a -> b -> c -> d -->

<!-- } -->

<!-- [1]:  paste0('Raw Data (n = ', data$a, ')') -->
<!-- [2]: paste0('Remove Errors (n = ', data$b, ')') -->
<!-- [3]: paste0('Identify Potential Customers (n = ', data$c, ')') -->
<!-- [4]: paste0('Select Top Priorities (n = ', data$d, ')') -->
<!-- ") -->

<!-- ``` -->



<!-- ### CONSORT diagram  {.unnumbered} -->

<!-- THIS SECTION IS UNDER CONSTRUCTION   -->

<!-- https://scriptsandstatistics.wordpress.com/2017/12/22/how-to-draw-a-consort-flow-diagram-using-r-and-graphviz/ -->

<!-- Note above is out of date via DiagrammeR -->




<!-- ======================================================= -->
## Alluvial/Sankey Diagrams { }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

We load the **networkD3** package to produce the diagram, and also **tidyverse** for the data preparation steps.  

```{r}
pacman::p_load(
  networkD3,
  tidyverse)
```

### Plotting from dataset {.unnumbered} 

Plotting the connections in a dataset. Below we demonstrate using this package on the case `linelist`. Here is an [online tutorial](https://www.r-graph-gallery.com/321-introduction-to-interactive-sankey-diagram-2.html).    

We begin by getting the case counts for each unique age category and hospital combination. We've removed values with missing age category for clarity. We also re-label the `hospital` and `age_cat` columns as `source` and `target` respectively. These will be the two sides of the alluvial diagram.  

```{r}
# counts by hospital and age category
links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = hospital,
         target = age_cat)
```

The dataset now look like this:  

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


Now we create a data frame of all the diagram nodes, under the column `name`. This consists of all the values for `hospital` and `age_cat`. Note that we ensure they are all class Character before combining them. and adjust the ID columns to be numbers instead of labels:  

```{r}
# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

nodes  # print
```
The we edit the `links` data frame, which we created above with `count()`. We add two numeric columns `IDsource` and `IDtarget` which will actually reflect/create the links between the nodes. These columns will hold the rownumbers (position) of the source and target nodes. 1 is subtracted so that these position numbers begin at 0 (not 1).  

```{r}
# match to numbers, not names
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1
```

The links dataset now looks like this:  

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Now plot the Sankey diagram with `sankeyNetwork()`. You can read more about each argument by running `?sankeyNetwork` in the console. Note that unless you set `iterations = 0` the order of your nodes may not be as expected. 


```{r}

# plot
######
p <- sankeyNetwork(
  Links = links,
  Nodes = nodes,
  Source = "IDsource",
  Target = "IDtarget",
  Value = "n",
  NodeID = "name",
  units = "TWh",
  fontSize = 12,
  nodeWidth = 30,
  iterations = 0)        # ensure node order is as in data
p
```



Here is an example where the patient Outcome is included as well. Note in the data preparation step we have to calculate the counts of cases between age and hospital, and separately between hospital and outcome - and then bind all these counts together with `bind_rows()`.  

```{r}
# counts by hospital and age category
age_hosp_links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = age_cat,          # re-name
         target = hospital)

hosp_out_links <- linelist %>% 
    drop_na(age_cat) %>% 
    select(hospital, outcome) %>% 
    count(hospital, outcome) %>% 
    rename(source = hospital,       # re-name
           target = outcome)

# combine links
links <- bind_rows(age_hosp_links, hosp_out_links)

# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

# Create id numbers
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1

# plot
######
p <- sankeyNetwork(Links = links,
                   Nodes = nodes,
                   Source = "IDsource",
                   Target = "IDtarget",
                   Value = "n",
                   NodeID = "name",
                   units = "TWh",
                   fontSize = 12,
                   nodeWidth = 30,
                   iterations = 0)
p

```


https://www.displayr.com/sankey-diagrams-r/



<!-- ======================================================= -->
## Event timelines { }

To make a timeline showing specific events, you can use the `vistime` package.

See this [vignette](https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning)

```{r}
# load package
pacman::p_load(vistime,  # make the timeline
               plotly    # for interactive visualization
               )
```

```{r, echo=F}
# reference: https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning

data <- read.csv(text="event, group, start, end, color
                       Event 1, Group A,2020-01-22,2020-01-22, #90caf9
                       Event 1, Group B,2020-01-23,2020-01-23, #90caf9
                       Event 1, Group C,2020-01-23,2020-01-23, #1565c0
                       Event 1, Group D,2020-01-25,2020-01-25, #f44336
                       Event 1, Group E,2020-01-25,2020-01-25, #90caf9
                       Event 1, Group F,2020-01-26,2020-01-26, #8d6e63
                       Event 1, Group G,2020-01-27,2020-01-27, #1565c0
                       Event 1, Group H,2020-01-27,2020-01-27, #90caf9
                       Event 1, Group I,2020-01-27,2020-01-27,#90a4ae
                       Event 2, Group A,2020-01-28,2020-01-28,#fc8d62
                       Event 2, Group C,2020-01-28,2020-01-28, #6a3d9a
                       Event 2, Group J,2020-01-28,2020-01-28, #90caf9
                       Event 2, Group J,2020-01-28,2020-01-28, #fc8d62
                       Event 2, Group J,2020-01-28,2020-01-28, #1565c0
")
```

Here is the events dataset we begin with:  

```{r message=FALSE, echo=F}
DT::datatable(data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



```{r}
p <- vistime(data)    # apply vistime

library(plotly)

# step 1: transform into a list
pp <- plotly_build(p)

# step 2: Marker size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "markers") pp$x$data[[i]]$marker$size <- 10
}

# step 3: text size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textfont$size <- 10
}


# step 4: text position
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textposition <- "right"
}

#print
pp

```



<!-- ======================================================= -->
## DAGs { }

You can build a DAG manually using the **DiagammeR** package and DOT language as described above.  

Alternatively, there are packages like **ggdag** and **daggity**

[Introduction to DAGs ggdag vignette](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html)   

[Causal inference with dags in R](https://www.r-bloggers.com/2019/08/causal-inference-with-dags-in-r/#:~:text=In%20a%20DAG%20all%20the,for%20drawing%20and%20analyzing%20DAGs.)  





<!-- ======================================================= -->
## Resources { }



Much of the above regarding the DOT language is adapted from the tutorial [at this site](https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/)  

Another more in-depth [tutorial on DiagammeR](http://rich-iannone.github.io/DiagrammeR/)

This page on [Sankey diagrams](https://www.displayr.com/sankey-diagrams-r/
)  




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/diagrams.Rmd-->


# Combinations analysis { }  

```{r echo=F, out.width= "75%", warning=F, message=F}
pacman::p_load(tidyverse,
               UpSetR,
               ggupset)

# Adds new symptom variables to the linelist, with random "yes" or "no" values 
linelist_sym <- linelist %>% 
  mutate(fever  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.80, 0.20)),
         chills = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.20, 0.80)),
         cough  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.9, 0.15)),
         aches  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.10, 0.90)),
         vomit = sample(c("yes", "no"), nrow(linelist), replace = T))

linelist_sym_2 <- linelist_sym %>% 
     
  #mutate(fever = ifelse(fever == "yes", colnames(linelist)[which(colnames(linelist) == "fever")]))
   mutate(across(.cols = c(fever, chills, cough, aches, vomit),
                 .fns = ~+(.x == "yes")))   

     
  #mutate(across(c("fever", "chills", "cough", "aches", "vomit"), ~ifelse(.x = "yes", colnames(.)[which(colnames(.) == "fever")], 0)))   
  

# Make the plot
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, vomit),
  sets = c("fever", "chills", "cough", "aches", "vomit"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```



This analysis plots the frequency of different **combinations** of values/responses. In this example, we plot the frequency at which cases exhibited various combinations of symptoms.  

This analysis is also often called:  

* **"Multiple response analysis"**  
* **"Sets analysis"**  
* **"Combinations analysis"**  

In the example plot above, five symptoms are shown. Below each vertical bar is a line and dots indicating the combination of symptoms reflected by the bar above. To the right, horizontal bars reflect the frequency of each individual symptom.  

The first method we show uses the package **ggupset**, and the second uses the package **UpSetR**. 




  



<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, warning=F, message=F}
pacman::p_load(
  tidyverse,     # data management and visualization
  UpSetR,        # special package for combination plots
  ggupset)       # special package for combination plots
```

<!-- ======================================================= -->
### Import data {.unnumbered}  


To begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  



```{r, echo=F}
# import the linelist into R
linelist_sym <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist_sym <- import("linelist_cleaned.rds")
```


This linelist includes five "yes/no" variables on reported symptoms. We will need to transform these variables a bit to use the **ggupset** package to make our plot. View the data (scroll to the right to see the symptoms variables).  

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_sym, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Re-format values {.unnumbered}  

To align with the format expected by **ggupset** we convert the "yes" and "no" the the actual symptom name, using `case_when()` from **dplyr**. If "no", we set the value as blank, so the values are either `NA` or the symptom.  
 

```{r, warning=F, message=F}
# create column with the symptoms named, separated by semicolons
linelist_sym_1 <- linelist_sym %>% 

  # convert the "yes" and "no" values into the symptom name itself
  # if old value is "yes", new value is "fever", otherwise set to missing (NA)
mutate(fever = ifelse(fever == "yes", "fever", NA), 
       chills = ifelse(chills == "yes", "chills", NA),
       cough = ifelse(cough == "yes", "cough", NA),
       aches = ifelse(aches == "yes", "aches", NA),
       vomit = ifelse(vomit == "yes", "vomit", NA))
```

Now we make two final columns:  

1. Concatenating (gluing together) all the symptoms of the patient (a character column)  
2. Convert the above column to class *list*, so it can be accepted by **ggupset** to make the plot  

See the page on [Characters and strings] to learn more about the `unite()` function from **stringr**

```{r, warning=F, message=F}
linelist_sym_1 <- linelist_sym_1 %>% 
  unite(col = "all_symptoms",
        c(fever, chills, cough, aches, vomit), 
        sep = "; ",
        remove = TRUE,
        na.rm = TRUE) %>% 
  mutate(
    # make a copy of all_symptoms column, but of class "list" (which is required to use ggupset() in next step)
    all_symptoms_list = as.list(strsplit(all_symptoms, "; "))
    )
```

View the new data. Note the two columns towards the right end - the pasted combined values, and the list

```{r, echo=F, , warning=F, message=F}
DT::datatable(head(linelist_sym_1,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


<!-- ======================================================= -->
## **ggupset** {  }

Load the package

```{r}
pacman::p_load(ggupset)
```


Create the plot. We begin with a `ggplot()` and `geom_bar()`, but then we add the special function `scale_x_upset()` from the **ggupset**.  

```{r, warning=F, message=F}
ggplot(
  data = linelist_sym_1,
  mapping = aes(x = all_symptoms_list)) +
geom_bar() +
scale_x_upset(
  reverse = FALSE,
  n_intersections = 10,
  sets = c("fever", "chills", "cough", "aches", "vomit"))+
labs(
  title = "Signs & symptoms",
  subtitle = "10 most frequent combinations of signs and symptoms",
  caption = "Caption here.",
  x = "Symptom combination",
  y = "Frequency in dataset")

```
  
More information on **ggupset** can be found [online](https://rdrr.io/cran/ggupset/man/scale_x_upset.html) or offline in the package documentation in your RStudio Help tab `?ggupset`.  


<!-- ======================================================= -->
## `UpSetR` {  }

The **UpSetR** package allows more customization of the plot, but it can be more difficult to execute:


**Load package**  

```{r}
pacman::p_load(UpSetR)
```

**Data cleaning**  

We must convert the `linelist` symptoms values to 1 / 0. 

```{r}
linelist_sym_2 <- linelist_sym %>% 
     # convert the "yes" and "no" values into 1s and 0s
     mutate(fever = ifelse(fever == "yes", 1, 0), 
            chills = ifelse(chills == "yes", 1, 0),
            cough = ifelse(cough == "yes", 1, 0),
            aches = ifelse(aches == "yes", 1, 0),
            vomit = ifelse(vomit == "yes", 1, 0))
            
```

If you are interested in a more efficient command, you can take advantage of the `+()` function, which converts to 1s and 0s based on a logical statement. This command utilizes the `across()` function to change multiple columns at once (read more in [Cleaning data and core functions](#clean_across)).  

```{r, eval=F, echo=T}
# Efficiently convert "yes" to 1 and 0
linelist_sym_2 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into 1s and 0s
  mutate(across(c(fever, chills, cough, aches, vomit), .fns = ~+(.x == "yes")))
```


Now make the plot using the custom function `upset()` - using only the symptoms columns. You must designate which "sets" to compare (the names of the symptom columns). Alternatively, use `nsets = ` and `order.by = "freq"` to only show the top X combinations.  

```{r, warning=F, message=F}

# Make the plot
linelist_sym_2 %>% 
  UpSetR::upset(
       sets = c("fever", "chills", "cough", "aches", "vomit"),
       order.by = "freq",
       sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
       empty.intersections = "on",
       # nsets = 3,
       number.angles = 0,
       point.size = 3.5,
       line.size = 2, 
       mainbar.y.label = "Symptoms Combinations",
       sets.x.label = "Patients with Symptom")

```


<!-- ======================================================= -->
## Resources {  }

[The github page on UpSetR](https://github.com/hms-dbmi/UpSetR)  

[A Shiny App version - you can upload your own data](https://gehlenborglab.shinyapps.io/upsetr/)  

[*documentation - difficult to interpret](https://cran.r-project.org/web/packages/UpSetR/UpSetR.pdf)  


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/combination_analysis.Rmd-->


# Phylogenetic trees {}  


<!-- ======================================================= -->

## Overview {}


**Phylogenetic trees** are used to visualize and describe the relatedness and evolution of organisms based on the sequence of their genetic code.  

They can be constructed from genetic sequences using distance-based methods (such as neighbor-joining method) or character-based methods (such as maximum likelihood and Bayesian Markov Chain Monte Carlo method). Next-generation sequencing (NGS) has become more affordable and is becoming more widely used in public health to describe pathogens causing infectious diseases. Portable sequencing devices decrease the turn around time and hold promises to make data available for the support of outbreak investigation in real-time. NGS data can be used to identify the origin or source of an outbreak strain and its propagation, as well as determine presence of antimicrobial resistance genes. To visualize the genetic relatedness between samples a phylogenetic tree is constructed.  

In this page we will learn how to use the **ggtree** package, which allows for combined visualization of phylogenetic trees with additional sample data in form of a dataframe. This will enable us to observe patterns and improve understanding of the outbreak dynamic.

```{r, phylogenetic_trees_overview_graph, out.width=c('80%'), fig.align='center', fig.show='hold', echo = FALSE}

pacman::p_load(here, ggplot2, dplyr, ape, ggtree, treeio, ggnewscale)

tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))

sample_data <- read.csv(here::here("data","phylo", "sample_data_Shigella_tree.csv"),sep=",", na.strings=c("NA"), head = TRUE, stringsAsFactors=F)


ggtree(tree, layout="circular", branch.length='none') %<+% sample_data + # the %<+% is used to add your dataframe with sample data to the tree
  aes(color=I(Belgium))+ # color the branches according to a variable in your dataframe
  scale_color_manual(name = "Sample Origin", # name of your color scheme (will show up in the legend like this)
                    breaks = c("Yes", "No"), # the different options in your variable
                   labels = c("NRCSS Belgium", "Other"), # how you want the different options named in your legend, allows for formatting
                 values= c("blue", "black"), # the color you want to assign to the variable 
                 na.value = "black") + # color NA values in black as well
  new_scale_color()+ # allows to add an additional color scheme for another variable
     geom_tippoint(aes(color=Continent), size=1.5)+ # color the tip point by continent, you may change shape adding "shape = "
scale_color_brewer(name = "Continent",  # name of your color scheme (will show up in the legend like this)
                       palette="Set1", # we choose a set of colors coming with the brewer package
                   na.value="grey")+ # for the NA values we choose the color grey
  theme(legend.position= "bottom")

```

<!-- ======================================================= -->

## Preparation {}

### Load packages {.unnumbered}  

This code chunk shows the loading of required packages. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, phylogenetic_trees_loading_packages}
pacman::p_load(
  rio,             # import/export
  here,            # relative file paths
  tidyverse,       # general data management and visualization
  ape,             # to import and export phylogenetic files
  ggtree,          # to visualize phylogenetic files
  treeio,          # to visualize phylogenetic files
  ggnewscale)      # to add additional layers of color schemes

```

### Import data {.unnumbered}  

The data for this page can be downloaded with the instructions on the [Download handbook and data] page.  

There are several different formats in which a phylogenetic tree can be stored (eg. Newick, NEXUS, Phylip). A common one is the Newick file format (.nwk), which is the standard for representing trees in computer-readable form. This means an entire tree can be expressed in a string format such as  "((t2:0.04,t1:0.34):0.89,(t5:0.37,(t4:0.03,t3:0.67):0.9):0.59); ", listing all nodes and tips and their relationship (branch length) to each other.  

Note: It is important to understand that the phylogenetic tree file in itself does not contain sequencing data, but is merely the result of the genetic distances between the sequences. We therefore cannot extract sequencing data from a tree file.

First, we use the `read.tree()` function from **ape** package to import a Newick phylogenetic tree file in .txt format, and store it in a list object of class "phylo". If necessary, use the  `here()` function from the **here** package to specify the relative file path.

Note: In this case the newick tree is saved as a .txt file for easier handling and downloading from Github.

```{r, echo=F}
tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))
```


```{r, echo=T, eval=F}
tree <- ape::read.tree("Shigella_tree.txt")
```

We inspect our tree object and see it contains 299 tips (or samples) and 236 nodes.  

```{r}
tree
```

Second, we import a table stored as a .csv file with additional information for each sequenced sample, such as gender, country of origin and attributes for antimicrobial resistance, using the `import()` function from the **rio** package:

```{r, echo=F}
sample_data <- import(here("data", "phylo", "sample_data_Shigella_tree.csv"))
```

```{r, echo=T, eval=F}
sample_data <- import("sample_data_Shigella_tree.csv")
```

Below are the first 50 rows of the data:  

```{r message=FALSE, echo=F}
DT::datatable(head(sample_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Clean and inspect {.unnumbered}  

We clean and inspect our data: In order to assign the correct sample data to the phylogenetic tree, the values in the column `Sample_ID` in the `sample_data` data frame need to match the `tip.labels` values in the `tree` file: 

We check the formatting of the `tip.labels` in the `tree` file by looking at the first 6 entries using with `head()` from **base** R.
```{r, phylogenetic_trees_inspect_sampledata}
head(tree$tip.label) 
```

We also make sure the first column in our `sample_data` data frame is `Sample_ID`. We look at the column names of our dataframe using  `colnames()` from **base** R.

```{r}
colnames(sample_data)   
```

We look at the `Sample_IDs` in the data frame to make sure the formatting is the same than in the `tip.label` (eg. letters are all capitals, no extra underscores `_` between letters and numbers, etc.)

```{r}
head(sample_data$Sample_ID) # we again inspect only the first 6 using head()
```

We can also compare if all samples are present in the `tree` file and vice versa by generating a logical vector of TRUE or FALSE where they do or do not match. These are not printed here, for simplicity.  

```{r, eval=F}
sample_data$Sample_ID %in% tree$tip.label

tree$tip.label %in% sample_data$Sample_ID
```

We can use these vectors to show any sample IDs that are not on the tree (there are none).  

```{r}
sample_data$Sample_ID[!tree$tip.label %in% sample_data$Sample_ID]
```

Upon inspection we can see that the format of `Sample_ID` in the dataframe corresponds to the format of sample names at the `tip.labels`. These do not have to be sorted in the same order to be matched.

We are ready to go!




<!-- ======================================================= -->

## Simple tree visualization {}


### Different tree layouts {.unnumbered}  

**ggtree** offers many different layout formats and some may be more suitable for your specific purpose than others. Below are a few demonstrations. For other options see this [online book](http://yulab-smu.top/treedata-book/chapter4.html).  

Here are some example tree layouts:
```{r, phylogenetic_trees_example_formats, out.width=c('50%'), fig.show='hold'}

ggtree(tree)                                            # simple linear tree
ggtree(tree,  branch.length = "none")                   # simple linear tree with all tips aligned
ggtree(tree, layout="circular")                         # simple circular tree
ggtree(tree, layout="circular", branch.length = "none") # simple circular tree with all tips aligned

```

### Simple tree plus sample data {.unnumbered}  

The **%<+%** operator is used to connect the `sample_data` data frame to the `tree` file.
The most easy annotation of your tree is the addition of the sample names at the tips, as well as coloring of tip points and if desired the branches:

Here is an example of a circular tree: 
```{r, phylogenetic_trees_adding_sampledata, fig.align='center', warning=F, message=F}

ggtree(tree, layout = "circular", branch.length = 'none') %<+% sample_data + # %<+% adds dataframe with sample data to tree
  aes(color = I(Belgium))+                       # color the branches according to a variable in your dataframe
  scale_color_manual(
    name = "Sample Origin",                      # name of your color scheme (will show up in the legend like this)
    breaks = c("Yes", "No"),                     # the different options in your variable
    labels = c("NRCSS Belgium", "Other"),        # how you want the different options named in your legend, allows for formatting
    values = c("blue", "black"),                  # the color you want to assign to the variable 
    na.value = "black") +                        # color NA values in black as well
  new_scale_color()+                             # allows to add an additional color scheme for another variable
    geom_tippoint(
      mapping = aes(color = Continent),          # tip color by continent. You may change shape adding "shape = "
      size = 1.5)+                               # define the size of the point at the tip
  scale_color_brewer(
    name = "Continent",                    # name of your color scheme (will show up in the legend like this)
    palette = "Set1",                      # we choose a set of colors coming with the brewer package
    na.value = "grey") +                    # for the NA values we choose the color grey
  geom_tiplab(                             # adds name of sample to tip of its branch 
    color = 'black',                       # (add as many text lines as you wish with + , but you may need to adjust offset value to place them next to each other)
    offset = 1,
    size = 1,
    geom = "text",
    align = TRUE)+    
  ggtitle("Phylogenetic tree of Shigella sonnei")+       # title of your graph
  theme(
    axis.title.x = element_blank(), # removes x-axis title
    axis.title.y = element_blank(), # removes y-axis title
    legend.title = element_text(    # defines font size and format of the legend title
      face = "bold",
      size = 12),   
    legend.text=element_text(       # defines font size and format of the legend text
      face = "bold",
      size = 10),  
    plot.title = element_text(      # defines font size and format of the plot title
      size = 12,
      face = "bold"),  
    legend.position = "bottom",     # defines placement of the legend
    legend.box = "vertical",        # defines placement of the legend
    legend.margin = margin())   
```

You can export your tree plot with `ggsave()` as you would any other ggplot object. Written this way, `ggsave()` saves the last image produced to the file path you specify. Remember that you can use `here()` and relative file paths to easily save in subfolders, etc.  

```{r, eval=F}
ggsave("example_tree_circular_1.png", width = 12, height = 14)

```


<!-- ======================================================= -->

## Tree manipulation {}

Sometimes you may have a very large phylogenetic tree and you are only interested in one part of the tree. For example, if you produced a tree including historical or international samples to get a large overview of where your dataset might fit in the bigger picture. But then to look closer at your data you want to inspect only that portion of the bigger tree.

Since the phylogenetic tree file is just the output of sequencing data analysis, we can not manipulate the order of the nodes and branches in the file itself. These have already been determined in previous analysis from the raw NGS data. We are able though to zoom into parts, hide parts and even subset part of the tree. 

### Zoom in {.unnumbered}  

If you don't want to "cut" your tree, but only inspect part of it more closely you can zoom in to view a specific part.

First, we plot the entire tree in linear format and add numeric labels to each node in the tree.
```{r, phylogenetic_trees_zoom_in, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(tree,) %<+% sample_data +
  geom_tiplab(size = 1.5) +                # labels the tips of all branches with the sample name in the tree file
  geom_text2(
    mapping = aes(subset = !isTip,
                  label = node),
    size = 5,
    color = "darkred",
    hjust = 1,
    vjust = 1)                            # labels all the nodes in the tree

p  # print

```

To zoom in to one particular branch (sticking out to the right), use `viewClade()` on the ggtree object `p` and provide the node number to get a closer look:
```{r phylogenetic_trees_zoom_in_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

viewClade(p, node = 452)

```

### Collapsing branches {.unnumbered} 

However, we may want to ignore this branch and can collapse it at that same node (node nr. 452) using `collapse()`. This tree is defined as `p_collapsed`. 

```{r phylogenetic_trees_collapse_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

p_collapsed <- collapse(p, node = 452)
p_collapsed
```

For clarity, when we print `p_collapsed`, we add a `geom_point2()` (a blue diamond) at the node of the collapsed branch.  
```{r}
p_collapsed + 
geom_point2(aes(subset = (node == 452)),  # we assign a symbol to the collapsed node
            size = 5,                     # define the size of the symbol
            shape = 23,                   # define the shape of the symbol
            fill = "steelblue")           # define the color of the symbol
```

### Subsetting a tree {.unnumbered} 

If we want to make a more permanent change and create a new, reduced tree to work with we can subset part of it with `tree_subset()`. Then you can save it as new newick tree file or .txt file. 

First, we inspect the tree nodes and tip labels in order to decide what to subset.  

```{r, phylogenetic_trees_subsetting, out.width=c('50%'), fig.show='hold', fig.align='center'}
ggtree(
  tree,
  branch.length = 'none',
  layout = 'circular') %<+% sample_data +               # we add the asmple data using the %<+% operator
  geom_tiplab(size = 1)+                                # label tips of all branches with sample name in tree file
  geom_text2(
    mapping = aes(subset = !isTip, label = node),
    size = 3,
    color = "darkred") +                                # labels all the nodes in the tree
 theme(
   legend.position = "none",                            # removes the legend all together
   axis.title.x = element_blank(),
   axis.title.y = element_blank(),
   plot.title = element_text(size = 12, face="bold"))
```

Now, say we have decided to subset the tree at node 528 (keep only tips within this branch after node 528) and we save it as a new `sub_tree1` object:

```{r}
sub_tree1 <- tree_subset(
  tree,
  node = 528)                                            # we subset the tree at node 528
```

Lets have a look at the subset tree 1:

```{r}
ggtree(sub_tree1) +
  geom_tiplab(size = 3) +
  ggtitle("Subset tree 1")
```

You can also subset based on one particular sample, specifying how many nodes "backwards" you want to include. Let's subset the same part of the tree based on a sample, in this case S17BD07692, going back 9 nodes and we save it as a new `sub_tree2` object:

```{r}
sub_tree2 <- tree_subset(
  tree,
  "S17BD07692",
  levels_back = 9) # levels back defines how many nodes backwards from the sample tip you want to go
```

Lets have a look at the subset tree 2:

```{r}
ggtree(sub_tree2) +
  geom_tiplab(size =3)  +
  ggtitle("Subset tree 2")

```

You can also save your new tree either as a Newick type or even a text file using the `write.tree()` function from **ape** package:

```{r, eval=F, phylogenetic_trees_write_tree}
# to save in .nwk format
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.nwk')

# to save in .txt format
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.txt')

```

### Rotating nodes in a tree {.unnumbered} 


As mentioned before we cannot change the order of tips or nodes in the tree, as this is based on their genetic relatedness and is not subject to visual manipulation. But we can rote branches around nodes if that eases our visualization.

First, we plot our new subset tree 2 with node labels to choose the node we want to manipulate and store it an a ggtree plot object `p`.

```{r, phylogenetic_trees_rotating_1, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) +  
  geom_tiplab(size = 4) +
  geom_text2(aes(subset=!isTip, label=node), # labels all the nodes in the tree
             size = 5,
             color = "darkred", 
             hjust = 1, 
             vjust = 1) 
p
```

We can then manipulate nodes by applying **ggtree::rotate()** or **ggtree::flip()**: 
Note: to illustrate which nodes we are manipulating we first apply the **geom_hilight()** function from **ggtree** to highlight the samples in the nodes we are interested in and store that ggtree plot object in a new object `p1`.

```{r, phylogenetic_trees_rotating_2, out.width=c('50%'), fig.show='hold', fig.align='center'}

p1 <- p + geom_hilight(  # highlights node 39 in blue, "extend =" allows us to define the length of the color block
  node = 39,
  fill = "steelblue",
  extend = 0.0017) +  
geom_hilight(            # highlights the node 37 in yellow
  node = 37,
  fill = "yellow",
  extend = 0.0017) +               
ggtitle("Original tree")


p1 # print
```

Now we can rotate node 37 in object `p1` so that the samples on node 38 move to the top. We store the rotated tree in a new object `p2`.
```{r}
p2 <- rotate(p1, 37) + 
      ggtitle("Rotated Node 37")


p2   # print
```

Or we can use the flip command to rotate node 36 in object `p1` and switch node 37 to the top and node 39 to the bottom. We store the flipped tree in a new object `p3`.
```{r}

p3 <-  flip(p1, 39, 37) +
      ggtitle("Rotated Node 36")


p3   # print
```

### Example subtree with sample data annotation {.unnumbered} 

Lets say we are investigating the cluster of cases with clonal expansion which occurred in 2017 and 2018 at node 39 in our sub-tree. We add the year of strain isolation as well as travel history and color by country to see origin of other closely related strains:

```{r, phylogenetic_trees_inspect_subset_example, out.width=c('80%'), fig.show='hold', fig.align='center', warning=F, message=F}

ggtree(sub_tree2) %<+% sample_data +     # we use th %<+% operator to link to the sample_data
  geom_tiplab(                          # labels the tips of all branches with the sample name in the tree file
    size = 2.5,
    offset = 0.001,
    align = TRUE) + 
  theme_tree2()+
  xlim(0, 0.015)+                       # set the x-axis limits of our tree
  geom_tippoint(aes(color=Country),     # color the tip point by continent
                size = 1.5)+ 
  scale_color_brewer(
    name = "Country", 
    palette = "Set1", 
    na.value = "grey")+
  geom_tiplab(                          # add isolation year as a text label at the tips
    aes(label = Year),
    color = 'blue',
    offset = 0.0045,
    size = 3,
    linetype = "blank" ,
    geom = "text",
    align = TRUE)+ 
  geom_tiplab(                          # add travel history as a text label at the tips, in red color
    aes(label = Travel_history),
    color = 'red',
    offset = 0.006,
    size = 3,
    linetype = "blank",
    geom = "text",
    align = TRUE)+ 
  ggtitle("Phylogenetic tree of Belgian S. sonnei strains with travel history")+  # add plot title
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+                    # add a label to the x-axis 
  theme(
    axis.title.x = element_text(size = 10),
    axis.title.y = element_blank(),
    legend.title = element_text(face = "bold", size = 12),
    legend.text = element_text(face = "bold", size = 10),
    plot.title = element_text(size = 12, face = "bold"))

```

Our observation points towards an import event of strains from Asia, which then circulated in Belgium over the years and seem to have caused our latest outbreak.

<!-- ======================================================= -->

## More complex trees: adding heatmaps of sample data {.unnumbered}


We can add more complex information, such as categorical presence of antimicrobial resistance genes and numeric values for actually measured resistance to antimicrobials in form of a heatmap using the **ggtree::gheatmap()** function.

First we need to plot our tree (this can be either linear or circular) and store it in a new ggtree plot object `p`: We will use the sub_tree from part 3.)
```{r, phylogenetic_trees_sampledata_heatmap, out.width=c('60%'), fig.align='center', fig.show='hold'}

p <- ggtree(sub_tree2, branch.length='none', layout='circular') %<+% sample_data +
  geom_tiplab(size =3) + 
 theme(
   legend.position = "none",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(
      size = 12,
      face = "bold",
      hjust = 0.5,
      vjust = -15))
p

```

Second, we prepare our data. To visualize different variables with new color schemes, we subset our dataframe to the desired variable. It is important to add the `Sample_ID` as rownames otherwise it cannot match the data to the tree `tip.labels`:

In our example we want to look at gender and mutations that could confer resistance to Ciprofloxacin, an important first line antibiotic used to treat Shigella infections.

We create a dataframe for gender: 
```{r, phylogenetic_trees_sampledata_heatmap_data}
gender <- data.frame("gender" = sample_data[,c("Gender")])
rownames(gender) <- sample_data$Sample_ID
```

We create a dataframe for mutations in the gyrA gene, which confer Ciprofloxacin resistance:
```{r}
cipR <- data.frame("cipR" = sample_data[,c("gyrA_mutations")])
rownames(cipR) <- sample_data$Sample_ID

```
We create a dataframe for the measured minimum inhibitory concentration (MIC) for Ciprofloxacin from the laboratory:
```{r}
MIC_Cip <- data.frame("mic_cip" = sample_data[,c("MIC_CIP")])
rownames(MIC_Cip) <- sample_data$Sample_ID
```

We create a first plot adding a binary heatmap for gender to the phylogenetic tree and storing it in a new ggtree plot object `h1`:
```{r, phylogenetic_trees_sampledata_heatmap_gender, out.width=c('70%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender,                                 # we add a heatmap layer of the gender dataframe to our tree plot
                offset = 10,                               # offset shifts the heatmap to the right,
                width = 0.10,                              # width defines the width of the heatmap column,
                color = NULL,                              # color defines the boarder of the heatmap columns
         colnames = FALSE) +                               # hides column names for the heatmap
  scale_fill_manual(name = "Gender",                       # define the coloring scheme and legend for gender
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1

```

Then we add information on mutations in the gyrA gene, which confer resistance to Ciprofloxacin:

Note: The presence of chromosomal point mutations in WGS data was prior determined using the PointFinder tool developed by Zankari et al. (see reference in the additional references section)

First, we assign a new color scheme to our existing plot object `h1` and store it in a now object `h2`. This enables us to define and change the colors for our second variable in the heatmap.
```{r}
h2 <- h1 + new_scale_fill() 
```

Then we add the second heatmap layer to `h2` and store the combined plots in a new object `h3`:

```{r, phylogenetic_trees_sampledata_heatmap_cip_genes, out.width=c('80%'), fig.show='hold', fig.align='center'}

h3 <- gheatmap(h2, cipR,         # adds the second row of heatmap describing Ciprofloxacin resistance mutations
               offset = 12, 
               width = 0.10, 
               colnames = FALSE) +
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
h3
```

We repeat the above process, by first adding a new color scale layer to our existing object `h3`, and then adding the continuous data on the minimum inhibitory concentration (MIC) of Ciprofloxacin for each strain to the resulting object `h4` to produce the final object `h5`:
```{r, phylogenetic_trees_sampledata_heatmap_cip_MIC, out.width=c('90%'), fig.show='hold', fig.align='center'}
# First we add the new coloring scheme:
h4 <- h3 + new_scale_fill()

# then we combine the two into a new plot:
h5 <- gheatmap(h4, MIC_Cip,  
               offset = 14, 
               width = 0.10,
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",  # here we define a gradient color scheme for the continuous variable of MIC
                      low = "yellow", high = "red",
                      breaks = c(0, 0.50, 1.00),
                      na.value = "white") +
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h5

```

We can do the same exercise for a linear tree:
```{r, phylogenetic_trees_sampledata_heatmap_linear_1, out.width=c('80%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) %<+% sample_data +
  geom_tiplab(size = 3) + # labels the tips
  theme_tree2()+
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+
  xlim(0, 0.015)+
 theme(legend.position = "none",
      axis.title.y = element_blank(),
      plot.title = element_text(size = 12, 
                                face = "bold",
                                hjust = 0.5,
                                vjust = -15))
p
```

First we add gender:  

```{r, phylogenetic_trees_sampledata_heatmap_linear_2, out.width=c('80%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender, 
                offset = 0.003,
                width = 0.1, 
                color="black", 
         colnames = FALSE)+
  scale_fill_manual(name = "Gender",
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1
```


Then we add Ciprofloxacin resistance mutations after adding another color scheme layer:


```{r, phylogenetic_trees_sampledata_heatmap_linear_3, out.width=c('80%'), fig.show='hold', fig.align='center'}

h2 <- h1 + new_scale_fill()
h3 <- gheatmap(h2, cipR,   
               offset = 0.004, 
               width = 0.1,
               color = "black",
                colnames = FALSE)+
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
 h3
```

Then we add the minimum inhibitory concentration determined by the laboratory (MIC):

```{r, phylogenetic_trees_sampledata_heatmap_linear_4, out.width=c('80%'), fig.show='hold', fig.align='center'}

h4 <- h3 + new_scale_fill()
h5 <- gheatmap(h4, MIC_Cip, 
               offset = 0.005,  
               width = 0.1,
               color = "black", 
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",
                      low = "yellow", high = "red",
                      breaks = c(0,0.50,1.00),
                      na.value = "white")+
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 8),
        legend.box = "horizontal", legend.margin = margin())+
  guides(shape = guide_legend(override.aes = list(size = 2)))
h5

```


<!-- ======================================================= -->
## Resources {}

http://hydrodictyon.eeb.uconn.edu/eebedia/index.php/Ggtree# Clade_Colors
https://bioconductor.riken.jp/packages/3.2/bioc/vignettes/ggtree/inst/doc/treeManipulation.html
https://guangchuangyu.github.io/ggtree-book/chapter-ggtree.html
https://bioconductor.riken.jp/packages/3.8/bioc/vignettes/ggtree/inst/doc/treeManipulation.html

Ea Zankari, Rosa Allesøe, Katrine G Joensen, Lina M Cavaco, Ole Lund, Frank M Aarestrup, PointFinder: a novel web tool for WGS-based detection of antimicrobial resistance associated with chromosomal point mutations in bacterial pathogens, Journal of Antimicrobial Chemotherapy, Volume 72, Issue 10, October 2017, Pages 2764–2768, https://doi.org/10.1093/jac/dkx217


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/phylogenetic_trees.Rmd-->


# Interactive plots { }  

Data visualisation is increasingly required to be interrogable by the audience. Consequently, is is becoming common to create interactive plots. There are several ways to include these but the two most common are **plotly** and **shiny**. 

In this page we will focus on converting an existing `ggplot()` plot into an interactive plot with **plotly**. You can read more about **shiny** in the [Dashboards with Shiny] page. What is worth mentioning is that interactive plots are only useable in HTML format R markdown documents, not PDF or Word documents.

Below is a basic epicurve that has been transformed to be interactive using the integration of **ggplot2** and **plotly** (hover your mouse over the plot, zoom in, or click items in the legend). 

```{r plotly_demo, out.width=c('75%'), out.height=c('500px'), echo=F, warning=F, message=F}
pacman::p_load(plotly, rio, here, ggplot2, dplyr, lubridate)
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## these buttons are superfluous/distracting
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- linelist %>% 
  mutate(outcome = if_else(is.na(outcome), "Unknown", outcome),
         date_earliest = if_else(is.na(date_infection), date_onset, date_infection),
         week_earliest = floor_date(date_earliest, unit = "week",week_start = 1))%>% 
  count(week_earliest, outcome) %>% 
  ggplot()+
  geom_col(aes(week_earliest, n, fill = outcome))+
  xlab("Week of infection/onset") + ylab("Cases per week")+
  theme_minimal()

p %>% 
  ggplotly() %>% 
  partial_bundle() %>% 
  config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)

```

<!-- ======================================================= -->
## Preparation {  }

### Load packages {.unnumbered}  

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  


```{r}
pacman::p_load(
  rio,       # import/export
  here,      # filepaths
  lubridate, # working with dates
  plotly,    # interactive plots
  scales,    # quick percents
  tidyverse  # data management and visualization
  ) 
```

### Start with a `ggplot()` {.unnumbered}  

In this page we assume that you are beginning with a `ggplot()` plot that you want to convert to be interactive. We will build several of these plots in this page, using the case `linelist` used in many pages of this handbook.  


### Import data {.unnumbered}

To begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```






  
<!-- ======================================================= -->
## Plot with `ggplotly()` {  }

The function `ggplotly()` from the **plotly** package makes it easy to convert a `ggplot()` to be interactive. Simply save your `ggplot()` and then pipe it to the `ggplotly()` function.  


Below, we plot a simple line representing the proportion of cases who died in a given week:  

We begin by creating a summary dataset of each epidemiological week, and the percent of cases with a known outcome that died.  

```{r}
weekly_deaths <- linelist %>%
  group_by(epiweek = floor_date(date_onset, "week")) %>%  # create and group data by epiweek column
  summarise(                                              # create new summary data frame:
    n_known_outcome = sum(!is.na(outcome), na.rm=T),      # number of cases per group with known outcome
    n_death  = sum(outcome == "Death", na.rm=T),          # number of cases per group who died
    pct_death = 100*(n_death / n_known_outcome)           # percent of cases with known outcome who died
  )
```
Here is the first 50 rows of the `weekly_deaths` dataset.  

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_deaths, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
Then we create the plot with **ggplot2**, using `geom_line()`.  

```{r, warning=F, message=F}
deaths_plot <- ggplot(data = weekly_deaths)+            # begin with weekly deaths data
  geom_line(mapping = aes(x = epiweek, y = pct_death))  # make line 

deaths_plot   # print
```


We can make this interactive by simply passing this plot to `ggplotly()`, as below. Hover your mouse over the line to show the x and y values. You can zoom in on the plot, and drag it around. You can also see icons in the upper-right of the plot. In order, they allow you to:  

* Download the current view as a PNG image  
* Zoom in with a select box  
* "Pan", or move across the plot by clicking and dragging the plot  
* Zoom in, zoom out, or return to default zoom  
* Reset axes to defaults  
* Toggle on/off "spike lines" which are dotted lines from the interactive point extending to the x and y axes  
* Adjustments to whether data show when you are not hovering on the line  


```{r}
deaths_plot %>% plotly::ggplotly()
```

Grouped data work with `ggplotly()` as well. Below, a weekly epicurve is made, grouped by outcome. The stacked bars are interactive. Try clicking on the different items in the legend (they will appear/disappear).  


```{r plot_show, eval=F}
# Make epidemic curve with incidence2 pacakge
p <- incidence2::incidence(
  linelist,
  date_index = date_onset,
  interval = "weeks",
  groups = outcome) %>% plot(fill = outcome)
```

```{r, echo=T, eval=F}
# Plot interactively  
p %>% plotly::ggplotly()
```
  
```{r, warning = F, message = F, , out.width=c('95%'), out.height=c('500px'), echo=FALSE}
p %>% 
  ggplotly() %>% 
  partial_bundle() 
```
  
<!-- ======================================================= -->
## Modifications {  }

### File size {.unnumbered}  

When exporting in an R Markdown generated HTML (like this book!) you want to make the plot as small data size as possible (with no negative side effects in most cases). For this, just pipe the interactive plot to `partial_bundle()`, also from **plotly**.  

```{r plot_tidyshow, eval=F}
p <- p %>% 
  plotly::ggplotly() %>%
  plotly::partial_bundle()
```

### Buttons {.unnumbered}  

Some of the buttons on a standard plotly are superfluous and can be distracting, so you can remove them. You can do this simply by piping the output into `config()` from **plotly** and specifying which buttons to remove. In the below example we specify in advance the names of the buttons to remove, and provide them to the argument `modeBarButtonsToRemove = `. We also set `displaylogo = FALSE` to remove the plotly logo.  

```{r plot_tidyshow2, eval=F}
## these buttons are distracting and we want to remove them
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- p %>%          # re-define interactive plot without these buttons
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```



<!-- ======================================================= -->
## Heat tiles {  }

You can make almost any `ggplot()` plot interactive, including heat tiles. In the page on [Heat plots] you can read about how to make the below plot, which displays the proportion of days per week that certain facilities reported data to their province.  

Here is the code, although we will not describe it in depth here.  

```{r  message=F, warning=F}
# import data
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds"))

# aggregate data into Weeks for Spring district
agg_weeks <- facility_count_data %>% 
  filter(District == "Spring",
         data_date < as.Date("2020-08-01")) %>% 
  mutate(week = aweek::date2week(
    data_date,
    start_date = "Monday",
    floor_day = TRUE,
    factor = TRUE)) %>% 
  group_by(location_name, week, .drop = F) %>%
  summarise(
    n_days          = 7,
    n_reports       = n(),
    malaria_tot     = sum(malaria_tot, na.rm = T),
    n_days_reported = length(unique(data_date)),
    p_days_reported = round(100*(n_days_reported / n_days))) %>% 
  ungroup(location_name, week) %>% 
  right_join(tidyr::expand(., week, location_name)) %>% 
  mutate(week = aweek::week2date(week))

# create plot
metrics_plot <- ggplot(agg_weeks,
       aes(x = week,
           y = location_name,
           fill = p_days_reported))+
  geom_tile(colour="white")+
  scale_fill_gradient(low = "orange", high = "darkgreen", na.value = "grey80")+
  scale_x_date(expand = c(0,0),
               date_breaks = "2 weeks",
               date_labels = "%d\n%b")+
  theme_minimal()+ 
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),
    legend.key.width  = grid::unit(0.6,"cm"),
    axis.text.x = element_text(size=12),
    axis.text.y = element_text(vjust=0.2),
    axis.ticks = element_line(size=0.4),
    axis.title = element_text(size=12, face="bold"),
    plot.title = element_text(hjust=0,size=14,face="bold"),
    plot.caption = element_text(hjust = 0, face = "italic")
    )+
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, April-May 2019",
       caption = "7-day weeks beginning on Mondays.")

metrics_plot # print
```

Below, we make it interactive and modify it for simple buttons and file size.  

```{r,  out.width=c('95%'), out.height=c('500px')}
metrics_plot %>% 
  plotly::ggplotly() %>% 
  plotly::partial_bundle() %>% 
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```

<!-- ## Maps {.unnumbered}   -->

<!-- You can also make `ggplot()` GIS maps interactive, although it makes a bit more care.  -->

<!-- THIS SECTION IS UNDER CONSTRUCTION  -->

<!-- Although **plotly** works well with `ggplot2::geom_sf` in RStudio, when you try to include its outputs in R Markdown HTML files (like this book), it doesn't work well.   -->

<!-- So instead you can use {**plotly**}'s own mapping tools which can be tricky but are easy when you know how. Read on...   -->

<!-- We're going to use Covid-19 incidence across African countries for this example. The data used can be found on the [World Health Organisation website](https://covid19.who.int/table).   -->

<!-- You'll also need a new type of file, a GeoJSON, which is sort of similar to a shp file for those familiar with GIS. For this book, we used one from [here](https://geojson-maps.ash.ms).   -->

<!-- GeoJSON files are stored in R as complex lists and you'll need to maipulate them a little. -->

<!-- ```{r, echo=T,} -->
<!-- ## You need two new packages: {rjson} and {purrr} -->
<!-- pacman::p_load(plotly, rjson, purrr) -->

<!-- ## This is a simplified version of the WHO data -->
<!-- df <- rio::import(here::here("data", "gis", "covid_incidence.csv")) -->

<!-- ## Load your geojson file -->
<!-- geoJSON <- rjson::fromJSON(file=here::here("data", "gis", "africa_countries.geo.json")) -->

<!-- ## Here are some of the properties for each element of the object -->
<!-- head(geoJSON$features[[1]]$properties) -->

<!-- ``` -->


<!-- This is the tricky part. For {**plotly**} to match your incidence data to GeoJSON, the countries in the geoJSON need an id in a specific place in the list of lists. For this we need to build a basic function: -->
<!-- ```{r} -->
<!-- ## The property column we need to choose here is "sovereignt" as it is the names for each country -->
<!-- give_id <- function(x){ -->

<!--   x$id <- x$properties$sovereignt  ## Take sovereignt from properties and set it as the id -->

<!--   return(x) -->
<!-- } -->

<!-- ## Use {purrr} to apply this function to every element of the features list of the geoJSON object -->
<!-- geoJSON$features <- purrr::map(.x = geoJSON$features, give_id) -->
<!-- ``` -->

<!-- <!-- ======================================================= --> -->
<!-- ### Maps - plot {  } -->

<!-- UNDER CONSTRUCTION -->

<!-- ```{r, echo=FALSE, eval=FALSE, out.width=c('95%'), out.height=c('500px'),warning=F} -->
<!-- plotly::plot_ly() %>%  -->
<!--   plotly::add_trace(                    #The main plot mapping functionn -->
<!--     type="choropleth", -->
<!--     geojson=geoJSON, -->
<!--     locations=df$Name,          #The column with the names (must match id) -->
<!--     z=df$Cumulative_incidence,  #The column with the incidence values -->
<!--     zmin=0, -->
<!--     zmax=57008, -->
<!--     colorscale="Viridis", -->
<!--     marker=list(line=list(width=0)) -->
<!--   ) %>% -->
<!--   colorbar(title = "Cases per million") %>% -->
<!--   layout(title = "Covid-19 cumulative incidence", -->
<!--                  geo = list(scope = 'africa')) %>%  -->
<!--   config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove) -->
<!-- ``` -->

<!-- ======================================================= -->
## Resources {  }

Plotly is not just for R, but also works well with Python (and really any data science language as it's built in JavaScript). You can read more about it on the [plotly website](https://plotly.com/r/)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/interactive_plots.Rmd-->

# (PART) Reports and dashboards {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_reports_dashboards.Rmd-->


# Reports with R Markdown { }  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_overview.png"))
```

R Markdown is a widely-used tool for creating automated, reproducible, and share-worthy outputs, such as reports. It can generate static or interactive outputs, in Word, pdf, html, powerpoint, and other formats. 

An R Markdown script intersperces R code and text such that the script actually  *becomes your output document*. You can create an entire formatted document, including narrative text (can be dynamic to change based on your data), tables, figures, bullets/numbers, bibliographies, etc. 

Such documents can be produced to update on a routine basis (e.g. daily surveillance reports) and/or run on subsets of data (e.g. reports for each jurisdiction).  

Other pages in this handbook expand on this topic:  

* The page [Organizing routine reports] demonstrates how to routinize your report production with auto-generated time-stamped folders.  
* The page [Dashboards with R Markdown] explains how to format a R Markdown report as a dashboard.  


Of note, the [R4Epis](https://r4epis.netlify.app/) project has developed template R Markdown scripts for common outbreaks and surveys scenarios encountered at MSF project locations.  


<!-- ======================================================= -->
## Preparation {  }

**Background to R Markdown**

To explain some of the concepts and packages involved:

* **Markdown** is a “language” that allows you to write a document using plain text, that can be converted to html and other formats. It is not specific to R. Files written in Markdown have a ‘.md’ extension.
* **R Markdown**: is a variation on markdown that _is specific to R_ - it allows you to write a document using markdown to produce text *and to embed R code and display their outputs*. R Markdown files have '.Rmd' extension.  
* **rmarkdown - the package**: This is used by R to render the .Rmd file into the desired output. It's focus is converting the markdown (text) syntax, so we also need...
* **knitr**: This R package will read the code chunks, execute it, and 'knit' it back into the document. This is how tables and graphs are included alongside the text.
* **Pandoc**: Finally, pandoc actually convert the output into word/pdf/powerpoint etc. It is a software separate from R but is installed automatically with RStudio.  

In sum, the process that happens *in the background* (you do not need to know all these steps!) involves feeding the .Rmd file to **knitr**, which executes the R code chunks and creates a new .md (markdown) file which includes the R code and its rendered output. The .md file is then processed by pandoc to create the finished product: a Microsoft Word document, HTML file, powerpoint document, pdf, etc.  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/0_rmd.png"))
```

(source: https://rmarkdown.rstudio.com/authoring_quick_tour.html):

**Installation**

To create a R Markdown output, you need to have the following installed:

* The **rmarkdown** package (**knitr** will also be installed automatically)  
* Pandoc, which should come installed with RStudio. If you are not using RStudio, you can download Pandoc here: http://pandoc.org. 
* If you want to generate PDF output (a bit trickier), you will need to install LaTeX. For R Markdown users who have not installed LaTeX before, we recommend that you install TinyTeX (https://yihui.name/tinytex/). You can use the following commands: 

```{r, eval=F}
pacman::p_load(tinytex)     # install tinytex package
tinytex::install_tinytex()  # R command to install TinyTeX software
```

<!-- ======================================================= -->
## Getting started {  }

### Install rmarkdown R package {.unnumbered}

Install the **rmarkdown** R package. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, eval=F}
pacman::p_load(rmarkdown)
```

### Starting a new Rmd file {.unnumbered}

In RStudio, open a new R markdown file, starting with ‘File’, then ‘New file’ then ‘R markdown…’. 

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstarted.png"))
```

R Studio will give you some output options to pick from. In the example below we select "HTML" because we want to create an html document. The title and the author names are not important. If the output document type you want is not one of these, don't worry - you can just pick any one and change it in the script later. 

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstartedB.png"))
```

This will open up a new .Rmd script.

### Important to know {.unnumbered}
 
**The working directory**

The working directory of a markdown file is wherever the Rmd file itself is saved. For instance, if the R project is within `~/Documents/projectX ` and the Rmd file itself is in a subfolder `~/Documents/projectX/markdownfiles/markdown.Rmd`, the code `read.csv(“data.csv”)` within the markdown will look for a csv file in the `markdownfiles` folder, and not the root project folder where scripts within projects would normally automatically look.

To refer to files elsewhere, you will either need to use the full file path or use the **here** package. The **here** package sets the working directory to the root folder of the R project and is explained in detail in the [R projects] and [Import and export] pages of this handbook. For instance, to import a file called "data.csv" from within the `projectX` folder, the code would be `import(here(“data.csv”))`. 

Note that use of `setwd()` in R Markdown scripts is not recommended – it only applies to the code chunk that it is written in. 

**Working on a drive vs your computer**

Because R Markdown can run into pandoc issues when running on a shared network drive, it is recommended that your folder is on your local machine, e.g. in a project within ‘My Documents’. If you use Git (much recommended!), this will be familiar. For more details, see the handbook pages on [R on network drives] and [Errors and help].  


<!-- ======================================================= -->
## R Markdown components {  }

An R Markdown document can be edited in RStudio just like a standard R script. When you start a new R Markdown script, RStudio tries to be helpful by showing a template which explains the different section of an R Markdown script. 

The below is what appears when starting a new Rmd script intended to produce an html output (as per previous section). 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_defaultRMD.png"))
```

As you can see, there are three basic components to an Rmd file: YAML, Markdown text, and R code chunks.  

These will *create and become your document output*. See the diagram below:  

```{r out.width = "100%", out.height="150%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_translation.png"))
```



### YAML metadata {.unnumbered}

Referred to as the ‘YAML metadata’ or just ‘YAML’, this is at the top of the R Markdown document. This section of the script will tell your Rmd file what type of output to produce, formatting preferences, and other metadata such as document title, author, and date. There are other uses not mentioned here (but referred to in ‘Producing an output’). Note that indentation matters; tabs are not accepted but spaces are. 

This section must begin with a line containing just three dashes `---` and must close with a line containing just three dashes `---`. YAML parameters comes in `key:value` pairs. The placement of colons in YAML is important - the `key:value` pairs are separated by colons (not equals signs!).  

The YAML should begin with metadata for the document. The order of these primary YAML parameters (not indented) does not matter. For example:  

```yaml
title: "My document"
author: "Me"
date: "`r Sys.Date()`"
```

You can use R code in YAML values by writing it as in-line code (preceded by `r` within back-ticks) but also within quotes (see above example for `date: `).  

In the image above, because we clicked that our default output would be an html file, we can see that the YAML says `output: html_document`. However we can also change this to say `powerpoint_presentation` or `word_document` or even `pdf_document`.  


### Text {.unnumbered}

This is the narrative of your document, including the titles and headings. It is written in the "markdown" language, which is used across many different software.  

Below are the core ways to write this text. See more extensive documentation available on R Markdown "cheatsheet" at the [RStudio website](https://rstudio.com/resources/cheatsheets/).  

#### New lines {.unnumbered}  

Uniquely in R Markdown, to initiate a new line, enter *two spaces** at the end of the previous line and then Enter/Return.  



#### Case {.unnumbered}  

Surround your normal text with these character to change how it appears in the output.  

* Underscores (`_text_`) or single asterisk (`*text*`) to _italicise_
* Double asterisks (`**text**`) for **bold text**
* Back-ticks (````text````) to display text as code  

The actual appearance of the font can be set by using specific templates (specified in the YAML metadata; see example tabs).  

#### Color {.unnumbered}  

There is no simple mechanism to change the color of text in R Markdown. One work-around, *IF your output is an HTML file*, is to add an HTML line into the markdown text. The below HTML code will print a line of text in bold red.  

```md
<span style="color: red;">**_DANGER:_** This is a warning.</span>  
```

<span style="color: red;">**_DANGER:_** This is a warning.</span>  


#### Titles and headings {.unnumbered}  

A hash symbol in a text portion of a R Markdown script creates a heading. This is different than in a chunk of R code in the script, in which a hash symbol is a mechanism to comment/annotate/de-activate, as in a normal R script.  

Different heading levels are established with different numbers of hash symbols at the start of a new line. One hash symbol is a title or primary heading. Two hash symbols are a second-level heading. Third- and fourth-level headings can be made with successively more hash symbols.

```md
# First-level heading / title

## Second level heading  

### Third-level heading
```


#### Bullets and numbering {.unnumbered}  

Use asterisks (`*`) to created a bullets list. Finish the previous sentence, enter two spaces, Enter/Return *twice*, and then start your bullets. Include a space between the asterisk and your bullet text. After each bullet enter two spaces and then Enter/Return. Sub-bullets work the same way but are indented. Numbers work the same way but instead of an asterisk, write 1), 2), etc. Below is how your R Markdown script text might look.  


```md
Here are my bullets (there are two spaces after this colon):  

* Bullet 1 (followed by two spaces and Enter/Return)  
* Bullet 2 (followed by two spaces and Enter/Return)  
  * Sub-bullet 1 (followed by two spaces and Enter/Return)  
  * Sub-bullet 2 (followed by two spaces and Enter/Return)  
  
```


#### Comment out text {.unnumbered}

You can "comment out" R Markdown text just as you can use the "#" to comment out a line of R code in an R chunk. Simply highlight the text and press Ctrl+Shift+c (Cmd+Shift+c for Mac). The text will be surrounded by arrows and turn green. It will not appear in your output.  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_hide_text.png"))
```


### Code chunks {.unnumbered}

Sections of the script that are dedicated to running R code are called "chunks". This is where you may load packages, import data, and perform the actual data management and visualisation. There may be many code chunks, so they can help you organize your R code into parts, perhaps interspersed with text. To note:
These ‘chunks’ will appear to have a slightly different background colour from the narrative part of the document.  

Each chunk is opened with a line that starts with three back-ticks, and curly brackets that contain parameters for the chunk (`{ }`). The chunk ends with three more back-ticks.  

You can create a new chunk by typing it out yourself, by using the keyboard shortcut "Ctrl + Alt + i" (or Cmd + Shift + r in Mac), or by clicking the green 'insert a new code chunk' icon at the top of your script editor.

Some notes about the contents of the curly brackets `{ }`:

*	They start with ‘r’ to indicate that the language name within the chunk is R
*	After the r you can optionally write a chunk "name" – these are not necessary but can help you organise your work. Note that if you name your chunks, you should ALWAYS use unique names or else R will complain when you try to render.  
*	The curly brackets can include other options too, written as `tag=value`, such as:  
  * `eval = FALSE` to not run the R code  
  * `echo = FALSE` to not print the chunk's R source code in the output document  
  * `warning = FALSE` to not print warnings produced by the R code  
  * `message = FALSE` to not print any messages produced by the R code  
  * `include =` either TRUE/FALSE whether to include chunk outputs (e.g. plots) in the document
  * `out.width = ` and `out.height =` - provide in style `out.width = "75%"`  
  * `fig.align = "center"` adjust how a figure is aligned across the page  
  * `fig.show='hold'` if your chunk prints multiple figures and you want them printed next to each other (pair with `out.width = c("33%", "67%")`. Can also set as `fig.show='asis'` to show them below the code that generates them, `'hide'` to hide, or `'animate'` to concatenate multiple into an animation.  
* A chunk header must be written in *one line*  
* Try to avoid periods, underscores, and spaces. Use hyphens ( - ) instead if you need a separator.  

Read more extensively about the **knitr** options [here](https://yihui.org/knitr/options/).  

Some of the above options can be configured with point-and-click using the setting buttons at the top right of the chunk. Here, you can specify which parts of the chunk you want the rendered document to include, namely the code, the outputs, and the warnings. This will come out as written preferences within the curly brackets, e.g. `echo=FALSE` if you specify you want to ‘Show output only’.  


```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunk.png"))
```

There are also two arrows at the top right of each chunk, which are useful to run code within a chunk, or all code in prior chunks. Hover over them to see what they do.


For global options to be applied to all chunks in the script, you can set this up within your very first R code chunk in the script. For instance, so that only the outputs are shown for each code chunk and not the code itself, you can include this command in the R code chunk:  

```{r, eval=F}
knitr::opts_chunk$set(echo = FALSE) 
```



#### In-text R code {.unnumbered}  

You can also include minimal R code within back-ticks. Within the back-ticks, begin the code with "r" and a space, so RStudio knows to evaluate the code as R code. See the example below.  

The example below shows multiple heading levels, bullets, and uses R code for the current date (`Sys.Date()`) to evaluate into a printed date.

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_text.png"))
```



  
The example above is simple (showing the current date), but using the same syntax you can display values produced by more complex R code (e.g. to calculate the min, median, max of a column). You can also integrate R objects or values that were created in R code chunks earlier in the script.  

As an example, the script below calculates the proportion of cases that are aged less than 18 years old, using **tidyverse** functions, and creates the objects `less18`, `total`, and `less18prop`. This dynamic value is inserted into subsequent text. We see how it looks when knitted to a word document.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunktext.png"))
```


### Images {.unnumbered}  

You can include images in your R Markdown one of two ways:  

```{r, eval=F}
![]("path/to/image.png")  
```

If the above does not work, try using `knitr::include_graphics()`  

```{r, eval=F}
knitr::include_graphics("path/to/image.png")
```

(remember, your file path could be written using the **here** package)

```{r, eval=F}
knitr::include_graphics(here::here("path", "to", "image.png"))
```


### Tables {.unnumbered}  

Create a table using hyphens ( - ) and bars ( | ). The number of hyphens before/between bars allow the number of spaces in the cell before the text begins to wrap.  


```md
Column 1 |Column  2 |Column 3
---------|----------|--------
Cell A   |Cell B    |Cell C
Cell D   |Cell E    |Cell F
```

The above code produces the table below:  

Column 1 |Column  2 |Column 3
---------|----------|--------
Cell A   |Cell B    |Cell C
Cell D   |Cell E    |Cell F


### Tabbed sections {.unnumbered}  

For HTML outputs, you can arrange the sections into "tabs". Simply add `.tabset` in the curly brackets `{ }` that are placed *after a heading*. Any sub-headings beneath that heading (until another heading of the same level) will appear as tabs that the user can click through. Read more [here](https://bookdown.org/yihui/rmarkdown-cookbook/html-tabs.html)  



```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/tabbed_script.png"))
knitr::include_graphics(here::here("images", "markdown/tabbed_view.gif"))

```


You can add an additional option `.tabset-pills` after `.tabset` to give the tabs themselves a "pilled" appearance. Be aware that when viewing the tabbed HTML output, the Ctrl+f search functionality will only search "active" tabs, not hidden tabs.  





<!-- ======================================================= -->
## File structure {}

There are several ways to structure your R Markdown and any associated R scripts. Each has advantages and disadvantages:  

* Self-contained R Markdown - everything needed for the report is imported or created within the R Markdown  
  * Source other files - You can run external R scripts with the `source()` command and use their outputs in the Rmd  
  * Child scripts - an alternate mechanism for `source()`  
* Utilize a "runfile" - Run commands in an R script *prior to* rendering the R Markdown  


### Self-contained Rmd {.unnumbered}  

For a relatively simple report, you may elect to organize your R Markdown script such that it is "self-contained" and does not involve any external scripts.  

Everything you need to run the R markdown is imported or created within the Rmd file, including all the code chunks and package loading. This "self-contained" approach is appropriate when you do not need to do much data processing (e.g. it brings in a clean or semi-clean data file) and the rendering of the R Markdown will not take too long.

In this scenario, one logical organization of the R Markdown script might be:  

1) Set global **knitr** options  
2) Load packages  
3) Import data  
4) Process data  
5) Produce outputs (tables, plots, etc.)  
6) Save outputs, if applicable (.csv, .png, etc.)  

#### Source other files {.unnumbered}

One variation of the "self-contained" approach is to have R Markdown code chunks "source" (run) other R scripts. This can make your R Markdown script less cluttered, more simple, and easier to organize. It can also help if you want to display final figures at the beginning of the report. In this approach, the final R Markdown script simply combines pre-processed outputs into a document.  

One way to do this is by providing the R scripts (file path and name with extension) to the **base** R command `source()`.  

```{r, eval=F}
source("your-script.R", local = knitr::knit_global())
# or sys.source("your-script.R", envir = knitr::knit_global())
```

Note that when using `source()` *within* the R Markdown, the external files will still be run *during the course of rendering your Rmd file*. Therefore, each script is run every time you render the report. Thus, having these `source()` commands *within* the R Markdown does not speed up your run time, nor does it greatly assist with de-bugging, as error produced will still be printed when producing the R Markdown.  

An alternative is to utilize the `child = ` **knitr** option. EXPLAIN MORE TO DO

You must be aware of various R *environments*. Objects created within an environment will not necessarily be available to the environment used by the R Markdown.  



### Runfile {.unnumbered}  

This approach involves utilizing the R script that contains the `render()` command(s) to pre-process objects that feed into the R markdown.  

For instance, you can load the packages, load and clean the data, and even create the graphs of interest prior to `render()`. These steps can occur in the R script, or in other scripts that are sourced. As long as these commands occur in the same RStudio session and objects are saved to the environment, the objects can then be called within the Rmd content. Then the R markdown itself will only be used for the final step - to produce the output with all the pre-processed objects. This is much easier to de-bug if something goes wrong.

This approach is helpful for the following reasons:  

* More informative error messages - these messages will be generated from the R script, not the R Markdown. R Markdown errors tend to tell you which chunk had a problem, but will not tell you which line.  
* If applicable, you can run long processing steps in advance of the `render()` command - they will run only once. 


In the example below, we have a separate R script in which we pre-process a `data` object into the R Environment and then render the "create_output.Rmd" using `render()`.  

```{r, eval=F}
data <- import("datafile.csv") %>%       # Load data and save to environment
  select(age, hospital, weight)          # Select limited columns

rmarkdown::render(input = "create_output.Rmd")   # Create Rmd file
```





### Folder strucutre {.unnumbered}  

Workflow also concerns the overall folder structure, such as having an 'output' folder for created documents and figures, and 'data' or 'inputs' folders for cleaned data. We do not go into further detail here, but check out the [Organizing routine reports] page.  







## Producing the document  

You can produce the document in the following ways:  

* Manually by pressing the "Knit" button at the top of the RStudio script editor (fast and easy)  
* Run the `render()` command (executed outside the R Markdown script)  


### Option 1: "Knit" button {.unnumbered}  

When you have the Rmd file open, press the 'Knit' icon/button at the top of the file. 

R Studio will you show the progress within an ‘R Markdown’ tab near your R console. The document will automatically open when complete.  

The document will be saved in the same folder as your R markdown script, and with the same file name (aside from the extension). This is obviously not ideal for version control (it will be over-written each tim you knit, unless moved manually), as you may then need to rename the file yourself (e.g. add a date).  

This is RStudio’s shortcut button for the `render()` function from **rmarkdown**. This approach only compatible with a self-contained R markdown, where all the needed components exist or are sourced within the file.  

```{r out.width = "90%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/4_progress.png"))
```



### Option 2: `render()` command {.unnumbered}

Another way to produce your R Markdown output is to run the `render()` function (from the **rmarkdown** package). You must execute this command *outside* the R Markdown script - so either in a separate R script (often called a "run file"), or as a stand-alone command in the R Console. 

```{r, eval=F}
rmarkdown::render(input = "my_report.Rmd")
```

As with "knit", the default settings will save the Rmd output to the same folder as the Rmd script, with the same file name (aside from the file extension). For instance “my_report.Rmd” when knitted will create “my_report.docx” if you are knitting to a word document. However, by using `render()` you have the option to use different settings. `render()` can accept arguments including:  

* `output_format = ` This is the output format to convert to (e.g. `"html_document"`, `"pdf_document"`, `"word_document"`, or `"all"`). You can also specify this in the YAML inside the R Markdown script.  
* `output_file = ` This is the name of the output file (and file path). This can be created via R functions like `here()` or `str_glue()` as demonstrated below.  
* `output_dir = ` This is an output directory (folder) to save the file. This allows you to chose an alternative other than the directory the Rmd file is saved to.  
* `output_options = ` You can provide a list of options that will override those in the script YAML (e.g. )
* `output_yaml = ` You can provide path to a .yml file that contains YAML specifications  
* `params = ` See the section on parameters below  
* See the complete list [here](https://pkgs.rstudio.com/rmarkdown/reference/render.html)  

As one example, to improve version control, the following command will save the output file within an ‘outputs’ sub-folder, with the current date in the file name. To create the file name, the function `str_glue()` from the **stringr** package is use to 'glue' together static strings (written plainly) with dynamic R code (written in curly brackets). For instance if it is April 10th 2021, the file name from below will be “Report_2021-04-10.docx”. See the page on [Characters and strings] for more details on `str_glue()`.  

```{r, eval=F}
rmarkdown::render(
  input = "create_output.Rmd",
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx")) 
```

As the file renders, the RStudio Console will show you the rendering progress up to 100%, and a final message to indicate that the rendering is complete. 



###  Options 3: **reportfactory**  package {.unnumbered}  

The R package **reportfactory** offers an alternative method of organising and compiling R Markdown reports *catered to scenarios where you run reports routinely (e.g. daily, weekly...).* It eases the compilation of multiple R Markdown files and the organization of their outputs. In essence, it provides a "factory" from which you can run the R Markdown reports, get automatically date- and time-stamped folders for the outputs, and have "light" version control.  

Read more about this work flow in the page on [Organizing routine reports].  



<!-- ======================================================= -->
## Parameterised reports {  }

You can use parameterisation to make a report dynamic, such that it can be run with specific setting (e.g. a specific date or place or with certain knitting options). Below, we focus on the basics, but there is more [detail online](https://bookdown.org/yihui/rmarkdown/parameterized-reports.html) about parameterized reports.

Using the Ebola linelist as an example, let’s say we want to run a standard surveillance report for each hospital each day. We show how one can do this using parameters.

*Important: dynamic reports are also possible without the formal parameter structure (without `params:`), using simple R objects in an adjacent R script. This is explained at the end of this section.*



### Setting parameters {.unnumbered}

You have several options for specifying parameter values for your R Markdown output.  

#### Option 1: Set parameters within YAML {.unnumbered}

Edit the YAML to include a `params: ` option, with indented statements for each parameter you want to define. In this example we create parameters `date` and `hospital`, for which we specify values. These values are subject to change each time the report is run. If you use the "Knit" button to produce the output, the parameters will have these default values. Likewise, if you use `render()` the parameters will have these default values unless otherwise specified in the `render()` command.  


```yaml
---
title: Surveillance report
output: html_document
params:
 date: 2021-04-10
 hospital: Central Hospital
---
```

In the background, these parameter values are contained within a read-only list called `params`. Thus, you can insert the parameter values in R code as you would another R object/value in your environment. Simply type `params$` followed by the parameter name. For example `params$hospital` to represent the hospital name ("Central Hospital" by default).  

Note that parameters can also hold values `true` or `false`, and so these can be included in your **knitr** options for a R chunk. For example, you can set `{r, eval=params$run}` instead of `{r, eval=FALSE}`, and now whether the chunk runs or not depends on the value of a parameter `run:`.  

Note that for parameters that are dates, they will be input as a string. So for `params$date` to be interpreted in R code it will likely need to be wrapped with `as.Date()` or a similar function to convert to class Date.  




#### Option 2: Set parameters within `render()` {.unnumbered}  

As mentioned above, as alternative to pressing the "Knit" button to produce the output is to execute the `render()` function from a separate script. In this later case, you can specify the parameters to be used in that rendering to the `params = ` argument of `render()`. 

Note than any parameter values provided here will *overwrite* their default values if written within the YAML. We write the values in quotation marks as in this case they should be defined as character/string values.  

The below command renders "surveillance_report.Rmd", specifies a dynamic output file name and folder, and provides a `list()` of two parameters and their values to the argument `params = `.  

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = list(date = "2021-04-10", hospital  = "Central Hospital"))
```


#### Option 3: Set parameters using a Graphical User Interface {.unnumbered}  

For a more interactive feel, you can also use the Graphical User Interface (GUI) to manually select values for parameters. To do this we can click the drop-down menu next to the ‘Knit’ button and choose ‘Knit with parameters’. 

A pop-up will appear allowing you to type in values for the parameters that are established in the document's YAML. 

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUI.png"))
```

You can achieve the same through a `render()` command by specifying `params = "ask"`, as demonstrated below.  

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = “ask”)
```


However, typing values into this pop-up window is subject to error and spelling mistakes. You may prefer to add restrictions to the values that can be entered through drop-down menus. You can do this by adding in the YAML several specifications for each `params: ` entry. 

* `label: ` is how the title for that particular drop-down menu  
* `value: ` is the default (starting) value  
* `input: ` set to `select` for drop-down menu  
* `choices: ` Give the eligible values in the drop-down menu  

Below, these specifications are written for the `hospital` parameter.  

```yaml
---
title: Surveillance report
output: html_document
params:
 date: 2021-04-10
 hospital: 
  label: “Town:”
  value: Central Hospital
  input: select
  choices: [Central Hospital, Military Hospital, Port Hospital, St. Mark's Maternity Hospital (SMMH)]
---
```

When knitting (either via the 'knit with parameters' button or by `render()`), the pop-up window will have drop-down options to select from.  

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUIB.png"))
```




### Parameterized example {.unnumbered} 

The following code creates parameters for `date` and `hospital`, which are used in the R Markdown as `params$date` and `params$hospital`, respectively.  

In the resulting report output, see how the data are filtered to the specific hospital, and the plot title refers to the correct hospital and date. We use the "linelist_cleaned.rds" file here, but it would be particularly appropriate if the linelist itself also had a datestamp within it to align with parameterised date. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_Rmdexample.png"))
```

Knitting this produces the final output with the default font and layout.

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_RmdexampleB.png"))
```


### Parameterisation without `params` {.unnumbered}

If you are rendering a R Markdown file with `render()` from a separate script, you can actually create the impact of parameterization without using the `params:` functionality.  

For instance, in the *R script* that contains the `render()` command, you can simply define `hospital` and `date` as two R objects (values) before the `render()` command. In the R Markdown, you would not need to have a `params:` section in the YAML, and we would refer to the `date` object rather than `params$date` and `hospital` rather than `params$hospital`. 

```{r, eval=F}
# This is a R script that is separate from the R Markdown

# define R objects
hospital <- "Central Hospital"
date <- "2021-04-10"

# Render the R markdown
rmarkdown::render(input = "create_output.Rmd") 
```

Following this approach means means you can not “knit with parameters”, use the GUI, or include knitting options within the parameters. However it allows for simpler code, which may be advantageous.  


<!-- ======================================================= -->

## Looping reports  {  }

We may want to run a report multiple times, varying the input parameters, to produce a report for each jurisdictions/unit. This can be done using tools for *iteration*, which are explained in detail in the page on [Iteration, loops, and lists]. Options include the **purrr** package, or use of a *for loop* as explained below.  

Below, we use a simple *for loop* to generate a surveillance report for all hospitals of interest. This is done with one command (instead of manually changing the hospital parameter one-at-a-time). The command to render the reports must exist in a separate script *outside* the report Rmd. This script will also contain defined objects to "loop through" - today’s date, and a vector of hospital names to loop through.  



```{r, eval=F}
hospitals <- c("Central Hospital",
                "Military Hospital", 
                "Port Hospital",
                "St. Mark's Maternity Hospital (SMMH)") 
```

We then feed these values one-at-a-time into the `render()` command using a loop, which runs the command once for each value in the `hospitals` vector. The letter `i` represents the index position (1 through 4) of the hospital currently being used in that iteration, such that `hospital_list[1]` would be “Central Hospital”. This information is supplied in two places in the `render()` command:  

1) To the file name, such that the file name of the first iteration if produced on 10th April 2021 would be “Report_Central Hospital_2021-04-10.docx”, saved in the ‘output’ subfolder of the working directory.  
2) To `params = ` such that the Rmd uses the hospital name internally whenever the `params$hospital` value is called (e.g. to filter the dataset to the particular hospital only). In this example, four files would be created - one for each hospital.

```{r, eval=F}
for(i in 1:length(hospitals)){
  rmarkdown::render(
    input = "surveillance_report.Rmd",
    output_file = str_glue("output/Report_{hospitals[i]}_{Sys.Date()}.docx"),
    params = list(hospital  = hospitals[i]))
}       
```



<!-- In the scenario where you are f not using this strict form of parameterisation but saving objects to the environment, as discussed at the end of the parameterisation section, the render function would look like this: -->

<!-- ```md -->
<!-- for(i in 1:length(hospital_list)){ -->
<!-- rmarkdown::render("surveillance_report.Rmd", -->
<!--                   output_file = paste0("output/Report_", hospital_list[i], refdate, ".docx") -->
<!-- }        -->
<!-- ``` -->
<!-- The text within the markdown would then need to refer to `hospital_list[i]` and `refdate`.  -->






<!-- ======================================================= -->
## Templates  

By using a template document that contains any desired formatting, you can adjust the aesthetics of how the Rmd output will look. You can create for instance an MS Word or Powerpoint file that contains pages/slides with the desired dimensions, watermarks, backgrounds, and fonts. 

### Word documents {.unnumbered}

To create a template, start a new word document (or use an existing output with  formatting the suits you), and edit fonts by defining the Styles. In Style,Headings 1, 2, and 3 refer to the various markdown header levels (`# Header 1`, `## Header 2` and `### Header 3` respectively). Right click on the style and click 'modify' to change the font formatting as well as the paragraph (e.g. you can introduce page breaks before certain styles which can help with spacing). Other aspects of the word document such as margins, page size, headers etc, can be changed like a usual word document you are working directly within. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/7_template.png"))
```

### Powerpoint documents {.unnumbered}

As above, create a new slideset or use an existing powerpoint file with the desired formatting. For further editing, click on 'View' and 'Slide Master'. From here you can change the 'master' slide appearance by editing the text formatting in the text boxes, as well as the background/page dimensions for the overall page. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/8_ppttemplate.png"))
```

Unfortunately, editing powerpoint files is slightly less flexible:

* A first level header (`# Header 1`) will automatically become the title of a new slide,
* A `## Header 2` text will not come up as a subtitle but text within the slide's main textbox (unless you find a way to maniuplate the Master view). 
* Outputted plots and tables will automatically go into new slides. You will need to combine them, for instance the the **patchwork** function to combine ggplots, so that they show up on the same page. See this [blog post](https://mattherman.info/blog/ppt-patchwork/) about using the **patchwork** package to put multiple images on one slide.  

See the [**officer** package](https://davidgohel.github.io/officer/) for a tool to work more in-depth with powerpoint presentations.  




### Integrating templates into the YAML {.unnumbered}

Once a template is prepared, the detail of this can be added in the YAML of the Rmd underneath the 'output' line and underneath where the document type is specified (which goes to a separate line itself). Note `reference_doc` can be used for powerpoint slide templates.

It is easiest to save the template in the same folder as where the Rmd file is (as in the example below), or in a subfolder within. 

```yaml
---
title: Surveillance report
output: 
 word_document:
  reference_docx: "template.docx"
params:
 date: 2021-04-10
 hospital: Central Hospital
template:
 
---
```

### Formatting HTML files {.unnumbered}

HTML files do not use templates, but can have the styles configured within the YAML. HTMLs are interactive documents, and are particularly flexible. We cover some basic options here. 

* Table of contents: We can add a table of contents with `toc: true` below, and also specify that it remains viewable ("floats") as you scroll, with `toc_float: true`.

* Themes: We can refer to some pre-made themes, which come from a Bootswatch theme library. In the below example we use cerulean. Other options include: journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, and yeti. 

* Highlight: Configuring this changes the look of highlighted text (e.g. code within chunks that are shown). Supported styles include default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark, and textmate.  

Here is an example of how to integrate the above options into the YAML.

```yaml
---
title: "HTML example"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: kate
    
---
```

Below are two examples of HTML outputs which both have floating tables of contents, but different theme and highlight styles selected:


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/9_html.png"))
```


## Dynamic content  

In an HTML output, your report content can be dynamic. Below are some examples:  

### Tables {.unnumbered}  

In an HTML report, you can print data frame / tibbles such that the content is dynamic, with filters and scroll bars. There are several packages that offer this capability.  

To do this with the **DT** package, as is used throughout this handbook, you can insert a code chunk like this:  

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/10_dynamictable.png"))
```

The function `datatable()` will print the provided data frame as a dynamic table for the reader. You can set `rownames = FALSE` to simplify the far left-side of the table. `filter = "top"` provides a filter over each column. In the `option()` argument provide a list of other specifications. Below we include two: `pageLength = 5` set the number of rows that appear as 5 (the remaining rows can be viewed by paging through arrows), and `scrollX=TRUE` enables a scrollbar on the bottom of the table (for columns that extend too far to the right).  

If your dataset is very large, consider only showing the top X rows by wrapping the data frame in `head()`.  


### HTML widgets {.unnumbered}

[HTML widgets for R](http://www.htmlwidgets.org/) are a special class of R packages that enable increased interactivity by utilizing JavaScript libraries. You can embed them in HTML R Markdown outputs.  

Some common examples of these widgets include:  

* Plotly (used in this handbook page and in the [Interative plots] page)
* visNetwork (used in the [Transmission Chains] page of this handbook)  
* Leaflet (used in the [GIS Basics] page of this handbook)  
* dygraphs (useful for interactively showing time series data)  
* DT (`datatable()`) (used to show dynamic tables with filter, sort, etc.)  

The `ggplotly()` function from **plotly** is particularly easy to use. See the [Interactive plots] page. 


## Resources {  }

Further information can be found via:

* https://bookdown.org/yihui/rmarkdown/
* https://rmarkdown.rstudio.com/articles_intro.html

A good explainer of markdown vs knitr vs Rmarkdown is here: https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/rmarkdown.Rmd-->


# Organizing routine reports {  }  

This page covers the **reportfactory** package, which is an *accompaniment to using R Markdown for reports*. 

In scenarios where you run reports routinely (daily, weekly, etc.), it eases the compilation of multiple R Markdown files and the organization of their outputs. In essence, it provides a "factory" from which you can run the R Markdown reports, get automatically date- and time-stamped folders for the outputs, and have "light" version control.  

**reportfactory** is one of the packages developed by RECON (R Epidemics Consortium). Here is their [website](https://www.repidemicsconsortium.org/) and [Github](https://github.com/reconverse).  


## Preparation

### Load packages {.unnumbered}  

From within RStudio, install the latest version of the **reportfactory** package from Github.  

You can do this via the **pacman** package with `p_load_current_gh()` which will force intall of the latest version from Github. Provide the character string "reconverse/reportfactory", which specifies the Github organization (reconverse) and repository (reportfactory). You can also use `install_github()` from the **remotes** package, as an alternative.

```{r, eval=FALSE}
# Install and load the latest version of the package from Github
pacman::p_load_current_gh("reconverse/reportfactory")
#remotes::install_github("reconverse/reportfactory") # alternative
```


## New factory  

To create a new factory, run the function `new_factory()`. This will create a new self-contained R project folder. By default:  

* The factory will be added to your working directory
* The name of the factory R project will be called "new_factory.Rproj"  
* Your RStudio session will "move in" to this R project  

```{r, eval=F}
# This will create the factory in the working directory
new_factory()
```

Looking inside the factory, you can see that sub-folders and some files were created automatically.  


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new2.png"))
```

* The *report_sources* folder will hold your R Markdown scripts, which generate your reports  
* The *outputs* folder will hold the report outputs (e.g. HTML, Word, PDF, etc.)  
* The *scripts* folder can be used to store other R scripts (e.g. that are sourced by your Rmd scripts)  
* The *data* folder can be used to hold your data ("raw" and "clean" subfolders are included)  
* A *.here* file, so you can use the **here** package to call files in sub-folders by their relation to this root folder (see [R projects] page for details)  
* A *gitignore* file was created in case you link this R project to a Github repository (see [Version control and collaboration with Github])  
* An empty README file, for if you use a Github repository  


<span style="color: orange;">**_CAUTION:_** depending on your computer's setting, files such as ".here" may exist but be invisible.</span>  

Of the default settings, below are several that you might want to adjust within the `new_factory()` command:  

* `factory = ` - Provide a name for the factory folder (default is "new_factory")  
* `path = ` - Designate a file path for the new factory (default is the working directory)  
* `report_sources = ` Provide an alternate name for the subfolder which holds the R Markdown scripts (default is "report_sources")  
* `outputs = ` Provide an alternate name for the folder which holds the report outputs (default is "outputs")  

See `?new_factory` for a complete list of the arguments.  


When you create the new factory, your R session is transferred to the new R project, so you should again load the **reportfactory** package.  

```{r, eval=FALSE}
pacman::p_load(reportfactory)
```

Now you can run a the `factory_overview()` command to see the internal structure (all folders and files) in the factory.  

```{r, eval=F}
factory_overview()            # print overview of the factory to console
```

The following "tree" of the factory's folders and files is printed to the R console. Note that in the "data" folder there are sub-folders for "raw" and "clean" data, and example CSV data. There is also "example_report.Rmd" in the "report_sources" folder.    

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview.png"))
```


## Create a report  

From within the factory R project, create a R Markdown report just as you would normally, and save it into the "report_sources" folder. See the [R Markdown][Reports with R Markdown] page for instructions. For purposes of example, we have added the following to the factory:  

* A new R markdown script entitled "daily_sitrep.Rmd", saved within the "report_sources" folder  
* Data for the report ("linelist_cleaned.rds"), saved to the "clean" sub-folder within the "data" folder  

We can see using `factory_overview()` our R Markdown in the "report_sources" folder and the data file in the "clean" data folder (highlighted):

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview2.png"))
```

Below is a screenshot of the beginning of the R Markdown "daily_sitrep.Rmd". You can see that the output format is set to be HTML, via the YAML header `output: html_document`. 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new_rmd.png"))
```

In this simple script, there are commands to:  

* Load necessary packages  
* Import the linelist data using a filepath from the **here** package (read more in the page on [Import and export])  

```{r, eval=F}
linelist <- import(here("data", "clean", "linelist_cleaned.rds"))
```

* Print a summary table of cases, and export it with `export()` as a .csv file  
* Print an epicurve, and export it with `ggsave()` as a .png file  


You can review just the list of R Markdown reports in the "report_sources" folder with this command:  

```{r, eval=F}
list_reports()
```



## Compile  

In a report factory, to "compile" a R Markdown report means that the .Rmd script will be run and the output will be produced (as specified in the script YAML e.g. as HTML, Word, PDF, etc).  

*The factory will automatically create a date- and time-stamped folder for the outputs in the "outputs" folder.*  

The report itself and any exported files produced by the script (e.g. csv, png, xlsx) will be saved into this folder. In addition, the Rmd script itself will be saved in this folder, so you have a record of that version of the script.  

This contrasts with the normal behavior of a "knitted" R Markdown, which saves outputs to the location of the Rmd script. This default behavior can result in crowded, messy folders. The factory aims to improve organization when one needs to run reports frequently.  

### Compile by name {.unnumbered}  

You can compile a specific report by running `compile_reports()` and providing the Rmd script name (without .Rmd extension) to `reports = `. For simplicity, you can skip the `reports = ` and just write the R Markdown name in quotes, as below.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile1.png"))
```


This command would compile only the "daily_sitrep.Rmd" report, saving the HTML report, and the .csv table and .png epicurve exports into a date- and time-stamped sub-folder specific to the report, within the "outputs" folder.  

Note that if you choose to provide the .Rmd extension, you must correctly type the extension as it is saved in the file name (.rmd vs. .Rmd).  

Also note that when you compile, you may see several files temporarily appear in the "report_sources" folder - but they will soon disappear as they are transferred to the correct "outputs" folder. 

### Compile by number {.unnumbered}

You can also specify the Rmd script to compile by providing a number or vector of numbers to `reports = `. The numbers must align with the order the reports appear when you run `list_reports()`.  

```{r, eval=F}
# Compile the second and fourth Rmds in the "report_sources" folder
compile_reports(reports = c(2, 4))
```



### Compile all {.unnumbered}

You can compile *all* the R Markdown reports in the "report_sources" folder by setting the `reports = ` argument to TRUE.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_all.png"))
```


### Compile from sub-folder {.unnumbered}  

You can add sub-folders to the "report_sources" folder. To run an R Markdown report from a subfolder, simply provide the name of the folder to `subfolder = `. Below is an example of code to compile a Rmd report that lives in a sub_folder of "report_sources".  

```{r, eval=F}
compile_reports(
     reports = "summary_for_partners.Rmd",
     subfolder = "for_partners")
```

You can compile all Rmd reports within a subfolder by providing the subfolder name to `reports = `, with a slash on the end, as below.  

```{r, eval=F}
compile_reports(reports = "for_partners/")
```


### Parameterization {.unnumbered}

As noted in the page on [Reports with R Markdown], you can run reports with specified parameters. You can pass these parameters as a list to `compile_reports()` via the `params = ` argument. For example, in this fictional report there are three parameters provided to the R Markdown reports.  

```{r, eval=F}
compile_reports(
  reports = "daily_sitrep.Rmd",
  params = list(most_recent_data = TRUE,
                region = "NORTHERN",
                rates_denominator = 10000),
  subfolder = "regional"
)
```


### Using a "run-file" {.unnumbered}  

If you have multiple reports to run, consider creating a R script that contains all the `compile_reports()` commands. A user can simply run all the commands in this R script and all the reports will compile. You can save this "run-file" to the "scripts" folder.  



## Outputs  

After we have compiled the reports a few times, the "outputs" folder might look like this (highlights added for clarity):  


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview_all.png"))
```


* Within "outputs", sub-folders have been created for each Rmd report  
* Within those, further sub-folders have been created for each unique compiling  
  * These are date- and time-stamped ("2021-04-23_T11-07-36" means 23rd April 2021 at 11:07:36)  
  * You can edit the date/time-stamp format. See `?compile_reports`
* Within each date/time compiled folder, the report output is stored (e.g. HTML, PDF, Word) along with the Rmd script (version control!) and any other exported files (e.g. table.csv, epidemic_curve.png)  

Here is a view inside one of the date/time-stamped folders, for the "daily_sitrep" report. The file path is highlighted in yellow for emphasis.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_folder.png"))
```


Finally, below is a screenshot of the HTML report output.  


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_html.png"))
```

You can use `list_outputs()` to review a list of the outputs.  




## Miscellaneous  

### Knit {.unnumbered} 

You can still "knit" one of your R Markdown reports by pressing the "Knit" button, if you want. If you do this, as by default, the outputs will appear in the folder where the Rmd is saved - the "report_sources" folder. In prior versions of **reportfactory**, having any non-Rmd files in "report_sources" would prevent compiling, but this is no longer the case. You can run `compile_reports()` and no error will occur.  

### Scripts {.unnumbered}  

We encourage you to utilize the "scripts" folder to store "runfiles" or .R scripts that are sourced by your .Rmd scripts. See the page on [R Markdown][Reports with R Markdown] for tips on how to structure your code across several files.  


### Extras {.unnumbered} 

* With **reportfactory**, you can use the function `list_deps()` to list all packages required across all the reports in the entire factory.  

* There is an accompanying package in development called **rfextras** that offers more helper functions to assist you in building reports, such as:  
  * `load_scripts()` - sources/loads all .R scripts in a given folder (the "scripts" folder by default)  
  * `find_latest()` - finds the latest version of a file (e.g. the latest dataset)




<!-- ======================================================= -->
## Resources {  }

See the **reportfactory** package's [Github page](https://github.com/reconverse/reportfactory)

See the **rfextras** package's [Github page](https://github.com/reconhub/rfextras)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/reportfactory.Rmd-->


# Dashboards with R Markdown { }

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_output.png"))
```

This page will cover the basic use of the **flexdashboard** package. This package allows you to easily format R Markdown output as a dashboard with panels and pages. The dashboard content can be text, static figures/tables or interactive graphics.  

Advantages of **flexdashboard**:  

* It requires minimal non-standard R coding - with very little practice you can quickly create a dashboard  
* The dashboard can usually be emailed to colleagues as a self-contained HTML file - no server required  
* You can combine **flexdashboard** with **shiny**, **ggplotly**, and other *"html widgets"* to add interactivity  

Disadvantages of **flexdashboard**:  

* Less customization as compared to using **shiny** alone to create a dashboard  


Very comprehensive tutorials on using **flexdashboard** that informed this page can be found in the Resources section. Below we describe the core features and give an example of building a dashboard to explore an outbreak, using the case `linelist` data.  


## Preparation

### Load packages {.unnumbered}  

In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r}
pacman::p_load(
  rio,             # data import/export     
  here,            # locate files
  tidyverse,       # data management and visualization
  flexdashboard,   # dashboard versions of R Markdown reports
  shiny,           # interactive figures
  plotly           # interactive figures
)
```

### Import data {.unnumbered}  

We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, <a href='https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds' class='download-button'>click to download the "clean" linelist</a> (as .rds file). Import data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details). 

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

The first 50 rows of the linelist are displayed below.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


## Create new R Markdown  

After you have installed the package, create a new R Markdown file by clicking through to *File > New file > R Markdown*. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new1.png"))
```


In the window that opens, select "From Template" and select the "Flex Dashboard" template. You will then be prompted to name the document. In this page's example, we will name our R Markdown as "outbreak_dashboard.Rmd".  
  

```{r out.width = "100%", out.height="75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new2.png"))
```




## The script  

The script is an R Markdown script, and so has the same components and organization as described in the page on [Reports with R Markdown]. We briefly re-visit these and highlight differences from other R Markdown output formats.  

### YAML {.unnumbered}  

At the top of the script is the "YAML" header. This must begin with three dashes `---` and must close with three dashes `---`. YAML parameters comes in `key:value` pairs. **The indentation and placement of colons in YAML is important** - the `key:value` pairs are separated by colons (not equals signs!). 

The YAML should begin with metadata for the document. The order of these primary YAML parameters (not indented) does not matter. For example:  

```{r, eval=F}
title: "My document"
author: "Me"
date: "`r Sys.Date()`"
```

You can use R code in YAML values by putting it like in-line code (preceeded by `r` within backticks) but also within quotes (see above for Date).  

A required YAML parameter is `output: `, which specifies the type of file to be produced (e.g. `html_document`, `pdf_document`, `word_document`, or `powerpoint_presentation`). For **flexdashboard** this parameter value is a bit confusing - it must be set as `output:flexdashboard::flex_dashboard`. Note the single and double colons, and the underscore. This YAML output parameter is often followed by *an additional colon* and indented sub-parameters (see `orientation: ` and `vertical_layout: ` parameters below).  

```{r, eval=F}
title: "My dashboard"
author: "Me"
date: "`r Sys.Date()`"
output:
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
```

As shown above, indentations (2 spaces) are used for sub-parameters. In this case, do not forget to put an additional colon after the primary, like `key:value:`.  

If appropriate, logical values should be given in YAML in lowercase (`true`, `false`, `null`). If a colon is part of your value (e.g. in the title) put the value in quotes. See the examples in sections below.  



### Code chunks {.unnumbered}  

An R Markdown script can contain multiple code "chunks" - these are areas of the script where you can write multiple-line R code and they function just like mini R scripts.  

Code chunks are created with three back-ticks and curly brackets with a lowercase "r" within. The chunk is closed with three backticks. You can create a new chunk by typing it out yourself, by using the keyboard shortcut "Ctrl + Alt + i" (or Cmd + Shift + r in Mac), or by clicking the green 'insert a new code chunk' icon at the top of your script editor. Many examples are given below.  


### Narrative text {.unnumbered}  

Outside of an R code "chunk", you can write narrative text. As described in the page on [Reports with R Markdown], you can italicize text by surrounding it with one asterisk (*), or bold by surrounding it with two asterisks (**). Recall that bullets and numbering schemes are sensitive to newlines, indentation, and finishing a line with two spaces.  

You can also insert in-line R code into text as described in the [Reports with R Markdown] page, by surrounding the code with backticks and starting the command with "r": `` ` 1+1` ``(see example with date above).  



### Headings {.unnumbered}  

Different heading levels are established with different numbers of hash symbols, as described in the [Reports with R Markdown] page.  

In **flexdashboard**, a primary heading (#) creates a "page" of the dashboard. Second-level headings (##) create a column or a row depending on your `orientation:` parameter (see details below). Third-level headings (###) create panels for plots, charts, tables, text, etc.   

```md
# First-level heading (page)

## Second level heading (row or column)  

### Third-level heading (pane for plot, chart, etc.)
```





## Section attributes  

As in a normal R markdown, you can specify attributes to apply to parts of your dashboard by including `key=value` options after a heading, within curly brackets `{ }`. For example, in a typical HTML R Markdown report you might organize sub-headings into tabs with `## My heading {.tabset}`.  

Note that these attributes are written after a *heading* in a text portion of the script. These are different than the **knitr** options inserted within at the top of R code chunks, such as `out.height = `.  

Section attributes specific to **flexdashboard** include:  

* `{data-orientation=}` Set to either `rows` or `columns`. If your dashboard has multiple pages, add this attribute to each page to indicate orientation (further explained in [layout section](#layout)).  
* `{data-width=}` and `{data-height=}` set relative size of charts, columns, rows laid out in the same dimension (horizontal or vertical). Absolute sizes are adjusted to best fill the space on any display device thanks to the [flexbox](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Using_CSS_flexible_boxes) engine.  
     * Height of charts also depends on whether you set the YAML parameter `vertical_layout: fill` or `vertical_layout: scroll`. If set to scroll, figure height will reflect the traditional `fig.height = ` option in the R code chunk.  
     * See complete size documentation at the [flexdashboard website](https://rmarkdown.rstudio.com/flexdashboard/using.html#sizing)  
* `{.hidden}` Use this to exclude a specific page from the navigation bar  
* `{data-navbar=}` Use this in a page-level heading to nest it within a navigation bar drop-down menu. Provide the name (in quotes) of the drop-down menu. See example below.  


## Layout {#layout}  

Adjust the layout of your dashboard in the following ways:  

* Add pages, columns/rows, and charts with R Markdown headings (e.g. #, ##, or ###)  
* Adjust the YAML parameter `orientation:` to either `rows` or `columns`  
* Specify whether the layout fills the browser or allows scrolling  
* Add tabs to a particular section heading  


### Pages {.unnumbered}  

First-level headings (#) in the R Markdown will represent "pages" of the dashboard. By default, pages will appear in a navigation bar along the top of the dashboard.  

```{r, out.height = c('100%'), out.width = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_script.png"))
```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_view.png"))
```



You can group pages into a "menu" within the top navigation bar by adding the attribute `{data-navmenu=}` to the page heading. Be careful - do not include spaces around the equals sign otherwise it will not work!  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_script.png"))
```


Here is what the script produces:  


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_view.png"))
```

You can also convert a page or a column into a "sidebar" on the left side of the dashboard by adding the `{.sidebar}` attribute. It can hold text (viewable from any page), or if you have integrated **shiny** interactivity it can be useful to hold user-input controls such as sliders or drop-down menus.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_view.png"))
```




### Orientation {.unnumbered}  

Set the `orientation:` yaml parameter to indicate how your second-level (##) R Markdown headings should be interpreted - as either `orientation: columns` or `orientation: rows`. 

Second-level headings (##) will be interpreted as new columns or rows based on this `orientation` setting.  

If you set `orientation: columns`, second-level headers will create new columns in the dashboard. The below dashboard has one page, containing two columns, with a total of three panels. You can adjust the relative width of the columns with `{data-width=}` as shown below.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_view.png"))
```

If you set `orientation: rows`, second-level headers will create new rows instead of columns. Below is the same script as above, but `orientation: rows` so that second-level headings produce rows instead of columns. You can adjust the relative *height* of the rows with `{data-height=}` as shown below.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_view.png"))
```

If your dashboard has multiple pages, you can designate the orientation for each specific page by adding the `{data-orientation=}` attribute the header of each page (specify either `rows` or `columns` without quotes).  

### Tabs {.unnumbered} 

You can divide content into tabs with the `{.tabset}` attribute, as in other HTML R Markdown outputs.  

Simply add this attribute after the desired heading. Sub-headings under that heading will be displayed as tabs. For example, in the example script below column 2 on the right (##) is modified so that the epidemic curve and table panes (###) are displayed in tabs.  

You can do the same with rows if your orientation is rows.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_view.png"))
```


## Adding content  

Let's begin to build a dashboard. Our simple dashboard will have 1 page, 2 columns, and 4 panels. We will build the panels piece-by-piece for demonstration.  

You can easily include standard R outputs such as text, ggplots, and tables (see [Tables for presentation] page). Simply code them within an R code chunk as you would for any other R Markdown script.  

Note: you can download the finished Rmd script and HTML dashboard output - see the [Download handbook and data] page.  


### Text {.unnumbered}  

You can type in Markdown text and include *in-line* code as for any other R Markdown output. See the [Reports with R Markdown] page for details. 

In this dashboard we include a summary text panel that includes dynamic text showing the latest hospitalisation date and number of cases reported in the outbreak. 

### Tables {.unnumbered}  

You can include R code chunks that print outputs such as tables. But the output will look best and respond to the window size if you use the `kable()` function from **knitr** to display your tables. The **flextable** functions may produce tables that are shortened / cut-off.  

For example, below we feed the `linelist()` through a `count()` command to produce a summary table of cases by hospital. Ultimately, the table is piped to `knitr::kable()` and the result has a scroll bar on the right. You can read more about customizing your table with `kable()` and **kableExtra** [here](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_view.png"))
```


If you want to show a dynamic table that allows the user to filter, sort, and/or click through "pages" of the data frame, use the package **DT** and it's function `datatable()`, as in the code below.  

The example code below, the data frame `linelist` is printed. You can set `rownames = FALSE` to conserve horizontal space, and `filter = "top"` to have filters on top of every column. A list of other specifications can be provided to `options = `. Below, we set `pageLength = ` so that 5 rows appear and `scrollX = ` so the user can use a scroll bar on the bottom to scroll horizontally. The argument `class = 'white-space: nowrap'` ensures that each row is only one line (not multiple lines). You can read about other possible arguments and values [here](https://rstudio.github.io/DT/?_ga=2.2810736.1321860763.1619286819-369061888.1601594705) or by entering `?datatable`

```{r, eval=F}
DT::datatable(linelist, 
              rownames = FALSE, 
              options = list(pageLength = 5, scrollX = TRUE), 
              class = 'white-space: nowrap' )
```

### Plots {.unnumbered}  

You can print plots to a dashboard pane as you would in an R script. In our example, we use the **incidence2** package to create an "epicurve" by age group with two simple commands (see [Epidemic curves] page). However, you could use `ggplot()` and print a plot in the same manner.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_view.png"))
```


### Interactive plots {.unnumbered}  

You can also pass a standard ggplot or other plot object to `ggplotly()` from the **plotly** package (see the [Interactive plots] page). This will make your plot interactive, allow the reader to "zoom in", and show-on-hover the value of every data point (in this scenario the number of cases per week and age group in the curve).  

```{r, eval=F}
age_outbreak <- incidence(linelist, date_onset, "week", groups = age_cat)
plot(age_outbreak, fill = age_cat, col_pal = muted, title = "") %>% 
  plotly::ggplotly()
```

Here is what this looks like in the dashboard (gif). This interactive functionality will still work even if you email the dashboard as a static file (not online on a server).  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_ggplotly.gif"))
```

### HTML widgets {.unnumbered}

[HTML widgets for R](http://www.htmlwidgets.org/) are a special class of R packages that increases interactivity by utilizing JavaScript libraries. You can embed them in R Markdown outputs (such as a flexdashboard) and in Shiny dashboards.  

Some common examples of these widgets include:  

* Plotly (used in this handbook page and in the [Interative plots] page)
* visNetwork (used in the [Transmission Chains] page of this handbook)  
* Leaflet (used in the [GIS Basics] page of this handbook)  
* dygraphs (useful for interactively showing time series data)  
* DT (`datatable()`) (used to show dynamic tables with filter, sort, etc.)  

Below we demonstrate adding an epidemic transmission chain which uses visNetwork to the dashboard. The script shows only the new code added to the "Column 2" section of the R Markdown script. You can find the code in the [Transmission chains] page of this handbook.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain_script.png"))
```

Here is what the script produces:  

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain.gif"))
```



## Code organization

You may elect to have all code within the R Markdown **flexdashboard** script. Alternatively, to have a more clean and concise dashboard script you may choose to call upon code/figures that are hosted or created in external R scripts. This is described in greater detail in the [Reports with R Markdown] page. 


## Shiny  

Integrating the R package **shiny** can make your dashboards even more reactive to user input. For example, you could have the user select a jurisdiction, or a date range, and have panels react to their choice (e.g. filter the data displayed). To embed **shiny** reactivity into **flexdashboard**, you need only make a few changes to your **flexdashboard** R Markdown script.  

You can use **shiny** to produce apps/dashboards *without* flexdashboard too. The handbook page on [Dashboards with Shiny] gives an overview of this approach, including primers on **shiny** syntax, app file structure, and options for sharing/publishing (including free server options). These syntax and general tips translate into the **flexdashboard** context as well.  

Embedding **shiny** in **flexdashboard** is however, a fundamental change to your flexdashboard. It will no longer produce an HTML output that you can send by email and anyone could open and view. Instead, it will be an "app". The "Knit" button at the top of the script will be replaced by a "Run document" icon, which will open an instance of the interactive the dashboard locally on your computer.  

Sharing your dashboard will now require that you either:  

* Send the Rmd script to the viewer, they open it in R on their computer, and run the app, or  
* The app/dashboard is hosted on a server accessible to the viewer  

Thus, there are benefits to integrating **shiny**, but also complications. If easy sharing by email is a priority and you don't need **shiny** reactive capabilities, consider the reduced interactivity offered by `ggplotly()` as demonstrated above.    

Below we give a very simple example using the same "outbreak_dashboard.Rmd" as above. Extensive documentation on integrating Shiny into **flexdashboard** is available online [here](https://rmarkdown.rstudio.com/flexdashboard/shiny.html).  



### Settings {.unnumbered}  

Enable **shiny** in a **flexdashboard** by adding the YAML parameter `runtime: shiny` at the same indentation level as `output: `, as below:  

```md
---
title: "Outbreak dashboard (Shiny demo)"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny
---

```

It is also convenient to enable a "side bar" to hold the shiny input widgets that will collect information from the user. As explained above, create a column and indicate the `{.sidebar}` option to create a side bar on the left side. You can add text and R chunks containing the **shiny** `input` commands within this column.  

If your app/dashboard is hosted on a server and may have multiple simultaneous users, name the first R code chunk as `global`. Include the commands to import/load your data in this chunk. This special named chunk is treated differently, and the data imported within it are only imported once (not continuously) and are available for all users. This improves the start-up speed of the app.  

### Worked example {.unnumbered}  

Here we adapt the flexdashboard script "outbreak_dashboard.Rmd" to include **shiny**. We will add the capability for the user to select a hospital from a drop-down menu, and have the epidemic curve reflect only cases from that hospital, with a dynamic plot title. We do the following:  

* Add `runtime: shiny` to the YAML  
* Re-name the setup chunk as `global`  
* Create a sidebar containing:  
  * Code to create a vector of unique hospital names  
  * A `selectInput()` command (**shiny** drop-down menu) with the choice of hospital names. The selection is saved as `hospital_choice`, which can be referenced in later code as `input$hospital_choice`  
* The epidemic curve code (column 2) is wrapped within `renderPlot({ })`, including:  
  * A filter on the dataset restricting the column `hospital` to the current value of `input$hospital_choice`  
  * A dynamic plot title that incorporates `input$hospital_choice`  
  
Note that any code referencing an `input$` value must be within a `render({})` function (to be reactive).  

Here is the top of the script, including YAML, global chunk, and sidebar:  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script1.png"))
```
  
Here is the Column 2, with the reactive epicurve plot:  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script2.png"))
```

And here is the dashboard:  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_view.gif"))
```




### Other examples {.unnumbered}  

To read a health-related example of a Shiny-**flexdashboard** using the **shiny** interactivity and the **leaflet** mapping widget, see this chapter of the online book [Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-dashboardswithshiny.html).  




## Sharing  

Dashboards that do not contain Shiny elements will output an HTML file (.html), which can be emailed (if size permits). This is useful, as you can send the "dashboard" report and not have to set up a server to host it as a website.  

If you have embedded **shiny**, you will not be able to send an output by email, but you can send the script itself to an R user, or host the dashboard on a server as explained above.  


## Resources  

Excellent tutorials that informed this page can be found below. If you review these, most likely within an hour you can have your own dashboard.  

https://bookdown.org/yihui/rmarkdown/dashboards.html

https://rmarkdown.rstudio.com/flexdashboard/

https://rmarkdown.rstudio.com/flexdashboard/using.html

https://rmarkdown.rstudio.com/flexdashboard/examples.html
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/flexdashboard.Rmd-->


# Dashboards with Shiny { }  

Dashboards are often a great way to share results from analyses with others. Producing a dashboard with **shiny** requires a relatively advanced knowledge of the R language, but offers incredible customization and possibilities.  

<!-- One of the largest drawbacks of `R` is its usability for people who are new to or have no experience with programming languages. While these skills are very valuable, most people will find that this represents a barrier to sharing analyses, especially in multidisciplinary environments. It requires some work to maintain an `R` installation, and not everyone will be comfortable running shared code, even if it's well documented and easy to read. This is *especially* true when users have to change parameters of code!  -->

<!-- R based dashboards are also advantageous in that they centralise how code is run - when the same code is run on different machines, often people will have to deal with differing file paths, different R versions, and different package installations. For this reason, dashboards are a great way to share code with others in a user friendly way! -->

It is recommended that someone learning dashboards with **shiny** has good knowledge of data transformation and visualisation, and is comfortable debugging code, and writing functions. Working with dashboards is not intuitive when you're starting, and is difficult to understand at times, but is a great skill to learn and gets much easier with practice!

This page will give a short overview of how to make dashboards with **shiny** and its extensions. 
For an alternative method of making dashboards that is faster, easier, but perhaps less customizeable, see the page on **flextable** ([Dashboards with R Markdown]).  



## Preparation  


### Load packages {.unnumbered}  

In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

We begin by installing the **shiny** R package:  

```{r, eval = FALSE}
pacman::p_load("shiny")
```


### Import data {.unnumbered}  

If you would like to follow-along with this page, see this section of the [Download handbook and data](#data_shiny). There are links to download the R scripts and data files that produce the final Shiny app.  

If you try to re-construct the app using these files, please be aware of the R project folder structure that is created over the course of the demonstration (e.g. folders for "data" and for "funcs").  



<!-- ======================================================= -->
## The structure of a shiny app {  }

### Basic file structures {.unnumbered}  

To understand `shiny`, we first need to understand how the file structure of an app works! We should make a brand new directory before we start. This can actually be made easier by choosing _New project_ in _Rstudio_, and choosing _Shiny Web Application_. This will create the basic structure of a shiny app for you.

When opening this project, you'll notice there is a `.R` file already present called _app.R_. It is *essential* that we have one of two basic file structures:

1. One file called _app.R_, *or*  
2. Two files, one called _ui.R_ and the other _server.R_  

In this page, we will use the first approach of having one file called *app.R*. Here is an example script:  

```{r, eval = FALSE}
# an example of app.R

library(shiny)

ui <- fluidPage(

    # Application title
    titlePanel("My app"),

    # Sidebar with a slider input widget
    sidebarLayout(
        sidebarPanel(
            sliderInput("input_1")
        ),

        # Show a plot 
        mainPanel(
           plotOutput("my_plot")
        )
    )
)

# Define server logic required to draw a histogram
server <- function(input, output) {
     
     plot_1 <- reactive({
          plot_func(param = input_1)
     })
     
    output$my_plot <- renderPlot({
       plot_1()
    })
}


# Run the application 
shinyApp(ui = ui, server = server)


```


If you open this file, you'll notice that two objects are defined - one called `ui` and another called `server`. These objects *must* be defined in *every* shiny app and are central to the structure of the app itself! In fact, the only difference between the two file structures described above is that in structure 1, both `ui` and `server` are defined in one file, whereas in structure 2 they are defined in separate files. Note: we can also (and we should if we have a larger app) have other .R files in our structure that we can `source()` into our app.



### The server and the ui {.unnumbered}

We next need to understand what the `server` and `ui` objects actually _do_. *Put simply, these are two objects that are interacting with each other whenever the user interacts with the shiny app.*

The UI element of a shiny app is, on a basic level, R code that creates an HTML interface. This means everything that is *displayed* in the UI of an app. This generally includes:

* "Widgets" - dropdown menus, check boxes, sliders, etc that can be interacted with by the user
* Plots, tables, etc - outputs that are generated with R code
* Navigation aspects of an app - tabs, panes, etc. 
* Generic text, hyperlinks, etc
* HTML and CSS elements (addressed later)

The most important thing to understand about the UI is that it *receives inputs* from the user and *displays outputs* from the server. There is no *active* code running in the ui *at any time* - all changes seen in the UI are passed through the server (more or less). So we have to make our plots, downloads, etc in the server

The server of the shiny app is where all code is being run once the app starts up. The way this works is a little confusing. The server function will effectively _react_ to the user interfacing with the UI, and run chunks of code in response. If things change in the server, these will be passed back up to the ui, where the changes can be seen. Importantly, the code in the server will be executed *non-consecutively* (or it's best to think of it this way). Basically, whenever a ui input affects a chunk of code in the server, it will run automatically, and that output will be produced and displayed.

This all probably sounds very abstract for now, so we'll have to dive into some examples to get a clear idea of how this actually works. 


### Before you start to build an app {.unnumbered}

Before you begin to build an app, its immensely helpful to know *what* you want to build. Since your UI will be written in code, you can't really visualise what you're building unless you are aiming for something specific. For this reason, it is immensely helpful to look at lots of examples of shiny apps to get an idea of what you can make - even better if you can look at the source code behind these apps! Some great resources for this are:

* The [Rstudio app gallery](https://shiny.rstudio.com/gallery/)  

Once you get an idea for what is possible, it's also helpful to map out what you want yours to look like - you can do this on paper or in any drawing software (PowerPoint, MS paint, etc.). It's helpful to start simple for your first app! There's also no shame in using code you find online of a nice app as a template for your work - its much easier than building something from scratch!



## Building a UI 

When building our app, its easier to work on the UI first so we can see what we're making, and not risk the app failing because of any server errors. As mentioned previously, its often good to use a template when working on the UI. There are a number of standard layouts that can be used with shiny that are available from the base shiny package, but it's worth noting that there are also a number of package extensions such as `shinydashboard`. We'll use an example from base shiny to start with. 

A shiny UI is generally defined as a series of nested functions, in the following order

1. A function defining the general layout (the most basic is `fluidPage()`, but more are available)
2. Panels within the layout such as:
     - a sidebar (`sidebarPanel()`)
     - a "main" panel (`mainPanel()`)
     - a tab (`tabPanel()`)
     - a generic "column" (`column()`)
3. Widgets and outputs - these can confer inputs to the server (widgets) or outputs from the server (outputs)
     - Widgets generally are styled as `xxxInput()` e.g. `selectInput()`
     - Outputs are generally styled as `xxxOutput()` e.g. `plotOutput()`

It's worth stating again that these can't be visualised easily in an abstract way, so it's best to look at an example! Lets consider making a basic app that visualises our malaria facility count data by district. This data has a lot of differnet parameters, so it would be great if the end user could apply some filters to see the data by age group/district as they see fit! We can use a very simple shiny layout to start - the sidebar layout. This is a layout where widgets are placed in a sidebar on the left, and the plot is placed on the right.

Lets plan our app - we can start with a selector that lets us choose the district where we want to visualise data, and another to let us visualise the age group we are interested in. We'll aim to use these filters to show an epicurve that reflects these parameters. So for this we need:

1. Two dropdown menus that let us choose the district we want, and the age group we're interested in. 
2. An area where we can show our resulting epicurve.

This might look something like this:

```{r, eval = FALSE}

library(shiny)

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve")
    )
    
  )
)


```


When app.R is run with the above UI code (with no active code in the `server` portion of app.R) the layout appears looking like this - note that there will be no plot if there is no server to render it, but our inputs are working!

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "simple_UI_view.png"))
```

This is a good opportunity to discuss how widgets work - note that each widget is accepting an `inputId`, a `label`, and a series of other options that are specific to the widget type. This `inputId` is extremely important - these are the IDs that are used to pass information from the UI to the server. For this reason, they *must be unique*. You should make an effort to name them something sensible, and specific to what they are interacting with in cases of larger apps.

You should read documentation carefully for full details on what each of these widgets do. Widgets will pass specific types of data to the server depending on the widget type, and this needs to be fully understood. For example, `selectInput()` will pass a character type to the server:

- If we select _Spring_ for the first widget here, it will pass the character object `"Spring"` to the server. 
- If we select two items from the dropdown menu, they will come through as a character vector (e.g. `c("Spring", "Bolo")`).

Other widgets will pass different types of object to the server! For example:

- `numericInput()` will pass a numeric type object to the server
- `checkboxInput()` will pass a logical type object to the server (`TRUE` or `FALSE`)

It's also worth noting the *named vector* we used for the age data here. For many widgets, using a named vector as the choices will display the *names* of the vector as the display choices, but pass the selected *value* from the vector to the server. I.e. here someone can select "15+" from the drop-down menu, and the UI will pass `"malaria_rdt_15"` to the server - which happens to be the name of the column we're interested in!


There are loads of widgets that you can use to do lots of things with your app. Widgets also allow you to upload files into your app, and download outputs. There are also some excellent shiny extensions that give you access to more widgets than base shiny - the **shinyWidgets** package is a great example of this. To look at some examples you can look at the following links:

- [base shiny widget gallery](https://shiny.rstudio.com/gallery/widget-gallery.html)
- [shinyWidgets gallery](https://github.com/dreamRs/shinyWidgets)



## Loading data into our app

The next step in our app development is getting the server up and running. To do this however, we need to get some data into our app, and figure out all the calculations we're going to do. A shiny app is not straightforward to debug, as it's often not clear where errors are coming from, so it's ideal to get all our data processing and visualisation code working before we start making the server itself.

So given we want to make an app that shows epi curves that change based on user input, we should think about what code we would need to run this in a normal R script. We'll need to:

1. Load our packages
2. Load our data
3. Transform our data
4. Develop a _function_ to visualise our data based on user inputs

This list is pretty straightforward, and shouldn't be too hard to do. It's now important to think about which parts of this process need to *be done only once* and which parts need to *run in response to user inputs*. This is because shiny apps generally run some code before running, which is only performed once. It will help our app's performance if as much of our code can be moved to this section. For this example, we only need to load our data/packages and do basic transformations once, so we can put that code *outside the server*. This means the only thing we'll need in the server is the code to visualise our data. Lets develop all of these componenets in a script first. However, since we're visualising our data with a function, we can also put the code _for the function_ outside the server so our function is in the environment when the app runs!

First lets load our data. Since we're working with a new project, and we want to make it clean, we can create a new directory called data, and add our malaria data in there. We can run this code below in a testing script we will eventually delete when we clean up the structure of our app.

```{r, echo = TRUE}
pacman::p_load("tidyverse", "lubridate")

# read data
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

print(malaria_data)


```


It will be easier to work with this data if we use tidy data standards, so we should also transform into a longer data format, where age group is a column, and cases is another column. We can do this easily using what we've learned in the [Pivoting data] page.  


```{r, echo = TRUE}

malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")

print(malaria_data)

```

And with that we've finished preparing our data! This crosses items 1, 2, and 3 off our list of things to develop for our "testing R script". The last, and most difficult task will be building a function to produce an epicurve based on user defined parameters. As mentioned previously, it's *highly recommended* that anyone learning shiny first look at the section on functional programming ([Writing functions]) to understand how this works!

When defining our function, it might be hard to think about what parameters we want to include. For functional programming with shiny, every relevent parameter will generally have a widget associated with it, so thinking about this is usually quite easy! For example in our current app, we want to be able to filter by district, and have a widget for this, so we can add a district parameter to reflect this. We *don't* have any app functionality to filter by facility (for now), so we don't need to add this as a parameter. Lets start by making a function with three parameters:

1. The core dataset
2. The district of choice
3. The age group of choice

```{r}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}

```


We won't go into great detail about this function, as it's relatively simple in how it works. One thing to note however, is we handle errors by returning `NULL` when it would otherwise give an error. This is because when a shiny server produces a `NULL` object instead of a plot object, nothing will be shown in the ui! This is important, as otherwise errors will often cause your app to stop working.  

Another thing to note is the use of the `%in%` operator when evaluating the `district` input. As mentioned above, this could arrive as a character vector with multiple values, so using `%in%` is more flexible than say, `==`.  

Let's test our function!

```{r, echo = TRUE, warning = FALSE}

plot_epicurve(malaria_data, district = "Bolo", agegroup = "malaria_rdt_0-4")

```

With our function working, we now have to understand how this all is going to fit into our shiny app. We mentioned the concept of _startup code_ before, but lets look at how we can actually incorporate this into the structure of our app. There are two ways we can do this!

1. Put this code in your _app.R_ file at the start of the script (above the UI), or  
2. Create a new file in your app's directory called _global.R_, and put the startup code in this file.

It's worth noting at this point that it's generally easier, especially with bigger apps, to use the second file structure, as it lets you separate your file structure in a simple way. Lets fully develop a this global.R script now. Here is what it could look like:


```{r, eval = F}
# global.R script

pacman::p_load("tidyverse", "lubridate", "shiny")

# read data
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

# clean data and pivot longer
malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")


# define plotting function
plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  # create plot title
  if (!("All" %in% district)) {            
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  # filter to age group
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}



```


Easy! One great feature of shiny is that it will understand what files named _app.R_, _server.R_, _ui.R_, and _global.R_ are for, so there is no need to connect them to each other via any code. So just by having this code in _global.R_ in the directory it will run before we start our app!.  

We should also note that it would improve our app's organisation if we moved the plotting function to its own file - this will be especially helpful as apps become larger. To do this, we could make another directory called _funcs_, and put this function in as a file called _plot_epicurve.R_. We could then read this function in via the following command in _global.R_

```{r, eval = F}

source(here("funcs", "plot_epicurve.R"), local = TRUE)

```

Note that you should *always* specify `local = TRUE` in shiny apps, since it will affect sourcing when/if the app is published on a server. 

## Developing an app server

Now that we have most of our code, we just have to develop our server. This is the final piece of our app, and is probably the hardest to understand. The server is a large R function, but its helpful to think of it as a series of smaller functions, or tasks that the app can perform. It's important to understand that these functions are not executed in a linear order. There is an order to them, but it's not fully necessary to understand when starting out with shiny. At a very basic level, these tasks or functions will activate when there is a change in user inputs that affects them, *unless the developer has set them up so they behave differently*. Again, this is all quite abstract, but lets first go through the three basic types of shiny _objects_

1. Reactive sources - this is another term for user inputs. The shiny server has access to the outputs from the UI through the widgets we've programmed. Every time the values for these are changed, this is passed down to the server.

2. Reactive conductors - these are objects that exist *only* inside the shiny server. We don't actually need these for simple apps, but they produce objects that can only be seen inside the server, and used in other operations. They generally depend on reactive sources.

3. Endpoints - these are outputs that are passed from the server to the UI. In our example, this would be the epi curve we are producing. 

With this in mind lets construct our server step-by-step. We'll show our UI code again here just for reference:

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve")
    )
    
  )
)


```

From this code UI we have:

- Two inputs:
  - District selector (with an inputId of `select_district`)
  - Age group selector (with an inputId of `select_agegroup`)
- One output:
  - The epicurve (with an outputId of `malaria_epicurve`)

As stated previously, these unique names we have assigned to our inputs and outputs are crucial. They *must be unique* and are used to pass information between the ui and server. In our server, we access our inputs via the syntax `input$inputID` and outputs and passed to the ui through the syntax `output$output_name` Lets have a look at an example, because again this is hard to understand otherwise!

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
}


```


The server for a simple app like this is actually quite straightforward! You'll notice that the server is a function with three parameters - `input`, `output`, and `session` - this isn't that important to understand for now, but its important to stick to this setup! In our server we only have one task - this renders a plot based on our function we made earlier, and the inputs from the server. Notice how the names of the input and output objects correspond exactly to those in the ui.

To understand the basics of how the server reacts to user inputs, you should note that the output will know (through the underlying package) when inputs change, and rerun this function to create a plot every time they change. Note that we also use the `renderPlot()` function here - this is one of a family of class-specific functions that pass those objects to a ui output. There are a number of functions that behave similarly, but you need to ensure the function used matches the class of object you're passing to the ui! For example:

- `renderText()` - send text to the ui
- `renderDataTable` - send an interactive table to the ui.

Remember that these also need to match the output *function* used in the ui - so `renderPlot()` is paired with `plotOutput()`, and `renderText()` is matched with `textOutput()`. 

So we've finally made a functioning app! We can run this by pressing the Run App button on the top right of the script window in Rstudio. You should note that you can choose to run your app in your default browser (rather than Rstudio) which will more accurately reflect what the app will look like for other users.  


```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_simple_view.gif"))
```


It is fun to note that in the R console, the app is "listening"! Talk about reactivity!  

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "listening.png"))
```


<!-- TO DO: *ADD SOMETHING ON DOWNLOADING A ZIP FILE OF THE APP?*  -->



## Adding more functionality

At this point we've finally got a running app, but we have very little functionality. We also haven't really scratched the surface of what shiny can do, so there's a lot more to learn about! Lets continue to build our existing app by adding some extra features. Some things that could be nice to add could be: 

1. Some explanatory text 
2. A download button for our plot - this would provide the user with a high quality version of the image that they're generating in the app
3. A selector for specific facilities
4. Another dashboard page - this could show a table of our data.

This is a lot to add, but we can use it to learn about a bunch of different shiny featues on the way. There is so much to learn about shiny (it can get *very* advanced, but its hopefully the case that once users have a better idea of how to use it they can become more comfortable using external learning sources as well).



### Adding static text {.unnumbered}  

Lets first discuss adding static text to our shiny app. Adding text to our app is extremely easy, once you have a basic grasp of it. Since static text doesn't change in the shiny app (If you'd like it to change, you can use *text rendering* functions in the server!), all of shiny's static text is generally added in the ui of the app. We wont go through this in great detail, but you can add a number of different elements to your ui (and even custom ones) by interfacing R with *HTML* and *css*.

HTML and css are languages that are explicitly involved in user interface design. We don't need to understand these too well, but *HTML* creates objects in UI (like a text box, or a table), and *css* is generally used to change the style and aesthetics of those objects. Shiny has access to a large array of _HTML tags_ - these are present for objects that behave in a specific way, such as headers, paragraphs of text, line breaks, tables, etc. We can use some of these examples like this:

- `h1()` - this a a *header* tag, which will make enclosed text automatically larger, and change defaults as they pertain to the font face, colour etc (depending on the overall theme of your app). You can access _smaller and smaller_ sub-heading with `h2()` down to `h6()` as well. Usage looks like:
  * `h1("my header - section 1")`

- `p()` - this is a *paragraph* tag, which will make enclosed text similar to text in a body of text. This text will automatically wrap, and be of a relatively small size (footers could be smaller for example.) Think of it as the text body of a word document. Usage looks like:  

  * `p("This is a larger body of text where I am explaining the function of my app")`
  
- `tags$b()` and `tags$i()` - these are used to create bold `tags$b()` and italicised `tags$i()` with whichever text is enclosed!

- `tags$ul()`, `tags$ol()` and `tags$li()` - these are tags used in creating *lists*. These are all used within the syntax below, and allow the user to create either an ordered list (`tags$ol()`; i.e. numbered) or unordered list (`tags$ul()`, i.e. bullet points). `tags$li()` is used to denote items in the list, regardless of which type of list is used. e.g.:

```{r, eval = F}

tags$ol(
  
  tags$li("Item 1"),
  
  tags$li("Item 2"),
  
  tags$li("Item 3")
  
)

```

- `br()` and `hr()` - these tags create *linebreaks* and *horizontal lines* (with a linebreak) respectively. Use them to separate out the sections of your app and text! There is no need to pass any items to these tags (parentheses can remain empty).


- `div()` - this is a *generic* tag that can *contain anything*, and can be *named anything*. Once you progress with ui design, you can use these to compartmentalize your ui, give specific sections specific styles, and create interactions between the server and UI elements. We won't go into these in detail, but they're worth being aware of!

Note that every one of these objects can be accessed through `tags$...` or for some, just the function. These are effectively synonymous, but it may help to use the `tags$...` style if you'd rather be more explicit and not overwrite the functions accidentally. This is also by no means an exhaustive list of tags available. There is a full list of all tags available in shiny  [here](https://shiny.rstudio.com/articles/tag-glossary.html) and even more can be used by inserting HTML directly into your ui!


If you're feeling confident, you can also add any *css styling elements* to your HTML tags with the `style` argument in any of them. We won't go into how this works in detail, but one tip for testing aesthetic changes to a UI is using the HTML inspector mode in chrome (of your shiny app you are running in browser), and editing the style of objects yourself!

Lets add some text to our app

```{r, eval = F}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         h4("Options"),
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
    tags$ul(
      tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
      tags$li(tags$b("data_date"), " - the date the data were collected at"),
      tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
      tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
      tags$li(tags$b("District"), " - the district the data were collected at"),
      tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
      tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
    )
    
  )
)
)



```

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_text_view.png"))
```


### Adding a link {.unnumbered}

To add a link to a website, use `tags$a()` with the link and display text as shown below. To have as a standalone paragraph, put it within `p()`. To have only a few words of a sentence linked, break the sentence into parts and use `tags$a()` for the hyperlinked part. To ensure the link opens in a *new* browser window, add `target = "_blank"` as an argument.  

```{r, eval=F}
tags$a(href = "www.epiRhandbook.com", "Visit our website!")
```



### Adding a download button {.unnumbered}

Lets move on to the second of the three features. A download button is a fairly common thing to add to an app and is fairly easy to make. We need to add another Widget to our ui, and we need to add another output to our server to attach to it. We can also introduce *reactive conductors* in this example!


Lets update our ui first - this is easy as shiny comes with a widget called `downloadButton()` - lets give it an inputId and a label.

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = FALSE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # horizontal line
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```
 
Note that we've also added in a `hr()` tag - this adds a horizontal line separating our control widgets from our download widgets. This is another one of the HTML tags that we discussed previously.

Now that we have our ui ready, we need to add the server component. Downloads are done in the server with the `downloadHandler()` function. Similar to our plot, we need to attach it to an output that has the same inputId as the download button. This function takes two arguments - `filename` and `content` - these are both functions. As you might be able to guess, `filename` is used to specify the name of the downloaded file, and `content` is used to specify what should be downloaded. `content` contain a function that you would use to save data locally - so if you were downloading a csv file you could use `rio::export()`. Since we're downloading a plot, we'll use `ggplot2::ggsave()`. Lets look at how we would program this (we won't add it to the server yet). 

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
  output$download_epicurve <- downloadHandler(
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```


Note that the `content` function always takes a `file` argument, which we put where the output file name is specified. You might also notice that we're repeating code here - we are using our `plot_epicurve()` function twice in this server, once for the download and once for the image displayed in the app. While this wont massively affect performance, this means that the code to generate this plot will have to be run when the user changes the widgets specifying the district and age group, *and* again when you want to download the plot. In larger apps, suboptimal decisions like this one will slow things down more and more, so it's good to learn how to make our app more efficient in this sense. What would make more sense is if we had a way to run the epicurve code when the districts/age groups are changes, *and let that be used by* the renderPlot() and downloadHandler() functions. This is where reactive conductors come in! 

Reactive conductors are objects that are created in the shiny server in a *reactive* way, but are not outputted - they can just be used by other parts of the server. There are a number of different kinds of *reactive conductors*, but we'll go through the basic two.

1.`reactive()` - this is the most basic reactive conductor - it will react whenever any inputs used inside of it change (so our district/age group widgets)  
2. `eventReactive()`- this rective conductor works the same as `reactive()`, except that the user can specify which inputs cause it to rerun. This is useful if your reactive conductor takes a long time to process, but this will be explained more later.  

Lets look at the two examples:

```{r, eval = FALSE}

malaria_plot_r <- reactive({
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})


# only runs when the district selector changes!
malaria_plot_er <- eventReactive(input$select_district, {
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})



```

When we use the `eventReactive()` setup, we can specify which inputs cause this chunk of code to run - this isn't very useful to us at the moment, so we can leave it for now. Note that you can include multiple inputs with `c()`

Lets look at how we can integrate this into our server code:


```{r, eval = FALSE}

server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  })
  
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```

You can see we're just calling on the output of our reactive we've defined in both our download and plot rendering functions. One thing to note that often trips people up is you have to use the outputs of reactives as if they were functions - so you *must add empty brackets at the end of them* (i.e. `malaria_plot()` is correct, and `malaria_plot` is not). Now that we've added this solution our app is a little tidyer, faster, and easier to change since all our code that runs the epicurve function is in one place.


```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "download_button_view.png"))
```


### Adding a facility selector {.unnumbered}  

Lets move on to our next feature - a selector for specific facilities. We'll implement another parameter into our function so we can pass this as an argument from our code. Lets look at doing this first - it just operates off the same principles as the other parameters we've set up. Lets update and test our function.


```{r, echo = TRUE}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot", facility = "All") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
    if (!("All" %in% facility)) {
    data <- data %>%
      filter(location_name == facility)
    
    plot_title_facility <- facility
    
  } else {
    
    plot_title_facility <- "all facilities"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }

  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}; {plot_title_facility}"),
      subtitle = agegroup_title
    )
  
  
  
}
```

Let's test it:  

```{r, warning=F, message=F}

plot_epicurve(malaria_data, district = "Spring", agegroup = "malaria_rdt_0-4", facility = "Facility 1")

```


With all the facilites in our data, it isn't very clear which facilities correspond to which districts - and the end user won't know either. This might make using the app quite unintuitive. For this reason, we should make the facility options in the UI change dynamically as the user changes the district - so one filters the other! Since we have so many variables that we're using in the options, we might also want to generate some of our options for the ui in our _global.R_ file _from the data_. For example, we can add this code chunk to _global.R_ after we've read our data in:



```{r, , message =  FALSE}

all_districts <- c("All", unique(malaria_data$District))

# data frame of location names by district
facility_list <- malaria_data %>%
  group_by(location_name, District) %>%
  summarise() %>% 
  ungroup()

```

Let's look at them:  

```{r}
all_districts
```


```{r}
facility_list
```


We can pass these new variables to the ui without any issue, since they are globally visible by both the server and the ui! Lets update our UI:


```{r, eval = FALSE}


ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = all_districts,
              selected = "All",
              multiple = FALSE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # selector for facility
         selectInput(
           inputId = "select_facility",
           label = "Select Facility",
           choices = c("All", facility_list$location_name),
           selected = "All"
         ),
         
         # horizontal line
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```


Notice how we're now passing variables for our choices instead of hard coding them in the ui! This might make our code more compact as well! Lastly, we'll have to update the server. It will be easy to update our function to incorporate our new input (we just have to pass it as an argument to our new parameter), but we should remember we also want the ui to update dynamically when the user changes the selected district. It is important to understand here that we *can change the parameters and behaviour of widgets* while the app is running, but this needs to be done *in the server*. We need to understand a new way to output to the server to learn how to do this.

The functions we need to understand how to do this are known as *observer* functions, and are similar to *reactive* functions in how they behave. They have one key difference though:

- Reactive functions do not directly affect outputs, and produce objects that can be seen in other locations in the server
- Observer functions *can* affect server outputs, but do so via side effects of other functions. (They can also do other things, but this is their main function in practice)

Similar to reactive functions, there are two flavours of observer functions, and they are divided by the same logic that divides reactive functions:

1. `observe()` - this function runs whenever any inputs used inside of it change
2. `observeEvent()` - this function runs when a *user-specified* input changes

We also need to understand the shiny-provided functions that update widgets. These are fairly straightforward to run - they first take the `session` object from the server function (this doesn't need to be understood for now), and then the `inputId` of the function to be changed. We then pass new versions of all parameters that are already taken by `selectInput()` - these will be automatically updated in the widget. 

Lets look at an isolated example of how we could use this in our server. When the user changes the district, we want to filter our tibble of facilities by district, and update the choices to *only reflect those that are available in that district* (and an option for all facilities)

```{r, eval = FALSE}

observe({
  
  if (input$select_district == "All") {
    new_choices <- facility_list$location_name
  } else {
    new_choices <- facility_list %>%
      filter(District == input$select_district) %>%
      pull(location_name)
  }
  
  new_choices <- c("All", new_choices)
  
  updateSelectInput(session, inputId = "select_facility",
                    choices = new_choices)
  
})


```

And that's it! we can add it into our server, and that behaviour will now work. Here's what our new server should look like:

```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  
  
}

```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_menu_view.gif"))
```







### Adding another tab with a table {.unnumbered}

Now we'll move on to the last component we want to add to our app. We'll want to separate our ui into two tabs, one of which will have an interactive table where the user can see the data they are making the epidemic curve with. To do this, we can use the packaged ui elements that come with shiny relevant to tabs. On a basic level, we can enclose most of our main panel in this general structure:

```{r, eval = FALSE}


# ... the rest of ui

mainPanel(
  
  tabsetPanel(
    type = "tabs",
    tabPanel(
      "Epidemic Curves",
      ...
    ),
    tabPanel(
      "Data",
      ...
    )
  )
)


```

Lets apply this to our ui. We also will want to use the **DT** package here - this is a great package for making interactive tables from pre-existing data. We can see it being used for `DT::datatableOutput()` in this example.

```{r, echo = FALSE}
library(DT)
```

```{r, eval = FALSE}
ui <- fluidPage(
     
     titlePanel("Malaria facility visualisation app"),
     
     sidebarLayout(
          
          sidebarPanel(
               # selector for district
               selectInput(
                    inputId = "select_district",
                    label = "Select district",
                    choices = all_districts,
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for age group
               selectInput(
                    inputId = "select_agegroup",
                    label = "Select age group",
                    choices = c(
                         "All ages" = "malaria_tot",
                         "0-4 yrs" = "malaria_rdt_0-4",
                         "5-14 yrs" = "malaria_rdt_5-14",
                         "15+ yrs" = "malaria_rdt_15"
                    ), 
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for facility
               selectInput(
                    inputId = "select_facility",
                    label = "Select Facility",
                    choices = c("All", facility_list$location_name),
                    selected = "All"
               ),
               
               # horizontal line
               hr(),
               downloadButton(
                    outputId = "download_epicurve",
                    label = "Download plot"
               )
               
          ),
          
          mainPanel(
               tabsetPanel(
                    type = "tabs",
                    tabPanel(
                         "Epidemic Curves",
                         plotOutput("malaria_epicurve")
                    ),
                    tabPanel(
                         "Data",
                         DT::dataTableOutput("raw_data")
                    )
               ),
               br(),
               hr(),
               p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
               tags$ul(
                    tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
                    tags$li(tags$b("data_date"), " - the date the data were collected at"),
                    tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
                    tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
                    tags$li(tags$b("District"), " - the district the data were collected at"),
                    tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
                    tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
               )
               
               
          )
     )
)


```


Now our app is arranged into tabs! Lets make the necessary edits to the server as well. Since we dont need to manipulate our dataset at all before we render it this is actually very simple - we just render the malaria_data dataset via DT::renderDT() to the ui!


```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  # render data table to ui
  output$raw_data <- DT::renderDT(
    malaria_data
  )
  
  
}


```


```{r, out.width=c('100%', '100%'), fig.show='hold', echo = F, fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_table_view.gif"))
```


## Sharing shiny apps

Now that you've developed your app, you probably want to share it with others - this is the main advantage of shiny after all! We can do this by sharing the code directly, or we could publish on a server. If we share the code, others will be able to see what you've done and build on it, but this will negate one of the main advantages of shiny - *it can eliminate the need for end-users to maintain an R installation*. For this reason, if you're sharing your app with users who are not comfortable with R, it is much easier to share an app that has been published on a server. 

If you'd rather share the code, you could make a .zip file of the app, or better yet, *publish your app on github and add collaborators.* You can refer to the section on github for further information here.

However, if we're publishing the app online, we need to do a little more work. Ultimately, we want your app to be able to be accessed via a web URL so others can get quick and easy access to it. Unfortunately, to publish you app on a server, you need to have access to a server to publish it on! There are a number of hosting options when it comes to this:

- _shinyapps.io_: this is the easiest place to publish shiny apps, as it has the smallest amount of configuration work needed, and has some free, but limited licenses.

- _RStudio Connect_: this is a far more powerful version of an R server, that can perform many operations, including publishing shiny apps. It is however, harder to use, and less recommended for first-time users.

For the purposes of this document, we will use _shinyapps.io_, since it is easier for first time users. You can make a free account here to start - there are also different price plans for server licesnses if needed. The more users you expect to have, the more expensive your price plan may have to be, so keep this under consideration. If you're looking to create something for a small set of individuals to use, a free license may be perfectly suitable, but a public facing app may need more licenses.

First we should make sure our app is suitable for publishing on a server. In your app, you should restart your R session, and ensure that it runs without running any extra code. This is important, as an app that requires package loading, or data reading not defined in your app code won't run on a server. Also note that you can't have any *explicit* file paths in your app - these will be invalid in the server setting - using the `here` package solves this issue very well. Finally, if you're reading data from a source that requires user-authentication, such as your organisation's servers, this will not generally work on a server. You will need to liase with your IT department to figure out how to whitelist the shiny server here.

*signing up for account*

Once you have your account, you can navigate to the tokens page under _Accounts_. Here you will want to add a new token - this will be used to deploy your app. 

From here, you should note that the url of your account will reflect the name of your app - so if your app is called _my_app_, the url will be appended as _xxx.io/my_app/_. Choose your app name wisely! Now that you are all ready, click deploy - if successful this will run your app on the web url you chose!

*something on making apps in documents?*

## Further reading

So far, we've covered a lot of aspects of shiny, and have barely scratched the surface of what is on offer for shiny. While this guide serves as an introduction, there is loads more to learn to fully understand shiny. You should start making apps and gradually add more and more functionality


## Recommended extension packages

The following represents a selection of high quality shiny extensions that can help you get a lot more out of shiny. In no particular order:

- **shinyWidgets** - this package gives you many many more widgets that can be used in your app. Run `shinyWidgets::shinyWidgetsGallery()` to see a selection of available widgets with this package. See examples [here](https://github.com/dreamRs/shinyWidgets)  

- **shinyjs** - this is an excellent package that gives the user the ability to greatly extend shiny's utility via a series of javascript. The applications of this package range from very simple to highly advanced, but you might want to first use it to manipulate the ui in simple ways, like hiding/showing elements, or enabling/disabling buttons. Find out more [here](https://deanattali.com/shinyjs/basic)

- **shinydashboard** - this package massively expands the available ui that can be used in shiny, specifically letting the user create a complex dashboard with a variety of complex layouts. See more [here](https://rstudio.github.io/shinydashboard/)

- **shinydashboardPlus** - get even more features out of the **shinydashboard** framework! See more [here](https://rinterface.github.io/shinydashboardPlus/articles/shinydashboardPlus.html)

- **shinythemes** - change the default css theme for your shiny app with a wide range of preset templates! See more [here](https://rstudio.github.io/shinythemes/)


There are also a number of packages that can be used to create interactive outputs that are shiny compatible. 

- **DT** is semi-incorporated into base-shiny, but provides a great set of functions to create interactive tables.

- **plotly** is a package for creating interactive plots that the user can manipulate in app. You can also convert your plot to interactive versions via `plotly::ggplotly()`! As alternatives, **dygraphs** and **highcharter** are also excellent.


## Recommended resources



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/shiny_basics.Rmd-->

# (PART) Miscellaneous {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_misc.Rmd-->

# Writing functions  


<!-- ======================================================= -->
## Preparation {  }


### Load packages {-}

This code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load installed packages with  `library()` from **base** R. See the page on [R basics] for more information on R packages.  

```{r, echo=F, warning=F, message=F}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  janitor,      # adding totals and percents to tables
  scales,       # easily convert proportions to percents  
  flextable,     # converting tables to HTML
  purrr,          #makes functional programming easier
  readr,          #to read csv files
  highcharter     #to create highchart object and draw particular plot

  )
```

### Import data {-}

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data.

We will also use in the last part of this page some data on H7N9 flu from 2013.

```{r, echo=F}
# import the linelists into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

flu_china <- rio::import(here::here("data", "case_linelists", "fluH7N9_China_2013.csv"))

```


## Functions  

Functions are helpful in programming since they allow to make codes easier to understand, somehow shorter and less prone to errors (given there were no errors in the function itself).

If you have come so far to this handbook, it means you have came across endless functions since in R, every operation is a function call
`+, for, if, [, $, { …`. For example `x + y` is the same as`'+'(x, y)`

R is one the languages that offers the most possibility to work with functions and give enough tools to the user to easily write them. We should not think about functions as fixed at the top or at the end of the programming chain, R offers the possibility to use them as if they were vectors and even to use them inside other functions, lists...

Lot of very advanced resources on functional programming exist and we will only give here an insight to help you start with functional programming with short practical examples. You are then encouraged to visit the links on references to read more about it.





## Why would you use a function? 

Before answering this question, it is important to note that you have already had tips to get to write your very first R functions in the page on [Iteration, loops, and lists] of this handbook. In fact, use of "if/else" and loops is often a core part of many of our functions since they easily help to either broaden the application of our code allowing multiple conditions or to iterate codes for repeating tasks.

- I am repeating multiple times the same block of code to apply it to a different variable or data?

- Getting rid of it will it substantially shorten my overall code and make it run quicker?

- Is it possible that the code I have written is used again but with a different value at many places of the code?

If the answer to one of the previous questions is "YES", then you probably need to write a function

## How does R  build functions?

Functions in R have three main components:

- the `formals()` which is the list of arguments which controls how we can call the function

- the `body()` that is the code inside the function i.e. within the brackets or following the parenthesis depending on how we write it

and,

- the `environment()` which will help locate the function's variables and determines how the function finds value.
 
Once you have created your function, you can verify each of these components by calling the function associated.
 

## Basic syntax and structure

- A function will need to be named properly so that its job is easily understandable as soon as we read its name. Actually this is already the case with majority of the base R architecture. Functions like  `mean()`, `print()`, `summary()` have names that are very straightforward 

- A function will need arguments, such as the data to work on and other objects that can be static values among other options  

- And finally a function will give an output based on its core task and the arguments it has been given. Usually we will use the built-in functions as `print()`, `return()`... to produce the output. The output can be a logical value, a number, a character, a data frame...in short any kind of R object.

Basically this is the composition of a function:

```{r, eval=FALSE}

function_name <- function(argument_1, argument_2, argument_3){
  
           function_task
  
           return(output)
}


```

We can create our first function that will be called `contain_covid19()`. 

```{r}

contain_covid19 <- function(barrier_gest, wear_mask, get_vaccine){
  
                            if(barrier_gest == "yes" & wear_mask == "yes" & get_vaccine == "yes" ) 
       
                            return("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


```

We can then verify the components of our newly created function.

```{r}

formals(contain_covid19)
body(contain_covid19)
environment(contain_covid19)

```


Now we will test our function. To call our written function, you use it as you use all R functions i.e by writing the function name and adding the required arguments.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "yes")

```

We can write again the name of each argument for precautionary reasons. But without specifying them, the code should work since R has in memory the positioning of each argument. So as long as you put the values of the arguments in the correct order, you can skip writing the arguments names when calling the functions.

```{r}

contain_covid19("yes", "yes", "yes")

```

Then let's look what happens if one of the values is `"no"` or **not** `"yes"`.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "no")
```

If we provide an argument that is not recognized, we get an error: 

```{r, eval=F}
contain_covid19(barrier_gest = "sometimes", wear_mask = "yes", get_vaccine = "no")
```

`Error in contain_covid19(barrier_gest = "sometimes", wear_mask = "yes",  : 
  could not find function "contain_covid19"`


<span style="color: black;">**_NOTE:_** Some functions  (most of time very short and straightforward) may not need a name and can be used directly on a line of code or inside another function to do quick task. They are called **anonymous functions** .</span>

For instance below is a first anonymous function that   keeps only character variables the dataset.

```{r, eval=F}
linelist %>% 
  dplyr::slice_head(n=10) %>%  #equivalent to R base "head" function and that return first n observation of the  dataset
  select(function(x) is.character(x)) 
```
  
```{r, echo=F}
linelist %>% 
  dplyr::slice_head(n=10) %>%  #equivalent to R base "head" function and that return first n observation of the  dataset
  select(function(x) is.character(x)) %>%  
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```


Then another function that selects every second observation of our dataset (may be relevant when we have longitudinal data with many records per patient for instance after having ordered by date or visit).
In this case, the proper function writing outside dplyr would be `function (x) (x%%2 == 0)` to apply to the vector containing all row numbers.


```{r, eval=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>% # add indices of each obs as rownames to clearly see the final selection
   filter(row_number() %%2 == 0)
```

```{r, echo=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>%    # add indices of each obs as rownames to clearly see the final selection
   filter(row_number() %%2 == 0) %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


A possible base R code for the same task would be:

```{r, eval = F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),]
```

```{r, echo=F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),] %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


<span style="color: orange;">**_CAUTION:_** Though it is true that using functions can help us with our code, it can nevertheless be  time consuming to write some functions or to fix one if it has not been thought thoroughly, written adequately and is returning errors as a result. For this reason it is often recommended to first write the R code, make sure it does what we intend it to do, and then transform it into a function with its three main components as listed above. </span>

## Examples  

### Return proportion tables for several columns {.unnumbered}  

Yes, we already have nice functions in many packages allowing to summarize information in a very easy and nice way. But we will still try to make our own, in our first steps to getting used to writing functions.

In this example we want to show how writing a simple function would avoid you copy-pasting the same code multiple times.

```{r}

proptab_multiple <- function(my_data, var_to_tab){
  
  #print the name of each variable of interest before doing the tabulation
  print(var_to_tab)

  with(my_data,
       rbind( #bind the results of the two following function by row
        #tabulate the variable of interest: gives only numbers
          table(my_data[[var_to_tab]], useNA = "no"),
          #calculate the proportions for each variable of interest and round the value to 2 decimals
         round(prop.table(table(my_data[[var_to_tab]]))*100,2)
         )
       )
}


proptab_multiple(linelist, "gender")

proptab_multiple(linelist, "age_cat")

proptab_multiple(linelist, "outcome")


```

<span style="color: darkgreen;">**_TIP:_** As shown above, it is very important to comment your functions as you would do for the general programming. Bear in mind that a function's aim is to make a code ready to read, shorter and more efficient. Then one should be able to understand what the function does just by reading its name and should have more details reading the comments.</span>


A second option is to use this function in another one via a loop to make the process at once:

```{r}


for(var_to_tab in c("gender","age_cat",  "outcome")){
  
  print(proptab_multiple(linelist, var_to_tab))
  
}

```

A simpler way could be using the base R "apply" instead of a "for loop" as expressed below:

```{r, include= FALSE, eval=FALSE}

base::lapply(linelist[,c("gender","age_cat", "outcome")], table)

```


<span style="color: darkgreen;">**_TIP:_** R is often defined as a functional programming language and almost anytime you run a line of code you are using some built-in functions. A good habit to be more comfortable with writing functions is to often have an internal look at how the basic functions you are using daily are built. The shortcut to do so is selecting the function name and then clicking on`Ctrl+F2` or `fn+F2` or `Cmd+F2` (depending on your computer) .</span>

## Using **purrr**: writing functions that can be iteratively applied

### Modify class of multiple columns in a dataset {.unnumbered}  

Let's say many character variables in the original `linelist` data need to be changes to "factor" for analysis and plotting purposes. Instead of repeating the step several times, we can just use `lapply()` to do the transformation of all variables concerned on a single line of code.


<span style="color: orange;">**_CAUTION:_** `lapply()` returns a list, thus its use may require an additional modification as a last step.</span>


```{r, include=FALSE}

linelist_factor1 <- linelist %>%
      lapply(
          function(x) if(is.character(x)) as.factor(x) else x) %>%
      as.data.frame() %>% 
      glimpse()

```


The same step can be done using `map_if()` function from the **purrr** package

```{r}

linelist_factor2 <- linelist %>%
  purrr::map_if(is.character, as.factor)


linelist_factor2 %>%
        glimpse()

```


### Iteratively produce graphs for different levels of a variable {.unnumbered}

We will produce here pie chart to look at the distribution of patient's outcome in China during the H7N9 outbreak for each province. Instead of repeating the code for each of them, we will just apply a function that we will create.

```{r}

#precising options for the use of highchart
options(highcharter.theme =   highcharter::hc_theme_smpl(tooltip = list(valueDecimals = 2)))


#create a function called "chart_outcome_province" that takes as argument the dataset and the name of the province for which to plot the distribution of the outcome.

chart_outcome_province <- function(data_used, prov){
  
  tab_prov <- data_used %>% 
    filter(province == prov,
           !is.na(outcome))%>% 
    group_by(outcome) %>% 
    count() %>%
    adorn_totals(where = "row") %>% 
    adorn_percentages(denominator = "col", )%>%
    mutate(
        perc_outcome= round(n*100,2))
  
  
  tab_prov %>%
    filter(outcome != "Total") %>% 
  highcharter::hchart(
    "pie", hcaes(x = outcome, y = perc_outcome),
    name = paste0("Distibution of the outcome in:", prov)
    )
  
}

chart_outcome_province(flu_china, "Shanghai")
chart_outcome_province(flu_china,"Zhejiang")
chart_outcome_province(flu_china,"Jiangsu")


```



### Iteratively produce tables for different levels of a variable {.unnumbered}

Here we will create three indicators to summarize in a table and we would like to produce this table for each of the provinces. Our indicators are the delay between onset and hospitalization, the percentage of recovery and the median age of cases.

```{r}


indic_1 <- flu_china %>% 
  group_by(province) %>% 
  mutate(
    date_hosp= strptime(date_of_hospitalisation, format = "%m/%d/%Y"),
    date_ons= strptime(date_of_onset, format = "%m/%d/%Y"), 
    delay_onset_hosp= as.numeric(date_hosp - date_ons)/86400,
    mean_delay_onset_hosp = round(mean(delay_onset_hosp, na.rm=TRUE ), 0)) %>%
  select(province, mean_delay_onset_hosp)  %>% 
  distinct()
     

indic_2 <-  flu_china %>% 
            filter(!is.na(outcome)) %>% 
            group_by(province, outcome) %>% 
            count() %>%
            pivot_wider(names_from = outcome, values_from = n) %>% 
    adorn_totals(where = "col") %>% 
    mutate(
        perc_recovery= round((Recover/Total)*100,2))%>% 
  select(province, perc_recovery)
    
    
    
indic_3 <-  flu_china %>% 
            group_by(province) %>% 
            mutate(
                    median_age_cases = median(as.numeric(age), na.rm = TRUE)
            ) %>% 
  select(province, median_age_cases)  %>% 
  distinct()

#join the three indicator datasets

table_indic_all <- indic_1 %>% 
  dplyr::left_join(indic_2, by = "province") %>% 
        left_join(indic_3, by = "province")


#print the indicators in a flextable


print_indic_prov <-  function(table_used, prov){
  
  #first transform a bit the dataframe for printing ease
  indic_prov <- table_used %>%
    filter(province==prov) %>%
    pivot_longer(names_to = "Indicateurs", cols = 2:4) %>% 
   mutate( indic_label = factor(Indicateurs,
   levels= c("mean_delay_onset_hosp","perc_recovery","median_age_cases"),
   labels=c("Mean delay onset-hosp","Percentage of recovery", "Median age of the cases"))
   ) %>% 
    ungroup(province) %>% 
    select(indic_label, value)
  

    tab_print <- flextable(indic_prov)  %>%
    theme_vanilla() %>% 
    flextable::fontsize(part = "body", size = 10) 
    
    
     tab_print <- tab_print %>% 
                  autofit()   %>%
                  set_header_labels( 
                indic_label= "Indicateurs", value= "Estimation") %>%
    flextable::bg( bg = "darkblue", part = "header") %>%
    flextable::bold(part = "header") %>%
    flextable::color(color = "white", part = "header") %>% 
    add_header_lines(values = paste0("Indicateurs pour la province de: ", prov)) %>% 
bold(part = "header")
 
 tab_print <- set_formatter_type(tab_print,
   fmt_double = "%.2f",
   na_str = "-")

tab_print 
    
}




print_indic_prov(table_indic_all, "Shanghai")
print_indic_prov(table_indic_all, "Jiangsu")


```


## Tips and best Practices for well functioning functions

Functional programming is meant to ease code and facilitates its reading. It should produce the contrary. The tips below will help you having a clean code and easy to read code. 


### Naming and syntax {.unnumbered}

- Avoid using character that could have been easily already taken by other functions already existing in your environment

- It is recommended for the function name to be short and straightforward to understand for another reader

- It is preferred to use verbs as the function name and nouns for the argument names.


### Column names and tidy evaluation {.unnumbered}  

If you want to know how to reference *column names* that are provided to your code as arguments, read this [tidyverse programming guidance](https://dplyr.tidyverse.org/articles/programming.html). Among the topics covered are *tidy evaluation* and use of the *embrace* `{{ }}` "double braces"

For example, here is a quick skeleton template code from page tutorial mentioned just above:  

```{r, eval=F}

var_summary <- function(data, var) {
  data %>%
    summarise(n = n(), min = min({{ var }}), max = max({{ var }}))
}
mtcars %>% 
  group_by(cyl) %>% 
  var_summary(mpg)

```


### Testing and Error handling {.unnumbered}

The more complicated a function's task the higher the possibility of errors. Thus it is sometimes necessary to add some verification within the funtion to help quickly understand where the error is from and find a way t fix it.

- It can be more than recommended to introduce a check on the missingness of one argument using `missing(argument)`. This simple check can return "TRUE" or "FALSE" value.

```{r , error=TRUE}

contain_covid19_missing <- function(barrier_gest, wear_mask, get_vaccine){
  
  if (missing(barrier_gest)) (print("please provide arg1"))
  if (missing(wear_mask)) print("please provide arg2")
  if (missing(get_vaccine)) print("please provide arg3")


  if (!barrier_gest == "yes" | wear_mask =="yes" | get_vaccine == "yes" ) 
       
       return ("you can do better")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_missing(get_vaccine = "yes")

```


- Use `stop()` for more detectable errors.

```{r, error=TRUE}

contain_covid19_stop <- function(barrier_gest, wear_mask, get_vaccine){
  
  if(!is.character(barrier_gest)) (stop("arg1 should be a character, please enter the value with `yes`, `no` or `sometimes"))
  
  if (barrier_gest == "yes" & wear_mask =="yes" & get_vaccine == "yes" ) 
       
       return ("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_stop(barrier_gest=1, wear_mask="yes", get_vaccine = "no")

```

- As we see when we run most of the built-in functions, there are messages and warnings that can pop-up in certain conditions. We can integrate those in our written functions by using the functions `message()` and `warning()`.

- We can handle errors also by using `safely()` which takes one function as an argument and executes it in a safe way. In fact the function will execute without stopping if it encounters an error. `safely()` returns as output a **list** with two objects which are the results and the error it "skipped".

We can verify by first running the `mean()` as  function, then run it with `safely()`.


```{r, warning=FALSE}

map(linelist, mean)
```


```{r, warning=FALSE}
safe_mean <- safely(mean)
linelist %>% 
  map(safe_mean)

```


As said previously, well commenting our codes is already a good way for having documentation in our work.  


<!-- ======================================================= -->
## Resources


[R for Data Science link](https://r4ds.had.co.nz/functions.html)   

[Cheatsheet advance R programming](https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf)

[Cheatsheet purr Package](https://purrr.tidyverse.org/)

[Video-ACM talk by Hadley Wickham: The joy of functional programming (how does map_dbl work)](https://youtube.videoken.com/embed/bzUmK0Y07ck)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/writing_functions.Rmd-->


# Directory interactions { }  

In this page we cover common scenarios where you create, interact with, save, and import with directories (folders).  


## Preparation  

### **fs** package {.unnumbered}  

The **fs** package is a **tidyverse** package that facilitate directory interactions, improving on some of the **base** R functions. In the sections below we will often use functions from **fs**.  

```{r}
pacman::p_load(
  fs,             # file/directory interactions
  rio,            # import/export
  here,           # relative file pathways
  tidyverse)      # data management and visualization
```


### Print directory as a dendrogram tree {.unnumbered}  

Use the function `dir_tree()` from **fs**.  

Provide the folder filepath to `path = ` and decide whether you want to show only one level (`recurse = FALSE`) or all files in all sub-levels (`recurse = TRUE`). Below we use `here()` as shorthand for the R project and specify its sub-folder "data", which contains all the data used for this R handbook. We set it to display all files within "data" and its sub-folders (e.g. "cache", "epidemic models", "population", "shp", and "weather").  


```{r}
fs::dir_tree(path = here("data"), recurse = TRUE)
```


## List files in a directory  

To list just the file names in a directory you can use `dir()` from **base** R. For example, this command lists the file names of the files in the "population" subfolder of the "data" folder in an R project. The relative filepath is provided using `here()` (which you can read about more in the [Import and export] page).  

```{r}
# file names
dir(here("data", "gis", "population"))
```

To list the full file paths of the directory's files, you can use you can use `dir_ls()` from **fs**. A **base** R alternative is `list.files()`.  

```{r}
# file paths
dir_ls(here("data", "gis", "population"))
```

To get all the metadata information about each file in a directory, (e.g. path, modification date, etc.) you can use `dir_info()` from **fs**.  

This can be particularly useful if you want to extract the last modification time of the file, for example if you want to import the most recent version of a file. For an example of this, see the [Import and export] page.     

```{r, eval=F}
# file info
dir_info(here("data", "gis", "population"))
```

Here is the data frame returned. Scroll to the right to see all the columns.  

```{r, echo=F}
DT::datatable(dir_info(here("data", "gis", "population")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

## File information  

To extract metadata information about a specific file, you can use `file_info()` from **fs** (or `file.info()` from **base** R).  

```{r, eval=F}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, echo=F}
DT::datatable(file_info(here("data", "case_linelists", "linelist_cleaned.rds")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Here we use the `$` to index the result and return only the `modification_time` value.  

```{r}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))$modification_time
```




## Check if exists  

### R objects {.unnumbered}  

You can use `exists()` from **base** R to check whether an R object exists *within* R (supply the object name in quotes).  

```{r}
exists("linelist")
```

Note that some **base** R packages use generic object names like "data" behind the scenes, that will appear as TRUE unless `inherit = FALSE` is specified. This is one reason to not name your dataset "data".  

```{r}
exists("data")
exists("data", inherit = FALSE)
```

If you are writing a function, you should use `missing()` from **base** R to check if an argument is present or not, instead of `exists()`.  



### Directories {.unnumbered}  

To check whether a directory exists, provide the file path (and file name) to `is_dir()` from **fs**. Scroll to the right to see that `TRUE` is printed.    

```{r}
is_dir(here("data"))
```

An alternative is `file.exists()` from **base** R.  


### Files {.unnumbered}  

To check if a specific file exists, use `is_file()` from **fs**. Scroll to the right to see that `TRUE` is printed.  

```{r}
is_file(here("data", "case_linelists", "linelist_cleaned.rds"))
```

A **base** R alternative is `file.exists()`.  



## Create  

### Directories {.unnumbered}  

To create a new directory (folder) you can use `dir_create()` from **fs**. If the directory already exists, it will not be overwritten and no error will be returned. 

```{r, eval=F}
dir_create(here("data", "test"))
```

An alternative is `dir.create()` from **base** R, which will show an error if the directory already exists. In contrast, `dir_create()` in this scenario will be silent.  

### Files {.unnumbered}  

You can create an (empty) file with `file_create()` from **fs**. If the file already exists, it will not be over-written or changed.  

```{r, eval=F}
file_create(here("data", "test.rds"))
```

A **base** R alternative is `file.create()`. But if the file already exists, this option will truncate it. If you use `file_create()` the file will be left unchanged.  


### Create if does not exists {.unnumbered}  

UNDER CONSTRUCTION  


## Delete

### R objects {.unnumbered}  

Use `rm()` from **base** R to remove an R object.  

### Directories {.unnumbered}  

Use `dir_delete()` from **fs**. 


### Files {.unnumbered}  

You can delete files with `file_delete()` from **fs**.  



## Running other files  

### `source()` {.unnumbered}  

To run one R script from another R script, you can use the `source()` command (from **base** R).

```{r, eval=F}
source(here("scripts", "cleaning_scripts", "clean_testing_data.R"))
```

This is equivalent to viewing the above R script and clicking the "Source" button in the upper-right of the script. This will execute the script but will do it silently (no output to the R console) unless specifically intended. See the page on [Interactive console] for examples of using `source()` to interact with a user via the R console in question-and-answer mode.  

```{r, fig.align = "center", out.height = '300%', echo=F}
knitr::include_graphics(here::here("images", "source_button.png"))
```


### `render()` {.unnumbered}  

`render()` is a variation on `source()` most often used for R markdown scripts. You provide the `input = ` which is the R markdown file, and also the `output_format = ` (typically either "html_document", "pdf_document", "word_document", "") 

See the page on [Reports with R Markdown] for more details. Also see the documentation for `render()` [here](https://rmarkdown.rstudio.com/docs/reference/render.html) or by entering `?render`.  



### Run files in a directory {.unnumbered}

You can create a *for loop* and use it to `source()` every file in a directory, as identified with `dir()`. 

```{r, eval=F}
for(script in dir(here("scripts"), pattern = ".R$")) {   # for each script name in the R Project's "scripts" folder (with .R extension)
  source(here("scripts", script))                        # source the file with the matching name that exists in the scripts folder
}
```

If you only want to run certain scripts, you can identify them by name like this:  

```{r, eval=F}

scripts_to_run <- c(
     "epicurves.R",
     "demographic_tables.R",
     "survival_curves.R"
)

for(script in scripts_to_run) {
  source(here("scripts", script))
}

```



Here is a [comparison](https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html) of the **fs** and **base** R functions.  

### Import files in a directory  {.unnumbered}

See the page on [Import and export] for importing and exporting individual files.  

Also see the [Import and export] page for methods to automatically import the most recent file, based on a date in the file name *or* by looking at the file meta-data.  

See the page on [Iteration, loops, and lists] for an example with the package **purrr** demonstrating:  

* Splitting a data frame and saving it out as multiple CSV files  
* Splitting a data frame and saving each part as a separate sheet within one Excel workbook  
* Importing multiple CSV files and combining them into one dataframe  
* Importing an Excel workbook with multiple sheets and combining them into one dataframe  




## **base** R  

See below the functions `list.files()` and `dir()`, which perform the same operation of listing files within a specified directory. You can specify `ignore.case =` or a specific pattern to look for. 

```{r, eval=F}
list.files(path = here("data"))

list.files(path = here("data"), pattern = ".csv")
# dir(path = here("data"), pattern = ".csv")

list.files(path = here("data"), pattern = "evd", ignore.case = TRUE)

```

If a file is currently "open", it will display in your folder with a tilde in front, like "~$hospital_linelists.xlsx".  


<!-- ======================================================= -->
## Resources {  }

https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/directories.Rmd-->

# Version control and collaboration with Git and Github

This chapter presents an overview of using Git to collaborate with others.
More extensive tutorials can be
found at the bottom in the Resources section.

## What is Git?

Git is a **version control** software that allows tracking changes in a
folder. It can be used like the "track change" option in Word, LibreOffice or
Google docs, but for all types of files. It is one of the most powerful
and most used options for version control.

**Why have I never heard of it? -** While people with a developer
background routinely learn to use version control software (Git,
Mercurial, Subversion or others), few of us from
quantitative disciplines are taught these skills. Consequently, most epidemiologists never
hear of it during their studies, and have to learn it on the fly.

**Wait, I heard of Github, is it the same?** - Not exactly, but you
often use them together, and we will show you how to. In short:

-   **Git** is the version control system, a piece of software. You can use it
    locally on your computer or to synchronize a folder with a
    host **website**. By default, one uses a terminal to give Git
    instructions in command-line.

-   You can use a **Git client/interface** to avoid the command-line and
    perform the same actions (at least for the simple, super common
    ones).

-   If you want to store your folder in a **host website** to
    collaborate with others, you may create an account at Github,
    Gitlab, Bitbucket or others.

So you could use the client/interface **Github Desktop**, which uses
**Git** in the background to manage your files, both locally on your
computer, and remotely on a **Github** server.

## Why use the combo Git and Github?

Using **Git** facilitates:

1)  Archiving documented versions with incremental changes so that you
    can easily revert backwards to any previous state
2)  Having parallel *branches*, i.e. developing/"working" versions with
    structured ways to integrate the changes after review

This can be done locally on your computer, even if you don't collaborate
with other people. Have you ever:

-   regretted having deleted a section of code, only to realize two
    months later that you actually needed it?


-   come back on a project that had been on pause and attempted to
    remember whether you had made that tricky modification in one of the
    models?

-   had a *file model_1.R* and another file *model_1\_test.R* and a file
    *model_1\_not_working.R* to try things out?

-   had a file *report.Rmd*, a file *report_full.Rmd*, a file
    *report_true_final.Rmd*, a file *report_final_20210304.Rmd*, a file
    *report_final_20210402.Rmd* and cursed your archiving skills?

Git will help with all that, and is worth to learn for that alone.


However, it becomes even more powerful when used with a online repository
such as Github to support **collaborative projects**. This facilitates:

-   Collaboration: others can review, comment on, and
    accept/decline changes

-   Sharing your code, data, and outputs, and invite feedback
    from the public (or privately, with your team)

and avoids:

-   "Oops, I forgot to send the last version and now you need to
    redo two days worth of work on this new file"

-   Mina, Henry and Oumar all worked at the same time on one script and
    need to manually merge their changes

-   Two people try to modify the same file on Dropbox and Sharepoint 
    and this creates a synchronization error.

### This sounds complicated, I am not a programmer {-}

It can be. Examples of advanced uses can be quite scary. However, much
like R, or even Excel, you don't need to become an expert to reap the
benefits of the tool. Learning a *small number of functions and notions*
lets you track your changes, synchronize your files on a online
repository and collaborate with your colleagues in a very short amount
of time.

Due to the learning curve, emergency context may not be the best of time
to learn these tools. But learning can be achieved by steps. Once you acquire 
a couple of notions, your workflow can be quite efficient and fast.
If you are not working on a project where collaborating with people
through Git is a necessity, **it is actually a good time to get
confident using it** in solo before diving in collaboration.

## Setup

### Install Git {.unnumbered}

*Git* is the engine behind the scenes on your computer, which tracks
changes, branches (versions), merges, and reverting. **You must first
install *Git* from <https://git-scm.com/downloads>.**

### Install an interface (optional but recommended) {.unnumbered}

Git has its own language of commands, which can be typed into a command
line terminal. However, there are many clients/interfaces and as non-developpers, in your
day-to-day use, you will rarely _need_ to interact with Git directly and 
interface usually provide nice visualisation tools for file modifications or branches. 

Many options exist, on all OS, from beginner friendly to more complex ones. 
Good options for beginners include the RStudio Git pane and 
[Github Desktop](https://desktop.github.com/), which we will showcase in 
this chapter.
Intermediate (more powerfull, but more complex) options include Source Tree, 
Gitkracken, Smart Git and others.

Quick explanation on [Git clients](-%09https:/happygitwithr.com/git-client.html#git-client).

*Note: since interfaces actually all use Git internally, you can try several of
them, switch from one to another on a given project, use the console punctually 
for an action your interface does not support, or even perform any number of 
actions online on Github.*

As noted below, you may occasionally have to write Git commands into a
terminal such as the RStudio terminal pane (a tab adjacent to the R
Console) or the Git Bash terminal.


### Github account {.unnumbered}

Sign-up for a free account at [github.com](github.com).

You may be offered to set-up two-factor authentication with an app on
your phone. Read more in the Github [help
documents](https://docs.github.com/en/github/authenticating-to-github/securing-your-account-with-two-factor-authentication-2fa).

If you use Github Desktop, you can enter your Gitub credentials after
installation following these
[steps](https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/authenticating-to-github).
If you don't do it know, credentials will be asked later when you try to
clone a project from Github.

## Vocabulary, concepts and basic functions

As when learning R, there is a bit of vocabulary to remember to
understand Git. Here are the [basics to get you
going](https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/)
/ [interactive tutorial](learngitbranching.js.org). In the next
sections, we will show how to use interfaces, but it is good
to have the vocabulary and concepts in mind, to build your mental model,
and as you'll need them when using interfaces anyway.

### Repository {.unnumbered}

A Git *repository* ("*repo*") is a folder that contains all the
sub-folders and files for your project (data, code, images, etc.) and
their revision histories. When you begin tracking changes in the
repository with it, Git will create a hidden folder that contains
all tracking information. A typical Git repository is
your *R Project* folder (see handbook page on [R projects]).

We will show how to create (_initialize_) a Git repository 
from Github, Github Desktop or Rstudio in the next
sections.

### Commits {.unnumbered}

A *commit* is a **snapshot** of the project at a given time. 
When you make a change to the project, you will make a new commit
to track the changes (the delta) made to your
files. For example, perhaps you edited some lines of code and updated a
related dataset. Once your changes are saved, you can bundle these
changes together into one "commit".

Each commit has a unique ID (a *hash*). For version control purposes,
you can revert your project back in time based on commits, so it is best
to keep them relatively small and coherent. You will also attach a brief
description of the changes called the "commit message".

*Staged changes*? To stage changes is to add them to the *staging area*
in preparation for the next commit. The idea is that you can finely
decide which changes to include in a given commit. For example, if you
worked on model specification in one script, and later on a figure in
another script, it would make sense to have two different commits (it would be easier
in case you wanted to revert the changes on the figure but not the model).


### Branches {.unnumbered}

A branch represents an *independent line* of changes in your repo, a
parallel, alternate version of your project files. 


Branches are useful to test changes before they are incorporated into
the *main* branch, which is usually the primary/final/"live" version of
your project. When you are done experimenting on a branch, you can bring
the changes into your *main* branch, by *merging* it, or delete it, if
the changes were not so successful.

*Note: you do not have to collaborate with other people to use branches,
nor need to have a remote online repository.*



### Local and remote repositories {.unnumbered}

To *clone* is to create a copy of a Git repository in another place.

For example, you can *clone* a online repository _from_ Github locally on
your computer, or begin with a local repository and clone
it online _to_ Github.

When you have cloned a repository, the project files exist in
two places:

-   the *LOCAL* repository on your physical computer. This
    is where you make the actual changes to the files/code.

-   the *REMOTE*, online repository: the versions of your project files
    in the Github repository (or on any other web
    host).

To synchronize these repositories, we will use more functions. Indeed,
unlike Sharepoint, Dropbox or other synchronizing software, Git does
not automatically update your local repository based or what's online,
or vice-versa. You get to choose when and how to synchronize.

-   `git fetch` downloads the new changes from the remote repository but does not 
change your local repository. Think of it as checking the state of the remote repository.

-   `git pull` downloads the new changes from the remote repositories
    and update your local repository.

-   When you have made one or several commits locally, you can
    `git push` the commits to the remote repository. This sends your
    changes on Github so that other people can see and pull them if
    they want to.


## Get started: create a new repository

There are many ways to create new repositories. You can do it from the
console, from Github, from an interface.

Two general approaches to set-up are:


-   Create a new R Project from an existing or new Github repository
    (*preferred for beginners*), or
-   Create a Github repository for an existing R project


### Start-up files {.unnumbered}

When you create a new repository, you can optionally create 
all of the below files, or you can add them to your repository at a later stage.
They would typically live in the "root" folder of the repository.

-   A *README* file is a file that someone can read to understand why
    your project exists and what else they should know to use it. It
    will be empty at first, but you should complete it later.

-   A *.gitignore* file is a text file where each line would contain
    folders or files that Git should ignore (not track changes). Read
    more about it and see examples
    [here](https://www.freecodecamp.org/news/gitignore-what-is-it-and-how-to-add-to-repo/).

-   You can choose a *license* for your work, so that other people
    know under which conditions they can use or reproduce your work. For more
    information, see the [Creative Commons
    licenses](https://creativecommons.org/licenses/).

### Create a new repository in Github {.unnumbered}

To create a new repository, log into Github and look for the green
button to create a new repository. This now empty repository can be
cloned locally to your computer (see next section).

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_new.png"))
```

You must choose if you want your repository to be **public** (visible to
everyone on the internet) or **private** (only visible to those with
permission). This has important implications if your data are sensitive.
If your repository is private you will encounter some quotas in advanced
special circumstances, such as if you are using Github *actions* to
automatically run your code in the cloud.
 
### Clone from a Github repository {.unnumbered}

You can *clone* an existing Github repository to create
a new local R project on your computer.

The Github repository could be one that already exists and contains
content, or could be an empty repository that you just created. In this
latter case you are essentially creating the Github repo and local R
project at the same time (see instructions above).

_Note_: if you do not have contributing rights on a Github repository, 
it is possible to first _fork_ the repository to your profile, and then
proceed with the other actions. Forking is explained at the end of this 
chapter, but we recommend that you read the other sections first.

Step 1: Navigate in Github to the repository, click on the green "**Code**"
button and copy the **HTTPS clone URL** (see image below)

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone.png"))
```

The next step can be performed in any interface. We will illustrate with
Rstudio and Github desktop.

#### In Rstudio {.unnumbered}

In RStudio, start a new R project by clicking *File \> New Project \>
Version Control \> Git*

-   When prompted for the "Repository URL", paste the HTTPS URL from
    Github\
-   Assign the R project a short, informative name\
-   Choose where the new R Project will be saved locally\
-   Check "Open in new session" and click "Create project"


You are now in a new, local, RStudio project that is a clone of the
Github repository. This local project and the Github repository are now
linked.

#### In Github Desktop {.unnumbered}

-   Click on *File \> Clone a repository*

-   Select the URL tab

-   Paste the HTTPS URL from Github in the first box

-   Select the folder in which you want to have your local repository

-   Click "CLONE"

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone_desktop.png"))
```

### New Github repo from existing R project {.unnumbered}

An alternative setup scenario is that you have an existing R project
with content, and you want to create a Github repository for it.

1)  Create a new, empty Github repository for the project (see
    instructions above)\
2)  Clone this repository locally (see HTTPS instructions above)\
3)  Copy all the content from your pre-existing R
    project (codes, data, etc.) into this new empty, local, repository (e.g. use copy and paste).\
4)  Open your new project in RStudio, and go to the Git pane. The new files should
    register as file changes, now tracked by Git. Therefore, you can
    bundle these changes as a *commit* and *push* them up to Github.
    Once *pushed*, the repository on Github will reflect all the files.
    
See the Github workflow section below for details on this process.

### What does it look like now? {.unnumbered}

#### In RStudio {-}

Once you have cloned a Github repository to a new R project, 
you now see in RStudio a "Git" tab. This tab appears in the same RStudio pane
as your R Environment:

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Git_console.png"))
```

Please note the buttons circled in the image above, as they will be
referenced later (from left to right):

-   Button to *commit* the saved file changes to the local
    branch (this will open a new window)
-   Blue arrow to *pull* (update your local version of the branch with
    any changes made on the remote/Github version of that branch)
-   Green arrow to *push* (send any commits/changes for your local
    version of the branch to the remote/Github version of that branch)
-   The Git tab in RStudio
-   Button to create a NEW branch using whichever local branch is shown
    to the right as the base. *You almost always want to branch off from
    the main branch (after you first pull to update the main branch)*
-   The branch you are currently working in
-   Changes you made to code or other files will appear below

#### In Github Desktop {-}

Github Desktop is an independent application that allows you to manage
all your repositories. When you open it, the interface allows you to
choose the repository you want to work on, and then to perform basic Git
actions from there.

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_interface.png"))
```


## Git + Github workflow

### Process overview {.unnumbered}

Once you have completed the setup (described above), you will have a
Github repo that is connected (*cloned*) to a local R project. The
*main* branch (created by default) is the so-called "live" version of
*all* the files. When you want to make modifications, it is a good
practice to create a *new branch* from the *main* branch (like "Make a
Copy"). This is a typical workflow in Git because creating a branch is
easy and fast.


A typical workflow is as follow:

1.  Make sure that your local repository is up-to-date, update it if
    not

2.  Go to the branch you were working on previously, or create a new
    branch to try out some things


3.  Work on the files locally on your computer, make one or several
    commits to this branch

4.  Update the remote version of the branch with your changes (push)

5.  When you are satisfied with your branch, you can merge the online
    version of the working branch into the online "main" branch to
    transfer the changes

Other team members may be doing the same thing with their own branches,
or perhaps contributing commits into your working branch as well. 

We go through the above process step-by-step in more detail below.
Here is a schematic we've developed - it's in the format of a two-way
table so it should help epidemiologists understand.

```{r echo=F, out.height='150%', out.width='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_table.png"))
```

Here's [another diagram](https://build5nines.com/introduction-to-git-version-control-workflow/).

*Note: until recently, the term "master" branch was used, but it is now
referred to as "main" branch.*

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "GitHub-Flow.png"))
```

Image
[source](https://build5nines.com/introduction-to-git-version-control-workflow/)

## Create a new branch

When you select a branch to work on, **Git resets your working directory
the way it was the last time you were on this branch**.

### In Rstudio Git pane {.unnumbered}

Ensure you are in the "main" branch, and then click on the purple icon to
create a new branch (see image above).

-   You will be prompted to name your branch with a one-word descriptive
    name (can use underscores if needed).
-   You will see that locally, you are still in the same R project, but
    you are no longer working on the "main" branch.
-   Once created, the new branch will also appear in the Github website
    as a branch.
    
You can visualize branches in the Git Pane in Rstudio after clicking on "History"

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_rstudio_branchs.png"))
```


### In Github Desktop {.unnumbered}

The process is very much similar, you are prompted to give your branch
a name. After, you will be prompted to "Publish you branch to Github" to
make the new branch appear in the remote repo as well.


```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_new_branch.png"))
```

### In console {.unnumbered}

What is actually happening behind the scenes is that you create a new
branch with `git branch`, then go to the branch with
`git checkout` (_i.e._ tell Git that your next commits will occur there). 
From your git repository:

```{bash, eval = FALSE}
git branch my-new-branch  # Create the new branch branch
git checkout my-new-branch # Go to the branch
git checkout -b my-new-branch # Both at once (shortcut)
```


For more information about using the console, see the section on
Git commands at the end.

## Commit changes

Now you can edit code, add new files, update datasets, etc.


Every one of your changes is tracked, *once the respective file is
saved*. Changed files will appear in the RStudio Git tab, in Github
Desktop, or using the command `git status` in the terminal (see below).

Whenever you make substantial changes (e.g. adding or updating a section of
code), pause and *commit* those changes. Think of a commit as a "batch"
of changes related to a common purpose. You can always continue to
revise a file after having committed changes on it.

*Advice on commits*: generally, it is better to make small commits, that
can be easily reverted if a problem arises, to commit together
modifications related to a common purpose. To achieve this, you will
find that *you should commit often*. At the beginning, you'll probably
forget to commit often, but then the habit kicks in.

### In Rstudio {.unnumbered}

The example below shows that, since the last commit, the R Markdown script "collaboration.Rmd" has changed, 
and several PNG images were added.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking2.png"))
```

You might be wondering what the yellow, blue, green, and red squares next to
the file names represent. Here is a snapshot from the [RStudio
cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)
that explains their meaning. Note that changes with yellow "?" can still
be staged, committed, and pushed.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking.png"))
```

-   Press the "Commit" button in the Git tab, which will open a new
    window (shown below)

-   Click on a file name in the upper-left box

-   Review the changes you made to that file (highlighted below in green
    or red)

-   "Stage" the file, which will include those changes in the commit. Do
    this by checking the box next to the file name. Alternatively, you
    can highlight multiple file names and then click "Stage"

-   Write a commit message that is short but descriptive (required)

-   Press the "Commit" button. A pop-up box will appear showing success
    or an error message.


Now you can make more changes and more commits, as many times as you would like

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_commit.png"))
```

### In Github Desktop {.unnumbered}

You can see the list of the files that were changed on the left. If
you select a text file, you will see a summary of the modifications that were made
in the right pane (the view will not work on more complex files like .docs or .xlsx).

To stage the changes, just tick the little box near file names. When you
have selected the files you want to add to this commit, give the commit
a name, optionally a description and then click on the **commit**
button.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_commit.png"))
```

### In console {.unnumbered}

The two functions used behind the scenes are `git add` to select/stage
files and `git commit` to actually do the commit.

```{bash, eval = FALSE}
git status # see the changes 

git add new_pages/collaboration.Rmd  # select files to commit (= stage the changes)

git commit -m "Describe commit from Github Desktop" # commit the changes with a message

git log  # view information on past commits
```


### Amend a previous commit {.unnumbered}

What happens if you commit some changes, carry on working, and realize
that you made changes that should "belong" to the past commit (in your opinion). 
Fear not! You can append these changes to your previous commit.

In Rstudio, it should be pretty obvious as there is a "Amend previous commit" 
box on the same line as the COMMIT button. 

For some unclear reason, the functionality has not been implemented 
as such in Github Desktop, but there is a (conceptually awkward but easy)
way around. If you have committed **but not pushed** your changes yet, 
an "UNDO" button appears just under the COMMIT button. Click on it and 
it will revert your commit (but keep your staged files and your commit message). 
Save your changes, add new files to the commit if necessary and commit again.

In the console:  

```{bash, eval = FALSE}
git add [YOUR FILES] # Stage your new changes

git commit --amend  # Amend the previous commit

git commit --amend -m "An updated commit message"  # Amend the previous commit AND update the commit message
```


_Note: think before modifying commits that are already public and shared with your collaborators_.


## Pull and push changes up to Github

"First PULL, then PUSH"

It is good practice to *fetch* and *pull* before you begin working on
your project, to update the branch version on your local computer with
any changes that have been made to it in the remote/Github version.

PULL often. Don't hesitate. *Always pull before pushing*.

When your changes are made and committed and you are happy with the 
state of your project, you can *push* your commits up
to the remote/Github version of your branch.


Rince and repeat while you are working on the repository.

**Note:** it is much easier to revert changes that were committed but not 
pushed (i.e. are still local) than to revert changes that were pushed to the
remote repository (and perhaps already pulled by someone else), so it is better 
to push when you are done with introducing changes on the task that 
you were working on.


#### In Rstudio {.unnumbered}

*PULL* - First, click the "Pull" icon (downward arrow) which fetches and
pulls at the same time.

*PUSH* - Clicking the green "Pull" icon (upward arrow). You may be asked
to enter your Github username and password. The first time you are
asked, you may need to enter two Git command lines into the *Terminal*:

-   **git config --global user.email
    "[you\@example.com](mailto:you@example.com){.email}"** (your Github
    email address), and\
-   **git config --global user.name "Your Github username"**

To learn more about how to enter these commands, see the section below
on Git commands.

***TIP:*** Asked to provide your password too often? See these chapters
10 & 11 of this
[tutorial](https://happygitwithr.com/credential-caching.html#credential-caching)
to connect to a repository using a SSH key (more
complicated)  


#### In Github Desktop {.unnumbered}

Click on the "Fetch origin" button to check if there are new commits on
the remote repository.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_fetch_button.png"))
```

If Git finds new commits on the remote repository, the button will
change into a "Pull" button. Because the same button is used to push and
pull, you cannot push your changes if you don't pull before.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_pull_button.png"))
```

You can go to the "History" tab (near the "Changes" tab) to see all
commits (yours and others). This is a nice way of acquainting yourself
with what your collaborators did. You can read the commit message, the
description if there is one, and compare the code of the two files using
the *diff* pane.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_history.png"))
```

Once all remote changes have been pulled, and at least one local change
has been committed, you can push by clicking on the same button.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_push_button.png"))
```

#### Console {.unnumbered}

Without surprise, the commands are *fetch*, *pull* and *push*.

```{bash, eval = FALSE}
git fetch  # are there new commits in the remote directory?
git pull   # Bring remote commits into your local branch
git push   # Puch local commits of this branch to the remote branch
```


### I want to pull but I have local work {.unnumbered}

This can happen sometimes: 
you made some changes on your local repository, but the remote
repository has commits that you didn't pull. 


Git will refuse to pull because it might overwrite your changes. 
There are several strategies to keep your changes, 
well described in [Happy Git with R](https://happygitwithr.com/pull-tricky.html), 
among which the two main ones are:
- commit your changes, fetch remote changes, pull them in, resolve conflicts 
if needed (see section below), and push everything online
- `stash` your changes, which sort of stores them aside, pull, unstash 
(restore), and then commit, solve any conflicts, and push. 

If the files concerned by the remote changes and the files concerned 
by your local changes do not overlap, Git may solve conflicts automatically.

In Github Desktop, this can be done with buttons. To stash, go to _Branch > Stash all changes_.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_stash.png"))
```



## Merge branch into Main 

If you have finished making changes, you can begin the process of
merging those changes into the main branch. Depending on your situation,
this may be fast, or you may have deliberate review and approval
steps involving teammates.

### Locally in Github Desktop {.unnumbered}

One can merge branches locally using Github Desktop. First, go to
(checkout) the branch that will be the recipient of the commits, in other words, the
branch you want to update. Then go to the menu *Branch \> Merge into
current branch* and click. A box will allow you to select the branch you
want to import from.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_merge.png"))
```

### In console {.unnumbered}

First move back to the branch that will be the recipient of the changes.
This is usually *master*, but it could be another branch. Then merge your
working branch into master.

```{bash, eval = FALSE}
git checkout master  # Go back to master (or to the branch you want to move your )
git merge this_fancy_new_branch
```

[This
page](https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging)
shows a more advanced example of branching and explains a bit what is
happening behind the scenes.

### In Github: submitting pull requests {.unnumbered}


While it is totally possible to merge two branches locally, or without
informing anybody, a merge may be discussed or investigated by several
people before being integrated to the master branch. To help with the
process, Github offers some discussion features around the merge: the
**pull request**.

A pull request (a "PR") is a request to merge one branch into another 
(in other words, a request that _your working branch be pulled into the "main" branch_). 
A pull request typically involves multiple commits. A pull request usually begins a conversation and review 
process before it is accepted and the branch is merged. For example, 
you can read pull request discussions on [dplyr's
github](https://github.com/tidyverse/dplyr/pulls).


You can submit a pull request (PR) directly form the website (as
illustrated bellow) or from Github Desktop.

-   Go to Github repository (online)
-   View the tab "Pull Requests" and click the "New pull request" button
-   Select from the drop-down menu to merge your branch into main
-   Write a detailed Pull Request comment and click "Create Pull
    Request".

In the image below, the branch "forests" has been selected to be merged
into "main":

```{r echo=F, out.width = '100%', out.height='150%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request2.png"))
```

Now you should be able to see the pull request (example image below):

-   Review the tab "Files changed" to see how the "main" branch would
    change if the branch were merged.\
-   On the right, you can request a review from members of your team by
    tagging their Github ID. If you like, you can set the repository
    settings to require one approving review in order to merge into
    main.\
-   Once the pull request is approved, a button to
    "Merge pull request" will become active. Click this.\
-   Once completed, delete your branch as explained below.

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request.png"))
```

### Resolving conflicts {.unnumbered}

When two people modified the same line(s) at the same time, a
merge conflict arises. Indeed, Git refuses to make a decision about
which version to keep, but it helps you find where the
conflict is. **DO NOT PANIC**. Most of the time, it is pretty straightforward
to resolve.

For example, on Github:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict2.png"))
```


After the merge raised a conflict, open the file in your favorite editor.
The conflict will be indicated by series of characters:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict3.png"))
```

The text between *\<\<\<\<\<\<\< HEAD* and *=======* comes from your
local repository, and the one between *=======* and *\>\>\>\>\>\>\>* from the
the other branch (which may be origin, master or any branch of
your choice).

You need to decide which version of the code you prefer (or even write a
third, including changes from both sides if pertinent), delete the rest
and remove all the marks that Git added *(\<\<\<\<\<\<\< HEAD, =======,
\>\>\>\>\>\>\> origin/master/your_branch_name*). 

Then, save the file, stage it and commit it : this is the commit 
that makes the merged version "official". Do not forget to push afterwards.

The more often you and your collaborators pull and push, the smaller the
conflicts will be.


*Note: If you feel at ease with the console, there are more [advanced
merging
options](https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging)
(e.g. ignoring whitespace, giving a collaborator priority etc.).*

### Delete your branch {.unnumbered}

Once a branch was merged into master and is no longer needed, you can
delete it.

#### Github + Rstudio

Go to the repository on Github and click the button to view all the
branches (next to the drop-down to select branches). Now find your
branch and click the trash icon next to it. Read more detail on deleting
a branch
[here](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository#deleting-a-branch).

Be sure to also delete the branch locally on your computer. This will
not happen automatically.

-   From RStudio, make sure you are in the Main branch
-   Switch to typing Git commands in the RStudio "Terminal" (the tab
    adjacent to the R console), and type: **git branch -d
    branch_name**, where "branch_name" is the name of your branch to be
    deleted
-   Refresh your Git tab and the branch should be gone


#### In Github Desktop

Just checkout the branch you want to delete, and go to the menu
*Branch \> Delete*.


### Forking {.unnumbered}

You can fork a project if you would like to contribute to it but 
do not have the rights to do so, or if you just 
want to modify it for your personal use. A 
short description of forking can be found [here](https://guides.github.com/activities/forking/).

On Github, click on the "Fork" button:  

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_1.png"))
```

This will clone the original repository, but in your own profile. So now, 
there are two versions of the repository **on Github**: the original one,
that you cannot modify, and the cloned version in your profile.

Then, you can proceed to clone your version of the online repository locally 
on your computer, using any of the methods described in previous sections. 
Then, you can create a new branch, make changes, commit and push them 
_to your remote repository_.

Once you are happy with the result you can create a Pull Request 
from Github or Github Desktop to begin the conversation with the 
owners/maintainers of the original repository.


**What if you need some newer commits from the official repository?**

Imagine that someone makes a critical modification to the official repository,
which you want to include to your cloned version.
It is possible to synchronize your fork with the official repository. 
It involves using the terminal, but it is not too complicated. 
You mostly need to remember that:
- _upstream_ = the official repository, the one that you could not modify
- _origin_ = your version of the repository on your Github profile


You can read [this tutorial](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork) or follow along below: 


First, type in your Git terminal (inside your repo):  

```{bash, eval = FALSE}
git remote -v
```
 
If you have not yet configured the upstream repository you should 
see two lines, beginning by _origin_. They show the remote repo 
that `fetch` and `push` point to. Remember, _origin_ is the conventional
nickname for your own version of the repository on Github. For example:  

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_2.png"))
```

Now, add a new remote repository:  

```{bash, eval = FALSE}
git remote add upstream https://github.com/appliedepi/epirhandbook_eng.git
```
 
Here the address is the address that Github generates when you clone
a repository (see section on cloning). Now you will have four remote pointers:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_3.png"))
```

Now that the setup is done, whenever you want to get the changes from 
the original (_upstream_) repository, you just have to go (_checkout_) to 
the branch you want to update and type:

```{bash, eval = FALSE}
git fetch upstream # Get the new commits from the remote repository
git checkout the_branch_you_want_to_update
git merge upstream/the_branch_you_want_to_update  # Merge the upstream branch into your branch.
git push # Update your own version of the remote repo
```

If there are conflicts, you will have to solve them, as explained 
in the Resolving conflicts section. 


**Summary**: forking is cloning, but on the Github server side. 
The rest of the actions are typical collaboration workflow actions 
(clone, push, pull, commit, merge, submit pull requests...).

_Note: while forking is a concept, not a Git command, it also exist on other Web hosts, like [Bitbucket](https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow)._


```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_4.png"))
```


## What we learned

You have learned how to:  

- setup Git to track modifications in your folders,  
- connect your local repository to a remote online repository,  
- commit changes,  
- synchronize your local and remote repositories.  

All this should get you going and be enough for most of your needs as 
epidemiologists. We usually do not have as advanced usage as developers. 

However, know that should you want (or need) to go further, Git offers more power to simplify 
commit histories, revert one or several commits, cherry-pick commits, etc. 
Some of it may sound like pure wizardry, but now that you have the basics, 
it is easier to build on it.


Note that while the Git pane in Rstudio and Github Desktop are good for 
beginners / day-to-day usage in our line of work, they do not offer an 
interface to some of the intermediate / advanced Git functions. 
Some more complete interfaces allows you to do more with point-and-click 
(usually at the cost of a more complex layout). 

Remember that since you can use any tool at any point to track your repository, 
you can very easily install an interface to try it out sometimes, 
or to perform some less common complex task occasionally, 
while preferring a simplified interface for the rest of time (e.g. using 
Github Desktop most of the time, and switching to SourceTree or Gitbash for some specific tasks).


## Git commands {#git}


### Recommended learning {.unnumbered}

To learn Git commands in an interactive tutorial, see [this
website](https://learngitbranching.js.org/).

### Where to enter commands {.unnumbered}

You enter commands in a Git shell.

*Option 1* You can open a new Terminal in RStudio. This tab is next to
the R Console. If you cannot type any text in it, click on the
drop-down menu below "Terminal" and select "New terminal". Type the
commands at the blinking space in front of the dollar sign "\$".

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_terminal.png"))
```

*Option 2* You can also open a *shell* (a terminal to enter commands) by
clicking the blue "gears" icon in the Git tab (near the RStudio
Environment). Select "Shell" from the drop-down menu. A new window will
open where you can type the commands after the dollar sign "\$".

*Option 3* Right click to open "Git Bash here" which will open the same
sort of terminal, or open *Git Bash* form your application list.
[More beginner-friendly informations on Git Bash](https://happygitwithr.com/shell.html), 
how to find it and some bash commands you will need.

### Sample commands {.unnumbered}

Below we present a few common git commands. When you use them, keep in mind
which branch is active (checked-out), as that will change the action!

In the commands below, <name> represents a branch name. 
<commit_hash> represents the hash ID of a specific
commit. <num> represents a number. Do not type the
\< or \> symbols.

| Git command              | Action                                                                   |
|--------------------------|--------------------------------------------------------------------------|
| `git branch <name>`      | Create a new branch with the name <name>                                 |
| `git checkout <name>`    | Switch current branch to <name>                                          |
| `git checkout -b <name>` | Shortcut to create new branch *and* switch to it                         |
| `git status`             | See untracked changes                                                    |
| `git add <file>`         | Stage a file                                                             |
| `git commit -m <message>`| Commit currently staged changes to current branch with message |
| `git fetch`              | Fetch commits from remote repository                                     |
| `git pull`               | Pull commits from remote repository in current branch                    |
| `git push`               | Push local commits to remote directory                          |
| `git switch`             | An alternative to `git checkout` that is being phased in to Git |
| `git merge <name>`       | Merge <name> branch into current branch                         |
| `git rebase <name>`      | Append commits from current branch on to <name> branch          |



<!-- ======================================================= -->

## Resources

Much of this page was informed by [this "Happy Git with R"
website](https://happygitwithr.com/) by Jenny Bryan. There is a very helpful
section of this website that helps you troubleshoot common Git and
R-related errors.

The [Github.com documentation and start
guide](https://docs.github.com/en/github).

The RStudio ["IDE"
cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)
which includes tips on Git with RStudio.

<https://ohi-science.org/news/github-going-back-in-time>

**Git commands for beginners**

An [interactive
tutorial](learngitbranching.js.org) to learn
Git commands.

<https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/>:
good for learning the absolute basics to track changes in one folder on
you own computer.

Nice schematics to understand branches:
<https://speakerdeck.com/alicebartlett/git-for-humans>


**Tutorials covering both basic and more advanced subjects**

<https://tutorialzine.com/2016/06/learn-git-in-30-minutes>

<https://dzone.com/articles/git-tutorial-commands-and-operations-in-git>
<https://swcarpentry.github.io/git-novice/> (short course)
<https://rsjakob.gitbooks.io/git/content/chapter1.html>

The [Pro Git book](https://git-scm.com/book/en/v2) is considered an official reference. 
While some chapters are ok, it is usually a bit _technical_. It is probably a good resource 
once you have used Git a bit and want to learn a bit more precisely 
what happens and how to go further.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/collaboration.Rmd-->


# Common errors  

This page includes a running list of common errors and suggests solutions for troubleshooting them.  


## Interpreting error messages  

R errors can be cryptic at times, so Google is your friend. Search the error message with "R" and look for recent posts in [StackExchange.com](StackExchange.com), [stackoverflow.com](stackoverflow.com), [community.rstudio.com](community.rstudio.com), twitter (#rstats), and other forums used by programmers to filed questions and answers. Try to find recent posts that have solved similar problems.  

If after much searching you cannot find an answer to your problem, consider creating a *reproducible example* ("reprex") and posting the question yourself. See the page on [Getting help] for tips on how to create and post a reproducible example to forums. 


## Common errors  

Below, we list some common errors and potential explanations/solutions. Some of these are borrowed from Noam Ross who analyzed the most common forum posts on Stack Overflow about R error messages (see analysis [here](https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md))  


### Typo errors {.unnumbered}  

```
Error: unexpected symbol in:
"  geom_histogram(stat = "identity")+
  tidyquant::geom_ma(n=7, size = 2, color = "red" lty"
```
If you see "unexpected symbol", check for missing commas  



### Package errors {.unnumbered}  

```
could not find function "x"...
```
This likely means that you typed the function name incorrectly, or forgot to install or load a package.


```
Error in select(data, var) : unused argument (var)
```
You think you are using `dplyr::select()` but the `select()` function has been masked by `MASS::select()` - specify `dplyr::` or re-order your package loading so that dplyr is after all the others.

Other common masking errors stem from: `plyr::summarise()` and `stats::filter()`. Consider using the [**conflicted** package](https://www.tidyverse.org/blog/2018/06/conflicted/).




```
Error in install.packages : ERROR: failed to lock directory ‘C:\Users\Name\Documents\R\win-library\4.0’ for modifying
Try removing ‘C:\Users\Name\Documents\R\win-library\4.0/00LOCK’
```

If you get an error saying you need to remove an "00LOCK" file, go to your "R" library in your computer directory (e.g. R/win-library/) and look for a folder called "00LOCK". Delete this manually, and try installing the package again. A previous install process was probably interrupted, which led to this.  




### Object errors {.unnumbered}  

```
No such file or directory:
```
If you see an error like this when you try to export or import: Check the spelling of the file and filepath, and if the path contains slashes make sure they are forward `/` and not backward `\`. Also make sure you used the correct file extension (e.g. .csv, .xlsx).  


```
object 'x' not found 
```
This means that an object you are referencing does not exist. Perhaps code above did not run properly?  


```
Error in 'x': subscript out of bounds
```
This means you tried to access something (an element of a vector or a list) that was not there.  




### Function syntax errors {.unnumbered}

```
# ran recode without re-stating the x variable in mutate(x = recode(x, OLD = NEW)
Error: Problem with `mutate()` input `hospital`.
x argument ".x" is missing, with no default
i Input `hospital` is `recode(...)`.
```
This error above (`argument .x is missing, with no default`) is common in `mutate()` if you are supplying a function like `recode()` or `replace_na()` where it expects you to provide the column name as the first argument. This is easy to forget.  



### Logic errors {.unnumbered}  

```
Error in if
```

This likely means an `if` statement was applied to something that was not TRUE or FALSE.  


### Factor errors {.unnumbered}  

```
#Tried to add a value ("Missing") to a factor (with replace_na operating on a factor)
Problem with `mutate()` input `age_cat`.
i invalid factor level, NA generated
i Input `age_cat` is `replace_na(age_cat, "Missing")`.invalid factor level, NA generated
```
If you see this error about invalid factor levels, you likely have a column of class Factor (which contains pre-defined levels) and tried to add a new value to it. Convert it to class Character before adding a new value.  


### Plotting errors {.unnumbered}  

`Error: Insufficient values in manual scale. 3 needed but only 2 provided.`
ggplot() scale_fill_manual() values = c("orange", "purple") ... insufficient for number of factor levels ... consider whether NA is now a factor level...

```
Can't add x object
```
You probably have an extra `+` at the end of a ggplot command that you need to delete.


### R Markdown errors {.unnumbered}  

If the error message contains something like `Error in options[[sprintf("fig.%s", i)]]`, check that your knitr options at the top of each chunk correctly use the `out.width = ` or `out.height = ` and *not* `fig.width=` and `fig.height=`.

### Miscellaneous {.unnumbered}  

Consider whether you re-arranged piped **dplyr** verbs and didn't replace a pipe in the middle, or didn't remove a pipe from the end after re-arranging.

 


<!-- ======================================================= -->
## Resources { }

This is another blog post that lists common [R programming errors faced by beginners](https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/errors.Rmd-->


# Getting help  

This page covers how to get help by posting a Github issue or by posting a reproducible example ("reprex") to an online forum.  




## Github issues  

Many R packages and projects have their code hosted on the website Github.com. You can communicate directly with authors via this website by posting an "Issue".  

Read more about how to store your work on Github in the page [Collaboration and Github]. 

On Github, each project is contained within a *repository*. Each repository contains code, data, outputs, help documentation, etc. There is also a vehicle to communicate with the authors called "Issues".  

See below the Github page for the **incidence2** package (used to make epidemic curves). You can see the "Issues" tab highlighted in yellow. You can see that there are 5 open issues.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues.png"))
```

Once in the Issues tab, you can see the open issues. Review them to ensure your problem is not already being addressed. You can open a new issue by clicking the green button on the right. You will need a Github account to do this. 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues2.png"))
```


  
In your issue, follow the instructions below to provide a minimal, reproducible example. And please be courteous! Most people developing R packages and projects are doing so in their spare time (like this handbook!).  

To read more advanced materials about handling issues in your own Github repository, check out the Github [documentation on Issues](https://guides.github.com/features/issues/).  



## Reproducible example  

Providing a reproducible example ("reprex") is key to getting help when posting in a forum or in a Github issue. People want to help you, but you have to give them an example that they can work with on their own computer. The example should:  

* Demonstrate the problem you encountered  
* Be *minimal*, in that it includes only the data and code required to reproduce your problem  
* Be *reproducible*, such that all objects (e.g. data), package calls (e.g. `library()` or `p_load()`) are included

*Also, be sure you do not post any sensitive data with the reprex!* You can create example data frames, or use one of the data frames built into R (enter `data()` to open a list of these datasets).  



### The **reprex** package {.unnumbered}  

The **reprex** package can assist you with making a reproducible example:  

1) **reprex** is installed with **tidyverse**, so load either package  

```{r, eval=F}
# install/load tidyverse (which includes reprex)
pacman::p_load(tidyverse)
```

2) Begin an R script that creates your problem, step-by-step, starting from loading packages and data.  

```{r, eval=F}
# load packages
pacman::p_load(
     tidyverse,  # data mgmt and vizualization
     outbreaks)  # example outbreak datasets

# flu epidemic case linelist
outbreak_raw <- outbreaks::fluH7N9_china_2013  # retrieve dataset from outbreaks package

# Clean dataset
outbreak <- outbreak_raw %>% 
     mutate(across(contains("date"), as.Date))

# Plot epidemic

ggplot(data = outbreak)+
     geom_histogram(
          mapping = aes(x = date_of_onset),
          binwidth = 7
     )+
  scale_x_date(
    date_format = "%d %m"
  )

```
*Copy* all the code to your clipboard, and run the following command:  

```{r, eval=F}
reprex::reprex()
```

You will see an HTML output appear in the RStudio Viewer pane. It will contain all your code and any warnings, errors, or plot outputs. This output is also copied to your clipboard, so you can post it directly into a Github issue or a forum post.  

```{r, out.width=c('100%', '100%'), warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_reprex_RStudio1.png"))
```


* If you set `session_info = TRUE` the output of `sessioninfo::session_info()` with your R and R package versions will be included  
* You can provide a working directory to `wd = `  
* You can read more about the arguments and possible variations at the [documentation]() or by entering `?reprex`

In the example above, the `ggplot()` command did not run because the arguemnt `date_format =` is not correct - it should be `date_labels = `.  


### Minimal data {.unnumbered}  

The helpers need to be able to use your data - ideally they need to be able to create it *with code*.  

To create a minumal dataset, consider anonymising and using only a subset of the observations. 

UNDER CONSTRUCTION - you can also use the function `dput()` to create minimal dataset.  




## Posting to a forum  

Read lots of forum posts. Get an understanding for which posts are well-written, and which ones are not.  

1) First, decide whether to ask the question at all. Have you *thoroughly* reviewed the forum website, trying various search terms, to see if your question has already been asked?  

2) Give your question an informative title (not "Help! this isn't working").  

3) Write your question:  

* Introduce your situation and problem  
* Link to posts of similar issues and explain how they do not answer your question  
* Include any relevant information to help someone who does not know the context of your work  
* Give a minimal reproducible example with your R session information  
* Use proper spelling, grammar, punctuation, and break your question into paragraphs so that it is easier to read  

4) Monitor your question once posted to respond to any requests for clarification. Be courteous and gracious - often the people answering are volunteering their time to help you. If you have a follow-up question consider whether it should be a separate posted question.  

5) Mark the question as answered, *if* you get an answer that meets the *original* request. This helps others later quickly recognize the solution.  


Read these posts about [how to ask a good question](https://stackoverflow.com/help/how-to-ask) the [Stack overflow code of conduct](https://stackoverflow.com/conduct).  


<!-- ======================================================= -->
## Resources { }


Tidyverse page on how to [get help!](https://www.tidyverse.org/help/#:~:text=When%20you%20want%20to%20make,to%20load%20the%20reprex%20package.&text=Enter%20reprex()%20in%20the,preview%20of%20your%20rendered%20reprex.)

Tips on [producing a minimal dataset](https://xiangxing98.github.io/R_Learning/R_Reproducible.nb.html#producing-a-minimal-dataset)

Documentation for the [dput function](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/dput)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/help.Rmd-->


# R on network drives { }  

 


<!-- ======================================================= -->
## Overview {  }

Using R on network or "company" shared drives can present additional challenges. This page contains approaches, common errors, and suggestions on troubleshooting gained from our experience working through these issues. These include tips for the particularly delicate situations involving R Markdown.  


**Using R on Network Drives: Overarching principles**  

1) You must get administrator access for your computer. Setup RStudio specifically to run as administrator.  
2) Save packages to a library on a lettered drive (e.g. "C:") when possible. Use a package library whose path begins with "\\\" as little as possible.  
3) the **rmarkdown** package must **not** be in a "\\\" package library, as then it can't connect to TinyTex or Pandoc.  




## RStudio as administrator  

When you click the RStudio icon to open RStudio, do so with a right-click. Depending on your machine, you may see an option to "Run as Administrator". Otherwise, you may see an option to select Properties (then there should appear a window with the option "Compatibility", and you can select a checkbox "Run as Administrator").  




## Useful commands 

Below are some useful commands when trying to troubleshoot issues using R on network drives.  

You can return the path(s) to package libraries that R is using. They will be listed in the order that R is using to install/load/search for packages. Thus, if you want R to use a different default library, you can switch the order of these paths (see below).  

```{r, eval=F}
# Find libraries
.libPaths()                   # Your library paths, listed in order that R installs/searches. 
                              # Note: all libraries will be listed, but to install to some (e.g. C:) you 
                              # may need to be running RStudio as an administrator (it won't appear in the 
                              # install packages library drop-down menu) 
```

You may want to switch the order of the package libraries used by R. For example if R is picking up a library location that begins with "\\\" and one that begins with a letter e.g. "D:". You can adjust the order of `.libPaths()` with the following code.  

````{r, eval=F}
# Switch order of libraries
# this can effect the priority of R finding a package. E.g. you may want your C: library to be listed first
myPaths <- .libPaths() # get the paths
myPaths <- c(myPaths[2], myPaths[1]) # switch them
.libPaths(myPaths) # reassign them
```

If you are having difficulties with R Markdown connecting to Pandoc, begin with this code to find out where RStudio thinks your Pandoc installation is.  

```{r, eval=F}
# Find Pandoc
Sys.getenv("RSTUDIO_PANDOC")  # Find where RStudio thinks your Pandoc installation is
```

If you want to see which library a package is loading from, try the below code:  

```{r, eval=F}
# Find a package
# gives first location of package (note order of your libraries)
find.package("rmarkdown", lib.loc = NULL, quiet = FALSE, verbose = getOption("verbose")) 
```



<!-- ======================================================= -->
## Troubleshooting common errors {  }


**"Failed to compile...tex in rmarkdown"**  

* Check the installation of TinyTex, or install TinyTex to C: location. See the [R basics] page on how to install TinyTex.  

```{r, eval=F}
# check/install tinytex, to C: location
tinytex::install_tinytex()
tinytex:::is_tinytex() # should return TRUE (note three colons)
```


**Internet routines cannot be loaded**  

For example, `Error in tools::startDynamicHelp() : internet routines cannot be loaded`  

* Try selecting 32-bit version from RStudio via Tools/Global Options.  
  * note: if 32-bit version does not appear in menu, make sure you are not using RStudio v1.2.  
* Alternatively, try uninstalling R and re-installing with different bit version (32 instead of 64)


**C: library does not appear as an option when I try to install packages manually**

* Run RStudio as an administrator, then this option will appear.  
* To set-up RStudio to always run as administrator (advantageous when using an Rproject where you don't click RStudio icon to open)... right-click the Rstudio icon 

The image below shows how you can manually select the library to install a package to. This window appears when you open the Packages RStudio pane and click "Install".  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "network_install.png"))
```

**Pandoc 1 error**  

If you are getting "pandoc error 1" when knitting R Markdowns scripts on network drives:  

* Of multiple library locations, have the one with a lettered drive listed first (see codes above)  
* The above solution worked when knitting on local drive but while on a networked internet connection  
* See more tips here: https://ciser.cornell.edu/rmarkdown-knit-to-html-word-pdf/  

**Pandoc Error 83**  

The error will look something like this: `can't find file...rmarkdown...lua...`. This means that it was unable to find this file.  

See https://stackoverflow.com/questions/58830927/rmarkdown-unable-to-locate-lua-filter-when-knitting-to-word  

Possibilities:  

1) Rmarkdown package is not installed  
2) Rmarkdown package is not findable  
3) An admin rights issue.  

It is possible that R is not able to find the **rmarkdown** package file, so check which library the **rmarkdown** package lives (see code above). If the package is installed to a library that in inaccessible (e.g. starts with "\\\") consider manually moving it to C: or other named drive library. Be aware that the **rmarkdown** package has to be able to connect to TinyTex installation, so can not live in a library on a network drive.


**Pandoc Error 61**  

For example: `Error: pandoc document conversion failed with error 61`  or `Could not fetch...`  

* Try running RStudio as administrator (right click icon, select run as admin, see above instructions)  
* Also see if the specific package that was unable to be reached can be moved to C: library.

**LaTex error (see below)**

An error like: `! Package pdftex.def Error: File 'cict_qm2_2020-06-29_files/figure-latex/unnamed-chunk-5-1.png' not found: using draft setting.` or `Error: LaTeX failed to compile file_name.tex.`  

* See https://yihui.org/tinytex/r/#debugging for debugging tips.  
* See file_name.log for more info.


**Pandoc Error 127**  

This could be a RAM (space) issue. Re-start your R session and try again. 


**Mapping network drives**

Mapping a network drive can be risky. Consult with your IT department before attempting this.  

A tip borrowed from this [forum discussion](https://stackoverflow.com/questions/48161177/r-markdown-openbinaryfile-does-not-exist-no-such-file-or-directory/55616529?noredirect=1#comment97966859_55616529): 

How does one open a file "through a mapped network drive"?  

* First, you'll need to know the network location you're trying to access.  
* Next, in the Windows file manager, you will need to right click on "This PC" on the right hand pane, and select "Map a network drive".  
* Go through the dialogue to define the network location from earlier as a lettered drive.  
* Now you have two ways to get to the file you're opening. Using the drive-letter path should work.  


**Error in install.packages()**  

If you get an error that includes mention of a "lock" directory, for example: `Error in install.packages : ERROR: failed to lock directory...`

Look in your package library and you will see a folder whose name begins with "00LOCK". Try the following tips:  

* Manually delete the "00LOCK" folder directory from your package library. Try installing the package again.  
* You can also try the command `pacman::p_unlock()` (you can also put this command in the Rprofile so it runs every time project opens.). Then try installing the package again. It may take several tries.  
* Try running RStudio in Administrator mode, and try installing the packages one-by-one.  
* If all else fails, install the package to another library or folder (e.g. Temp) and then manually copy the package's folder over to the desired library.  






```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/network_drives.Rmd-->


# Data Table { }  
     
The handbook focusses on the **dplyr** “verb” functions and the **magrittr** pipe operator `%>%` as a method to clean and group data, but the **data.table** package offers an alternative method that you may encounter in your R career.  



<!-- ======================================================= -->
## Intro to data tables {  }

A data table is a 2-dimensional data structure like a data frame that allows complex grouping operations to be performed. The data.table syntax is structured so that operations can be performed on rows, columns and groups. 

The structure is **DT[i, j, by]**, separated by 3 parts; the **i, j** and **by** arguments. The **i** argument allows for subsetting of required rows, the **j** argument allows you to operate on columns and the **by** argument allows you operate on columns by groups.
  
This page will address the following topics:  

* Importing data and use of `fread()` and `fwrite()`
* Selecting and filtering rows using the **i** argument
* Using helper functions `%like%`, `%chin%`, `%between%` 
* Selecting and computing on columns using the **j** argument
* Computing by groups using the **by** argument
* Adding and updating data to data tables using `:=`

<!-- ======================================================= -->
## Load packages and import data { }

### Load packages {.unnumbered}  

Using the `p_load()` function from **pacman**, we load (and install if necessary) packages required for this analysis.
     
     
```{r}
pacman::p_load(
  rio,        # to import data
  data.table, # to group and clean data
  tidyverse,  # allows use of pipe (%>%) function in this chapter
  here 
  ) 
```


### Import data {.unnumbered}

This page will explore some of the core functions of **data.table** using the case linelist referenced throughout the handbook.

We import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the [Download book and data] page. The dataset is imported using the `import()` function from the **rio** package. See the page on [Import and export] for various ways to import data. From here we use `data.table()` to convert the data frame to a data table.

```{r}
linelist <- rio::import(here("data", "linelist_cleaned.xlsx")) %>% data.table()
```

The `fread()` function is used to directly import regular delimited files, such as .csv files, directly to a data table format. This function, and its counterpart, `fwrite()`, used for writing data.tables as regular delimited files are very fast and computationally efficient options for large databases.


The first 20 rows of `linelist`:  

```{r message=FALSE, echo=F, eval=FALSE}
DT::datatable(head(linelist,20), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Base R commands such as `dim()` that are used for data frames can also be used for data tables

```{r}
dim(linelist) #gives the number of rows and columns in the data table
```


<!-- ======================================================= -->
## The i argument: selecting and filtering rows{ }
     
Recalling the **DT[i, j, by]** structure, we can filter rows using either row numbers or logical expressions. The i argument is first; therefore, the syntax **DT[i]** or **DT[i,]** can be used. 

The first example retrieves the first 5 rows of the data table, the second example subsets cases are 18 years or over, and the third example subsets cases 18 years old or over but not diagnosed at the Central Hospital:


```{r, eval=F}
linelist[1:5] #returns the 1st to 5th row
linelist[age >= 18] #subsets cases are equal to or over 18 years
linelist[age >= 18 & hospital != "Central Hospital"] #subsets cases equal to or over 18 years old but not diagnosed at the Central Hospital

```

Using .N in the i argument represents the total number of rows in the data table. This can be used to subset on the row numbers: 

```{r, eval=F}
linelist[.N] #returns the last row
linelist[15:.N] #returns the 15th to the last row
```


### Using helper functions for filtering {.unnumbered}  

Data table uses helper functions that make subsetting rows easy. The `%like%` function is used to match a pattern in a column, `%chin%` is used to match a specific character, and the `%between%` helper function is used to match numeric columns within a prespecified range.

In the following examples we:
*  filter rows where the hospital variable contains “Hospital”
*  filter rows where the outcome is “Recover” or “Death”
*  filter rows in the age range 40-60

```{r, eval=F}
linelist[hospital %like% "Hospital"] #filter rows where the hospital variable contains “Hospital”
linelist[outcome %chin% c("Recover", "Death")] #filter rows where the outcome is “Recover” or “Death”
linelist[age %between% c(40, 60)] #filter rows in the age range 40-60

#%between% must take a vector of length 2, whereas %chin% can take vectors of length >= 1

```

## The j argument: selecting and computing on columns{ }

Using the **DT[i, j, by]** structure, we can select columns using numbers or names. The **j** argument is second; therefore, the syntax **DT[, j]** is used. To facilitate computations on the **j** argument, the column is wrapped using either `list()` or `.()`. 


### Selecting columns {.unnumbered} 

The first example retrieves the first, third and fifth columns of the data table, the second example selects all columns except the height, weight and gender columns. The third example uses the `.()` wrap to select the **case_id** and **outcome** columns.


```{r, eval=F}
linelist[ , c(1,3,5)]
linelist[ , -c("gender", "age", "wt_kg", "ht_cm")]
linelist[ , list(case_id, outcome)] #linelist[ , .(case_id, outcome)] works just as well

```

### Computing on columns {.unnumbered} 

By combining the **i** and **j** arguments it is possible to filter rows and compute on the columns. Using **.N** in the **j** argument also represents the total number of rows in the data table and can be useful to return the number of rows after row filtering.

In the following examples we:
* Count the number of cases that stayed over 7 days in hospital
* Calculate the mean age of the cases that died at the military hospital
* Calculate the standard deviation, median, mean age of the cases that recovered at the central hospital

```{r}
linelist[days_onset_hosp > 7 , .N]
linelist[hospital %like% "Military" & outcome %chin% "Death", .(mean(age, na.rm = T))] #na.rm = T removes N/A values
linelist[hospital == "Central Hospital" & outcome == "Recover", 
                 .(mean_age = mean(age, na.rm = T),
                   median_age = median(age, na.rm = T),
                   sd_age = sd(age, na.rm = T))] #this syntax does not use the helper functions but works just as well

```

Remember using the .() wrap in the j argument facilitates computation, returns a data table and allows for column naming.

## The by argument: computing by groups{ }

The **by** argument is the third argument in the **DT[i, j, by]** structure. The **by** argument accepts both a character vector and the `list()` or `.()` syntax. Using the `.()` syntax in the **by** argument allows column renaming on the fly.

In the following examples we:	
* group the number of cases by hospital
* in cases 18 years old or over, calculate the mean height and weight of cases according to gender and whether they recovered or died
* in admissions that lasted over 7 days, count the number of cases according to the month they were admitted and the hospital they were admitted to


````{r}
linelist[, .N, .(hospital)] #the number of cases by hospital
linelist[age > 18, .(mean_wt = mean(wt_kg, na.rm = T),
                             mean_ht = mean(ht_cm, na.rm = T)), .(gender, outcome)] #NAs represent the categories where the data is missing
linelist[days_onset_hosp > 7, .N, .(month = month(date_hospitalisation), hospital)]

```

Data.table also allows the chaining expressions as follows:

````{r}

linelist[, .N, .(hospital)][order(-N)][1:3] #1st selects all cases by hospital, 2nd orders the cases in descending order, 3rd subsets the 3 hospitals with the largest caseload


```

In these examples we are following the assumption that a row in the data table is equal to a new case, and so we can use the **.N** to represent the number of rows in the data table. Another useful function to represent the number of unique cases is `uniqueN()`, which returns the number of unique values in a given input. This is illustrated here:

````{r}

linelist[, .(uniqueN(gender))] #remember .() in the j argument returns a data table

```

The answer is 3, as the unique values in the gender column are m, f and N/A. Compare with the base R function `unique()`, which returns all the unique values in a given input:

````{r}

linelist[, .(unique(gender))]
```

To find the number of unique cases in a given month we would write the following:

````{r}

linelist[, .(uniqueN(case_id)), .(month = month(date_hospitalisation))]

```

## Adding and updating to data tables { }

The `:=` operator is used to add or update data in a data table. Adding columns to your data table can be done in the following ways:

````{r}

linelist[, adult := age >= 18] #adds one column
linelist[, c("child", "wt_lbs") := .(age < 18, wt_kg*2.204)] #to add multiple columns requires c("") and list() or .() syntax
linelist[, `:=` (bmi_in_range = (bmi > 16 & bmi < 40),
                         no_infector_source_data = is.na(infector) | is.na(source))] #this method uses := as a functional operator `:=`
linelist[, adult := NULL] #deletes the column

```


Further complex aggregations are beyond the scope of this introductory chapter, but the idea is to provide a popular and viable alternative to **dplyr** for grouping and cleaning data. The **data.table** package is a great package that allows for neat and readable code.


## Resources {  }

Here are some useful resources for more information:
* https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
* https://github.com/Rdatatable/data.table
* https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf
* https://www.machinelearningplus.com/data-manipulation/datatable-in-r-complete-guide/
* https://www.datacamp.com/community/tutorials/data-table-r-tutorial

You can perform any summary function on grouped data; see the Cheat Sheet here for more info:
https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_table.Rmd-->

