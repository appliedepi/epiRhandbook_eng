{
  "hash": "245102a1c176171dd424287ea23d4139",
  "result": {
    "markdown": "\n# Time series and outbreak detection { }  \n\n<!-- ======================================================= -->\n## Overview {  }\n\nThis tab demonstrates the use of several packages for time series analysis. \nIt primarily relies on packages from the [**tidyverts**](https://tidyverts.org/) \nfamily, but will also use the RECON [**trending**](https://github.com/reconhub/trending) \npackage to fit models that are more appropriate for infectious disease epidemiology. \n\nNote in the below example we use a dataset from the **surveillance** package \non Campylobacter in Germany (see the [data chapter](https://epirhandbook.com/download-handbook-and-data.html), \nof the handbook for details). However, if you wanted to run the same code on a dataset\nwith multiple countries or other strata, then there is an example code template for this in the \n[r4epis github repo](https://github.com/R4EPI/epitsa). \n\nTopics covered include:  \n\n1.  Time series data \n2.  Descriptive analysis \n3.  Fitting regressions\n4.  Relation of two time series \n5.  Outbreak detection\n6.  Interrupted time series\n\n\n<!-- ======================================================= -->\n## Preparation {  }\n\n### Packages {.unnumbered}\n\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize `p_load()` from **pacman**, which installs the package if necessary *and* loads it for use. You can also load packages with `library()` from **base** R. See the page on [R basics](https://epirhandbook.com/r-basics.html) for more information on R packages.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(rio,          # File import\n               here,         # File locator\n               tidyverse,    # data management + ggplot2 graphics\n               tsibble,      # handle time series datasets\n               slider,       # for calculating moving averages\n               imputeTS,     # for filling in missing values\n               feasts,       # for time series decomposition and autocorrelation\n               forecast,     # fit sin and cosin terms to data (note: must load after feasts)\n               trending,     # fit and assess models \n               tmaptools,    # for getting geocoordinates (lon/lat) based on place names\n               ecmwfr,       # for interacting with copernicus sateliate CDS API\n               stars,        # for reading in .nc (climate data) files\n               units,        # for defining units of measurement (climate data)\n               yardstick,    # for looking at model accuracy\n               surveillance  # for aberration detection\n               )\n```\n:::\n\n\n### Load data {.unnumbered}\n\nYou can download all the data used in this handbook via the instructions in the [Download handbook and data] page.  \n\nThe example dataset used in this section is weekly counts of campylobacter cases reported in Germany between 2001 and 2011. <a href='https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx' class='download-button'>\n\tYou can click here to download<span> this data file (.xlsx).</span></a> \n\nThis dataset is a reduced version of the dataset available in the [**surveillance**](https://cran.r-project.org/web/packages/surveillance/) package. \n(for details load the surveillance package and see `?campyDE`)\n\nImport these data with the `import()` function from the **rio** package (it handles many file types like .xlsx, .csv, .rds - see the [Import and export] page for details).\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# import the counts into R\ncounts <- rio::import(\"campylobacter_germany.xlsx\")\n```\n:::\n\n\nThe first 10 rows of the counts are displayed below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-3f80ffccdb1be279128b\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-3f80ffccdb1be279128b\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"2001-12-31T00:00:00Z\",\"2002-01-07T00:00:00Z\",\"2002-01-14T00:00:00Z\",\"2002-01-21T00:00:00Z\",\"2002-01-28T00:00:00Z\",\"2002-02-04T00:00:00Z\",\"2002-02-11T00:00:00Z\",\"2002-02-18T00:00:00Z\",\"2002-02-25T00:00:00Z\",\"2002-03-04T00:00:00Z\"],[514,913,1023,887,815,792,803,807,692,705]],\"container\":\"<table class=\\\"white-space: nowrap\\\">\\n  <thead>\\n    <tr>\\n      <th>date<\\/th>\\n      <th>case<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"scrollX\":true,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":1},{\"name\":\"date\",\"targets\":0},{\"name\":\"case\",\"targets\":1}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n### Clean data {.unnumbered}\n\nThe code below makes sure that the date column is in the appropriate format. \nFor this tab we will be using the **tsibble** package and so the `yearweek` \nfunction will be used to create a calendar week variable. There are several other\nways of doing this (see the [Working with dates](https://epirhandbook.com/working-with-dates.html)\npage for details), however for time series its best to keep within one framework (**tsibble**). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## ensure the date column is in the appropriate format\ncounts$date <- as.Date(counts$date)\n\n## create a calendar week variable \n## fitting ISO definitons of weeks starting on a monday\ncounts <- counts %>% \n     mutate(epiweek = yearweek(date, week_start = 1))\n```\n:::\n\n\n### Download climate data {.unnumbered} \n\nIn the *relation of two time series* section of this page, we will be comparing \ncampylobacter case counts to climate data. \n\nClimate data for anywhere in the world can be downloaded from the EU's Copernicus \nSatellite. These are not exact measurements, but based on a model (similar to \ninterpolation), however the benefit is global hourly coverage as well as forecasts.  \n\nYou can download each of these climate data files from the [Download handbook and data] page.  \n\nFor purposes of demonstration here, we will show R code to use the **ecmwfr** package to pull these data from the Copernicus \nclimate data store. You will need to create a free account in order for this to \nwork. The package website has a useful [walkthrough](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds)\nof how to do this. Below is example code of how to go about doing this, once you \nhave the appropriate API keys. You have to replace the X's below with your account\nIDs. You will need to download one year of data at a time otherwise the server times-out. \n\nIf you are not sure of the coordinates for a location you want to download data \nfor, you can use the **tmaptools** package to pull the coordinates off open street\nmaps. An alternative option is the [**photon**](https://github.com/rCarto/photon)\npackage, however this has not been released on to CRAN yet; the nice thing about \n**photon** is that it provides more contextual data for when there are several \nmatches for your search.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## retrieve location coordinates\ncoords <- geocode_OSM(\"Germany\", geometry = \"point\")\n\n## pull together long/lats in format for ERA-5 querying (bounding box) \n## (as just want a single point can repeat coords)\nrequest_coords <- str_glue_data(coords$coords, \"{y}/{x}/{y}/{x}\")\n\n\n## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)\n## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app\n## https://github.com/bluegreen-labs/ecmwfr\n\n## set up key for weather data \nwf_set_key(user = \"XXXXX\",\n           key = \"XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX\",\n           service = \"cds\") \n\n## run for each year of interest (otherwise server times out)\nfor (i in 2002:2011) {\n  \n  ## pull together a query \n  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax\n  ## change request to a list using addin button above (python to list)\n  ## Target is the name of the output file!!\n  request <- request <- list(\n    product_type = \"reanalysis\",\n    format = \"netcdf\",\n    variable = c(\"2m_temperature\", \"total_precipitation\"),\n    year = c(i),\n    month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n    day = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n            \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\",\n            \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\"),\n    time = c(\"00:00\", \"01:00\", \"02:00\", \"03:00\", \"04:00\", \"05:00\", \"06:00\", \"07:00\",\n             \"08:00\", \"09:00\", \"10:00\", \"11:00\", \"12:00\", \"13:00\", \"14:00\", \"15:00\",\n             \"16:00\", \"17:00\", \"18:00\", \"19:00\", \"20:00\", \"21:00\", \"22:00\", \"23:00\"),\n    area = request_coords,\n    dataset_short_name = \"reanalysis-era5-single-levels\",\n    target = paste0(\"germany_weather\", i, \".nc\")\n  )\n  \n  ## download the file and store it in the current working directory\n  file <- wf_request(user     = \"XXXXX\",  # user ID (for authentication)\n                     request  = request,  # the request\n                     transfer = TRUE,     # download the file\n                     path     = here::here(\"data\", \"Weather\")) ## path to save the data\n  }\n```\n:::\n\n\n### Load climate data {.unnumbered}\n\nWhether you downloaded the climate data via our handbook, or used the code above, you now should have 10 years of \".nc\" climate data files stored in the same folder on your computer.  \n\nUse the code below to import these files into R with the **stars** package. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define path to weather folder \nfile_paths <- list.files(\n  here::here(\"data\", \"time_series\", \"weather\"), # replace with your own file path \n  full.names = TRUE)\n\n## only keep those with the current name of interest \nfile_paths <- file_paths[str_detect(file_paths, \"germany\")]\n\n## read in all the files as a stars object \ndata <- stars::read_stars(file_paths)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \n```\n:::\n:::\n\n\nOnce these files have been imported as the object `data`, we will convert them to a data frame.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## change to a data frame \ntemp_data <- as_tibble(data) %>% \n  ## add in variables and correct units\n  mutate(\n    ## create an calendar week variable \n    epiweek = tsibble::yearweek(time), \n    ## create a date variable (start of calendar week)\n    date = as.Date(epiweek),\n    ## change temperature from kelvin to celsius\n    t2m = set_units(t2m, celsius), \n    ## change precipitation from metres to millimetres \n    tp  = set_units(tp, mm)) %>% \n  ## group by week (keep the date too though)\n  group_by(epiweek, date) %>% \n  ## get the average per week\n  summarise(t2m = as.numeric(mean(t2m)), \n            tp = as.numeric(mean(tp)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'epiweek'. You can override using the\n`.groups` argument.\n```\n:::\n:::\n\n\n\n\n\n<!-- ======================================================= -->\n## Time series data {  }\n\nThere are a number of different packages for structuring and handling time series\ndata. As said, we will focus on the **tidyverts** family of packages and so will\nuse the **tsibble** package to define our time series object. Having a data set\ndefined as a time series object means it is much easier to structure our analysis. \n\nTo do this we use the `tsibble()` function and specify the \"index\", i.e. the variable\nspecifying the time unit of interest. In our case this is the `epiweek` variable. \n\nIf we had a data set with weekly counts by province, for example, we would also \nbe able to specify the grouping variable using the `key = ` argument. \nThis would allow us to do analysis for each group. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define time series object \ncounts <- tsibble(counts, index = epiweek)\n```\n:::\n\n\nLooking at `class(counts)` tells you that on top of being a tidy data frame \n(\"tbl_df\", \"tbl\", \"data.frame\"), it has the additional properties of a time series\ndata frame (\"tbl_ts\"). \n\nYou can take a quick look at your data by using **ggplot2**. We see from the plot that\nthere is a clear seasonal pattern, and that there are no missings. However, there\nseems to be an issue with reporting at the beginning of each year; cases drop \nin the last week of the year and then increase for the first week of the next year. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## plot a line graph of cases by week\nggplot(counts, aes(x = epiweek, y = case)) + \n     geom_line()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/basic_plot-1.png){width=672}\n:::\n:::\n\n\n\n<span style=\"color: red;\">**_DANGER:_** Most datasets aren't as clean as this example. \nYou will need to check for duplicates and missings as below. </span>\n\n<!-- ======================================================= -->\n### Duplicates {.unnumbered}\n\n**tsibble** does not allow duplicate observations. So each row will need to be\nunique, or unique within the group (`key` variable). \nThe package has a few functions that help to identify duplicates. These include\n`are_duplicated()` which gives you a TRUE/FALSE vector of whether the row is a \nduplicate, and `duplicates()` which gives you a data frame of the duplicated rows. \n\nSee the page on [De-duplication](https://epirhandbook.com/de-duplication.html)\nfor more details on how to select rows you want. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## get a vector of TRUE/FALSE whether rows are duplicates\nare_duplicated(counts, index = epiweek) \n\n## get a data frame of any duplicated rows \nduplicates(counts, index = epiweek) \n```\n:::\n\n\n<!-- ======================================================= -->\n### Missings {.unnumbered}\n\nWe saw from our brief inspection above that there are no missings, but we also \nsaw there seems to be a problem with reporting delay around new year. \nOne way to address this problem could be to set these values to missing and then \nto impute values. The simplest form of time series imputation is to draw\na straight line between the last non-missing and the next non-missing value. \nTo do this we will use the **imputeTS** package function `na_interpolation()`. \n\nSee the [Missing data](https://epirhandbook.com/missing-data.html) page for other options for imputation.  \n\nAnother alternative would be to calculate a moving average, to try and smooth\nover these apparent reporting issues (see next section, and the page on [Moving averages](https://epirhandbook.com/moving-averages.html)). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create a variable with missings instead of weeks with reporting issues\ncounts <- counts %>% \n     mutate(case_miss = if_else(\n          ## if epiweek contains 52, 53, 1 or 2\n          str_detect(epiweek, \"W51|W52|W53|W01|W02\"), \n          ## then set to missing \n          NA_real_, \n          ## otherwise keep the value in case\n          case\n     ))\n\n## alternatively interpolate missings by linear trend \n## between two nearest adjacent points\ncounts <- counts %>% \n  mutate(case_int = imputeTS::na_interpolation(case_miss)\n         )\n\n## to check what values have been imputed compared to the original\nggplot_na_imputations(counts$case_miss, counts$case_int) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/missings-1.png){width=672}\n:::\n:::\n\n\n\n\n\n<!-- ======================================================= -->\n## Descriptive analysis {  }\n\n\n\n<!-- ======================================================= -->\n### Moving averages {#timeseries_moving .unnumbered}\n\nIf data is very noisy (counts jumping up and down) then it can be helpful to \ncalculate a moving average. In the example below, for each week we calculate the \naverage number of cases from the four previous weeks. This smooths the data, to \nmake it more interpretable. In our case this does not really add much, so we will\nstick to the interpolated data for further analysis. \nSee the [Moving averages](https://epirhandbook.com/moving-averages.html) page for more detail. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create a moving average variable (deals with missings)\ncounts <- counts %>% \n     ## create the ma_4w variable \n     ## slide over each row of the case variable\n     mutate(ma_4wk = slider::slide_dbl(case, \n                               ## for each row calculate the name\n                               ~ mean(.x, na.rm = TRUE),\n                               ## use the four previous weeks\n                               .before = 4))\n\n## make a quick visualisation of the difference \nggplot(counts, aes(x = epiweek)) + \n     geom_line(aes(y = case)) + \n     geom_line(aes(y = ma_4wk), colour = \"red\")\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/moving_averages-1.png){width=672}\n:::\n:::\n\n\n\n<!-- ======================================================= -->\n### Periodicity {.unnumbered}\n\nBelow we define a custom function to create a periodogram. See the [Writing functions] page for information about how to write functions in R.  \n\nFirst, the function is defined. Its arguments include a dataset with a column `counts`, `start_week = ` which is the first week of the dataset, a number to indicate how many periods per year (e.g. 52, 12), and lastly the output style (see details in the code below).  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Function arguments\n#####################\n## x is a dataset\n## counts is variable with count data or rates within x \n## start_week is the first week in your dataset\n## period is how many units in a year \n## output is whether you want return spectral periodogram or the peak weeks\n  ## \"periodogram\" or \"weeks\"\n\n# Define function\nperiodogram <- function(x, \n                        counts, \n                        start_week = c(2002, 1), \n                        period = 52, \n                        output = \"weeks\") {\n  \n\n    ## make sure is not a tsibble, filter to project and only keep columns of interest\n    prepare_data <- dplyr::as_tibble(x)\n    \n    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]\n    prepare_data <- dplyr::select(prepare_data, {{counts}})\n    \n    ## create an intermediate \"zoo\" time series to be able to use with spec.pgram\n    zoo_cases <- zoo::zooreg(prepare_data, \n                             start = start_week, frequency = period)\n    \n    ## get a spectral periodogram not using fast fourier transform \n    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)\n    \n    ## return the peak weeks \n    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period\n    \n    if (output == \"weeks\") {\n      periodo_weeks\n    } else {\n      periodo\n    }\n    \n}\n\n## get spectral periodogram for extracting weeks with the highest frequencies \n## (checking of seasonality) \nperiodo <- periodogram(counts, \n                       case_int, \n                       start_week = c(2002, 1),\n                       output = \"periodogram\")\n\n## pull spectrum and frequence in to a dataframe for plotting\nperiodo <- data.frame(periodo$freq, periodo$spec)\n\n## plot a periodogram showing the most frequently occuring periodicity \nggplot(data = periodo, \n                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + \n  geom_line() + \n  labs(x = \"Period (Weeks)\", y = \"Log(density)\")\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/periodogram-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## get a vector weeks in ascending order \npeak_weeks <- periodogram(counts, \n                          case_int, \n                          start_week = c(2002, 1), \n                          output = \"weeks\")\n```\n:::\n\n\n<span style=\"color: black;\">**_NOTE:_** It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) </span>\n\n<!-- ======================================================= -->\n### Decomposition {.unnumbered}\n\nClassical decomposition is used to break a time series down several parts, which\nwhen taken together make up for the pattern you see. \nThese different parts are:  \n\n* The trend-cycle (the long-term direction of the data)  \n* The seasonality (repeating patterns)  \n* The random (what is left after removing trend and season)  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## decompose the counts dataset \ncounts %>% \n  # using an additive classical decomposition model\n  model(classical_decomposition(case_int, type = \"additive\")) %>% \n  ## extract the important information from the model\n  components() %>% \n  ## generate a plot \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/decomposition-1.png){width=672}\n:::\n:::\n\n\n<!-- ======================================================= -->\n### Autocorrelation {.unnumbered}\n\nAutocorrelation tells you about the relation between the counts of each week \nand the weeks before it (called lags).  \n\nUsing the `ACF()` function, we can produce a plot which shows us a number of lines \nfor the relation at different lags. Where the lag is 0 (x = 0), this line would \nalways be 1 as it shows the relation between an observation and itself (not shown here). \nThe first line shown here (x = 1) shows the relation between each observation \nand the observation before it (lag of 1), the second shows the relation between \neach observation and the observation before last (lag of 2) and so on until lag of\n52 which shows the relation between each observation and the observation from 1 \nyear (52 weeks before).  \n\nUsing the `PACF()` function (for partial autocorrelation) shows the same type of relation \nbut adjusted for all other weeks between. This is less informative for determining\nperiodicity. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## using the counts dataset\ncounts %>% \n  ## calculate autocorrelation using a full years worth of lags\n  ACF(case_int, lag_max = 52) %>% \n  ## show a plot\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/autocorrelation-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## using the counts data set \ncounts %>% \n  ## calculate the partial autocorrelation using a full years worth of lags\n  PACF(case_int, lag_max = 52) %>% \n  ## show a plot\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/autocorrelation-2.png){width=672}\n:::\n:::\n\n\nYou can formally test the null hypothesis of independence in a time series (i.e. \nthat it is not autocorrelated) using the Ljung-Box test (in the **stats** package). \nA significant p-value suggests that there is autocorrelation in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## test for independance \nBox.test(counts$case_int, type = \"Ljung-Box\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  counts$case_int\nX-squared = 462.65, df = 1, p-value < 2.2e-16\n```\n:::\n:::\n\n\n\n<!-- ======================================================= -->\n## Fitting regressions {  }\n\nIt is possible to fit a large number of different regressions to a time series, \nhowever, here we will demonstrate how to fit a negative binomial regression - as \nthis is often the most appropriate for counts data in infectious diseases. \n\n<!-- ======================================================= -->\n### Fourier terms {.unnumbered}\n\nFourier terms are the equivalent of sin and cosin curves. The difference is that \nthese are fit based on finding the most appropriate combination of curves to explain\nyour data.  \n\nIf only fitting one fourier term, this would be the equivalent of fitting a sin \nand a cosin for your most frequently occurring lag seen in your periodogram (in our \ncase 52 weeks). We use the `fourier()` function from the **forecast** package.  \n\nIn the below code we assign using the `$`, as `fourier()` returns two columns (one \nfor sin one for cosin) and so these are added to the dataset as a list, called \n\"fourier\" - but this list can then be used as a normal variable in regression. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## add in fourier terms using the epiweek and case_int variabless\ncounts$fourier <- select(counts, epiweek, case_int) %>% \n  fourier(K = 1)\n```\n:::\n\n\n<!-- ======================================================= -->\n### Negative binomial {.unnumbered}\n\nIt is possible to fit regressions using base **stats** or **MASS**\nfunctions (e.g. `lm()`, `glm()` and `glm.nb()`). However we will be using those from \nthe **trending** package, as this allows for calculating appropriate confidence\nand prediction intervals (which are otherwise not available). \nThe syntax is the same, and you specify an outcome variable then a tilde (~) \nand then add your various exposure variables of interest separated by a plus (+). \n\nThe other difference is that we first define the model and then `fit()` it to the \ndata. This is useful because it allows for comparing multiple different models \nwith the same syntax. \n\n<span style=\"color: darkgreen;\">**_TIP:_** If you wanted to use rates, rather than \ncounts you could include the population variable as a logarithmic offset term, by adding \n`offset(log(population)`. You would then need to set population to be 1, before \nusing `predict()` in order to produce a rate. </span>\n\n<span style=\"color: darkgreen;\">**_TIP:_** For fitting more complex models such \nas ARIMA or prophet, see the [**fable**](https://fable.tidyverts.org/index.html) package.</span>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the fourier terms to account for seasonality\n    fourier)\n\n## fit your model using the counts dataset\nfitted_model <- trending::fit(model, data.frame(counts))\n\n## calculate confidence intervals and prediction intervals \nobserved <- predict(fitted_model, simulate_pi = FALSE)\n\nestimate_res <- data.frame(observed$result)\n\n## plot your regression \nggplot(data = estimate_res, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"Red\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/nb_reg-1.png){width=672}\n:::\n:::\n\n\n<!-- ======================================================= -->\n### Residuals {.unnumbered}\n\nTo see how well our model fits the observed data we need to look at the residuals. \nThe residuals are the difference between the observed counts and the counts \nestimated from the model. We could calculate this simply by using `case_int - estimate`, \nbut the `residuals()` function extracts this directly from the regression for us.\n\nWhat we see from the below, is that we are not explaining all of the variation \nthat we could with the model. It might be that we should fit more fourier terms, \nand address the amplitude. However for this example we will leave it as is. \nThe plots show that our model does worse in the peaks and troughs (when counts are\nat their highest and lowest) and that it might be more likely to underestimate \nthe observed counts. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## calculate the residuals \nestimate_res <- estimate_res %>% \n  mutate(resid = fitted_model$result[[1]]$residuals)\n\n## are the residuals fairly constant over time (if not: outbreaks? change in practice?)\nestimate_res %>%\n  ggplot(aes(x = epiweek, y = resid)) +\n  geom_line() +\n  geom_point() + \n  labs(x = \"epiweek\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## is there autocorelation in the residuals (is there a pattern to the error?)  \nestimate_res %>% \n  as_tsibble(index = epiweek) %>% \n  ACF(resid, lag_max = 52) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n```{.r .cell-code}\n## are residuals normally distributed (are under or over estimating?)  \nestimate_res %>%\n  ggplot(aes(x = resid)) +\n  geom_histogram(binwidth = 100) +\n  geom_rug() +\n  labs(y = \"count\") \n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n\n```{.r .cell-code}\n## compare observed counts to their residuals \n  ## should also be no pattern \nestimate_res %>%\n  ggplot(aes(x = estimate, y = resid)) +\n  geom_point() +\n  labs(x = \"Fitted\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-2-4.png){width=672}\n:::\n\n```{.r .cell-code}\n## formally test autocorrelation of the residuals\n## H0 is that residuals are from a white-noise series (i.e. random)\n## test for independence \n## if p value significant then non-random\nBox.test(estimate_res$resid, type = \"Ljung-Box\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  estimate_res$resid\nX-squared = 336.25, df = 1, p-value < 2.2e-16\n```\n:::\n:::\n\n\n<!-- ======================================================= -->\n## Relation of two time series {  }\n\nHere we look at using weather data (specifically the temperature) to explain \ncampylobacter case counts. \n\n<!-- ======================================================= -->\n### Merging datasets {.unnumbered}\n\nWe can join our datasets using the week variable. For more on merging see the \nhandbook section on [joining](https://epirhandbook.com/joining-data.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## left join so that we only have the rows already existing in counts\n## drop the date variable from temp_data (otherwise is duplicated)\ncounts <- left_join(counts, \n                    select(temp_data, -date),\n                    by = \"epiweek\")\n```\n:::\n\n\n<!-- ======================================================= -->\n### Descriptive analysis {.unnumbered}\n\nFirst plot your data to see if there is any obvious relation. \nThe plot below shows that there is a clear relation in the seasonality of the two\nvariables, and that temperature might peak a few weeks before the case number.\nFor more on pivoting data, see the handbook section on [pivoting data](https://epirhandbook.com/pivoting-data.html). \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts %>% \n  ## keep the variables we are interested \n  select(epiweek, case_int, t2m) %>% \n  ## change your data in to long format\n  pivot_longer(\n    ## use epiweek as your key\n    !epiweek,\n    ## move column names to the new \"measure\" column\n    names_to = \"measure\", \n    ## move cell values to the new \"values\" column\n    values_to = \"value\") %>% \n  ## create a plot with the dataset above\n  ## plot epiweek on the x axis and values (counts/celsius) on the y \n  ggplot(aes(x = epiweek, y = value)) + \n    ## create a separate plot for temperate and case counts \n    ## let them set their own y-axes\n    facet_grid(measure ~ ., scales = \"free_y\") +\n    ## plot both as a line\n    geom_line()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/basic_plot_bivar-1.png){width=672}\n:::\n:::\n\n\n<!-- ======================================================= -->\n### Lags and cross-correlation {.unnumbered}\n\nTo formally test which weeks are most highly related between cases and temperature. \nWe can use the cross-correlation function (`CCF()`) from the **feasts** package. \nYou could also visualise (rather than using `arrange`) using the `autoplot()` function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts %>% \n  ## calculate cross-correlation between interpolated counts and temperature\n  CCF(case_int, t2m,\n      ## set the maximum lag to be 52 weeks\n      lag_max = 52, \n      ## return the correlation coefficient \n      type = \"correlation\") %>% \n  ## arange in decending order of the correlation coefficient \n  ## show the most associated lags\n  arrange(-ccf) %>% \n  ## only show the top ten \n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tsibble: 10 x 2 [1W]\n        lag   ccf\n   <cf_lag> <dbl>\n 1      -4W 0.749\n 2      -5W 0.745\n 3      -3W 0.735\n 4      -6W 0.729\n 5      -2W 0.727\n 6      -7W 0.704\n 7      -1W 0.695\n 8      -8W 0.671\n 9       0W 0.649\n10      47W 0.638\n```\n:::\n:::\n\n\nWe see from this that a lag of 4 weeks is most highly correlated, \nso we make a lagged temperature variable to include in our regression. \n\n<span style=\"color: red;\">**_DANGER:_** Note that the first four weeks of our data\nin the lagged temperature variable are missing (`NA`) - as there are not four \nweeks prior to get data from. In order to use this dataset with the **trending** \n`predict()` function, we need to use the the `simulate_pi = FALSE` argument within\n`predict()` further down. If we did want to use the simulate option, then \nwe have to drop these missings and store as a new data set by adding `drop_na(t2m_lag4)` \nto the code chunk below.</span>  \n \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts <- counts %>% \n  ## create a new variable for temperature lagged by four weeks\n  mutate(t2m_lag4 = lag(t2m, n = 4))\n```\n:::\n\n\n\n<!-- ======================================================= -->\n### Negative binomial with two variables {.unnumbered}\n\nWe fit a negative binomial regression as done previously. This time we add the \ntemperature variable lagged by four weeks. \n\n<span style=\"color: orange;\">**_CAUTION:_** Note the use of `simulate_pi = FALSE`\nwithin the `predict()` argument. This is because the default behaviour of **trending** \nis to use the **ciTools** package to estimate a prediction interval. This does not \nwork if there are `NA` counts, and also produces more granular intervals. \nSee `?trending::predict.trending_model_fit` for details. </span>  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the fourier terms to account for seasonality\n    fourier + \n    ## use the temperature lagged by four weeks \n    t2m_lag4\n    )\n\n## fit your model using the counts dataset\nfitted_model <- trending::fit(model, data.frame(counts))\n\n## calculate confidence intervals and prediction intervals \nobserved <- predict(fitted_model, simulate_pi = FALSE)\n```\n:::\n\n\n\nTo investigate the individual terms, we can pull the original negative binomial\nregression out of the **trending** format using `get_model()` and pass this to the\n**broom** package `tidy()` function to retrieve exponentiated estimates and associated\nconfidence intervals.  \n\nWhat this shows us is that lagged temperature, after controlling for trend and seasonality, \nis similar to the case counts (estimate ~ 1) and significantly associated. \nThis suggests that it might be a good variable for use in predicting future case\nnumbers (as climate forecasts are readily available). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted_model %>% \n  ## extract original negative binomial regression\n  get_fitted_model() #%>% \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n\nCall:  glm.nb(formula = case_int ~ epiweek + fourier + t2m_lag4, data = data.frame(counts), \n    init.theta = 32.80689607, link = log)\n\nCoefficients:\n (Intercept)       epiweek  fourierS1-52  fourierC1-52      t2m_lag4  \n   5.825e+00     8.464e-05    -2.850e-01    -1.954e-01     6.672e-03  \n\nDegrees of Freedom: 504 Total (i.e. Null);  500 Residual\n  (4 observations deleted due to missingness)\nNull Deviance:\t    2015 \nResidual Deviance: 508.2 \tAIC: 6784\n```\n:::\n\n```{.r .cell-code}\n  ## get a tidy dataframe of results\n  #tidy(exponentiate = TRUE, \n  #     conf.int = TRUE)\n```\n:::\n\n\nA quick visual inspection of the model shows that it might do a better job of \nestimating the observed case counts. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimate_res <- data.frame(observed$result)\n\n## plot your regression \nggplot(data = estimate_res, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"Red\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/plot_nb_reg_bivar-1.png){width=672}\n:::\n:::\n\n\n\n#### Residuals {.unnumbered}\n\nWe investigate the residuals again to see how well our model fits the observed data. \nThe results and interpretation here are similar to those of the previous regression, \nso it may be more feasible to stick with the simpler model without temperature. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## calculate the residuals \nestimate_res <- estimate_res %>% \n  mutate(resid = case_int - estimate)\n\n## are the residuals fairly constant over time (if not: outbreaks? change in practice?)\nestimate_res %>%\n  ggplot(aes(x = epiweek, y = resid)) +\n  geom_line() +\n  geom_point() + \n  labs(x = \"epiweek\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## is there autocorelation in the residuals (is there a pattern to the error?)  \nestimate_res %>% \n  as_tsibble(index = epiweek) %>% \n  ACF(resid, lag_max = 52) %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n## are residuals normally distributed (are under or over estimating?)  \nestimate_res %>%\n  ggplot(aes(x = resid)) +\n  geom_histogram(binwidth = 100) +\n  geom_rug() +\n  labs(y = \"count\") \n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n```{.r .cell-code}\n## compare observed counts to their residuals \n  ## should also be no pattern \nestimate_res %>%\n  ggplot(aes(x = estimate, y = resid)) +\n  geom_point() +\n  labs(x = \"Fitted\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n\n```{.r .cell-code}\n## formally test autocorrelation of the residuals\n## H0 is that residuals are from a white-noise series (i.e. random)\n## test for independence \n## if p value significant then non-random\nBox.test(estimate_res$resid, type = \"Ljung-Box\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tBox-Ljung test\n\ndata:  estimate_res$resid\nX-squared = 339.52, df = 1, p-value < 2.2e-16\n```\n:::\n:::\n\n\n<!-- ======================================================= -->\n## Outbreak detection {  }\n\nWe will demonstrate two (similar) methods of detecting outbreaks here. \nThe first builds on the sections above. \nWe use the **trending** package to fit regressions to previous years, and then\npredict what we expect to see in the following year. If observed counts are above\nwhat we expect, then it could suggest there is an outbreak. \nThe second method is based on similar principles but uses the **surveillance** package,\nwhich has a number of different algorithms for aberration detection.\n\n<span style=\"color: orange;\">**_CAUTION:_** Normally, you are interested in the current year (where you only know counts up to the present week). So in this example we are pretending to be in week 39 of 2011.</span>\n\n<!-- ======================================================= -->\n### **trending** package {.unnumbered}\n\nFor this method we define a baseline (which should usually be about 5 years of data). \nWe fit a regression to the baseline data, and then use that to predict the estimates\nfor the next year. \n\n<!-- ======================================================= -->\n#### Cut-off date { -}\n\nIt is easier to define your dates in one place and then use these throughout the\nrest of your code.  \n\nHere we define a start date (when our observations started) and a cut-off date \n(the end of our baseline period - and when the period we want to predict for starts). \n~We also define how many weeks are in our year of interest (the one we are going to\nbe predicting)~.\nWe also define how many weeks are between our baseline cut-off and the end date \nthat we are interested in predicting for. \n\n\n<span style=\"color: black;\">**_NOTE:_** In this example we pretend to currently be at the end of September 2011 (\"2011 W39\").</span>  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define start date (when observations began)\nstart_date <- min(counts$epiweek)\n\n## define a cut-off week (end of baseline, start of prediction period)\ncut_off <- yearweek(\"2010-12-31\")\n\n## define the last date interested in (i.e. end of prediction)\nend_date <- yearweek(\"2011-12-31\")\n\n## find how many weeks in period (year) of interest\nnum_weeks <- as.numeric(end_date - cut_off)\n```\n:::\n\n\n\n<!-- ======================================================= -->\n#### Add rows {.unnumbered}\n\nTo be able to forecast in a tidyverse format, we need to have the right number \nof rows in our dataset, i.e. one row for each week up to the `end_date`defined above. \nThe code below allows you to add these rows for by a grouping variable - for example\nif we had multiple countries in one dataset, we could group by country and then \nadd rows appropriately for each. \nThe `group_by_key()` function from **tsibble** allows us to do this grouping \nand then pass the grouped data to **dplyr** functions, `group_modify()` and \n`add_row()`. Then we specify the sequence of weeks between one after the maximum week \ncurrently available in the data and the end week. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## add in missing weeks till end of year \ncounts <- counts %>%\n  ## group by the region\n  group_by_key() %>%\n  ## for each group add rows from the highest epiweek to the end of year\n  group_modify(~add_row(.,\n                        epiweek = seq(max(.$epiweek) + 1, \n                                      end_date,\n                                      by = 1)))\n```\n:::\n\n\n\n\n<!-- ======================================================= -->\n#### Fourier terms {.unnumbered}\n\nWe need to redefine our fourier terms - as we want to fit them to the baseline \ndate only and then predict (extrapolate) those terms for the next year. \nTo do this we need to combine two output lists from the `fourier()` function together; \nthe first one is for the baseline data, and the second one predicts for the \nyear of interest (by defining the `h` argument).  \n\n*N.b.* to bind rows we have to use `rbind()` (rather than tidyverse `bind_rows`) as\nthe fourier columns are a list (so not named individually). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define fourier terms (sincos) \ncounts <- counts %>% \n  mutate(\n    ## combine fourier terms for weeks prior to  and after 2010 cut-off date\n    ## (nb. 2011 fourier terms are predicted)\n    fourier = rbind(\n      ## get fourier terms for previous years\n      fourier(\n        ## only keep the rows before 2011\n        filter(counts, \n               epiweek <= cut_off), \n        ## include one set of sin cos terms \n        K = 1\n        ), \n      ## predict the fourier terms for 2011 (using baseline data)\n      fourier(\n        ## only keep the rows before 2011\n        filter(counts, \n               epiweek <= cut_off),\n        ## include one set of sin cos terms \n        K = 1, \n        ## predict 52 weeks ahead\n        h = num_weeks\n        )\n      )\n    )\n```\n:::\n\n\n<!-- ======================================================= -->\n#### Split data and fit regression {.unnumbered}\n\nWe now have to split our dataset in to the baseline period and the prediction \nperiod. This is done using the **dplyr** `group_split()` function after `group_by()`, \nand will create a list with two data frames, one for before your cut-off and one \nfor after.  \n\nWe then use the **purrr** package `pluck()` function to pull the datasets out of the\nlist (equivalent of using square brackets, e.g. `dat[[1]]`), and can then fit \nour model to the baseline data, and then use the `predict()` function for our data\nof interest after the cut-off.  \n\nSee the page on [Iteration, loops, and lists] to learn more about **purrr**.  \n\n<span style=\"color: orange;\">**_CAUTION:_** Note the use of `simulate_pi = FALSE`\nwithin the `predict()` argument. This is because the default behaviour of **trending** \nis to use the **ciTools** package to estimate a prediction interval. This does not \nwork if there are `NA` counts, and also produces more granular intervals. \nSee `?trending::predict.trending_model_fit` for details. </span>  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split data for fitting and prediction\ndat <- counts %>% \n  group_by(epiweek <= cut_off) %>%\n  group_split()\n\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier\n)\n\n# define which data to use for fitting and which for predicting\nfitting_data <- pluck(dat, 2)\npred_data <- pluck(dat, 1) %>% \n  select(case_int, epiweek, fourier)\n\n# fit model \nfitted_model <- trending::fit(model, data.frame(fitting_data))\n\n# get confint and estimates for fitted data\nobserved <- fitted_model %>% \n  predict(simulate_pi = FALSE)\n\n# forecast with data want to predict with \nforecasts <- fitted_model %>% \n  predict(data.frame(pred_data), simulate_pi = FALSE)\n\n## combine baseline and predicted datasets\nobserved <- bind_rows(observed$result, forecasts$result)\n```\n:::\n\n\nAs previously, we can visualise our model with **ggplot**. We highlight alerts with\nred dots for observed counts above the 95% prediction interval. \nThis time we also add a vertical line to label when the forecast starts. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## plot your regression \nggplot(data = observed, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"grey\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## plot in points for the observed counts above expected\n  geom_point(\n    data = filter(observed, case_int > upper_pi), \n    aes(y = case_int), \n    colour = \"red\", \n    size = 2) + \n  ## add vertical line and label to show where forecasting started\n  geom_vline(\n           xintercept = as.Date(cut_off), \n           linetype = \"dashed\") + \n  annotate(geom = \"text\", \n           label = \"Forecast\", \n           x = cut_off, \n           y = max(observed$upper_pi) - 250, \n           angle = 90, \n           vjust = 1\n           ) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n```\n:::\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/forecast_plot-1.png){width=672}\n:::\n:::\n\n\n\n\n<!-- ======================================================= -->\n#### Prediction validation {.unnumbered}\n\nBeyond inspecting residuals, it is important to investigate how good your model is\nat predicting cases in the future. This gives you an idea of how reliable your \nthreshold alerts are.  \n\nThe traditional way of validating is to see how well you can predict the latest \nyear before the present one (because you don't yet know the counts for the \"current year\"). \nFor example in our data set we would use the data from 2002 to 2009 to predict 2010, \nand then see how accurate those predictions are. Then refit the model to include\n2010 data and use that to predict 2011 counts.  \n\nAs can be seen in the figure below by *Hyndman et al* in [\"Forecasting principles \nand practice\"](https://otexts.com/fpp3/). \n\n![](https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png)\n*figure reproduced with permission from the authors* \n\nThe downside of this is that you are not using all the data available to you, and \nit is not the final model that you are using for prediction. \n\nAn alternative is to use a method called cross-validation. In this scenario you \nroll over all of the data available to fit multiple models to predict one year ahead. \nYou use more and more data in each model, as seen in the figure below from the \nsame [*Hyndman et al* text]((https://otexts.com/fpp3/). \nFor example, the first model uses 2002 to predict 2003, the second uses 2002 and \n2003 to predict 2004, and so on. \n![](https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png)\n*figure reproduced with permission from the authors*\n\nIn the below we use **purrr** package `map()` function to loop over each dataset. \nWe then put estimates in one data set and merge with the original case counts, \nto use the **yardstick** package to compute measures of accuracy. \nWe compute four measures including: Root mean squared error (RMSE), Mean absolute error\t\n(MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).\n\n<span style=\"color: orange;\">**_CAUTION:_** Note the use of `simulate_pi = FALSE`\nwithin the `predict()` argument. This is because the default behaviour of **trending** \nis to use the **ciTools** package to estimate a prediction interval. This does not \nwork if there are `NA` counts, and also produces more granular intervals. \nSee `?trending::predict.trending_model_fit` for details. </span>  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Cross validation: predicting week(s) ahead based on sliding window\n\n## expand your data by rolling over in 52 week windows (before + after) \n## to predict 52 week ahead\n## (creates longer and longer chains of observations - keeps older data)\n\n## define window want to roll over\nroll_window <- 52\n\n## define weeks ahead want to predict \nweeks_ahead <- 52\n\n## create a data set of repeating, increasingly long data\n## label each data set with a unique id\n## only use cases before year of interest (i.e. 2011)\ncase_roll <- counts %>% \n  filter(epiweek < cut_off) %>% \n  ## only keep the week and case counts variables\n  select(epiweek, case_int) %>% \n    ## drop the last x observations \n    ## depending on how many weeks ahead forecasting \n    ## (otherwise will be an actual forecast to \"unknown\")\n    slice(1:(n() - weeks_ahead)) %>%\n    as_tsibble(index = epiweek) %>% \n    ## roll over each week in x after windows to create grouping ID \n    ## depending on what rolling window specify\n    stretch_tsibble(.init = roll_window, .step = 1) %>% \n  ## drop the first couple - as have no \"before\" cases\n  filter(.id > roll_window)\n\n\n## for each of the unique data sets run the code below\nforecasts <- purrr::map(unique(case_roll$.id), \n                        function(i) {\n  \n  ## only keep the current fold being fit \n  mini_data <- filter(case_roll, .id == i) %>% \n    as_tibble()\n  \n  ## create an empty data set for forecasting on \n  forecast_data <- tibble(\n    epiweek = seq(max(mini_data$epiweek) + 1,\n                  max(mini_data$epiweek) + weeks_ahead,\n                  by = 1),\n    case_int = rep.int(NA, weeks_ahead),\n    .id = rep.int(i, weeks_ahead)\n  )\n  \n  ## add the forecast data to the original \n  mini_data <- bind_rows(mini_data, forecast_data)\n  \n  ## define the cut off based on latest non missing count data \n  cv_cut_off <- mini_data %>% \n    ## only keep non-missing rows\n    drop_na(case_int) %>% \n    ## get the latest week\n    summarise(max(epiweek)) %>% \n    ## extract so is not in a dataframe\n    pull()\n  \n  ## make mini_data back in to a tsibble\n  mini_data <- tsibble(mini_data, index = epiweek)\n  \n  ## define fourier terms (sincos) \n  mini_data <- mini_data %>% \n    mutate(\n    ## combine fourier terms for weeks prior to  and after cut-off date\n    fourier = rbind(\n      ## get fourier terms for previous years\n      forecast::fourier(\n        ## only keep the rows before cut-off\n        filter(mini_data, \n               epiweek <= cv_cut_off), \n        ## include one set of sin cos terms \n        K = 1\n        ), \n      ## predict the fourier terms for following year (using baseline data)\n      fourier(\n        ## only keep the rows before cut-off\n        filter(mini_data, \n               epiweek <= cv_cut_off),\n        ## include one set of sin cos terms \n        K = 1, \n        ## predict 52 weeks ahead\n        h = weeks_ahead\n        )\n      )\n    )\n  \n  \n  # split data for fitting and prediction\n  dat <- mini_data %>% \n    group_by(epiweek <= cv_cut_off) %>%\n    group_split()\n\n  ## define the model you want to fit (negative binomial) \n  model <- glm_nb_model(\n    ## set number of cases as outcome of interest\n    case_int ~\n      ## use epiweek to account for the trend\n      epiweek +\n      ## use the furier terms to account for seasonality\n      fourier\n  )\n\n  # define which data to use for fitting and which for predicting\n  fitting_data <- pluck(dat, 2)\n  pred_data <- pluck(dat, 1)\n  \n  # fit model \n  fitted_model <- trending::fit(model, fitting_data)\n  \n  # forecast with data want to predict with \n  forecasts <- fitted_model %>% \n    predict(data.frame(pred_data), simulate_pi = FALSE)\n  forecasts <- data.frame(forecasts$result[[1]]) %>% \n       ## only keep the week and the forecast estimate\n    select(epiweek, estimate)\n    \n  }\n  )\n\n## make the list in to a data frame with all the forecasts\nforecasts <- bind_rows(forecasts)\n\n## join the forecasts with the observed\nforecasts <- left_join(forecasts, \n                       select(counts, epiweek, case_int),\n                       by = \"epiweek\")\n\n## using {yardstick} compute metrics\n  ## RMSE: Root mean squared error\n  ## MAE:  Mean absolute error\t\n  ## MASE: Mean absolute scaled error\n  ## MAPE: Mean absolute percent error\nmodel_metrics <- bind_rows(\n  ## in your forcasted dataset compare the observed to the predicted\n  rmse(forecasts, case_int, estimate), \n  mae( forecasts, case_int, estimate),\n  mase(forecasts, case_int, estimate),\n  mape(forecasts, case_int, estimate),\n  ) %>% \n  ## only keep the metric type and its output\n  select(Metric  = .metric, \n         Measure = .estimate) %>% \n  ## make in to wide format so can bind rows after\n  pivot_wider(names_from = Metric, values_from = Measure)\n\n## return model metrics \nmodel_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n   rmse   mae  mase  mape\n  <dbl> <dbl> <dbl> <dbl>\n1  252.  199.  1.96  17.3\n```\n:::\n:::\n\n\n\n<!-- ======================================================= -->\n### **surveillance** package {.unnumbered}\n\nIn this section we use the **surveillance** package to create alert thresholds \nbased on outbreak detection algorithms. There are several different methods \navailable in the package, however we will focus on two options here. \nFor details, see these papers on the [application](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf)\nand [theory](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf)\nof the alogirthms used. \n\nThe first option uses the improved Farrington method. This fits a negative \nbinomial glm (including trend) and down-weights past outbreaks (outliers) to \ncreate a threshold level. \n\nThe second option use the glrnb method. This also fits a negative binomial glm \nbut includes trend and fourier terms (so is favoured here). The regression is used\nto calculate the \"control mean\" (~fitted values) - it then uses a computed \ngeneralized likelihood ratio statistic to assess if there is shift in the mean \nfor each week. Note that the threshold for each week takes in to account previous\nweeks so if there is a sustained shift an alarm will be triggered. \n(Also note that after each alarm the algorithm is reset)\n\nIn order to work with the **surveillance** package, we first need to define a \n\"surveillance time series\" object (using the `sts()` function) to fit within the \nframework. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define surveillance time series object\n## nb. you can include a denominator with the population object (see ?sts)\ncounts_sts <- sts(observed = counts$case_int[!is.na(counts$case_int)],\n                  start = c(\n                    ## subset to only keep the year from start_date \n                    as.numeric(str_sub(start_date, 1, 4)), \n                    ## subset to only keep the week from start_date\n                    as.numeric(str_sub(start_date, 7, 8))), \n                  ## define the type of data (in this case weekly)\n                  freq = 52)\n\n## define the week range that you want to include (ie. prediction period)\n## nb. the sts object only counts observations without assigning a week or \n## year identifier to them - so we use our data to define the appropriate observations\nweekrange <- cut_off - start_date\n```\n:::\n\n\n<!-- ======================================================= -->\n#### Farrington method {.unnumbered}\n\nWe then define each of our parameters for the Farrington method in a `list`. \nThen we run the algorithm using `farringtonFlexible()` and then we can extract the \nthreshold for an alert using `farringtonmethod@upperbound`to include this in our \ndataset. It is also possible to extract a TRUE/FALSE for each week if it triggered \nan alert (was above the threshold) using `farringtonmethod@alarm`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define control\nctrl <- list(\n  ## define what time period that want threshold for (i.e. 2011)\n  range = which(counts_sts@epoch > weekrange),\n  b = 9, ## how many years backwards for baseline\n  w = 2, ## rolling window size in weeks\n  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)\n  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)\n  trend = TRUE,\n  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)\n  thresholdMethod = \"nbPlugin\",\n  populationOffset = TRUE\n  )\n\n## apply farrington flexible method\nfarringtonmethod <- farringtonFlexible(counts_sts, ctrl)\n\n## create a new variable in the original dataset called threshold\n## containing the upper bound from farrington \n## nb. this is only for the weeks in 2011 (so need to subset rows)\ncounts[which(counts$epiweek >= cut_off & \n               !is.na(counts$case_int)),\n              \"threshold\"] <- farringtonmethod@upperbound\n```\n:::\n\n\nWe can then visualise the results in ggplot as done previously. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(counts, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in upper bound of aberration algorithm\n  geom_line(aes(y = threshold, colour = \"Alert threshold\"), \n            linetype = \"dashed\", \n            size = 1.5) +\n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Alert threshold\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic() + \n  ## remove title of legend \n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/plot_farrington-1.png){width=672}\n:::\n:::\n\n\n<!-- ======================================================= -->\n#### GLRNB method {.unnumbered}\n\nSimilarly for the GLRNB method we define each of our parameters for the in a `list`, \nthen fit the algorithm and extract the upper bounds.\n\n<span style=\"color: orange;\">**_CAUTION:_** This method uses \"brute force\" (similar to bootstrapping) for calculating thresholds, so can take a long time!</span>\n\nSee the [GLRNB vignette](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) \nfor details. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define control options\nctrl <- list(\n  ## define what time period that want threshold for (i.e. 2011)\n  range = which(counts_sts@epoch > weekrange),\n  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include\n  trend = TRUE,   ## whether to include trend or not\n  refit = FALSE), ## whether to refit model after each alarm\n  ## cARL = threshold for GLR statistic (arbitrary)\n     ## 3 ~ middle ground for minimising false positives\n     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)\n   c.ARL = 2,\n   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak\n   ret = \"cases\"     ## return threshold upperbound as case counts\n  )\n\n## apply the glrnb method\nglrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)\n\n## create a new variable in the original dataset called threshold\n## containing the upper bound from glrnb \n## nb. this is only for the weeks in 2011 (so need to subset rows)\ncounts[which(counts$epiweek >= cut_off & \n               !is.na(counts$case_int)),\n              \"threshold_glrnb\"] <- glrnbmethod@upperbound\n```\n:::\n\n\nVisualise the outputs as previously. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(counts, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in upper bound of aberration algorithm\n  geom_line(aes(y = threshold_glrnb, colour = \"Alert threshold\"), \n            linetype = \"dashed\", \n            size = 1.5) +\n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Alert threshold\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic() + \n  ## remove title of legend \n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/plot_glrnb-1.png){width=672}\n:::\n:::\n\n\n<!-- ======================================================= -->\n## Interrupted timeseries {  }\n\nInterrupted timeseries (also called segmented regression or intervention analysis), \nis often used in assessing the impact of vaccines on the incidence of disease. \nBut it can be used for assessing impact of a wide range of interventions or introductions. \nFor example changes in hospital procedures or the introduction of a new disease \nstrain to a population. \nIn this example we will pretend that a new strain of Campylobacter was introduced\nto Germany at the end of 2008, and see if that affects the number of cases. \nWe will use negative binomial regression again. The regression this time will be \nsplit in to two parts, one before the intervention (or introduction of new strain here) \nand one after (the pre and post-periods). This allows us to calculate an incidence rate ratio comparing the\ntwo time periods. Explaining the equation might make this clearer (if not then just\nignore!). \n\nThe negative binomial regression can be defined as follows: \n\n$$\\log(Y_t)= β_0 + β_1 \\times t+ β_2 \\times δ(t-t_0) + β_3\\times(t-t_0 )^+ + log(pop_t) + e_t$$\n\nWhere:\n$Y_t$is the number of cases observed at time $t$  \n$pop_t$ is the population size in 100,000s at time $t$ (not used here)  \n$t_0$ is the last year of the of the pre-period (including transition time if any)  \n$δ(x$ is the indicator function (it is 0 if x≤0 and 1 if x>0)  \n$(x)^+$ is the cut off operator (it is x if x>0 and 0 otherwise)  \n$e_t$ denotes the residual \nAdditional terms trend and season can be added as needed. \n\n$β_2 \\times δ(t-t_0) + β_3\\times(t-t_0 )^+$ is the generalised linear \npart of the post-period and is zero in the pre-period. \nThis means that the $β_2$ and $β_3$ estimates are the effects of the intervention. \n\nWe need to re-calculate the fourier terms without forecasting here, as we will use\nall the data available to us (i.e. retrospectively). Additionally we need to calculate\nthe extra terms needed for the regression. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## add in fourier terms using the epiweek and case_int variabless\ncounts$fourier <- select(counts, epiweek, case_int) %>% \n  as_tsibble(index = epiweek) %>% \n  fourier(K = 1)\n\n## define intervention week \nintervention_week <- yearweek(\"2008-12-31\")\n\n## define variables for regression \ncounts <- counts %>% \n  mutate(\n    ## corresponds to t in the formula\n      ## count of weeks (could probably also just use straight epiweeks var)\n    # linear = row_number(epiweek), \n    ## corresponds to delta(t-t0) in the formula\n      ## pre or post intervention period\n    intervention = as.numeric(epiweek >= intervention_week), \n    ## corresponds to (t-t0)^+ in the formula\n      ## count of weeks post intervention\n      ## (choose the larger number between 0 and whatever comes from calculation)\n    time_post = pmax(0, epiweek - intervention_week + 1))\n```\n:::\n\n\nWe then use these terms to fit a negative binomial regression, and produce a \ntable with percentage change. What this example shows is that there was no \nsignificant change. \n\n<span style=\"color: orange;\">**_CAUTION:_** Note the use of `simulate_pi = FALSE`\nwithin the `predict()` argument. This is because the default behaviour of **trending** \nis to use the **ciTools** package to estimate a prediction interval. This does not \nwork if there are `NA` counts, and also produces more granular intervals. \nSee `?trending::predict.trending_model_fit` for details. </span>  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define the model you want to fit (negative binomial) \nmodel <- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier + \n    ## add in whether in the pre- or post-period \n    intervention + \n    ## add in the time post intervention \n    time_post\n    )\n\n## fit your model using the counts dataset\nfitted_model <- trending::fit(model, counts)\n\n## calculate confidence intervals and prediction intervals \nobserved <- predict(fitted_model, simulate_pi = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## show estimates and percentage change in a table\nfitted_model %>% \n  ## extract original negative binomial regression\n  get_model() %>% \n  ## get a tidy dataframe of results\n  tidy(exponentiate = TRUE, \n       conf.int = TRUE) %>% \n  ## only keep the intervention value \n  filter(term == \"intervention\") %>% \n  ## change the IRR to percentage change for estimate and CIs \n  mutate(\n    ## for each of the columns of interest - create a new column\n    across(\n      all_of(c(\"estimate\", \"conf.low\", \"conf.high\")), \n      ## apply the formula to calculate percentage change\n            .f = function(i) 100 * (i - 1), \n      ## add a suffix to new column names with \"_perc\"\n      .names = \"{.col}_perc\")\n    ) %>% \n  ## only keep (and rename) certain columns \n  select(\"IRR\" = estimate, \n         \"95%CI low\" = conf.low, \n         \"95%CI high\" = conf.high,\n         \"Percentage change\" = estimate_perc, \n         \"95%CI low (perc)\" = conf.low_perc, \n         \"95%CI high (perc)\" = conf.high_perc,\n         \"p-value\" = p.value)\n```\n:::\n\n\nAs previously we can visualise the outputs of the regression. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nestimate_res <- data.frame(observed$result)\n\nggplot(estimate_res, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate, col = \"Estimate\")) + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add vertical line and label to show where forecasting started\n  geom_vline(\n           xintercept = as.Date(intervention_week), \n           linetype = \"dashed\") + \n  annotate(geom = \"text\", \n           label = \"Intervention\", \n           x = intervention_week, \n           y = max(observed$upper_pi), \n           angle = 90, \n           vjust = 1\n           ) + \n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Estimate\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Unknown or uninitialised column: `upper_pi`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(observed$upper_pi): no non-missing arguments to max; returning\n-Inf\n```\n:::\n\n::: {.cell-output-display}\n![](time_series_files/figure-html/plot_interrupted-1.png){width=672}\n:::\n:::\n\n\n\n<!-- ======================================================= -->\n## Resources {  }\n\n[forecasting: principles and practice textbook](https://otexts.com/fpp3/)  \n[EPIET timeseries analysis case studies](https://github.com/EPIET/TimeSeriesAnalysis)  \n[Penn State course](https://online.stat.psu.edu/stat510/lesson/1) \n[Surveillance package manuscript](https://www.jstatsoft.org/article/view/v070i10)\n\n\n\n\n\n",
    "supporting": [
      "time_series_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\r\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/datatables-binding-0.31/datatables.js\"></script>\r\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\r\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\r\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\r\n<link href=\"../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}