# Sampling

Assessing accurately if an intervention (vaccination campaign, distribution of bednets, etc) reached enough individuals could idealy be done by collecting data from ALL those individuals. For obvious logistical constraints it is impossible in a reasonable amount of time and with limited resources. Drawing a representative sample of the population targeted by the intervention, the target population, is a robust strategy to gather information. It allows us to make reliable inferences on the target population by gathering information on a portion of it.

```{r "concept behind drawing sampling", out.width=c('100%', '100%'), echo=FALSE}
# knitr::include_graphics(here::here("images", "gis_head_image.png"))
```

Sampling strategies to draw a representative sample differ for descriptive studies, typically field surveys assessing the proportion of a population reached by various interventions (vaccination campaign, bednet distribution, etc), or analtyical studies (case control or cohort studies).

Some crucial elements should be defined to reach that goal:

- Target population
- Sampling scheme
	- Choose the number of stages
	- Identify the primary, secondary, tertiary (or more) sampling units and their number
- Sample size

The last steps would be to draw the sample using the identified sampling frame(s) and collect information.

```{r "package setup"}
## load packages from CRAN
pacman::p_load(here,         # File locator
			   randomNames,  # Generate random names
			   dplyr,		 # Data management
			   tidyr,		 # Switching from wide to long
			   ggplot2,      # Ggplot2 graphics
			   metR,         # Filled contours used with ggplot2
			   gridExtra,	 # Plotting ggplot2 graphs side by side
			   DT 			 # Interactive tables
			)
``` 

## Target population

Whether it is for descriptive studies or analytical studies a clear understanding of the population targeted by the intervention of interest is necessary.

You should be able to answer the following (familiar) questions regarding the target population:

- Who: what were the age group, sex, or any other relevant characteristics of the population?
- Where: which regions, cities, villages, areas were targeted?
- When: when did the intervention start/end?

The group of people identified by the answer to those questions is also the same one you should draw your sample from. Any selection bias in your sampling strategy would make your sample less representative of your target population and decrease the robustness of your inferences.

This step might seem straightforward and simple, but it can become complex in a setting with high population mobility or with a very short or a very targeted intervention. Besides, any mistake at this stage will introduce selection biases.

## Sampling scheme

Deciding how many stages and wich sampling method you use at each stage is crucial. It directly impacts your final sapling size and the practical logistic of your data collection step.

There are two main groups of sampling methods:

- **Probability sampling**: every individual has a non-zero probability to be selected and we can estimate its selection probability. Traditionnaly, we want the selected individuals to have equal selection probabilities: a "self-weighted" sample. However, it is not always the case. If it is not a self-weighted sample we must be able to quantify the selection probability of every participant to weight the data appropriately (see Survey analysis). This broad group of methods includes:
	- **Simple random sampling** (SRS): it involves randomly selecting a smaller group of participants/sampling units out of a population/finite number of sampling units with a probability $p=\frac{n}{N}$.
	- **Systematic sampling**: it involves selecting a smaller group of participants/sampling units using a sampling frame. Unlike with the SRS, a constant sampling interval $k=\frac{n}{N}$ is used with the $1^{st}$ sampling unit randomly chosen between 1 and $k$ and every following $k^{th}$ sampling unit selected. It is a reasonable approximation of SRS.
	- **Cluster sampling**: it involves selecting homogeneous groups, clusters, out of a heterogeneous population composed of those groups. The clusters tend to be natural groups, eg: selecting villages to assess the vaccine coverage in a rural area.
	- **Stratified random sampling**: it involves selecting a smaller group of sampling units out of a population divided into smaller mutually exclusive homgeneous groups: strata. The sampling occurs within each strata. The identified strata should be meaningful regarding what we try to measure, eg: vaccine coverge in rural area vs in urban area. 
- **Non-probability sampling**: as opposed to probability sampling some individuals have no chance to be selected and we cannot necessarily estimate their selection probability. This group includes:
	- **Convenience sampling**
	- **Snowball sampling**
	- **Purposeful sampling** (quota samples, typical cases samples, etc)

The different probability sampling methods are not exclusive and can be used together in complex sampling schemes. You can multiply the number of stages of your sampling combining sampling methods at each stage, eg: 2 stage sampling with cluster sampling for the first stage and SRS for the second stage (see Other sampling methods and more complex designs).

Typically, equal selection probabilities in the sample, a self-weighted sample, is desirable because it keeps the analysis simple without the need to use weights. However, unequal selection probabilities can be useful in situations where a subgroup of the target population is of interest, but its proportion is low enough that its selection would be rare with equal selection probabilities. An option could be to stratify the sampling, but it could be quite an effort if we simply want to ensure that it is sampled enough to produce reasonable estimates for this subgroup. Another solution could be to oversample it, leading to unequal selection probabilities. The downside is that it implies [analyzing the data with some additional adjustments](link to the survey analysis chapter) to take that into account.

We will not cover non-probability sampling. This type of methods are more often used in qualitative research and do not allow us to make inferences on the distribution of statistics of interest in the target population.

```{r "sampling_methods", echo=TRUE, warning=FALSE, message=FALSE}
# code chunks to illustrate the probablity sampling methods
```

## Sample size calculation

In this chapter, we will focus on the sample size calculation for descriptive studies.

Several R packages XXXXXXXXX can help you make such calculations easily. However you still need to make choices and sometimes some assumptions. We will here use a parametric approach and explicit the formulae used to highlight the choices you need to make and how they matter.

For any sample size calculation it is necessary to decide/identify:

- The primary variable of interest
	- Categorical (eg: vaccinated/unvaccinated) or continuous data (eg: score measuring well being on a 1 to 10 scale)
- The error estimation, it requires two elements
	- The $\alpha$ level (type I error), typically 0.05
	- The acceptable margin of error/precision.
- An estimation of the variance of the primary variable of interest:
	- For continuous data: this could rely on the literature or it could be asusmed. As a guideline keep in mind that 95% of estimates based on the sample will fall within +/- 1.96 standard deviation of the mean. As long as you have a reasonble idea of what could be the mean in the target population and you can define a range of values around it to be able to capture the value in a sample (which will vary a bit) you can make some assumptions on the variance (the standard deviation is the square root of the variance).
	- For categorical data: it can be summarized as the proportion you expect to find in your sample. The most conservative assumption maximizing the variance would be 0.5.

You can see that a lot of reading is necessary to minimize the assumptions you have to make. When making assumptions, you will often consider several possible variations to consider a range of sample sizes. Your final sample size will sometimes be a tradeoff between conservative assumptions and logistical capacities. When making such tradeoffs, **you must also keep in mind that a sample size too big is a loss of time of resources, whereas a smaple size too small lead to estimates lacking precisions and potentially unreliable**.

The sampling scheme influences the sample size calculation. However, it is common to start by assuming a 1 stage SRS sampling and then adjust the sample size to the specificities of our sampling scheme (stratification or cluster sampling). In practice, cluster sampling and stratified sampling are mostly used in combination with SRS and/or systematic sampling in a multistage sampling. Below we will then assume it is also the case.

### SRS

It is the simplest approach in many ways. The sampling itself is straightforward, as is the anlysis. However, depending on the scale of your survey it often needs to be associated to other sampling methods to avoid logistical hassles.

Building a sampling frame listing all the students of a school to randomly choose some of them is easy enough (1 stage SRS survey). Building such a sampling frame including the students of all the schools of all the cities in an area is less convenient than maybe selecting some cities, then seleting some schools in those cities, and then randomly selecting students in those schools (3 stage surveys with cluster sampling at the first and second stages with SRS at the third stage). Even if the second solution will lead to a higher sample size (see below), it is way easier to plan, to get the necessary data, and eventually to realize.

When it comes to sample size calculation, a first step is often to assume that your use 1 stage sampling with SRS. Then you will take into account the specificities of your chosen design (see below).

```{r "Basic SRS", echo=TRUE, fig.align='center'}
# Let us create a mock sampling frame with 1000 students with their names and sex
set.seed(1) # So that the example can be exactly reproduced
student_list <- data.frame(
					sex=sample(rep(0:1, each=500), 1000, replace=FALSE)) %>% # 0 is male and 1 is female with a 1/1 ratio here,
				mutate(
					prob=ifelse(sex==0, 0.8, 0.6),		 	 # For some reason boys had a higher probability to be vaccinated than girls
					vacci_status=rbinom(n=1000, size=1, prob=prob)) %>% # 0 is unvaccinated and 1 is vaccinated
				mutate(names=randomNames(gender=sex)) # Random name generation
# In real life you will not know the vaccination status of all the students: it could be the reason you are doing the survey, but here we control everything. This will allow us to compare our estimate using our sample to the truth.

# Let us randomly select 250 students
selected_students <- student_list[sample(1:nrow(student_list), 250),]

# Vaccination coverage in our student population (the "true" vaccination coverage)
mean(student_list$vacci_status)

# Vaccination coverage based on our randomly selected students (the estimate of the vaccination coverage)
mean(selected_students$vacci_status)

# The estimate of the vaccination coverage based on the sample is pretty close to the one in the population

datatable(selected_students, rownames=FALSE)
```

#### Primary variable: continuous data

$$
n=\frac{sd^2Z^2}{\Delta^2}
$$

$sd^2$ is the estimate of the variance (squared standard deviation), and $Z=0.196$ for $\alpha=0.05$

```{r "SRS continuous 1", echo=TRUE, fig.align="center", fig.height=4, fig.width=9}
# Let us see what the different combinations of precision or prevalence lead to in terms of sample size using the example of the score assessing well being (between 0 and 10).
ssize <- expand.grid(
			sd=seq(0.1, 1.5, by=0.1),
			delta=seq(0.01, 1, length=100)) %>%
			mutate(n=(sd^2*qnorm(0.975)^2)/(delta^2))

ssize_explo <- ggplot(
					data=ssize,
					aes(x=delta, y=sd, z=log10(n))) +
					geom_contour_filled(
						breaks=seq(0, 4, by=0.5)) +
					labs(fill="Log10 sample size") +
					geom_contour(
						size=1,
						breaks=1:3,
						color="black") +
					geom_text_contour(
						breaks=1:3,
						color="black",
						rotate=FALSE,
						stroke=0.05) +
					geom_vline(
						xintercept=0.1,
						linetype="dashed",
						color="red",
						size=1) +
					ggtitle("Variation of the sample size (log10)\nfor various combinations of sd and delta") +
					theme_bw()

# Let us now restrict a bit the range of the assumptions we make
ssize <- expand.grid(
			sd=seq(0.4, 1, by=0.025),
			delta=0.1) %>%
			mutate(n=(sd^2*qnorm(0.975)^2)/(delta^2))

ssize_zoom <- ggplot(
				data=ssize,
				aes(x=sd, y=n)) +
				geom_line(
					linetype="dashed",
					color="red",
					size=1) +
				ggtitle("Variation of the sample size\nwith sd and assuming delta=0.1") +
				theme_bw()

grid.arrange(
	ssize_explo,
	ssize_zoom,
	ncol=2,
	widths=c(4, 3))
```

You can see that precision has a strong impact on the sample size. It should help you make pragmatic choices and restrict the range of values to consider based on your objectives and logistics. 

#### Primary variable: Categorical data

$$
n=\frac{p(1-p)Z^2}{\Delta^2}
$$

$p(1-p)$ is the estimate of the variance, and $Z=0.196$ for $\alpha=0.05$

```{r "SRS categorical 1", echo=TRUE, fig.align="center", fig.height=4, fig.width=9}
# Let us see what the different combinations of precision or prevalence lead to in terms of sample size
ssize <- expand.grid(
			p=seq(0.5, 0.95, by=0.05),
			delta=seq(0.01, 0.1, by=0.01)) %>%
			mutate(n=(p*(1-p)*qnorm(0.975)^2)/(delta^2))

ssize_explo <- ggplot(
					data=ssize,
					aes(x=delta, y=p, z=log10(n))) +
					geom_contour_filled(
						breaks=seq(0, 4, by=0.5)) +
					labs(fill="Log10 sample size") +
					geom_contour(
						size=1,
						breaks=1:3,
						color="black") +
					geom_text_contour(
						breaks=1:3,
						color="black",
						rotate=FALSE,
						stroke=0.05) +
					geom_vline(
						xintercept=0.05,
						linetype="dashed",
						color="red",
						size=1) +
					ggtitle("Variation of the sample size (log10)\nfor various combinations of p and delta") +
					theme_bw()

# Let us now restrict a bit the range of the assumptions we make
ssize <- expand.grid(
			p=seq(0.6, 0.95, by=0.025),
			delta=0.05) %>%
			mutate(n=(p*(1-p)*qnorm(0.975)^2)/(delta^2))

ssize_zoom <- ggplot(
				data=ssize,
				aes(x=p, y=n)) +
				geom_line(
					linetype="dashed",
					color="red",
					size=1) +
				ggtitle("Variation of the sample size\nwith p and assuming delta=0.05") +
				theme_bw()

grid.arrange(
	ssize_explo,
	ssize_zoom,
	ncol=2,
	widths=c(4, 3))
```

As for continuous data, precision has a strong impact on the sample size, but so does the proportion you assume in the target population.

### Systematic sampling

Systematic sampling is functionally equivalent to SRS: every individual in the sampling frame has the same selection probability. Sample size calculation is then commonly done assuming SRS.

There is a risk of adding a bias depending on the presence of patterns in the way the sampling frame is organized with this strategy though.

```{r "systematic and bias 1", echo=TRUE}
# Let us go back to our sampling frame created to illustrate SRS
# If your remember well boys had more chance to be vaccinated
# Now imagine that for administrative reasons the list of students is ordered by alternating boys and girls
student_list <- student_list %>% # We are just reordering the list to alternate boys and girls
					arrange(sex) %>%
					mutate(index=rep(1:500, 2)) %>%
					arrange(index, sex) %>%
					select(-index)

datatable(student_list %>% select(-prob), rownames=FALSE)

# We still want to select 250 students so let us calculate the sampling interval
interval <- nrow(student_list)/250
# Now the random beginning
set.seed(1)
beginning <- sample(1:interval, 1)

selected_students <- student_list[seq(beginning, nrow(student_list), by=interval),]

# Vaccination coverage in our student population (the "true" vaccination coverage)
mean(student_list$vacci_status)

# Vaccination coverage based on our selected students (the estimate of the vaccination coverage)
mean(selected_students$vacci_status)
```

You can see in this example that we are substantially overestimating the vaccination coverage based on our sample. It is because of the way the sampling frame (the list of students) is ordered and also because our sampling interval happens to be even. The conjunction of the two lead us to oversample or undersample boys if our sampling interval is even if our first selected student is a boy or a girl.

This toy example is just a cartoonish illustration. In practice it can be very difficult to detect such issues before it is too late. The simple solution to this is to randmoly shuffle your sampling frame before making the systematic selection.

```{r "systematic and bias 2", echo=TRUE}
# Let us reshuffle our ordered list of students
set.seed(2)
student_list <- student_list[sample(1:nrow(student_list), nrow(student_list), replace=FALSE),]

datatable(student_list %>% select(-prob), rownames=FALSE)

# Now let us select the 250 students again
interval <- nrow(student_list)/250
# Now the random beginning
beginning <- sample(1:interval, 1)

selected_students <- student_list[seq(beginning, nrow(student_list), by=interval),]

# Vaccination coverage in our student population (the "true" vaccination coverage)
mean(student_list$vacci_status)

# Vaccination coverage based on our selected students (the estimate of the vaccination coverage)
mean(selected_students$vacci_status)
```

### Other sampling methods and more complex designs
 
As soon as you associate various sampling methods by using a multistage sampling design, calculating the sample size will require some information based on the litterature (idealy) or some additional assumptions.

Adding several stages very often means that you use other methods (maybe in association) than SRS/systematic sampling. Cluster sampling typically will lead to sampling participants that are to some extent correlated to the participants of the same cluster. Analysing a sample as is assumes that their characteristics are independent. When cluster sampling has been used at any stage, this assumption does not hold: it then underestimates the variance of your primary variable of interest. This means that you need to inflate the sample size to compensate for this. By how much you need to inflate your sample size? This is the difficult part. **The constant by which you should multiply your sample size assuming SRS is called the "design effect" ($deff$).**

$$
deff=\frac{V_{design}}{V_{SRS}}
$$

$V_{SRS}$ is the variance in a sample using SRS.

$V_{design}$ is the variance in a sample drawn using our alternative design.

$deff$ is usually above 1 with multistage sampling, but the closer it is to 1 the closer the sample size is to the one you would get only with a 1 stage SRS.

This formula also means that if you can get some estimate of $deff$, based on the literature, things become quite simple. You can simply make your sample size calculation assuming SRS and multiply it by $deff$.

$$
n_{design}=deff \times n_{SRS}
$$

An important point though: what you find in the litterature is relevant only if you defined your design and sampling units in a similar way, eg: if your clusters are villages but some article used households it is not useful. Besides, the settings should be reasonably similar as well.

#### Cluster sampling

Cluster sampling is very common sampling strategy. It can be used as a 1 stage sampling scheme, but it is more frequently associated to other sampling methods such as SRS/systematic sampling in multistage sampling schemes.

A common 2 stage sampling scheme uses cluster sampling at the first stage, with probability proportional to size (PPS), and SRS as the second stage. The association of the two leads to an equal selection probability of all the individuals. This is convenient for two main reasons:

- The cluster sampling adds logistical flexibility despite the increased sample size, eg: we first select villages with a probability proportional to population size and then select individuals in the villages.

- The selected participants have an equal selection probability. This means that the sample does not need to be weighted during the analysis.

There are more combinations with more than 2 stages that could lead you to a self-weighted sample. However, keep in mind that oversampling some groups might be desirable too and weighting your data is not excindingly complex either (see the chapter on the analysis).

```{r "self weighting demonstration", echo=TRUE}
# Let us generate a list of villages with their sample sizes
villages <- data.frame(
			name=LETTERS[1:20],
			pop=as.integer(100*rlnorm(20, meanlog=3, sdlog=1.5)))

# I do not define a seed on purpose here. You can rerun the code several times with different situations but always ending with a self-weighted sample.

# Now let us select 10 clusters  by PPS (1 cluster could be 5 households chosen randomly, or any other way you want to define it that makes sense with your context)
villages$p1 <- villages$pop/sum(villages$pop) # Probability that a cluster is selected in the villages
clusters <- sample(villages$name, 10, replace=TRUE, prob=villages$p1)

# In each household we randomly select 1 person using SRS
sample <- data.frame(cluster=clusters) %>%
			left_join(.,
				villages,
				by=c("cluster"="name")) %>%
			mutate(
				p2=1/pop, # The probability of an individual to be sampled once a cluster was selected in their village
				p=p1*p2) # The overall selection probability is the product of p1 and p2

datatable(sample, rownames=FALSE)
```

#### Equal selection probabilities

Let us assume we use the common design describe above for now: 2 stage sampling with cluster sampling by PPS at the 1st stage and SRS at the 2nd stage.

With this sampling scheme, an alternative way to define $deff$ is:

$$
deff=1+(n-1)\delta
$$

$n$ is the average cluster size. Ideally the size of all the clusters is identical or very similar.

$\delta$ is the intra-class correlation (ICC). It reflects how similar individuals tend be in a cluster, eg: if 1 child had 2 doses of MCV, his/her siblings are more likely to have had 2 doses as well because they are raised and taken care of in a very similar way.

As for the $deff$, the litterature can provide reasonable values for $\delta$. But again, it is necessary to ensure that you are referring to comparable designs and settings.

Let us have a look at how the $deff$ behaves for various values of cluster size and ICC.

```{r "deff and n", echo=TRUE, fig.width=5, fig.height=4, fig.align="center"}
deff_var <- expand.grid(
				n=1:100,
				delta=seq(0.1, 0.8, by=0.1)) %>%
				mutate(deff=1+(n-1)*delta)

ggplot(
	data=deff_var,
	aes(x=n, y=delta, z=deff)) +
	geom_contour_filled(
		breaks=c(1, 1.1, 5, seq(10, 90, by=10))) +
	labs(fill="deff") +
	theme_bw()
```

This highlights two things:

- The smaller the number of individuals selected from the same cluster the lower the $deff$. For a constant sample size, smaller cluster size means more clusters. **If we have reasonable data/experience to pick a value for the $deff$ without information on $\delta$ it is safer to select a greater number of clusters of small sizes than the opposite.** You would be more likely to end up with a lower $deff$ than compared to your assumption than the opposite. Like a lot of things regarding sampling, it also comes down to the trade-offs you can do with logistical, financial, and time constraints you have to collect data.

- For a given cluster size, the more heterogenous it is (the lower the value of $\delta$) the more precise the estimates will be because it leads to a lower $deff$.

Another way to look at it: SRS can be viewed as an extreme case of cluster sampling with clusters of size 1 (leading to $deff=1$).

#### Unequal selection probabilities

If we use cluster sampling but with unequal selection probabilities, then an alternative way to see $deff$ is:

$$
deff=\frac{N\sum_{k=1}^K(n_kw^2_k)}{\sum_{k=1}^K(n_kw_k)^2}(1+(n-1)\delta)
$$

$n_k$ is the size of the cluster $k$.

$w_k$ is the weight of the individuals of the cluster $k$ and it is the inverse of the selection probability in this cluster.

$N=\frac{1}{K}\sum^K_{k=1}{n_k}$ is the total sample size.

$n=\overline{n_k}$ is average the cluster size.

$\delta$ is the ICC.

### Stratified sampling

Stratification becomes relevant when the target population is a mixture of reasonably homogeneous subgroups. It could be anything as long as it makes sense in regard to what your survey primarily measures, eg: rural vs urban, the various age groups, men vs women, etc. The identified strata are exclusive and their sum should cover the whole population. They identify more homogeneous partitions, reducing the variance of the statistics of interest within them. It leads to more precise estimates. Unlike for cluster sampling the more homogeneous the strata are, the more precise the estimates will be.

When it comes to sample size calculation stratification is the same as applying your sampling strategy independently in the defined strata. You could vary the ratio between the necessary sample size in each strata any way you want as long as you have the minimum precision you look for in each strata.

$$
N=\sum_{s=1}^{S}n_s
$$

$N$ is the total sample size including all the $S$ strata.

$n_s$ is the sample size for the strata $s$. It could be the same for every strata if the assumption used are conservative enough and you get the required miminum precision you want. So, if you use a 1 to 1 ratio with 2 strata, you multiply your sample size by 2. It is very powerful to control some confounding biases but can increase you total sample size really fast.

## Number of stages

The number of stages is the number of phases involved in the sample selection. If you randomly select individuals from a register: there is only one stage. There is no limit to the number of stages. However, adding a stage tends to increase the sample size, so it should simplify the logistical complexity in return to be worth it and/or allow you to answer some questions, eg: stratifying the sampling to be able to provide reasonable estimate in urban vs rural areas.

The difficult aspect is that choosing the sampling scheme will depend on the situation and the available logistics: there is no simple way to decide the number of stages. The positive aspect is that it means there is a LOT of freedom in defining the number of stages.

Choosing the number of stages goes hand in hand with choosing how you will sample at each stage. Sometimes the setting or the questions you want to answer will naturally orient your choices. But, even though I keep repeating it, the logistical, financial, and time constraints should also be taken into account.

### Identifying sampling units

## Drawing a sample using "sampling frame(s)"

Obtaining the information to build a sampling frame can be time consuming in resource limited settings. Demographic informations at the necessary scale can often be gathered from recent census surveys from the local statistics institute, other population based surveys such as [DHS](https://dhsprogram.com/) or [MICS](https://mics.unicef.org/), or even the EPI. Ensuring the data you use to build your sampling frame are up-tp-date and at the necessary geographical scale can be challenging, and can require some trade-offs.

Sampling frames do not need to be a simple list (paper or otherwise). Other recent data could be really powerful to create a sampling frame in a setting where there are a lot of IDPs, or a natural disaster has substantially modified population distribution, or simply because you cannot find traditional data sources reliable enough. GIS sampling has been used more frequently in recent years to draw samples in complex settings. Although it can look more complex, in many ways it is often a straigthforward application of SRS but using satelite images, or a map, or another GIS product as a sampling frame.

```{r "spatial sampling", echo=TRUE}

```
