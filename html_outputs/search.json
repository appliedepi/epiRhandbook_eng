[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Epidemiologist R Handbook",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#r-for-applied-epidemiology-and-public-health",
    "href": "index.html#r-for-applied-epidemiology-and-public-health",
    "title": "The Epidemiologist R Handbook",
    "section": "R for applied epidemiology and public health",
    "text": "R for applied epidemiology and public health\nUsage: This handbook has been used over 3 million times by 850,000 people around the world.\nObjective: Serve as a quick R code reference manual (online and offline) with task-centered examples that address common epidemiological problems.\nAre you just starting with R? Try our free interactive tutorials or synchronous, virtual intro course used by US CDC, WHO, and 400+ other health agencies and Field Epi Training Programs worldwide.\nLanguages: French (Français), Spanish (Español), Vietnamese (Tiếng Việt), Japanese (日本), Turkish (Türkçe), Portuguese (Português), Russian (Русский)\n\n\n\n\n\n\n\n Written by epidemiologists, for epidemiologists\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\nApplied Epi is a nonprofit organisation and grassroots movement of frontline epis from around the world. We write in our spare time to offer this resource to the community. Your encouragement and feedback is most welcome:\n\nVisit our website and join our contact list\n\ncontact@appliedepi.org, tweet @appliedepi, or LinkedIn\n\nSubmit issues to our Github repository\n\nWe offer live R training from instructors with decades of applied epidemiology experience - www.appliedepi.org/live.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-handbook",
    "href": "index.html#how-to-use-this-handbook",
    "title": "The Epidemiologist R Handbook",
    "section": "How to use this handbook",
    "text": "How to use this handbook\n\nBrowse the pages in the Table of Contents, or use the search box\nClick the “copy” icons to copy code\n\nYou can follow-along with the example data\n\nOffline version\nSee instructions in the Download handbook and data page.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "The Epidemiologist R Handbook",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis handbook is produced by an independent collaboration of epidemiologists from around the world drawing upon experience with organizations including local, state, provincial, and national health agencies, the World Health Organization (WHO), Doctors without Borders (MSF), hospital systems, and academic institutions.\nThis handbook is not an approved product of any specific organization. Although we strive for accuracy, we provide no guarantee of the content in this book.\n\nContributors\nEditor: Neale Batra\nAuthors: Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, Isaac Florence, Natalie Fischer, Aminata Ndiaye, Liza Coyer, Jonathan Polonsky, Yurie Izawa, Chris Bailey, Daniel Molling, Isha Berry, Emma Buajitti, Mathilde Mousset, Sara Hollis, Wen Lin\nReviewers and supporters: Pat Keating, Amrish Baidjoe, Annick Lenglet, Margot Charette, Danielly Xavier, Marie-Amélie Degail Chabrat, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Kate Kelsey, Berhe Etsay, John Rossow, Mackenzie Zendt, James Wright, Laura Haskins, Flavio Finger, Tim Taylor, Jae Hyoung Tim Lee, Brianna Bradley, Wayne Enanoria, Manual Albela Miranda, Molly Mantus, Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga\nIllustrations: Calder Fong\n\n\n\n\n\n\nFunding and support\nThis book was primarily a volunteer effort that took thousands of hours to create.\nThe handbook received some supportive funding via a COVID-19 emergency capacity-building grant from TEPHINET, the global network of Field Epidemiology Training Programs (FETPs).\nAdministrative support was provided by the EPIET Alumni Network (EAN), with special thanks to Annika Wendland. EPIET is the European Programme for Intervention Epidemiology Training.\nSpecial thanks to Médecins Sans Frontières (MSF) Operational Centre Amsterdam (OCA) for their support during the development of this handbook.\nThis publication was supported by Cooperative Agreement number NU2GGH001873, funded by the Centers for Disease Control and Prevention through TEPHINET, a program of The Task Force for Global Health. Its contents are solely the responsibility of the authors and do not necessarily represent the official views of the Centers for Disease Control and Prevention, the Department of Health and Human Services, The Task Force for Global Health, Inc. or TEPHINET.\n\n\nInspiration\nThe multitude of tutorials and vignettes that provided knowledge for development of handbook content are credited within their respective pages.\nMore generally, the following sources provided inspiration for this handbook:\nThe “R4Epis” project (a collaboration between MSF and RECON)\nR Epidemics Consortium (RECON)\nR for Data Science book (R4DS)\nbookdown: Authoring Books and Technical Documents with R Markdown\nNetlify hosts this website",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#terms-of-use-and-contribution",
    "href": "index.html#terms-of-use-and-contribution",
    "title": "The Epidemiologist R Handbook",
    "section": "Terms of Use and Contribution",
    "text": "Terms of Use and Contribution\n\nLicense\n Applied Epi Incorporated, 2021 This work is licensed by Applied Epi Incorporated under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nAcademic courses and epidemiologist training programs are welcome to contact us about use or adaptation of this material (email contact@appliedepi.org).\n\n\nCitation\nBatra, Neale, et al. The Epidemiologist R Handbook. 2021. \n\n\nContribution\nIf you would like to make a content contribution, please contact with us first via Github issues or by email. We are implementing a schedule for updates and are creating a contributor guide.\nPlease note that the epiRhandbook project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "new_pages/editorial_style.html",
    "href": "new_pages/editorial_style.html",
    "title": "1  Editorial and technical notes",
    "section": "",
    "text": "1.1 Approach and style\nThe potential audience for this book is large. It will surely be used by people very new to R, and also by experienced R users looking for best practices and tips. So it must be both accessible and succinct. Therefore, our approach was to provide just enough text explanation that someone very new to R can apply the code and follow what the code is doing.\nA few other points:",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Editorial and technical notes</span>"
    ]
  },
  {
    "objectID": "new_pages/editorial_style.html#approach-and-style",
    "href": "new_pages/editorial_style.html#approach-and-style",
    "title": "1  Editorial and technical notes",
    "section": "",
    "text": "This is a code reference book accompanied by relatively brief examples - not a thorough textbook on R or data science\n\nThis is a R handbook for use within applied epidemiology - not a manual on the methods or science of applied epidemiology\n\nThis is intended to be a living document - optimal R packages for a given task change often and we welcome discussion about which to emphasize in this handbook\n\n\nR packages\nSo many choices\nOne of the most challenging aspects of learning R is knowing which R package to use for a given task. It is a common occurrence to struggle through a task only later to realize - hey, there’s an R package that does all that in one command line!\nIn this handbook, we try to offer you at least two ways to complete each task: one tried-and-true method (probably in base R or tidyverse) and one special R package that is custom-built for that purpose. We want you to have a couple options in case you can’t download a given package or it otherwise does not work for you.\nIn choosing which packages to use, we prioritized R packages and approaches that have been tested and vetted by the community, minimize the number of packages used in a typical work session, that are stable (not changing very often), and that accomplish the task simply and cleanly\nThis handbook generally prioritizes R packages and functions from the tidyverse. Tidyverse is a collection of R packages designed for data science that share underlying grammar and data structures. All tidyverse packages can be installed or loaded via the tidyverse package. Read more at the tidyverse website.\nWhen applicable, we also offer code options using base R - the packages and functions that come with R at installation. This is because we recognize that some of this book’s audience may not have reliable internet to download extra packages.\nLinking functions to packages explicitly\nIt is often frustrating in R tutorials when a function is shown in code, but you don’t know which package it is from! We try to avoid this situation.\nIn the narrative text, package names are written in bold (e.g. dplyr) and functions are written like this: mutate(). We strive to be explicit about which package a function comes from, either by referencing the package in nearby text or by specifying the package explicitly in the code like this: dplyr::mutate(). It may look redundant, but we are doing it on purpose.\nSee the page on R basics to learn more about packages and functions.\n\n\nCode style\nIn the handbook, we frequently utilize “new lines”, making our code appear “long”. We do this for a few reasons:\n\nWe can write explanatory comments with # that are adjacent to each little part of the code\n\nGenerally, longer (vertical) code is easier to read\n\nIt is easier to read on a narrow screen (no sideways scrolling needed)\n\nFrom the indentations, it can be easier to know which arguments belong to which function\n\nAs a result, code that could be written like this:\n\nlinelist %&gt;% \n  group_by(hospital) %&gt;%  # group rows by hospital\n  slice_max(date, n = 1, with_ties = F) # if there's a tie (of date), take the first row\n\n…is written like this:\n\nlinelist %&gt;% \n  group_by(hospital) %&gt;% # group rows by hospital\n  slice_max(\n    date,                # keep row per group with maximum date value \n    n = 1,               # keep only the single highest row \n    with_ties = F)       # if there's a tie (of date), take the first row\n\nR code is generally not affected by new lines or indentations. When writing code, if you initiate a new line after a comma it will apply automatic indentation patterns.\nWe also use lots of spaces (e.g. n = 1 instead of n=1) because it is easier to read. Be kind to the people reading your code!\n\n\nNomenclature\nIn this handbook, we generally reference “columns” and “rows” instead of “variables” and “observations”. As explained in this primer on “tidy data”, most epidemiological statistical datasets consist structurally of rows, columns, and values.\nVariables contain the values that measure the same underlying attribute (like age group, outcome, or date of onset). Observations contain all values measured on the same unit (e.g. a person, site, or lab sample). So these aspects can be more difficult to tangibly define.\nIn “tidy” datasets, each column is a variable, each row is an observation, and each cell is a single value. However some datasets you encounter will not fit this mold - a “wide” format dataset may have a variable split across several columns (see an example in the Pivoting data page). Likewise, observations could be split across several rows.\nMost of this handbook is about managing and transforming data, so referring to the concrete data structures of rows and columns is more relevant than the more abstract observations and variables. Exceptions occur primarily in pages on data analysis, where you will see more references to variables and observations.\n\n\nNotes\nHere are the types of notes you may encounter in the handbook:\nNOTE: This is a note\nTIP: This is a tip.\nCAUTION: This is a cautionary note.\nDANGER: This is a warning.",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Editorial and technical notes</span>"
    ]
  },
  {
    "objectID": "new_pages/editorial_style.html#editorial-decisions",
    "href": "new_pages/editorial_style.html#editorial-decisions",
    "title": "1  Editorial and technical notes",
    "section": "1.2 Editorial decisions",
    "text": "1.2 Editorial decisions\nBelow, we track significant editorial decisions around package and function choice. If you disagree or want to offer a new tool for consideration, please join/start a conversation on our Github page.\nTable of package, function, and other editorial decisions\n\n\n\n\n\n\n\n\n\nSubject\nConsidered\nOutcome\nBrief rationale\n\n\n\n\nGeneral coding approach\ntidyverse, data.table, base\ntidyverse, with a page on data.table, and mentions of base alternatives for readers with no internet\ntidyverse readability, universality, most-taught\n\n\nPackage loading\nlibrary(),install.packages(), require(), pacman\npacman\nShortens and simplifies code for most multi-package install/load use-cases\n\n\nImport and export\nrio, many other packages\nrio\nEase for many file types\n\n\nGrouping for summary statistics\ndplyr group_by(), stats aggregate()\ndplyr group_by()\nConsistent with tidyverse emphasis\n\n\nPivoting\ntidyr (pivot functions), reshape2 (melt/cast), tidyr (spread/gather)\ntidyr (pivot functions)\nreshape2 is retired, tidyr uses pivot functions as of v1.0.0\n\n\nClean column names\nlinelist, janitor\njanitor\nConsolidation of packages emphasized\n\n\nEpiweeks\nlubridate, aweek, tsibble, zoo\nlubridate generally, the others for specific cases\nlubridate’s flexibility, consistency, package maintenance prospects\n\n\nggplot labels\nlabs(), ggtitle()/ylab()/xlab()\nlabs()\nall labels in one place, simplicity\n\n\nConvert to factor\nfactor(), forcats\nforcats\nits various functions also convert to factor in same command\n\n\nEpidemic curves\nincidence, ggplot2, EpiCurve\nincidence2 as quick, ggplot2 as detailed\ndependability\n\n\nConcatenation\npaste(), paste0(), str_glue(), glue()\nstr_glue()\nMore simple syntax than paste functions; within stringr",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Editorial and technical notes</span>"
    ]
  },
  {
    "objectID": "new_pages/editorial_style.html#major-revisions",
    "href": "new_pages/editorial_style.html#major-revisions",
    "title": "1  Editorial and technical notes",
    "section": "1.3 Major revisions",
    "text": "1.3 Major revisions\n\n\n\nDate\nMajor changes\n\n\n\n\n10 May 2021\nRelease of version 1.0.0\n\n\n20 Nov 2022\nRelease of version 1.0.1\n\n\n\nNEWS With version 1.0.1 the following changes have been implemented:\n\nUpdate to R version 4.2\n\nData cleaning: switched {linelist} to {matchmaker}, removed unnecessary line from case_when() example\n\nDates: switched {linelist} guess_date() to {parsedate} parse_date()\nPivoting: slight update to pivot_wider() id_cols=\n\nSurvey analysis: switched plot_age_pyramid() to age_pyramid(), slight change to alluvial plot code\n\nHeat plots: added ungroup() to agg_weeks chunk\n\nInteractive plots: added ungroup() to chunk that makes agg_weeks so that expand() works as intended\n\nTime series: added data.frame() around objects within all trending::fit() and predict() commands\n\nCombinations analysis: Switch case_when() to ifelse() and added optional across() code for preparing the data\n\nTransmission chains: Update to more recent version of {epicontacts}",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Editorial and technical notes</span>"
    ]
  },
  {
    "objectID": "new_pages/editorial_style.html#session-info-r-rstudio-packages",
    "href": "new_pages/editorial_style.html#session-info-r-rstudio-packages",
    "title": "1  Editorial and technical notes",
    "section": "1.4 Session info (R, RStudio, packages)",
    "text": "1.4 Session info (R, RStudio, packages)\nBelow is the information on the versions of R, RStudio, and R packages used during this rendering of the Handbook.\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31 ucrt)\n os       Windows 11 x64 (build 22621)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       Europe/Stockholm\n date     2024-06-19\n pandoc   3.1.11 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.2   2023-12-11 [2] CRAN (R 4.3.2)\n digest        0.6.35  2024-03-11 [1] CRAN (R 4.3.3)\n evaluate      0.23    2023-11-01 [2] CRAN (R 4.3.2)\n fastmap       1.1.1   2023-02-24 [2] CRAN (R 4.3.2)\n htmltools     0.5.8   2024-03-25 [1] CRAN (R 4.3.3)\n htmlwidgets   1.6.4   2023-12-06 [2] CRAN (R 4.3.2)\n jsonlite      1.8.8   2023-12-04 [2] CRAN (R 4.3.2)\n knitr         1.45    2023-10-30 [2] CRAN (R 4.3.2)\n rlang         1.1.3   2024-01-10 [2] CRAN (R 4.3.2)\n rmarkdown     2.26    2024-03-05 [1] CRAN (R 4.3.3)\n rstudioapi    0.15.0  2023-07-07 [2] CRAN (R 4.3.2)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.3.2)\n xfun          0.43    2024-03-25 [1] CRAN (R 4.3.3)\n\n [1] C:/Users/ngulu864/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.2/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Editorial and technical notes</span>"
    ]
  },
  {
    "objectID": "new_pages/data_used.html",
    "href": "new_pages/data_used.html",
    "title": "2  Download handbook and data",
    "section": "",
    "text": "2.1 Download offline handbook\nYou can download the offline version of this handbook as an HTML file so that you can view the file in your web browser even if you no longer have internet access. If you are considering offline use of the Epi R Handbook here are a few things to consider:\nThere are two ways you can download the handbook:",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Download handbook and data</span>"
    ]
  },
  {
    "objectID": "new_pages/data_used.html#download-offline-handbook",
    "href": "new_pages/data_used.html#download-offline-handbook",
    "title": "2  Download handbook and data",
    "section": "",
    "text": "When you open the file it may take a minute or two for the images and the ‘Table of Contents’ to load.\n\nThe offline handbook has a slightly different layout - one very long page with Table of Contents on the left. To search for specific terms use Ctrl+f (Cmd-f).\n\nSee the Suggested packages page to assist you with installing appropriate R packages before you lose internet connectivity. You will not be able to install new packages without internet access.\n\nInstall our R package epirhandbook that contains all the example data (install process described below).\n\n\n\nOption 1: Use download link\nFor quick access, right-click this link and select “Save link as”.\nIf on a Mac, use Cmd+click. If on a mobile, press and hold the link and select “Save link”. The handbook will download to your device. If a screen with raw HTML code appears, ensure you have followed the above instructions or try Option 2.\n\n\nOption 2: Use our R package\nWe offer an R package called epirhandbook. It includes a function download_book() that downloads the handbook file from our Github repository to your computer.\nThis package also contains a function get_data() that downloads all the example data to your computer.\nRun the following code to install our R package epirhandbook from the Github repository appliedepi. This package is not on CRAN, so use the special function p_install_gh() to install it from Github.\n\n# install the latest version of the Epi R Handbook package\npacman::p_install_gh(\"appliedepi/epirhandbook\")\n\nNow, load the package for use in your current R session:\n\n# load the package for use\npacman::p_load(epirhandbook)\n\nNext, run the package’s function download_book() (with empty parentheses) to download the handbook to your computer. Assuming you are in RStudio, a window will appear allowing you to select a save location.\n\n# download the offline handbook to your computer\ndownload_book()",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Download handbook and data</span>"
    ]
  },
  {
    "objectID": "new_pages/data_used.html#download-data-to-follow-along",
    "href": "new_pages/data_used.html#download-data-to-follow-along",
    "title": "2  Download handbook and data",
    "section": "2.2 Download data to follow along",
    "text": "2.2 Download data to follow along\nTo “follow along” with the handbook pages, you can download the example data and outputs.\n\nUse our R package\nThe easiest approach to download all the data is to install our R package epirhandbook. It contains a function get_data() that saves all the example data to a folder of your choice on your computer.\nTo install our R package epirhandbook, run the following code. This package is not on CRAN, so use the function p_install_gh() to install it. The input is referencing our Github organisation (“appliedepi”) and the epirhandbook package.\n\n# install the latest version of the Epi R Handbook package\npacman::p_install_gh(\"appliedepi/epirhandbook\")\n\nNow, load the package for use in your current R session:\n\n# load the package for use\npacman::p_load(epirhandbook)\n\nNext, use the package’s function get_data() to download the example data to your computer. Run get_data(\"all\") to get all the example data, or provide a specific file name and extension within the quotes to retrieve only one file.\nThe data have already been downloaded with the package, and simply need to be transferred out to a folder on your computer. A pop-up window will appear, allowing you to select a save folder location. We suggest you create a new “data” folder as there are almost 80 files (including example data and example outputs).\n\n# download all the example data into a folder on your computer\nget_data(\"all\")\n\n# download only the linelist example data into a folder on your computer\nget_data(file = \"linelist_cleaned.rds\")\n\n\n# download a specific file into a folder on your computer\nget_data(\"linelist_cleaned.rds\")\n\nOnce you have used get_data() to save a file to your computer, you will still need to import it into R. see the Import and export page for details.\nIf you wish, you can review all the data used in this handbook in the “data” folder of our Github repository.\n\n\nDownload one-by-one\nThis option involves downloading the data file-by-file from our Github repository via either a link or an R command specific to the file. Some file types allow a download button, while others can be downloaded via an R command.\n\nCase linelist\nThis is a fictional Ebola outbreak, expanded by the handbook team from the ebola_sim practice dataset in the outbreaks package.\n\nClick to download the “raw” linelist (.xlsx). The “raw” case linelist is an Excel spreadsheet with messy data. Use this to follow-along with the Cleaning data and core functions page.\nClick to download the “clean” linelist (.rds). Use this file for all other pages of this handbook that use the linelist. A .rds file is an R-specific file type that preserves column classes. This ensures you will have only minimal cleaning to do after importing the data into R.\n\nOther related files:\n\nClick to download the “clean” linelist as an Excel file\nPart of the cleaning page uses a “cleaning dictionary” (.csv file). You can load it directly into R by running the following commands:\n\n\npacman::p_load(rio) # install/load the rio package\n\n# import the file directly from Github\ncleaning_dict &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/cleaning_dict.csv\")\n\n\n\nMalaria count data\nThese data are fictional counts of malaria cases by age group, facility, and day. A .rds file is an R-specific file type that preserves column classes. This ensures you will have only minimal cleaning to do after importing the data into R.\n Click to download the malaria count data (.rds file) \n\n\nLikert-scale data\nThese are fictional data from a Likert-style survey, used in the page on Demographic pyramids and Likert-scales. You can load these data directly into R by running the following commands:\n\npacman::p_load(rio) # install/load the rio package\n\n# import the file directly from Github\nlikert_data &lt;- import(\"https://raw.githubusercontent.com/appliedepi/epirhandbook_eng/master/data/likert_data.csv\")\n\n\n\nFlexdashboard\nBelow are links to the file associated with the page on Dashboards with R Markdown:\n\nTo download the R Markdown for the outbreak dashboard, right-click this link (Cmd+click for Mac) and select “Save link as”.\n\nTo download the HTML dashboard, right-click this link (Cmd+click for Mac) and select “Save link as”.\n\n\n\nContact Tracing\nThe Contact Tracing page demonstrated analysis of contact tracing data, using example data from Go.Data. The data used in the page can be downloaded as .rds files by clicking the following links:\n Click to download the case investigation data (.rds file) \n Click to download the contact registration data (.rds file) \n Click to download the contact follow-up data (.rds file) \nNOTE: Structured contact tracing data from other software (e.g. KoBo, DHIS2 Tracker, CommCare) may look different. If you would like to contribute alternative sample data or content for this page, please contact us.\nTIP: If you are deploying Go.Data and want to connect to your instance’s API, see the Import and export page (API section) and the Go.Data Community of Practice.\n\n\nGIS\nShapefiles have many sub-component files, each with a different file extention. One file will have the “.shp” extension, but others may have “.dbf”, “.prj”, etc. You will need to have all of these different files within the same folder to use the shapefile.\nThe GIS basics page provides links to the Humanitarian Data Exchange website where you can download the shapefiles directly as zipped files.\nFor example, the health facility points data can be downloaded here. Download “hotosm_sierra_leone_health_facilities_points_shp.zip”. Once saved to your computer, “unzip” the folder. You will see several files with different extensions (e.g. “.shp”, “.prj”, “.shx”) - all these must be saved to the same folder on your computer. Then to import into R, provide the file path and name of the “.shp” file to st_read() from the sf package (as described in the GIS basics page).\nIf you follow Option 1 to download all the example data (via our R package epirhandbook), all the shapefiles are included.\nAlternatively, you can download the shapefiles from the R Handbook Github “data” folder (see the “gis” sub-folder). However, be aware that you will need to download each sub-file individually to your computer. In Github, click on each file individually and download them by clicking on the “Download” button. Below, you can see how the shapefile “sle_adm3” consists of many files - each of which would need to be downloaded from Github.\n\n\n\n\n\n\n\n\n\n\n\nPhylogenetic trees\nSee the page on Phylogenetic trees. Newick file of phylogenetic tree constructed from whole genome sequencing of 299 Shigella sonnei samples and corresponding sample data (converted to a text file). The Belgian samples and resulting data are kindly provided by the Belgian NRC for Salmonella and Shigella in the scope of a project conducted by an ECDC EUPHEM Fellow, and will also be published in a manuscript. The international data are openly available on public databases (National Center for Biotechnology Information, NCBI) and have been previously published.\n\nTo download the “Shigella_tree.txt” phylogenetic tree file, right-click this link (Cmd+click for Mac) and select “Save link as”.\n\nTo download the “sample_data_Shigella_tree.csv” with additional information on each sample, right-click this link (Cmd+click for Mac) and select “Save link as”.\n\nTo see the new, created subset-tree, right-click this link (Cmd+click for Mac) and select “Save link as”. The .txt file will download to your computer.\n\nYou can then import the .txt files with read.tree() from the ape package, as explained in the page.\n\nape::read.tree(\"Shigella_tree.txt\")\n\n\n\nStandardization\nSee the page on Standardised rates. You can load the data directly from our Github repository on the internet into your R session with the following commands:\n\n# install/load the rio package\npacman::p_load(rio) \n\n##############\n# Country A\n##############\n# import demographics for country A directly from Github\nA_demo &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics.csv\")\n\n# import deaths for country A directly from Github\nA_deaths &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryA.csv\")\n\n##############\n# Country B\n##############\n# import demographics for country B directly from Github\nB_demo &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics_2.csv\")\n\n# import deaths for country B directly from Github\nB_deaths &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryB.csv\")\n\n\n###############\n# Reference Pop\n###############\n# import demographics for country B directly from Github\nstandard_pop_data &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/world_standard_population_by_sex.csv\")\n\n\n\nTime series and outbreak detection\nSee the page on Time series and outbreak detection. We use campylobacter cases reported in Germany 2002-2011, as available from the surveillance R package. This dataset has been adapted from the original, in that 3 months of data have been deleted from the end of 2011 for demonstration purposes.\n Click to download  Campylobacter in Germany (.xlsx) \nWe also use climate data from Germany 2002-2011 (temperature in degrees celsius and rain fail in millimetres) . These were downloaded from the EU Copernicus satellite reanalysis dataset using the ecmwfr package. You will need to download all of these and import them with stars::read_stars() as explained in the time series page.\n Click to download  Germany weather 2002 (.nc file) \n Click to download  Germany weather 2003 (.nc file) \n Click to download  Germany weather 2004 (.nc file) \n Click to download  Germany weather 2005 (.nc file) \n Click to download  Germany weather 2006 (.nc file) \n Click to download  Germany weather 2007 (.nc file) \n Click to download  Germany weather 2008 (.nc file) \n Click to download  Germany weather 2009 (.nc file) \n Click to download  Germany weather 2010 (.nc file) \n Click to download  Germany weather 2011 (.nc file) \n\n\nSurvey analysis\nFor the survey analysis page we use fictional mortality survey data based off MSF OCA survey templates. This fictional data was generated as part of the “R4Epis” project.\n Click to download  Fictional survey data (.xlsx) \n Click to download  Fictional survey data dictionary (.xlsx) \n Click to download  Fictional survey population data (.xlsx) \n\n\nShiny\nThe page on Dashboards with Shiny demonstrates the construction of a simple app to display malaria data.\nTo download the R files that produce the Shiny app:\nYou can  click here to download the app.R file that contains both the UI and Server code for the Shiny app.\nYou can  click here to download the facility_count_data.rds file that contains malaria data for the Shiny app. Note that you may need to store it within a “data” folder for the here() file paths to work correctly.\nYou can  click here to download the global.R file that should run prior to the app opening, as explained in the page.\nYou can  click here to download the plot_epicurve.R file that is sourced by global.R. Note that you may need to store it within a “funcs” folder for the here() file paths to work correctly.",
    "crumbs": [
      "About this book",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Download handbook and data</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html",
    "href": "new_pages/basics.html",
    "title": "3  R Basics",
    "section": "",
    "text": "3.1 Why use R?\nAs stated on the R project website, R is a programming language and environment for statistical computing and graphics. It is highly versatile, extendable, and community-driven.\nCost\nR is free to use! There is a strong ethic in the community of free and open-source material.\nReproducibility\nConducting your data management and analysis through a programming language (compared to Excel or another primarily point-click/manual tool) enhances reproducibility, makes error-detection easier, and eases your workload.\nCommunity\nThe R community of users is enormous and collaborative. New packages and tools to address real-life problems are developed daily, and vetted by the community of users. As one example, R-Ladies is a worldwide organization whose mission is to promote gender diversity in the R community, and is one of the largest organizations of R users. It likely has a chapter near you!",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#key-terms",
    "href": "new_pages/basics.html#key-terms",
    "title": "3  R Basics",
    "section": "3.2 Key terms",
    "text": "3.2 Key terms\nRStudio - RStudio is a Graphical User Interface (GUI) for easier use of R. Read more in the RStudio section.\nObjects - Everything you store in R - datasets, variables, a list of village names, a total population number, even outputs such as graphs - are objects which are assigned a name and can be referenced in later commands. Read more in the Objects section.\nFunctions - A function is a code operation that accept inputs and returns a transformed output. Read more in the Functions section.\nPackages - An R package is a shareable bundle of functions. Read more in the Packages section.\nScripts - A script is the document file that hold your commands. Read more in the Scripts section",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#learning",
    "href": "new_pages/basics.html#learning",
    "title": "3  R Basics",
    "section": "3.3 Resources for learning",
    "text": "3.3 Resources for learning\n\nResources within RStudio\nHelp documentation\nSearch the RStudio “Help” tab for documentation on R packages and specific functions. This is within the pane that also contains Files, Plots, and Packages (typically in the lower-right pane). As a shortcut, you can also type the name of a package or function into the R console after a question-mark to open the relevant Help page. Do not include parentheses.\nFor example: ?filter or ?diagrammeR.\nInteractive tutorials\nThere are several ways to learn R interactively within RStudio.\nRStudio itself offers a Tutorial pane that is powered by the learnr R package. Simply install this package and open a tutorial via the new “Tutorial” tab in the upper-right RStudio pane (which also contains Environment and History tabs).\nThe R package swirl offers interactive courses in the R Console. Install and load this package, then run the command swirl() (empty parentheses) in the R console. You will see prompts appear in the Console. Respond by typing in the Console. It will guide you through a course of your choice.\n\n\nCheatsheets\nThere are many PDF “cheatsheets” available on the RStudio website, for example:\n\nFactors with forcats package.\n\nDates and times with lubridate package.\n\nStrings with stringr package.\n\niterative opertaions with purrr package.\n\nData import.\n\nData transformation cheatsheet with dplyr package.\n\nR Markdown (to create documents like PDF, Word, Powerpoint…).\n\nShiny (to build interactive web apps).\n\nData visualization with ggplot2 package.\n\nCartography (GIS).\n\nleaflet package (interactive maps).\n\nPython with R (reticulate package).\n\nThis is an online R resource specifically for Excel users.\n\n\nTwitter\nR has a vibrant twitter community where you can learn tips, shortcuts, and news - follow these accounts:\n\nFollow us! @epiRhandbook\n\nR Function A Day @rfuntionaday is an incredible resource\n\nRStudio @RStudio\n\nRStudio Tips @rstudiotips\n\nR-Bloggers @Rbloggers\n\nR-ladies @RLadiesGlobal\n\nHadley Wickham @hadleywickham\n\nAlso:\n#epitwitter and #rstats\n\n\nFree online resources\nA definitive text is the R for Data Science book by Garrett Grolemund and Hadley Wickham.\nThe R4Epis project website aims to “develop standardised data cleaning, analysis and reporting tools to cover common types of outbreaks and population-based surveys that would be conducted in an MSF emergency response setting”. You can find R basics training materials, templates for RMarkdown reports on outbreaks and surveys, and tutorials to help you set them up.\n\n\nLanguages other than English\nMateriales de RStudio en Español\nIntroduction à R et au tidyverse (Francais)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#installation",
    "href": "new_pages/basics.html#installation",
    "title": "3  R Basics",
    "section": "3.4 Installation",
    "text": "3.4 Installation\n\nR and RStudio\nHow to install R\nVisit this website https://www.r-project.org/ and download the latest version of R suitable for your computer.\nHow to install RStudio\nVisit this website https://rstudio.com/products/rstudio/download/ and download the latest free Desktop version of RStudio suitable for your computer.\nPermissions\nNote that you should install R and RStudio to a drive where you have read and write permissions. Otherwise, your ability to install R packages (a frequent occurrence) will be impacted. If you encounter problems, try opening RStudio by right-clicking the icon and selecting “Run as administrator”. Other tips can be found in the page R on network drives.\nHow to update R and RStudio\nYour version of R is printed to the R Console at start-up. You can also run sessionInfo().\nTo update R, go to the website mentioned above and re-install R. Alternatively, you can use the installr package (on Windows) by running installr::updateR(). This will open dialog boxes to help you download the latest R version and update your packages to the new R version. More details can be found in the installr documentation.\nBe aware that the old R version will still exist in your computer. You can temporarily run an older version of R by clicking “Tools” -&gt; “Global Options” in RStudio and choosing an R version. This can be useful if you want to use a package that has not been updated to work on the newest version of R.\nTo update RStudio, you can go to the website above and re-download RStudio. Another option is to click “Help” -&gt; “Check for Updates” within RStudio, but this may not show the very latest updates.\nTo see which versions of R, RStudio, or packages were used when this Handbook as made, see the page on Editorial and technical notes.\n\n\nOther software you may need to install\n\nTinyTeX (for compiling an RMarkdown document to PDF).\n\nPandoc (for compiling RMarkdown documents).\n\nRTools (for building packages for R).\n\nphantomjs (for saving still images of animated networks, such as transmission chains).\n\n\nTinyTex\nTinyTex is a custom LaTeX distribution, useful when trying to produce PDFs from R.\nSee https://yihui.org/tinytex/ for more informaton.\nTo install TinyTex from R:\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()\n# to uninstall TinyTeX, run tinytex::uninstall_tinytex()\n\n\n\nPandoc\nPandoc is a document converter, a separate software from R. It comes bundled with RStudio and should not need to be downloaded. It helps the process of converting Rmarkdown documents to formats like .pdf and adding complex functionality.\n\n\nRTools\nRTools is a collection of software for building packages for R\nInstall from this website: https://cran.r-project.org/bin/windows/Rtools/\n\n\nphantomjs\nThis is often used to take “screenshots” of webpages. For example when you make a transmission chain with epicontacts package, an HTML file is produced that is interactive and dynamic. If you want a static image, it can be useful to use the webshot package to automate this process. This will require the external program “phantomjs”. You can install phantomjs via the webshot package with the command webshot::install_phantomjs().",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#rstudio",
    "href": "new_pages/basics.html#rstudio",
    "title": "3  R Basics",
    "section": "3.5 RStudio",
    "text": "3.5 RStudio\n\nRStudio orientation\nFirst, open RStudio.\nAs their icons can look very similar, be sure you are opening RStudio and not R.\nFor RStudio to work you must also have R installed on the computer (see above for installation instructions).\nRStudio is an interface (GUI) for easier use of R. You can think of R as being the engine of a vehicle, doing the crucial work, and RStudio as the body of the vehicle (with seats, accessories, etc.) that helps you actually use the engine to move forward! You can see the complete RStudio user-interface cheatsheet (PDF) here.\nBy default RStudio displays four rectangle panes.\n\n\n\n\n\n\n\n\n\nTIP: If your RStudio displays only one left pane it is because you have no scripts open yet.\nThe Source Pane\nThis pane, by default in the upper-left, is a space to edit, run, and save your scripts. Scripts contain the commands you want to run. This pane can also display datasets (data frames) for viewing.\nFor Stata users, this pane is similar to your Do-file and Data Editor windows.\nThe R Console Pane\nThe R Console, by default the left or lower-left pane in R Studio, is the home of the R “engine”. This is where the commands are actually run and non-graphic outputs and error/warning messages appear. You can directly enter and run commands in the R Console, but realize that these commands are not saved as they are when running commands from a script.\nIf you are familiar with Stata, the R Console is like the Command Window and also the Results Window.\nThe Environment Pane\nThis pane, by default in the upper-right, is most often used to see brief summaries of objects in the R Environment in the current session. These objects could include imported, modified, or created datasets, parameters you have defined (e.g. a specific epi week for the analysis), or vectors or lists you have defined during analysis (e.g. names of regions). You can click on the arrow next to a data frame name to see its variables.\nIn Stata, this is most similar to the Variables Manager window.\nThis pane also contains History where you can see commands that you can previously. It also has a “Tutorial” tab where you can complete interactive R tutorials if you have the learnr package installed. It also has a “Connections” pane for external connections, and can have a “Git” pane if you choose to interface with Github.\nPlots, Viewer, Packages, and Help Pane\nThe lower-right pane includes several important tabs. Typical plot graphics including maps will display in the Plot pane. Interactive or HTML outputs will display in the Viewer pane. The Help pane can display documentation and help files. The Files pane is a browser which can be used to open or delete files. The Packages pane allows you to see, install, update, delete, load/unload R packages, and see which version of the package you have. To learn more about packages see the packages section below.\nThis pane contains the Stata equivalents of the Plots Manager and Project Manager windows.\n\n\nRStudio settings\nChange RStudio settings and appearance in the Tools drop-down menu, by selecting Global Options. There you can change the default settings, including appearance/background color.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRestart\nIf your R freezes, you can re-start R by going to the Session menu and clicking “Restart R”. This avoids the hassle of closing and opening RStudio.\nCAUTION: Everything in your R environment will be removed when you do this.\n\n\nKeyboard shortcuts\nSome very useful keyboard shortcuts are below. See all the keyboard shortcuts for Windows, Max, and Linux Rstudio user interface cheatsheet.\n\n\n\n\n\n\n\n\n\nWindows/Linux\nMac\nAction\n\n\n\n\n\nEsc\nEsc\nInterrupt current command (useful if you accidentally ran an incomplete command and cannot escape seeing “+” in the R console).\n\n\nCtrl+s\nCmd+s\nSave (script).\n\n\nTab\nTab\nAuto-complete.\n\n\nCtrl + Enter\nCmd + Enter\nRun current line(s)/selection of code.\n\n\nCtrl + Shift + C\nCmd + Shift + c\nComment/uncomment the highlighted lines.\n\n\nAlt + -\nOption + -\nInsert &lt;-.\n\n\nCtrl + Shift + m\nCmd + Shift + m\nInsert %&gt;%.\n\n\nCtrl + l\nCmd + l\nClear the R console.\n\n\nCtrl + Alt + b\nCmd + Option + b\nRun from start to current. line\n\n\nCtrl + Alt + t\nCmd + Option + t\nRun the current code section (R Markdown).\n\n\nCtrl + Alt + i\nCmd + Shift + r\nInsert code chunk (into R Markdown).\n\n\nCtrl + Alt + c\nCmd + Option + c\nRun current code chunk (R Markdown).\n\n\nup/down arrows in R console\nSame\nToggle through recently run commands.\n\n\nShift + up/down arrows in script\nSame\nSelect multiple code lines.\n\n\nCtrl + f\nCmd + f\nFind and replace in current script.\n\n\nCtrl + Shift + f\nCmd + Shift + f\nFind in files (search/replace across many scripts).\n\n\nAlt + l\nCmd + Option + l\nFold selected code.\n\n\nShift + Alt + l\nCmd + Shift + Option+l\nUnfold selected code.\n\n\n\nTIP: Use your Tab key when typing to engage RStudio’s auto-complete functionality. This can prevent spelling errors. Press Tab while typing to produce a drop-down menu of likely functions and objects, based on what you have typed so far.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#functions",
    "href": "new_pages/basics.html#functions",
    "title": "3  R Basics",
    "section": "3.6 Functions",
    "text": "3.6 Functions\nFunctions are at the core of using R. Functions are how you perform tasks and operations. Many functions come installed with R, many more are available for download in packages (explained in the packages section), and you can even write your own custom functions!\nThis basics section on functions explains:\n\nWhat a function is and how they work.\n\nWhat function arguments are.\n\nHow to get help understanding a function.\n\nA quick note on syntax: In this handbook, functions are written in code-text with open parentheses, like this: filter(). As explained in the packages section, functions are downloaded within packages. In this handbook, package names are written in bold, like dplyr. Sometimes in example code you may see the function name linked explicitly to the name of its package with two colons (::) like this: dplyr::filter(). The purpose of this linkage is explained in the packages section.\n\n\nSimple functions\nA function is like a machine that receives inputs, carries out an action with those inputs, and produces an output. What the output is depends on the function.\nFunctions typically operate upon some object placed within the function’s parentheses. For example, the function sqrt() calculates the square root of a number:\n\nsqrt(49)\n\n[1] 7\n\n\nThe object provided to a function also can be a column in a dataset (see the Objects section for detail on all the kinds of objects). Because R can store multiple datasets, you will need to specify both the dataset and the column. One way to do this is using the $ notation to link the name of the dataset and the name of the column (dataset$column). In the example below, the function summary() is applied to the numeric column age in the dataset linelist, and the output is a summary of the column’s numeric and missing values.\n\n# Print summary statistics of column 'age' in the dataset 'linelist'\nsummary(linelist$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.07   23.00   84.00      86 \n\n\nNOTE: Behind the scenes, a function represents complex additional code that has been wrapped up for the user into one easy command.\n\n\n\nFunctions with multiple arguments\nFunctions often ask for several inputs, called arguments, located within the parentheses of the function, usually separated by commas.\n\nSome arguments are required for the function to work correctly, others are optional.\n\nOptional arguments have default settings.\n\nArguments can take character, numeric, logical (TRUE/FALSE), and other inputs.\n\nHere is a fun fictional function, called oven_bake(), as an example of a typical function. It takes an input object (e.g. a dataset, or in this example “dough”) and performs operations on it as specified by additional arguments (minutes = and temperature =). The output can be printed to the console, or saved as an object using the assignment operator &lt;-.\n\n\n\n\n\n\n\n\n\nIn a more realistic example, the age_pyramid() command below produces an age pyramid plot based on defined age groups and a binary split column, such as gender. The function is given three arguments within the parentheses, separated by commas. The values supplied to the arguments establish linelist as the data frame to use, age_cat5 as the column to count, and gender as the binary column to use for splitting the pyramid by color.\n\n# Create an age pyramid\nage_pyramid(data = linelist, age_group = \"age_cat5\", split_by = \"gender\")\n\n\n\n\n\n\n\n\nThe above command can be equivalently written as below, in a longer style with a new line for each argument. This style can be easier to read, and easier to write “comments” with # to explain each part (commenting extensively is good practice!). To run this longer command you can highlight the entire command and click “Run”, or just place your cursor in the first line and then press the Ctrl and Enter keys simultaneously.\n\n# Create an age pyramid\nage_pyramid(\n  data = linelist,        # use case linelist\n  age_group = \"age_cat5\", # provide age group column\n  split_by = \"gender\"     # use gender column for two sides of pyramid\n  )\n\n\n\n\n\n\n\n\nThe first half of an argument assignment (e.g. data =) does not need to be specified if the arguments are written in a specific order (specified in the function’s documentation). The below code produces the exact same pyramid as above, because the function expects the argument order: data frame, age_group variable, split_by variable.\n\n# This command will produce the exact same graphic as above\nage_pyramid(linelist, \"age_cat5\", \"gender\")\n\nA more complex age_pyramid() command might include the optional arguments to:\n\nShow proportions instead of counts (set proportional = TRUE when the default is FALSE)\n\nSpecify the two colors to use (pal = is short for “palette” and is supplied with a vector of two color names. See the objects page for how the function c() makes a vector)\n\nNOTE: For arguments that you specify with both parts of the argument (e.g. proportional = TRUE), their order among all the arguments does not matter.\n\nage_pyramid(\n  linelist,                    # use case linelist\n  \"age_cat5\",                  # age group column\n  \"gender\",                    # split by gender\n  proportional = TRUE,         # percents instead of counts\n  pal = c(\"orange\", \"purple\")  # colors\n  )\n\n\n\n\n\n\n\n\nTIP: Remember that you can put ? before a function to see what arguments the function can take, and which arguments are needed and which arguments have default values. For example `?age_pyramid’.\n\n\n\nWriting Functions\nR is a language that is oriented around functions, so you should feel empowered to write your own functions. Creating functions brings several advantages:\n\nTo facilitate modular programming - the separation of code in to independent and manageable pieces.\n\nReplace repetitive copy-and-paste, which can be error prone.\n\nGive pieces of code memorable names.\n\nHow to write a function is covered in-depth in the Writing functions page.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#packages",
    "href": "new_pages/basics.html#packages",
    "title": "3  R Basics",
    "section": "3.7 Packages",
    "text": "3.7 Packages\nPackages contain functions.\nAn R package is a shareable bundle of code and documentation that contains pre-defined functions. Users in the R community develop packages all the time catered to specific problems, it is likely that one can help with your work! You will install and use hundreds of packages in your use of R.\nOn installation, R contains “base” packages and functions that perform common elementary tasks. But many R users create specialized functions, which are verified by the R community and which you can download as a package for your own use. In this handbook, package names are written in bold. One of the more challenging aspects of R is that there are often many functions or packages to choose from to complete a given task.\n\nInstall and load\nFunctions are contained within packages which can be downloaded (“installed”) to your computer from the internet. Once a package is downloaded, it is stored in your “library”. You can then access the functions it contains during your current R session by “loading” the package.\nThink of R as your personal library: When you download a package, your library gains a new book of functions, but each time you want to use a function in that book, you must borrow,“load”, that book from your library.\nIn summary: to use the functions available in an R package, 2 steps must be implemented:\n\nThe package must be installed (once), and\n\nThe package must be loaded (each R session)\n\n\nYour library\nYour “library” is actually a folder on your computer, containing a folder for each package that has been installed. Find out where R is installed in your computer, and look for a folder called “win-library”. For example: R\\win-library\\4.4.1 (the 4.4.1 is the R version - you’ll have a different library for each R version you’ve downloaded).\nYou can print the file path to your library by entering .libPaths() (empty parentheses). This becomes especially important if working with R on network drives.\n\n\nInstall from CRAN\nMost often, R users download packages from CRAN. CRAN (Comprehensive R Archive Network) is an online public warehouse of R packages that have been published by R community members.\nAre you worried about viruses and security when downloading a package from CRAN? Read this article on the topic.\n\n\nHow to install and load\nIn this handbook, we suggest using the pacman package (short for “package manager”). It offers a convenient function p_load() which will install a package if necessary and load it for use in the current R session.\nThe syntax quite simple. Just list the names of the packages within the p_load() parentheses, separated by commas. This command will install the rio, tidyverse, and here packages if they are not yet installed, and will load them for use. This makes the p_load() approach convenient and concise if sharing scripts with others.\nNote that package names are case-sensitive.\n\n# Install (if necessary) and load packages for use\npacman::p_load(rio, tidyverse, here)\n\nHere we have used the syntax pacman::p_load() which explicitly writes the package name (pacman) prior to the function name (p_load()), connected by two colons ::. This syntax is useful because it also loads the pacman package (assuming it is already installed).\nThere are alternative base R functions that you will see often. The base R function for installing a package is install.packages(). The name of the package to install must be provided in the parentheses in quotes. If you want to install multiple packages in one command, they must be listed within a character vector c().\nNote: this command installs a package, but does not load it for use in the current session.\n\n# install a single package with base R\ninstall.packages(\"tidyverse\")\n\n# install multiple packages with base R\ninstall.packages(c(\"tidyverse\", \"rio\", \"here\"))\n\nInstallation can also be accomplished point-and-click by going to the RStudio “Packages” pane and clicking “Install” and searching for the desired package name.\nThe base R function to load a package for use (after it has been installed) is library(). It can load only one package at a time (another reason to use p_load()). You can provide the package name with or without quotes.\n\n# load packages for use, with base R\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(here)\n\nTo check whether a package in installed or loaded, you can view the Packages pane in RStudio. If the package is installed, it is shown there with version number. If its box is checked, it is loaded for the current session.\nInstall from Github\nSometimes, you need to install a package that is not yet available from CRAN. Or perhaps the package is available on CRAN but you want the development version with new features not yet offered in the more stable published CRAN version. These are often hosted on the website github.com in a free, public-facing code “repository”. Read more about Github in the handbook page on Version control and collaboration with Git and Github.\nTo download R packages from Github, you can use the function p_load_gh() from pacman, which will install the package if necessary, and load it for use in your current R session. Alternatives to install include using the remotes or devtools packages. Read more about all the pacman functions in the package documentation.\nTo install from Github, you have to provide more information. You must provide:\n\nThe Github ID of the repository owner\nThe name of the repository that contains the package\n\nOptional: The name of the “branch” (specific development version) you want to download\n\nIn the examples below, the first word in the quotation marks is the Github ID of the repository owner, after the slash is the name of the repository (the name of the package).\n\n# install/load the epicontacts package from its Github repository\np_load_gh(\"reconhub/epicontacts\")\n\nIf you want to install from a “branch” (version) other than the main branch, add the branch name after an “@”, after the repository name.\n\n# install the \"timeline\" branch of the epicontacts package from Github\np_load_gh(\"reconhub/epicontacts@timeline\")\n\nIf there is no difference between the Github version and the version on your computer, no action will be taken. You can “force” a re-install by instead using p_load_current_gh() with the argument update = TRUE. Read more about pacman in this online vignette\nInstall from ZIP or TAR\nYou could install the package from a URL:\n\npackageurl &lt;- \"https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz\"\ninstall.packages(packageurl, repos=NULL, type=\"source\")\n\nOr, download it to your computer in a zipped file:\nOption 1: using install_local() from the remotes package\n\nremotes::install_local(\"~/Downloads/dplyr-master.zip\")\n\nOption 2: using install.packages() from base R, providing the file path to the ZIP file and setting type = \"source and repos = NULL.\n\ninstall.packages(\"~/Downloads/dplyr-master.zip\", repos=NULL, type=\"source\")\n\n\n\n\nCode syntax\nFor clarity in this handbook, functions are sometimes preceded by the name of their package using the :: symbol in the following way: package_name::function_name()\nOnce a package is loaded for a session, this explicit style is not necessary. One can just use function_name(). However writing the package name is useful when a function name is common and may exist in multiple packages (e.g. plot()). Writing the package name will also load the package if it is not already loaded.\n\n# This command uses the package \"rio\" and its function \"import()\" to import a dataset\nlinelist &lt;- rio::import(\"linelist.xlsx\", which = \"Sheet1\")\n\n\n\nFunction help\nTo read more about a function, you can search for it in the Help tab of the lower-right RStudio. You can also run a command like ?thefunctionname (for example, to get help for the function p_load you would write ?p_load) and the Help page will appear in the Help pane. Finally, try searching online for resources.\n\n\nUpdate packages\nYou can update packages by re-installing them. You can also click the green “Update” button in your RStudio Packages pane to see which packages have new versions to install. Be aware that your old code may need to be updated if there is a major revision to how a function works!\n\n\nDelete packages\nUse p_delete() from pacman, or remove.packages() from base R.\n\n\nDependencies\nPackages often depend on other packages to work. These are called dependencies. If a dependency fails to install, then the package depending on it may also fail to install.\nSee the dependencies of a package with p_depends(), and see which packages depend on it with p_depends_reverse()\n\n\nMasked functions\nIt is not uncommon that two or more packages contain the same function name. For example, the package dplyr has a filter() function, but so does the package stats. The default filter() function depends on the order these packages are first loaded in the R session - the later one will be the default for the command filter().\nYou can check the order in your Environment pane of R Studio - click the drop-down for “Global Environment” and see the order of the packages. Functions from packages lower on that drop-down list will mask functions of the same name in packages that appear higher in the drop-down list. When first loading a package, R will warn you in the console if masking is occurring, but this can be easy to miss.\n\n\n\n\n\n\n\n\n\nHere are ways you can fix masking:\n\nSpecify the package name in the command. For example, use dplyr::filter()\n\nRe-arrange the order in which the packages are loaded (e.g. within p_load()), and start a new R session\n\n\n\nDetach / unload\nTo detach (unload) a package, use this command, with the correct package name and only one colon. Note that this may not resolve masking.\n\ndetach(package:PACKAGE_NAME_HERE, unload=TRUE)\n\n\n\nInstall older version\nSee this guide to install an older version of a particular package.\n\n\nSuggested packages\nSee the page on Suggested packages for a listing of packages we recommend for everyday epidemiology.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#scripts",
    "href": "new_pages/basics.html#scripts",
    "title": "3  R Basics",
    "section": "3.8 Scripts",
    "text": "3.8 Scripts\nScripts are a fundamental part of programming. They are documents that hold your commands (e.g. functions to create and modify datasets, print visualizations, etc). You can save a script and run it again later. There are many advantages to storing and running your commands from a script (vs. typing commands one-by-one into the R console “command line”):\n\nPortability - you can share your work with others by sending them your scripts.\n\nReproducibility - so that you and others know exactly what you did.\n\nVersion control - so you can track changes made by yourself or colleagues.\n\nCommenting/annotation - to explain to your colleagues what you have done.\n\n\nCommenting\nIn a script you can also annotate (“comment”) around your R code. Commenting is helpful to explain to yourself and other readers what you are doing. You can add a comment by typing the hash symbol (#) and writing your comment after it. The commented text will appear in a different color than the R code.\nAny code written after the # will not be run. Therefore, placing a # before code is also a useful way to temporarily block a line of code (“comment out”) if you do not want to delete it. You can comment out/in multiple lines at once by highlighting them and pressing Ctrl+Shift+c (Cmd+Shift+c in Mac).\n\n# A comment can be on a line by itself\n# import data\nlinelist &lt;- import(\"linelist_raw.xlsx\") %&gt;%   # a comment can also come after code\n# filter(age &gt; 50)                          # It can also be used to deactivate / remove a line of code\n  count()\n\nThere are a few general ideas to follow when writing your scripts in order to make them accessible. - Add comments on what you are doing and on why you are doing it.\n- Break your code into logical sections.\n- Accompany your code with a text step-by-step description of what you are doing (e.g. numbered steps).\n\n\nStyle\nIt is important to be conscious of your coding style - especially if working on a team. We advocate for the tidyverse style guide. There are also packages such as styler and lintr which help you conform to this style.\nA few very basic points to make your code readable to others:\n* When naming objects, use only lowercase letters, numbers, and underscores _, e.g. my_data\n* Use frequent spaces, including around operators, e.g. n = 1 and age_new &lt;- age_old + 3\n\n\nExample Script\nBelow is an example of a short R script. Remember, the better you succinctly explain your code in comments, the more your colleagues will like you!\n\n\n\n\n\n\n\n\n\n\n\n\nR markdown and Quarto\nAn R Markdown or Quarto script are types of R script in which the script itself becomes an output document (PDF, Word, HTML, Powerpoint, etc.). These are incredibly useful and versatile tools often used to create dynamic and automated reports.\nEven this website and handbook is produced with Quarto scripts!\nIt is worth noting that beginner R users can also use R Markdown - do not be intimidated! To learn more, see the handbook page on Reports with R Markdown documents.\n\n\n\nR notebooks\nThere is no difference between writing in a Rmarkdown vs an R notebook. However the execution of the document differs slightly. See this site for more details.\n\n\n\nShiny\nShiny apps/websites are contained within one script, which must be named app.R. This file has three components:\n\nA user interface (ui).\n\nA server function.\n\nA call to the shinyApp function.\n\nSee the handbook page on Dashboards with Shiny, or this online tutorial: Shiny tutorial\nIn previous versions, the above file was split into two files (ui.R and server.R)\n\n\nCode folding\nYou can collapse portions of code to make your script easier to read.\nTo do this, create a text header with #, write your header, and follow it with at least 4 of either dashes (-), hashes (#) or equals (=). When you have done this, a small arrow will appear in the “gutter” to the left (by the row number). You can click this arrow and the code below until the next header will collapse and a dual-arrow icon will appear in its place.\nTo expand the code, either click the arrow in the gutter again, or the dual-arrow icon. There are also keyboard shortcuts as explained in the RStudio section of this page.\nBy creating headers with #, you will also activate the Table of Contents at the bottom of your script (see below) that you can use to navigate your script. You can create sub-headers by adding more # symbols, for example # for primary, ## for secondary, and ### for tertiary headers.\nBelow are two versions of an example script. On the left is the original with commented headers. On the right, four dashes have been written after each header, making them collapsible. Two of them have been collapsed, and you can see that the Table of Contents at the bottom now shows each section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther areas of code that are automatically eligible for folding include “braced” regions with brackets { } such as function definitions or conditional blocks (if else statements). You can read more about code folding at the RStudio site.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#working-directory",
    "href": "new_pages/basics.html#working-directory",
    "title": "3  R Basics",
    "section": "3.9 Working directory",
    "text": "3.9 Working directory\nThe working directory is the root folder location used by R for your work - where R looks for and saves files by default. By default, it will save new files and outputs to this location, and will look for files to import (e.g. datasets) here as well.\nThe working directory appears in grey text at the top of the RStudio Console pane. You can also print the current working directory by running getwd() (leave the parentheses empty).\n\n\n\n\n\n\n\n\n\n\nRecommended approach\nSee the page on R projects for details on our recommended approach to managing your working directory.\n\nA common, efficient, and trouble-free way to manage your working directory and file paths is to combine these 3 elements in an R project-oriented workflow:\n\nAn R Project to store all your files (see page on R projects)\n\nThe here package to locate files (see page on Import and export)\n\nThe rio package to import/export files (see page on Import and export)\n\n\n\n\nSet by command\nUntil recently, many people learning R were taught to begin their scripts with a setwd() command.\nPlease instead consider using an R project-oriented workflow and read the reasons for not using setwd().\nIn brief, your work becomes specific to your computer. This means that file paths used to import and export files need to be changed if used on a different computer, or by different collaborators.\nAs noted above, although we do not recommend this approach in most circumstances, you can use the command setwd() with the desired folder file path in quotations, for example:\n\nsetwd(\"C:/Documents/R Files/My analysis\")\n\nDANGER: Setting a working directory with setwd() can be “brittle” if the file path is specific to one computer. Instead, use file paths relative to an R Project root directory, such as with the [here package].\n\n\n\nSet manually\nTo set the working directory manually (the point-and-click equivalent of setwd()), click the Session drop-down menu and go to “Set Working Directory” and then “Choose Directory”. This will set the working directory for that specific R session. Note: if using this approach, you will have to do this manually each time you open RStudio.\n\n\n\nWithin an R project\nIf using an R project, the working directory will default to the R project root folder that contains the “.rproj” file. This will apply if you open RStudio by clicking open the R Project (the file with “.rproj” extension).\n\n\n\nWorking directory in an R markdown\nIn an R markdown script, the default working directory is the folder the Rmarkdown file (.Rmd) is saved within. If using an R project and here package, this does not apply and the working directory will be here() as explained in the R projects page.\nIf you want to change the working directory of a stand-alone R markdown (not in an R project), if you use setwd() this will only apply to that specific code chunk. To make the change for all code chunks in an R markdown, edit the setup chunk to add the root.dir = parameter, such as below:\n\nknitr::opts_knit$set(root.dir = 'desired/directorypath')\n\nIt is much easier to just use the R markdown within an R project and use the here package.\n\n\n\nProviding file paths\nPerhaps the most common source of frustration for an R beginner (at least on a Windows machine) is typing in a file path to import or export data. There is a thorough explanation of how to best input file paths in the Import and export page, but here are a few key points:\nBroken paths\nBelow is an example of an “absolute” or “full address” file path. These will likely break if used by another computer. One exception is if you are using a shared/network drive.\nC:/Users/Name/Document/Analytic Software/R/Projects/Analysis2019/data/March2019.csv  \nSlash direction\nIf typing in a file path, be aware the direction of the slashes.\nUse forward slashes (/) to separate the components (“data/provincial.csv”). For Windows users, the default way that file paths are displayed is with back slashes (\\) - so you will need to change the direction of each slash. If you use the here package as described in the R projects page the slash direction is not an issue.\nRelative file paths\nWe generally recommend providing “relative” filepaths instead - that is, the path relative to the root of your R Project. You can do this using the here package as explained in the R projects page. A relativel filepath might look like this:\n\n# Import csv linelist from the data/linelist/clean/ sub-folders of an R project\nlinelist &lt;- import(here(\"data\", \"clean\", \"linelists\", \"marin_country.csv\"))\n\nEven if using relative file paths within an R project, you can still use absolute paths to import and export data outside your R project.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#objects",
    "href": "new_pages/basics.html#objects",
    "title": "3  R Basics",
    "section": "3.10 Objects",
    "text": "3.10 Objects\nEverything in R is an object, and R is an “object-oriented” language. These sections will explain:\n\nHow to create objects (&lt;-).\nTypes of objects (e.g. data frames, vectors..).\n\nHow to access subparts of objects (e.g. variables in a dataset).\n\nClasses of objects (e.g. numeric, logical, integer, double, character, factor).\n\n\n\nEverything is an object\nThis section is adapted from the R4Epis project.\n\nEverything you store in R - datasets, variables, a list of village names, a total population number, even outputs such as graphs - are objects which are assigned a name and can be referenced in later commands.\nAn object exists when you have assigned it a value (see the assignment section below). When it is assigned a value, the object appears in the Environment (see the upper right pane of RStudio). It can then be operated upon, manipulated, changed, and re-defined.\n\n\n\nDefining objects (&lt;-)\nCreate objects by assigning them a value with the &lt;- operator.\nYou can think of the assignment operator &lt;- as the words “is defined as”. Assignment commands generally follow a standard order:\nobject_name &lt;- value (or process/calculation that produce a value)\nFor example, you may want to record the current epidemiological reporting week as an object for reference in later code. In this example, the object current_week is created when it is assigned the value \"2018-W10\" (the quote marks make this a character value). The object current_week will then appear in the RStudio Environment pane (upper-right) and can be referenced in later commands.\nSee the R commands and their output in the boxes below.\n\ncurrent_week &lt;- \"2018-W10\"   # this command creates the object current_week by assigning it a value\ncurrent_week                 # this command prints the current value of current_week object in the console\n\n[1] \"2018-W10\"\n\n\nNOTE: Note the [1] in the R console output is simply indicating that you are viewing the first item of the output\nCAUTION: An object’s value can be over-written at any time by running an assignment command to re-define its value. Thus, the order of the commands run is very important.\nThe following command will re-define the value of current_week:\n\ncurrent_week &lt;- \"2018-W51\"   # assigns a NEW value to the object current_week\ncurrent_week                 # prints the current value of current_week in the console\n\n[1] \"2018-W51\"\n\n\nEquals signs =\nYou will also see equals signs in R code:\n\nA double equals sign == between two objects or values asks a logical question: “is this equal to that?”.\n\nYou will also see equals signs within functions used to specify values of function arguments (read about these in sections below), for example max(age, na.rm = TRUE).\n\nYou can use a single equals sign = in place of &lt;- to create and define objects, but this is discouraged. You can read about why this is discouraged here.\n\nDatasets\nDatasets are also objects (typically “data frames”) and must be assigned names when they are imported. In the code below, the object linelist is created and assigned the value of a CSV file imported with the rio package and its import() function.\n\n# linelist is created and assigned the value of the imported CSV file\nlinelist &lt;- import(\"my_linelist.csv\")\n\nYou can read more about importing and exporting datasets with the section on Import and export.\nCAUTION: A quick note on naming of objects:\n\nObject names must not contain spaces, but you should use underscore (_) or a period (.) instead of a space.\n\nObject names are case-sensitive (meaning that Dataset_A is different from dataset_A).\nObject names must begin with a letter (they cannot begin with a number like 1, 2 or 3).\n\nOutputs\nOutputs like tables and plots provide an example of how outputs can be saved as objects, or just be printed without being saved. A cross-tabulation of gender and outcome using the base R function table() can be printed directly to the R console (without being saved).\n\n# printed to R console only\ntable(linelist$gender, linelist$outcome)\n\n   \n    Death Recover\n  f  1227     953\n  m  1228     950\n\n\nBut the same table can be saved as a named object. Then, optionally, it can be printed.\n\n# save\ngen_out_table &lt;- table(linelist$gender, linelist$outcome)\n\n# print\ngen_out_table\n\n   \n    Death Recover\n  f  1227     953\n  m  1228     950\n\n\nColumns\nColumns in a dataset are also objects and can be defined, over-written, and created as described below in the section on Columns.\nYou can use the assignment operator from base R to create a new column. Below, the new column bmi (Body Mass Index) is created, and for each row the new value is result of a mathematical operation on the row’s value in the wt_kg and ht_cm columns.\n\n# create new \"bmi\" column using base R syntax\nlinelist$bmi &lt;- linelist$wt_kg / (linelist$ht_cm/100)^2\n\nHowever, in this handbook, we emphasize a different approach to defining columns, which uses the function mutate() from the dplyr package and piping with the pipe operator (%&gt;%). The syntax is easier to read and there are other advantages explained in the page on Cleaning data and core functions. You can read more about piping in the Piping section below.\n\n# create new \"bmi\" column using dplyr syntax\nlinelist &lt;- linelist %&gt;% \n  mutate(bmi = wt_kg / (ht_cm/100)^2)\n\n\n\n\nObject structure\nObjects can be a single piece of data (e.g. my_number &lt;- 24), or they can consist of structured data.\nThe graphic below is borrowed from this online R tutorial. It shows some common data structures and their names. Not included in this image is spatial data, which is discussed in the GIS basics page.\n\n\n\n\n\n\n\n\n\nIn epidemiology (and particularly field epidemiology), you will most commonly encounter data frames and vectors:\n\n\n\n\n\n\n\n\nCommon structure\nExplanation\nExample\n\n\n\nVectors | A container for a sequence of singular objects, all of the same class (e.g. numeric, character). | “Variables” (columns) in data frames are vectors (e.g. the column age_years). |\n\n\n\n\n\n\n\n\nData Frames\nVectors (e.g. columns) that are bound together that all have the same number of rows.\nlinelist is a data frame.\n\n\n\nNote that to create a vector that “stands alone” (is not part of a data frame) the function c() is used to combine the different elements. For example, if creating a vector of colors plot’s color scale: vector_of_colors &lt;- c(\"blue\", \"red2\", \"orange\", \"grey\")\n\n\n\nObject classes\nAll the objects stored in R have a class which tells R how to handle the object. There are many possible classes, but common ones include:\n\n\n\nClass\nExplanation\nExamples\n\n\n\n\n\nCharacter\nThese are text/words/sentences “within quotation marks”. Math cannot be done on these objects.\n“Character objects are in quotation marks”\n\n\n\nInteger\nNumbers that are whole only (no decimals)\n-5, 14, or 2000\n\n\n\nNumeric\nThese are numbers and can include decimals. If within quotation marks they will be considered character class.\n23.1 or 14\n\n\n\nFactor\nThese are vectors that have a specified order or hierarchy of values\nAn variable of economic status with ordered values\n\n\n\nDate\nOnce R is told that certain data are Dates, these data can be manipulated and displayed in special ways. See the page on Working with dates for more information.\n2018-04-12 or 15/3/1954 or Wed 4 Jan 1980\n\n\n\nLogical\nValues must be one of the two special values TRUE or FALSE (note these are not “TRUE” and “FALSE” in quotation marks)\nTRUE or FALSE\n\n\n\ndata.frame\nA data frame is how R stores a typical dataset. It consists of vectors (columns) of data bound together, that all have the same number of observations (rows).\nThe example AJS dataset named linelist_raw contains 68 variables with 300 observations (rows) each.\n\n\n\ntibble\ntibbles are a variation on data frame, the main operational difference being that they print more nicely to the console (display first 10 rows and only columns that fit on the screen)\nAny data frame, list, or matrix can be converted to a tibble with as_tibble()\n\n\n\nlist\nA list is like vector, but holds other objects that can be other different classes\nA list could hold a single number, and a data frame, and a vector, and even another list within it!\n\n\n\n\nYou can test the class of an object by providing its name to the function class(). Note: you can reference a specific column within a dataset using the $ notation to separate the name of the dataset and the name of the column.\n\nclass(linelist)         # class should be a data frame or tibble\n\n[1] \"data.frame\"\n\nclass(linelist$age)     # class should be numeric\n\n[1] \"numeric\"\n\nclass(linelist$gender)  # class should be character\n\n[1] \"character\"\n\n\nSometimes, a column will be converted to a different class automatically by R. Watch out for this! For example, if you have a vector or column of numbers, but a character value is inserted… the entire column will change to class character.\n\nnum_vector &lt;- c(1, 2 , 3, 4, 5) # define vector as all numbers\nclass(num_vector)          # vector is numeric class\n\n[1] \"numeric\"\n\nnum_vector[3] &lt;- \"three\"   # convert the third element to a character\nclass(num_vector)          # vector is now character class\n\n[1] \"character\"\n\n\nOne common example of this is when manipulating a data frame in order to print a table - if you make a total row and try to paste/glue together percents in the same cell as numbers (e.g. 23 (40%)), the entire numeric column above will convert to character and can no longer be used for mathematical calculations.Sometimes, you will need to convert objects or columns to another class.\n\n\n\nFunction\nAction\n\n\n\n\nas.character()\nConverts to character class\n\n\nas.numeric()\nConverts to numeric class\n\n\nas.integer()\nConverts to integer class\n\n\nas.Date()\nConverts to Date class - Note: see section on dates for details\n\n\nfactor()\nConverts to factor - Note: re-defining order of value levels requires extra arguments\n\n\n\nLikewise, there are base R functions to check whether an object IS of a specific class, such as is.numeric(), is.character(), is.double(), is.factor(), is.integer()\nHere is more online material on classes and data structures in R.\n\n\n\nColumns/Variables ($)\nA column in a data frame is technically a “vector” (see table above) - a series of values that must all be the same class (either character, numeric, logical, etc).\nA vector can exist independent of a data frame, for example a vector of column names that you want to include as explanatory variables in a model. To create a “stand alone” vector, use the c() function as below:\n\n# define the stand-alone vector of character values\nexplanatory_vars &lt;- c(\"gender\", \"fever\", \"chills\", \"cough\", \"aches\", \"vomit\")\n\n# print the values in this named vector\nexplanatory_vars\n\n[1] \"gender\" \"fever\"  \"chills\" \"cough\"  \"aches\"  \"vomit\" \n\n\nColumns in a data frame are also vectors and can be called, referenced, extracted, or created using the $ symbol. The $ symbol connects the name of the column to the name of its data frame. In this handbook, we try to use the word “column” instead of “variable”.\n\n# Retrieve the length of the vector age_years\nlength(linelist$age) # (age is a column in the linelist data frame)\n\nBy typing the name of the data frame followed by $ you will also see a drop-down menu of all columns in the data frame. You can scroll through them using your arrow key, select one with your Enter key, and avoid spelling mistakes!\n\n\n\n\n\n\n\n\n\nADVANCED TIP: Some more complex objects (e.g. a list, or an epicontacts object) may have multiple levels which can be accessed through multiple dollar signs. For example epicontacts$linelist$date_onset\n\n\n\nAccess/index with brackets ([ ])\nYou may need to view parts of objects, also called “indexing”, which is often done using the square brackets [ ]. Using $ on a data frame to access a column is also a type of indexing.\n\nmy_vector &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")  # define the vector\nmy_vector[5]                                  # print the 5th element\n\n[1] \"e\"\n\n\nSquare brackets also work to return specific parts of an returned output, such as the output of a summary() function:\n\n# All of the summary\nsummary(linelist$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.07   23.00   84.00      86 \n\n# Just the second element of the summary, with name (using only single brackets)\nsummary(linelist$age)[2]\n\n1st Qu. \n      6 \n\n# Just the second element, without name (using double brackets)\nsummary(linelist$age)[[2]]\n\n[1] 6\n\n# Extract an element by name, without showing the name\nsummary(linelist$age)[[\"Median\"]]\n\n[1] 13\n\n\nBrackets also work on data frames to view specific rows and columns. You can do this using the syntax data frame[rows, columns]:\n\n# View a specific row (2) from dataset, with all columns (don't forget the comma!)\nlinelist[2,]\n\n# View all rows, but just one column\nlinelist[, \"date_onset\"]\n\n# View values from row 2 and columns 5 through 10\nlinelist[2, 5:10] \n\n# View values from row 2 and columns 5 through 10 and 18\nlinelist[2, c(5:10, 18)] \n\n# View rows 2 through 20, and specific columns\nlinelist[2:20, c(\"date_onset\", \"outcome\", \"age\")]\n\n# View rows and columns based on criteria\n# *** Note the data frame must still be named in the criteria!\nlinelist[linelist$age &gt; 25 , c(\"date_onset\", \"outcome\", \"age\")]\n\n# Use View() to see the outputs in the RStudio Viewer pane (easier to read) \n# *** Note the capital \"V\" in View() function\nView(linelist[2:20, \"date_onset\"])\n\n# Save as a new object\nnew_table &lt;- linelist[2:20, c(\"date_onset\")] \n\nNote that you can also achieve the above row/column indexing on data frames and tibbles using dplyr syntax (functions filter() for rows, and select() for columns). Read more about these core functions in the Cleaning data and core functions page.\nTo filter based on “row number”, you can use the dplyr function row_number() with open parentheses as part of a logical filtering statement. Often you will use the %in% operator and a range of numbers as part of that logical statement, as shown below. To see the first N rows, you can also use the special dplyr function head().\n\n# View first 100 rows\nlinelist %&gt;% head(100)\n\n# Show row 5 only\nlinelist %&gt;% filter(row_number() == 5)\n\n# View rows 2 through 20, and three specific columns (note no quotes necessary on column names)\nlinelist %&gt;% \n     filter(row_number() %in% 2:20) %&gt;% \n     select(date_onset, outcome, age)\n\nWhen indexing an object of class list, single brackets always return with class list, even if only a single object is returned. Double brackets, however, can be used to access a single element and return a different class than list. Brackets can also be written after one another, as demonstrated below.\nThis visual explanation of lists indexing, with pepper shakers is humorous and helpful.\n\n# define demo list\nmy_list &lt;- list(\n  # First element in the list is a character vector\n  hospitals = c(\"Central\", \"Empire\", \"Santa Anna\"),\n  \n  # second element in the list is a data frame of addresses\n  addresses   = data.frame(\n    street = c(\"145 Medical Way\", \"1048 Brown Ave\", \"999 El Camino\"),\n    city   = c(\"Andover\", \"Hamilton\", \"El Paso\")\n    )\n  )\n\nHere is how the list looks when printed to the console. See how there are two named elements:\n\nhospitals, a character vector\n\naddresses, a data frame of addresses\n\n\nmy_list\n\n$hospitals\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\n$addresses\n           street     city\n1 145 Medical Way  Andover\n2  1048 Brown Ave Hamilton\n3   999 El Camino  El Paso\n\n\nNow we extract, using various methods:\n\nmy_list[1] # this returns the element in class \"list\" - the element name is still displayed\n\n$hospitals\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[1]] # this returns only the (unnamed) character vector\n\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[\"hospitals\"]] # you can also index by name of the list element\n\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[1]][3] # this returns the third element of the \"hospitals\" character vector\n\n[1] \"Santa Anna\"\n\nmy_list[[2]][1] # This returns the first column (\"street\") of the address data frame\n\n           street\n1 145 Medical Way\n2  1048 Brown Ave\n3   999 El Camino\n\n\n\n\n\nRemove objects\nYou can remove individual objects from your R environment by putting the name in the rm() function (no quote marks):\n\nrm(object_name)\n\nYou can remove all objects (clear your workspace) by running:\n\nrm(list = ls(all = TRUE))",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#piping",
    "href": "new_pages/basics.html#piping",
    "title": "3  R Basics",
    "section": "3.11 Piping (%>%)",
    "text": "3.11 Piping (%&gt;%)\nTwo general approaches to working with objects are:\n\nPipes/tidyverse - pipes send an object from function to function - emphasis is on the action, not the object.\n\nDefine intermediate objects - an object is re-defined again and again - emphasis is on the object.\n\n\n\nPipes\nSimply explained, the pipe operator passes an intermediate output from one function to the next.\nYou can think of it as saying “and then”. Many functions can be linked together with %&gt;%.\n\nPiping emphasizes a sequence of actions, not the object the actions are being performed on.\n\nPipes are best when a sequence of actions must be performed on one object.\n\nPipes can make code more clean and easier to read, more intuitive.\n\nPipe operators were first introduced through the magrittr package, which is part of tidyverse, and were specified as %&gt;%. In R 4.1.0, they introduced a base R pipe which is specified through |&gt;. The behaviour of the two pipes is the same, and they can be used somewhat interchangeably. However, there are a few key differences.\n\nThe %&gt;% pipe allows you to pass multiple arguments.\nThe %&gt;% pipe lets you drop parenthesis when calling a function with no other arguments (i.e. drop vs drop()).\nThe %&gt;% pipe allows you to start a pipe with . to create a function in your linking of code.\n\nFor these reasons, we recommend the magrittr pipe, %&gt;%, over the base R pipe, |&gt;.\nTo read more about the differences between base R and tidyverse (magrittr) pipes, see this blog post. For more information on the tidyverse approach, please see this style guide.\nHere is a fake example for comparison, using fictional functions to “bake a cake”. First, the pipe method:\n\n# A fake example of how to bake a cake using piping syntax\n\ncake &lt;- flour %&gt;%       # to define cake, start with flour, and then...\n  add(eggs) %&gt;%   # add eggs\n  add(oil) %&gt;%    # add oil\n  add(water) %&gt;%  # add water\n  mix_together(         # mix together\n    utensil = spoon,\n    minutes = 2) %&gt;%    \n  bake(degrees = 350,   # bake\n       system = \"fahrenheit\",\n       minutes = 35) %&gt;%  \n  let_cool()            # let it cool down\n\nNote that just like other R commands, pipes can be used to just display the result, or to save/re-save an object, depending on whether the assignment operator &lt;- is involved. See both below:\n\n# Create or overwrite object, defining as aggregate counts by age category (not printed)\nlinelist_summary &lt;- linelist %&gt;% \n  count(age_cat)\n\n\n# Print the table of counts in the console, but don't save it\nlinelist %&gt;% \n  count(age_cat)\n\n  age_cat    n\n1     0-4 1095\n2     5-9 1095\n3   10-14  941\n4   15-19  743\n5   20-29 1073\n6   30-49  754\n7   50-69   95\n8     70+    6\n9    &lt;NA&gt;   86\n\n\n%&lt;&gt;%\nThis is an “assignment pipe” from the magrittr package, which pipes an object forward and also re-defines the object. It must be the first pipe operator in the chain. It is shorthand. The below two commands are equivalent:\n\nlinelist &lt;- linelist %&gt;%\n  filter(age &gt; 50)\n\nlinelist %&lt;&gt;% filter(age &gt; 50)\n\n\n\n\nDefine intermediate objects\nThis approach to changing objects/data frames may be better if:\n\nYou need to manipulate multiple objects\n\nThere are intermediate steps that are meaningful and deserve separate object names\n\nRisks:\n\nCreating new objects for each step means creating lots of objects. If you use the wrong one you might not realize it!\n\nNaming all the objects can be confusing.\n\nErrors may not be easily detectable.\n\nEither name each intermediate object, or overwrite the original, or combine all the functions together. All come with their own risks.\nBelow is the same fake “cake” example as above, but using this style:\n\n# a fake example of how to bake a cake using this method (defining intermediate objects)\nbatter_1 &lt;- left_join(flour, eggs)\nbatter_2 &lt;- left_join(batter_1, oil)\nbatter_3 &lt;- left_join(batter_2, water)\n\nbatter_4 &lt;- mix_together(object = batter_3, utensil = spoon, minutes = 2)\n\ncake &lt;- bake(batter_4, degrees = 350, system = \"fahrenheit\", minutes = 35)\n\ncake &lt;- let_cool(cake)\n\nCombine all functions together - this is difficult to read:\n\n# an example of combining/nesting mutliple functions together - difficult to read\ncake &lt;- let_cool(bake(mix_together(batter_3, utensil = spoon, minutes = 2), degrees = 350, system = \"fahrenheit\", minutes = 35))",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#operators",
    "href": "new_pages/basics.html#operators",
    "title": "3  R Basics",
    "section": "3.12 Key operators and functions",
    "text": "3.12 Key operators and functions\nThis section details operators in R, such as:\n\nDefinitional operators.\n\nRelational operators (less than, equal too..).\n\nLogical operators (and, or…).\n\nHandling missing values.\n\nMathematical operators and functions (+/-, &gt;, sum(), median(), …).\n\nThe %in% operator.\n\n\n\nAssignment operators\n&lt;-\nThe basic assignment operator in R is &lt;-. Such that object_name &lt;- value.\nThis assignment operator can also be written as =. We advise use of &lt;- for general R use. We also advise surrounding such operators with spaces, for readability.\n&lt;&lt;-\nIf Writing functions, or using R in an interactive way with sourced scripts, then you may need to use this assignment operator &lt;&lt;- (from base R). This operator is used to define an object in a higher ‘parent’ R Environment. See this online reference.\n%&lt;&gt;%\nThis is an “assignment pipe” from the magrittr package, which pipes an object forward and also re-defines the object. It must be the first pipe operator in the chain. It is shorthand, as shown below in two equivalent examples:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(age_months = age_years * 12)\n\nThe above is equivalent to the below:\n\nlinelist %&lt;&gt;% mutate(age_months = age_years * 12)\n\n%&lt;+%\nThis is used to add data to phylogenetic trees with the ggtree package. See the page on Phylogenetic trees or this online resource book.\n\n\n\nRelational and logical operators\nRelational operators compare values and are often used when defining new variables and subsets of datasets. Here are the common relational operators in R:\n\n\n\nMeaning\nOperator\nExample\nExample Result\n\n\n\n\nEqual to\n==\n\"A\" == \"a\"\nFALSE (because R is case sensitive) Note that == (double equals) is different from = (single equals), which acts like the assignment operator &lt;-\n\n\nNot equal to\n!=\n2 != 0\nTRUE\n\n\nGreater than\n&gt;\n4 &gt; 2\nTRUE\n\n\nLess than\n&lt;\n4 &lt; 2\nFALSE\n\n\nGreater than or equal to\n&gt;=\n6 &gt;= 4\nTRUE\n\n\nLess than or equal to\n&lt;=\n6 &lt;= 4\nFALSE\n\n\nValue is missing\nis.na()\nis.na(7)\nFALSE (see page on Missing data)\n\n\nValue is not missing\n!is.na()\n!is.na(7)\nTRUE\n\n\n\nLogical operators, such as AND and OR, are often used to connect relational operators and create more complicated criteria. Complex statements might require parentheses ( ) for grouping and order of application.\n\n\n\n\n\n\n\nMeaning\nOperator\n\n\n\n\nAND\n&\n\n\nOR\n| (vertical bar)\n\n\nParentheses\n( ) Used to group criteria together and clarify order of operations\n\n\n\nFor example, below, we have a linelist with two variables we want to use to create our case definition, hep_e_rdt, a test result and other_cases_in_hh, which will tell us if there are other cases in the household. The command below uses the function case_when() to create the new variable case_def such that:\n\nlinelist_cleaned &lt;- linelist %&gt;%\n  mutate(case_def = case_when(\n    is.na(rdt_result) & is.na(other_case_in_home)            ~ NA_character_,\n    rdt_result == \"Positive\"                                 ~ \"Confirmed\",\n    rdt_result != \"Positive\" & other_cases_in_home == \"Yes\"  ~ \"Probable\",\n    TRUE                                                     ~ \"Suspected\"\n  ))\n\n\n\n\n\n\n\n\nCriteria in example above\nResulting value in new variable “case_def”\n\n\n\n\nIf the value for variables rdt_result and other_cases_in_home are missing\nNA (missing)\n\n\nIf the value in rdt_result is “Positive”\n“Confirmed”\n\n\nIf the value in rdt_result is NOT “Positive” AND the value in other_cases_in_home is “Yes”\n“Probable”\n\n\nIf one of the above criteria are not met\n“Suspected”\n\n\n\nNote that R is case-sensitive, so “Positive” is different than “positive”.\n\n\n\nMissing values\nIn R, missing values are represented by the special value NA (a “reserved” value) (capital letters N and A - not in quotation marks). If you import data that records missing data in another way (e.g. 99, “Missing”), you may want to re-code those values to NA. How to do this is addressed in the Import and export page.\nTo test whether a value is NA, use the special function is.na(), which returns TRUE or FALSE.\n\nrdt_result &lt;- c(\"Positive\", \"Suspected\", \"Positive\", NA)   # two positive cases, one suspected, and one unknown\nis.na(rdt_result)  # Tests whether the value of rdt_result is NA\n\n[1] FALSE FALSE FALSE  TRUE\n\n\nRead more about missing, infinite, NULL, and impossible values in the page on Missing data. Learn how to convert missing values when importing data in the page on Import and export.\n\n\n\nMathematics and statistics\nAll the operators and functions in this page are automatically available using base R.\n\nMathematical operators\nThese are often used to perform addition, division, to create new columns, etc. Below are common mathematical operators in R. Whether you put spaces around the operators is not important.\n\n\n\nPurpose\nExample in R\n\n\n\n\naddition\n2 + 3\n\n\nsubtraction\n2 - 3\n\n\nmultiplication\n2 * 3\n\n\ndivision\n30 / 5\n\n\nexponent\n2^3\n\n\norder of operations\n( )\n\n\n\n\n\nMathematical functions\n\n\n\nPurpose\nFunction\n\n\n\n\nrounding\nround(x, digits = n)\n\n\nrounding\njanitor::round_half_up(x, digits = n)\n\n\nceiling (round up)\nceiling(x)\n\n\nfloor (round down)\nfloor(x)\n\n\nabsolute value\nabs(x)\n\n\nsquare root\nsqrt(x)\n\n\nexponent\nexponent(x)\n\n\nnatural logarithm\nlog(x)\n\n\nlog base 10\nlog10(x)\n\n\nlog base 2\nlog2(x)\n\n\n\nNote: for round() the digits = specifies the number of decimal placed. Use signif() to round to a number of significant figures.\n\n\nScientific notation\nThe likelihood of scientific notation being used depends on the value of the scipen option.\nFrom the documentation of ?options: scipen is a penalty to be applied when deciding to print numeric values in fixed or exponential notation. Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.\nIf it is set to a low number (e.g. 0) it will be “turned on” always. To “turn off” scientific notation in your R session, set it to a very high number, for example:\n\n# turn off scientific notation\noptions(scipen = 999)\n\n\n\nRounding\nDANGER: round() uses “banker’s rounding” which rounds up from a .5 only if the upper number is even. Use round_half_up() from janitor to consistently round halves up to the nearest whole number. See this explanation\n\n# use the appropriate rounding function for your work\nround(c(2.5, 3.5))\n\n[1] 2 4\n\njanitor::round_half_up(c(2.5, 3.5))\n\n[1] 3 4\n\n\n\n\nStatistical functions\nCAUTION: The functions below will by default include missing values in calculations. Missing values will result in an output of NA, unless the argument na.rm = TRUE is specified. This can be written shorthand as na.rm = T.\n\n\n\nObjective\nFunction\n\n\n\n\nmean (average)\nmean(x, na.rm = T)\n\n\nmedian\nmedian(x, na.rm= T)\n\n\nstandard deviation\nsd(x, na.rm = T)\n\n\nquantiles*\nquantile(x, probs)\n\n\nsum\nsum(x, na.rm = T)\n\n\nminimum value\nmin(x, na.rm = T)\n\n\nmaximum value\nmax(x, na.rm = T)\n\n\nrange of numeric values\nrange(x, na.rm = T)\n\n\nsummary**\nsummary(x)\n\n\n\nNotes:\n\n*quantile(): x is the numeric vector to examine, and probs = is a numeric vector with probabilities within 0 and 1.0, e.g c(0.5, 0.8, 0.85)\n**summary(): gives a summary on a numeric vector including mean, median, and common percentiles\n\nDANGER: If providing a vector of numbers to one of the above functions, be sure to wrap the numbers within c() .\n\n# If supplying raw numbers to a function, wrap them in c()\nmean(1, 6, 12, 10, 5, 0)    # !!! INCORRECT !!!  \n\n[1] 1\n\nmean(c(1, 6, 12, 10, 5, 0)) # CORRECT\n\n[1] 5.666667\n\n\n\n\nOther useful functions\n\n\n\n\n\n\n\n\nObjective\nFunction\nExample\n\n\n\n\ncreate a sequence\nseq(from, to, by)\nseq(1, 10, 2)\n\n\nrepeat x, n times\nrep(x, ntimes)\nrep(1:3, 2) or rep(c(\"a\", \"b\", \"c\"), 3)\n\n\nsubdivide a numeric vector\ncut(x, n)\ncut(linelist$age, 5)\n\n\ntake a random sample\nsample(x, size)\nsample(linelist$id, size = 5, replace = TRUE)\n\n\n\n\n\n\n\n%in%\nA very useful operator for matching values, and for quickly assessing if a value is within a vector or data frame.\n\nmy_vector &lt;- c(\"a\", \"b\", \"c\", \"d\")\n\n\n\"a\" %in% my_vector\n\n[1] TRUE\n\n\"h\" %in% my_vector\n\n[1] FALSE\n\n\nTo ask if a value is not %in% a vector, put an exclamation mark (!) in front of the logic statement:\n\n# to negate, put an exclamation in front\n!\"a\" %in% my_vector\n\n[1] FALSE\n\n!\"h\" %in% my_vector\n\n[1] TRUE\n\n\n%in% is very useful when using the dplyr function case_when(). You can define a vector previously, and then reference it later. For example:\n\naffirmative &lt;- c(\"1\", \"Yes\", \"YES\", \"yes\", \"y\", \"Y\", \"oui\", \"Oui\", \"Si\")\n\nlinelist &lt;- linelist %&gt;% \n  mutate(child_hospitaled = case_when(\n    hospitalized %in% affirmative & age &lt; 18 ~ \"Hospitalized Child\",\n    TRUE                                      ~ \"Not\"))\n\nNote: If you want to detect a partial string, perhaps using str_detect() from stringr, it will not accept a character vector like c(\"1\", \"Yes\", \"yes\", \"y\"). Instead, it must be given a regular expression - one condensed string with OR bars, such as “1|Yes|yes|y”. For example, str_detect(hospitalized, \"1|Yes|yes|y\"). See the page on Characters and strings for more information.\nYou can convert a character vector to a named regular expression with this command:\n\naffirmative &lt;- c(\"1\", \"Yes\", \"YES\", \"yes\", \"y\", \"Y\", \"oui\", \"Oui\", \"Si\")\naffirmative\n\n[1] \"1\"   \"Yes\" \"YES\" \"yes\" \"y\"   \"Y\"   \"oui\" \"Oui\" \"Si\" \n\n# condense to \naffirmative_str_search &lt;- paste0(affirmative, collapse = \"|\")  # option with base R\naffirmative_str_search &lt;- str_c(affirmative, collapse = \"|\")   # option with stringr package\n\naffirmative_str_search\n\n[1] \"1|Yes|YES|yes|y|Y|oui|Oui|Si\"",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/basics.html#errors-warnings",
    "href": "new_pages/basics.html#errors-warnings",
    "title": "3  R Basics",
    "section": "3.13 Errors & warnings",
    "text": "3.13 Errors & warnings\nThis section explains:\n\nThe difference between errors and warnings\n\nGeneral syntax tips for writing R code\n\nCode assists\n\nCommon errors and warnings and troubleshooting tips can be found in the page on Errors and help.\n\n\nError versus Warning\nWhen a command is run, the R Console may show you warning or error messages in red text.\n\nA warning means that R has completed your command, but had to take additional steps or produced unusual output that you should be aware of.\nAn error means that R was not able to complete your command.\n\nLook for clues:\n\nThe error/warning message will often include a line number for the problem.\nIf an object “is unknown” or “not found”, perhaps you spelled it incorrectly, forgot to call a package with library(), or forgot to re-run your script after making changes.\n\nIf all else fails, copy the error message into Google along with some key terms - chances are that someone else has worked through this already!\n\n\n\nGeneral syntax tips\nA few things to remember when writing commands in R, to avoid errors and warnings:\n\nAlways close parentheses - tip: count the number of opening “(” and closing parentheses “)” for each code chunk\nAvoid spaces in column and object names. Use underscore ( _ ) or periods ( . ) instead\nKeep track of and remember to separate a function’s arguments with commas\nR is case-sensitive, meaning Variable_A is different from variable_A\n\n\n\n\nCode assists\nAny script (RMarkdown or otherwise) will give clues when you have made a mistake. For example, if you forgot to write a comma where it is needed, or to close a parentheses, RStudio will raise a flag on that line, on the right side of the script, to warn you.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "new_pages/transition_to_R.html",
    "href": "new_pages/transition_to_R.html",
    "title": "4  Transition to R",
    "section": "",
    "text": "4.1 From Excel\nTransitioning from Excel directly to R is a very achievable goal. It may seem daunting, but you can do it!\nIt is true that someone with strong Excel skills can do very advanced activities in Excel alone - even using scripting tools like VBA. Excel is used across the world and is an essential tool for an epidemiologist. However, complementing it with R can dramatically improve and expand your work flows.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transition to R</span>"
    ]
  },
  {
    "objectID": "new_pages/transition_to_R.html#from-excel",
    "href": "new_pages/transition_to_R.html#from-excel",
    "title": "4  Transition to R",
    "section": "",
    "text": "Benefits\nYou will find that using R offers immense benefits in time saved, more consistent and accurate analysis, reproducibility, shareability, and faster error-correction. Like any new software there is a learning “curve” of time you must invest to become familiar. The dividends will be significant and immense scope of new possibilities will open to you with R.\nExcel is a well-known software that can be easy for a beginner to use to produce simple analysis and visualizations with “point-and-click”. In comparison, it can take a couple weeks to become comfortable with R functions and interface. However, R has evolved in recent years to become much more friendly to beginners.\nMany Excel workflows rely on memory and on repetition - thus, there is much opportunity for error. Furthermore, generally the data cleaning, analysis methodology, and equations used are hidden from view. It can require substantial time for a new colleague to learn what an Excel workbook is doing and how to troubleshoot it. With R, all the steps are explicitly written in the script and can be easily viewed, edited, corrected, and applied to other datasets.\nTo begin your transition from Excel to R you must adjust your mindset in a few important ways:\n\n\nTidy data\nUse machine-readable “tidy” data instead of messy “human-readable” data. These are the three main requirements for “tidy” data, as explained in this tutorial on “tidy” data in R:\n\nEach variable must have its own column\n\nEach observation must have its own row\n\nEach value must have its own cell\n\nTo Excel users - think of the role that Excel “tables” play in standardizing data and making the format more predictable.\nAn example of “tidy” data would be the case linelist used throughout this handbook - each variable is contained within one column, each observation (one case) has it’s own row, and every value is in just one cell. Below you can view the first 50 rows of the linelist:\n\n\n\n\n\n\nThe main reason one encounters non-tidy data is because many Excel spreadsheets are designed to prioritize easy reading by humans, not easy reading by machines/software.\nTo help you see the difference, below are some fictional examples of non-tidy data that prioritize human-readability over machine-readability:\n\n\n\n\n\n\n\n\n\nProblems: In the spreadsheet above, there are merged cells which are not easily digested by R. Which row should be considered the “header” is not clear. A color-based dictionary is to the right side and cell values are represented by colors - which is also not easily interpreted by R (nor by humans with color-blindness!). Furthermore, different pieces of information are combined into one cell (multiple partner organizations working in one area, or the status “TBC” in the same cell as “Partner D”).\n\n\n\n\n\n\n\n\n\nProblems: In the spreadsheet above, there are numerous extra empty rows and columns within the dataset - this will cause cleaning headaches in R. Furthermore, the GPS coordinates are spread across two rows for a given treatment center. As a side note - the GPS coordinates are in two different formats!\n“Tidy” datasets may not be as readable to a human eye, but they make data cleaning and analysis much easier! Tidy data can be stored in various formats, for example “long” or “wide”“(see page on Pivoting data), but the principles above are still observed.\n\n\nFunctions\nThe R word “function” might be new, but the concept exists in Excel too as formulas. Formulas in Excel also require precise syntax (e.g. placement of semicolons and parentheses). All you need to do is learn a few new functions and how they work together in R.\n\n\nScripts\nInstead of clicking buttons and dragging cells you will be writing every step and procedure into a “script”. Excel users may be familiar with “VBA macros” which also employ a scripting approach.\nThe R script consists of step-by-step instructions. This allows any colleague to read the script and easily see the steps you took. This also helps de-bug errors or inaccurate calculations. See the R basics section on scripts for examples.\nHere is an example of an R script:\n\n\n\n\n\n\n\n\n\n\n\nExcel-to-R resources\nHere are some links to tutorials to help you transition to R from Excel:\n\nR vs. Excel\n\nRStudio course in R for Excel users\n\n\n\nR-Excel interaction\nR has robust ways to import Excel workbooks, work with the data, export/save Excel files, and work with the nuances of Excel sheets.\nIt is true that some of the more aesthetic Excel formatting can get lost in translation (e.g. italics, sideways text, etc.). If your work flow requires passing documents back-and-forth between R and Excel while retaining the original Excel formatting, try packages such as openxlsx.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transition to R</span>"
    ]
  },
  {
    "objectID": "new_pages/transition_to_R.html#from-stata",
    "href": "new_pages/transition_to_R.html#from-stata",
    "title": "4  Transition to R",
    "section": "4.2 From Stata",
    "text": "4.2 From Stata\n\nComing to R from Stata\nMany epidemiologists are first taught how to use Stata, and it can seem daunting to move into R. However, if you are a comfortable Stata user then the jump into R is certainly more manageable than you might think. While there are some key differences between Stata and R in how data can be created and modified, as well as how analysis functions are implemented – after learning these key differences you will be able to translate your skills.\nBelow are some key translations between Stata and R, which may be handy as your review this guide.\nGeneral notes\n\n\n\nSTATA\nR\n\n\n\n\nYou can only view and manipulate one dataset at a time\nYou can view and manipulate multiple datasets at the same time, therefore you will frequently have to specify your dataset within the code\n\n\nOnline community available through https://www.statalist.org/\nOnline community available through RStudio, StackOverFlow, and R-bloggers\n\n\nPoint and click functionality as an option\nMinimal point and click functionality\n\n\nHelp for commands available by help [command]\nHelp available by [function]? or search in the Help pane\n\n\nComment code using * or /// or /* TEXT */\nComment code using #\n\n\nAlmost all commands are built-in to Stata. New/user-written functions can be installed as ado files using ssc install [package]\nR installs with base functions, but typical use involves installing other packages from CRAN (see page on R basics)\n\n\nAnalysis is usually written in a do file\nAnalysis written in an R script in the RStudio source pane. R markdown scripts are an alternative.\n\n\n\nWorking directory\n\n\n\nSTATA\nR\n\n\n\n\nWorking directories involve absolute filepaths (e.g. “C:/usename/documents/projects/data/”)\nWorking directories can be either absolute, or relative to a project root folder by using the here package (see Import and export)\n\n\nSee current working directory with pwd\nUse getwd() or here() (if using the here package), with empty parentheses\n\n\nSet working directory with cd “folder location”\nUse setwd(“folder location”), or set_here(\"folder location) (if using here package)\n\n\n\nImporting and viewing data\n\n\n\nSTATA\nR\n\n\n\n\nSpecific commands per file type\nUse import() from rio package for almost all filetypes. Specific functions exist as alternatives (see Import and export)\n\n\nReading in csv files is done by import delimited “filename.csv”\nUse import(\"filename.csv\")\n\n\nReading in xslx files is done by import excel “filename.xlsx”\nUse import(\"filename.xlsx\")\n\n\nBrowse your data in a new window using the command browse\nView a dataset in the RStudio source pane using View(dataset). You need to specify your dataset name to the function in R because multiple datasets can be held at the same time. Note capital “V” in this function\n\n\nGet a high-level overview of your dataset using summarize, which provides the variable names and basic information\nGet a high-level overview of your dataset using summary(dataset)\n\n\n\nBasic data manipulation\n\n\n\nSTATA\nR\n\n\n\n\nDataset columns are often referred to as “variables”\nMore often referred to as “columns” or sometimes as “vectors” or “variables”\n\n\nNo need to specify the dataset\nIn each of the below commands, you need to specify the dataset - see the page on Cleaning data and core functions for examples\n\n\nNew variables are created using the command generate varname =\nGenerate new variables using the function mutate(varname = ). See page on Cleaning data and core functions for details on all the below dplyr functions.\n\n\nVariables are renamed using rename old_name new_name\nColumns can be renamed using the function rename(new_name = old_name)\n\n\nVariables are dropped using drop varname\nColumns can be removed using the function select() with the column name in the parentheses following a minus sign\n\n\nFactor variables can be labeled using a series of commands such as label define\nLabeling values can done by converting the column to Factor class and specifying levels. See page on Factors. Column names are not typically labeled as they are in Stata.\n\n\n\nDescriptive analysis\n\n\n\nSTATA\nR\n\n\n\n\nTabulate counts of a variable using tab varname\nProvide the dataset and column name to table() such as table(dataset$colname). Alternatively, use count(varname) from the dplyr package, as explained in Grouping data\n\n\nCross-tabulaton of two variables in a 2x2 table is done with tab varname1 varname2\nUse table(dataset$varname1, dataset$varname2 or count(varname1, varname2)\n\n\n\nWhile this list gives an overview of the basics in translating Stata commands into R, it is not exhaustive. There are many other great resources for Stata users transitioning to R that could be of interest:\n\nhttps://dss.princeton.edu/training/RStata.pdf\n\nhttps://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html\n\nhttp://r4stats.com/books/r4stata/",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transition to R</span>"
    ]
  },
  {
    "objectID": "new_pages/transition_to_R.html#from-sas",
    "href": "new_pages/transition_to_R.html#from-sas",
    "title": "4  Transition to R",
    "section": "4.3 From SAS",
    "text": "4.3 From SAS\n\nComing from SAS to R\nSAS is commonly used at public health agencies and academic research fields. Although transitioning to a new language is rarely a simple process, understanding key differences between SAS and R may help you start to navigate the new language using your native language. Below outlines the key translations in data management and descriptive analysis between SAS and R.\nGeneral notes\n\n\n\nSAS\nR\n\n\n\n\nOnline community available through SAS Customer Support\nOnline community available through RStudio, StackOverFlow, and R-bloggers\n\n\nHelp for commands available by help [command]\nHelp available by [function]? or search in the Help pane\n\n\nComment code using * TEXT ; or /* TEXT */\nComment code using #\n\n\nAlmost all commands are built-in. Users can write new functions using SAS macro, SAS/IML, SAS Component Language (SCL), and most recently, procedures Proc Fcmp and Proc Proto\nR installs with base functions, but typical use involves installing other packages from CRAN (see page on R basics)\n\n\nAnalysis is usually conducted by writing a SAS program in the Editor window.\nAnalysis written in an R script in the RStudio source pane. R markdown scripts are an alternative.\n\n\n\nWorking directory\n\n\n\nSAS\nR\n\n\n\n\nWorking directories can be either absolute, or relative to a project root folder by defining the root folder using %let rootdir=/root path; %include “&rootdir/subfoldername/filename”\nWorking directories can be either absolute, or relative to a project root folder by using the here package (see Import and export))\n\n\nSee current working directory with %put %sysfunc(getoption(work));\nUse getwd() or here() (if using the here package), with empty parentheses\n\n\nSet working directory with libname “folder location”\nUse setwd(“folder location”), or set_here(\"folder location) if using here package\n\n\n\nImporting and viewing data\n\n\n\nSAS\nR\n\n\n\n\nUse Proc Import procedure or using Data Step Infile statement.\nUse import() from rio package for almost all filetypes. Specific functions exist as alternatives (see Import and export)\n\n\nReading in csv files is done by using Proc Import datafile=”filename.csv” out=work.filename dbms=CSV; run; OR using Data Step Infile statement\nUse import(\"filename.csv\")\n\n\nReading in xslx files is done by using Proc Import datafile=”filename.xlsx” out=work.filename dbms=xlsx; run; OR using Data Step Infile statement\nUse import(“filename.xlsx”)\n\n\nBrowse your data in a new window by opening the Explorer window and select desired library and the dataset\nView a dataset in the RStudio source pane using View(dataset). You need to specify your dataset name to the function in R because multiple datasets can be held at the same time. Note capital “V” in this function\n\n\n\nBasic data manipulation\n\n\n\nSAS\nR\n\n\n\n\nDataset columns are often referred to as “variables”\nMore often referred to as “columns” or sometimes as “vectors” or “variables”\n\n\nNo special procedures are needed to create a variable. New variables are created simply by typing the new variable name, followed by an equal sign, and then an expression for the value\nGenerate new variables using the function mutate(). See page on Cleaning data and core functions for details on all the below dplyr functions.\n\n\nVariables are renamed using rename *old_name=new_name*\nColumns can be renamed using the function rename(new_name = old_name)\n\n\nVariables are kept using **keep**=varname\nColumns can be selected using the function select() with the column name in the parentheses\n\n\nVariables are dropped using **drop**=varname\nColumns can be removed using the function select() with the column name in the parentheses following a minus sign\n\n\nFactor variables can be labeled in the Data Step using Label statement\nLabeling values can done by converting the column to Factor class and specifying levels. See page on Factors. Column names are not typically labeled.\n\n\nRecords are selected using Where or If statement in the Data Step. Multiple selection conditions are separated using “and” command.\nRecords are selected using the function filter() with multiple selection conditions separated either by an AND operator (&) or a comma\n\n\nDatasets are combined using Merge statement in the Data Step. The datasets to be merged need to be sorted first using Proc Sort procedure.\ndplyr package offers a few functions for merging datasets. See page Joining Data for details.\n\n\n\nDescriptive analysis\n\n\n\nSAS\nR\n\n\n\n\nGet a high-level overview of your dataset using Proc Summary procedure, which provides the variable names and descriptive statistics\nGet a high-level overview of your dataset using summary(dataset) or skim(dataset) from the skimr package\n\n\nTabulate counts of a variable using proc freq data=Dataset; Tables varname; Run;\nSee the page on Descriptive tables. Options include table() from base R, and tabyl() from janitor package, among others. Note you will need to specify the dataset and column name as R holds multiple datasets.\n\n\nCross-tabulation of two variables in a 2x2 table is done with proc freq data=Dataset; Tables rowvar*colvar; Run;\nAgain, you can use table(), tabyl() or other options as described in the Descriptive tables page.\n\n\n\nSome useful resources:\nR for SAS and SPSS Users (2011)\nSAS and R, Second Edition (2014)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transition to R</span>"
    ]
  },
  {
    "objectID": "new_pages/transition_to_R.html#data-interoperability",
    "href": "new_pages/transition_to_R.html#data-interoperability",
    "title": "4  Transition to R",
    "section": "4.4 Data interoperability",
    "text": "4.4 Data interoperability\n\nsee the Import and export(importing.qmd) page for details on how the R package rio can import and export files such as STATA .dta files, SAS .xpt and.sas7bdat files, SPSS .por and.sav files, and many others.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Transition to R</span>"
    ]
  },
  {
    "objectID": "new_pages/packages_suggested.html",
    "href": "new_pages/packages_suggested.html",
    "title": "5  Suggested packages",
    "section": "",
    "text": "5.1 Packages from CRAN\n##########################################\n# List of useful epidemiology R packages #\n##########################################\n\n# This script uses the p_load() function from pacman R package, \n# which installs if package is absent, and loads for use if already installed\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n\n# Packages available from CRAN\n##############################\npacman::p_load(\n     \n     # learning R\n     ############\n     learnr,   # interactive tutorials in RStudio Tutorial pane\n     swirl,    # interactive tutorials in R console\n        \n     # project and file management\n     #############################\n     here,     # file paths relative to R project root folder\n     rio,      # import/export of many types of data\n     openxlsx, # import/export of multi-sheet Excel workbooks \n     \n     # package install and management\n     ################################\n     pacman,   # package install/load\n     renv,     # managing versions of packages when working in collaborative groups\n     remotes,  # install from github\n     \n     # General data management\n     #########################\n     tidyverse,    # includes many packages for tidy data wrangling and presentation\n          #dplyr,      # data management\n          #tidyr,      # data management\n          #ggplot2,    # data visualization\n          #stringr,    # work with strings and characters\n          #forcats,    # work with factors \n          #lubridate,  # work with dates\n          #purrr       # iteration and working with lists\n     linelist,     # cleaning linelists\n     naniar,       # assessing missing data\n     \n     # statistics  \n     ############\n     janitor,      # tables and data cleaning\n     gtsummary,    # making descriptive and statistical tables\n     rstatix,      # quickly run statistical tests and summaries\n     broom,        # tidy up results from regressions\n     lmtest,       # likelihood-ratio tests\n     easystats,\n          # parameters, # alternative to tidy up results from regressions\n          # see,        # alternative to visualise forest plots \n     \n     # epidemic modeling\n     ###################\n     epicontacts,  # Analysing transmission networks\n     EpiNow2,      # Rt estimation\n     EpiEstim,     # Rt estimation\n     projections,  # Incidence projections\n     incidence2,   # Make epicurves and handle incidence data\n     i2extras,     # Extra functions for the incidence2 package\n     epitrix,      # Useful epi functions\n     distcrete,    # Discrete delay distributions\n     \n     \n     # plots - general\n     #################\n     #ggplot2,         # included in tidyverse\n     cowplot,          # combining plots  \n     # patchwork,      # combining plots (alternative)     \n     RColorBrewer,     # color scales\n     ggnewscale,       # to add additional layers of color schemes\n\n     \n     # plots - specific types\n     ########################\n     DiagrammeR,       # diagrams using DOT language\n     incidence2,       # epidemic curves\n     gghighlight,      # highlight a subset\n     ggrepel,          # smart labels\n     plotly,           # interactive graphics\n     gganimate,        # animated graphics \n\n     \n     # gis\n     ######\n     sf,               # to manage spatial data using a Simple Feature format\n     tmap,             # to produce simple maps, works for both interactive and static maps\n     OpenStreetMap,    # to add OSM basemap in ggplot map\n     spdep,            # spatial statistics \n     \n     # routine reports\n     #################\n     rmarkdown,        # produce PDFs, Word Documents, Powerpoints, and HTML files\n     reportfactory,    # auto-organization of R Markdown outputs\n     officer,          # powerpoints\n     \n     # dashboards\n     ############\n     flexdashboard,    # convert an R Markdown script into a dashboard\n     shiny,            # interactive web apps\n     \n     # tables for presentation\n     #########################\n     knitr,            # R Markdown report generation and html tables\n     flextable,        # HTML tables\n     #DT,              # HTML tables (alternative)\n     #gt,              # HTML tables (alternative)\n     #huxtable,        # HTML tables (alternative) \n     \n     # phylogenetics\n     ###############\n     ggtree,           # visualization and annotation of trees\n     ape,              # analysis of phylogenetics and evolution\n     treeio            # to visualize phylogenetic files\n \n)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Suggested packages</span>"
    ]
  },
  {
    "objectID": "new_pages/packages_suggested.html#packages-from-github",
    "href": "new_pages/packages_suggested.html#packages-from-github",
    "title": "5  Suggested packages",
    "section": "5.2 Packages from Github",
    "text": "5.2 Packages from Github\nBelow are commmands to install two packages directly from Github repositories.\n\nThe development version of epicontacts contains the ability to make transmission trees with an temporal x-axis\n\nThe epirhandbook package contains all the example data for this handbook and can be used to download the offline version of the handbook.\n\n\n# Packages to download from Github (not available on CRAN)\n##########################################################\n\n# Development version of epicontacts (for transmission chains with a time x-axis)\npacman::p_install_gh(\"reconhub/epicontacts@timeline\")\n\n# The package for this handbook, which includes all the example data  \npacman::p_install_gh(\"appliedepi/epirhandbook\")",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Suggested packages</span>"
    ]
  },
  {
    "objectID": "new_pages/r_projects.html",
    "href": "new_pages/r_projects.html",
    "title": "6  R projects",
    "section": "",
    "text": "6.1 Suggested use\nA common, efficient, and trouble-free way to use R is to combine these 3 elements. One discrete work project is hosted within one R project. Each element is described in the sections below.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R projects</span>"
    ]
  },
  {
    "objectID": "new_pages/r_projects.html#suggested-use",
    "href": "new_pages/r_projects.html#suggested-use",
    "title": "6  R projects",
    "section": "",
    "text": "An R project\n\nA self-contained working environment with folders for data, scripts, outputs, etc.\n\n\nThe here package for relative filepaths\n\nFilepaths are written relative to the root folder of the R project - see Import and export for more information\n\n\nThe rio package for importing/exporting\n\nimport() and export() handle any file type by by its extension (e.g. .csv, .xlsx, .png)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R projects</span>"
    ]
  },
  {
    "objectID": "new_pages/r_projects.html#creating-an-r-project",
    "href": "new_pages/r_projects.html#creating-an-r-project",
    "title": "6  R projects",
    "section": "6.2 Creating an R project",
    "text": "6.2 Creating an R project\nTo create an R project, select “New Project” from the File menu.\n\nIf you want to create a new folder for the project, select “New directory” and indicate where you want it to be created.\n\nIf you want to create the project within an existing folder, click “Existing directory” and indicate the folder.\n\nIf you want to clone a Github repository, select the third option “Version Control” and then “Git”. See the page on Version control and collaboration with Git and Github for further details.\n\n\n\n\n\n\n\n\n\n\nThe R project you create will come in the form of a folder containing a .Rproj file. This file is a shortcut and likely the primary way you will open your project. You can also open a project by selecting “Open Project” from the File menu. Alternatively on the far upper right side of RStudio you will see an R project icon and a drop-down menu of available R projects.\nTo exit from an R project, either open a new project, or close the project (File - Close Project).\n\nSwitch projects\nTo switch between projects, click the R project icon and drop-down menu at the very top-right of RStudio. You will see options to Close Project, Open Project, and a list of recent projects.\n\n\n\n\n\n\n\n\n\n\n\nSettings\nIt is generally advised that you start RStudio each time with a “clean slate” - that is, with your workspace not preserved from your previous session. This will mean that your objects and results will not persist session-to-session (you must re-create them by running your scripts). This is good, because it will force you to write better scripts and avoid errors in the long run.\nTo set RStudio to have a “clean slate” each time at start-up:\n\nSelect “Project Options” from the Tools menu.\n\nIn the “General” tab, set RStudio to not restore .RData into workspace at startup, and to not save workspace to .RData on exit.\n\n\n\nOrganization\nIt is common to have subfolders in your project. Consider having folders such as “data”, “scripts”, “figures”, “presentations”. You can add folders in the typical way you would add a new folder for your computer. Alternatively, see the page on Directory interactions to learn how to create new folders with R commands.\n\n\nVersion control\nConsider a version control system. It could be something as simple as having dates on the names of scripts (e.g. “transmission_analysis_2020-10-03.R”) and an “archive” folder. Consider also having commented header text at the top of each script with a description, tags, authors, and change log.\nA more complicated method would involve using Github or a similar platform for version control. See the page on Version control and collaboration with Git and Github.\nOne tip is that you can search across an entire project or folder using the “Find in Files” tool (Edit menu). It can search and even replace strings across multiple files.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R projects</span>"
    ]
  },
  {
    "objectID": "new_pages/r_projects.html#examples",
    "href": "new_pages/r_projects.html#examples",
    "title": "6  R projects",
    "section": "6.3 Examples",
    "text": "6.3 Examples\nBelow are some examples of import/export/saving using here() from within an R projct. Read more about using the here package in the Import and export page.\nImporting linelist_raw.xlsx from the “data” folder in your R project\n\nlinelist &lt;- import(here(\"data\", \"linelist_raw.xlsx\"))\n\nExporting the R object linelist as “my_linelist.rds” to the “clean” folder within the “data” folder in your R project.\n\nexport(linelist, here(\"data\",\"clean\", \"my_linelist.rds\"))\n\nSaving the most recently printed plot as “epicurve_2021-02-15.png” within the “epicurves” folder in “outputs” folder in your R project.\n\nggsave(here(\"outputs\", \"epicurves\", \"epicurve_2021-02-15.png\"))",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R projects</span>"
    ]
  },
  {
    "objectID": "new_pages/r_projects.html#resources",
    "href": "new_pages/r_projects.html#resources",
    "title": "6  R projects",
    "section": "6.4 Resources",
    "text": "6.4 Resources\nRStudio webpage on using R projects",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R projects</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html",
    "href": "new_pages/importing.html",
    "title": "7  Import and export",
    "section": "",
    "text": "7.1 Overview\nWhen you import a “dataset” into R, you are generally creating a new data frame object in your R environment and defining it as an imported file (e.g. Excel, CSV, TSV, RDS) that is located in your folder directories at a certain file path/address.\nYou can import/export many types of files, including those created by other statistical programs (SAS, STATA, SPSS). You can also connect to relational databases.\nR even has its own data formats:",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#overview",
    "href": "new_pages/importing.html#overview",
    "title": "7  Import and export",
    "section": "",
    "text": "An RDS file (.rds) stores a single R object such as a data frame. These are useful to store cleaned data, as they maintain R column classes. Read more in this section.\n\nAn RData file (.Rdata) can be used to store multiple objects, or even a complete R workspace. Read more in this section.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#the-rio-package",
    "href": "new_pages/importing.html#the-rio-package",
    "title": "7  Import and export",
    "section": "7.2 The rio package",
    "text": "7.2 The rio package\nThe R package we recommend is: rio. The name “rio” is an abbreviation of “R I/O” (input/output).\nIts functions import() and export() can handle many different file types (e.g. .xlsx, .csv, .rds, .tsv). When you provide a file path to either of these functions (including the file extension like “.csv”), rio will read the extension and use the correct tool to import or export the file.\nThe alternative to using rio is to use functions from many other packages, each of which is specific to a type of file. For example, read.csv() (base R), read.xlsx() (openxlsx package), and write_csv() (readr pacakge), etc. These alternatives can be difficult to remember, whereas using import() and export() from rio is easy.\nrio’s functions import() and export() use the appropriate package and function for a given file, based on its file extension. See the end of this page for a complete table of which packages/functions rio uses in the background. It can also be used to import STATA, SAS, and SPSS files, among dozens of other file types.\nImport/export of shapefiles requires other packages, as detailed in the page on GIS basics.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#here",
    "href": "new_pages/importing.html#here",
    "title": "7  Import and export",
    "section": "7.3 The here package",
    "text": "7.3 The here package\nThe package here and its function here() make it easy to tell R where to find and to save your files - in essence, it builds file paths.\nUsed in conjunction with an R project, here allows you to describe the location of files in your R project in relation to the R project’s root directory (the top-level folder). This is useful when the R project may be shared or accessed by multiple people/computers. It prevents complications due to the unique file paths on different computers (e.g. \"C:/Users/Laura/Documents...\" by “starting” the file path in a place common to all users (the R project root).\nThis is how here() works within an R project:\n\nWhen the here package is first loaded within the R project, it places a small file called “.here” in the root folder of your R project as a “benchmark” or “anchor”\n\nIn your scripts, to reference a file in the R project’s sub-folders, you use the function here() to build the file path in relation to that anchor\nTo build the file path, write the names of folders beyond the root, within quotes, separated by commas, finally ending with the file name and file extension as shown below\n\nhere() file paths can be used for both importing and exporting\n\nFor example, below, the function import() is being provided a file path constructed with here().\n\nlinelist &lt;- import(here(\"data\", \"linelists\", \"ebola_linelist.xlsx\"))\n\nThe command here(\"data\", \"linelists\", \"ebola_linelist.xlsx\") is actually providing the full file path that is unique to the user’s computer:\n\"C:/Users/Laura/Documents/my_R_project/data/linelists/ebola_linelist.xlsx\"\nThe beauty is that the R command using here() can be successfully run on any computer accessing the R project.\nTIP: If you are unsure where the “.here” root is set to, run the function here() with empty parentheses.\nRead more about the here package at this link.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#file-paths",
    "href": "new_pages/importing.html#file-paths",
    "title": "7  Import and export",
    "section": "7.4 File paths",
    "text": "7.4 File paths\nWhen importing or exporting data, you must provide a file path. You can do this one of three ways:\n\nRecommended: provide a “relative” file path with the here package\n\nProvide the “full” / “absolute” file path\n\nManual file selection\n\n\n“Relative” file paths\nIn R, “relative” file paths consist of the file path relative to the root of an R project. They allow for more simple file paths that can work on different computers (e.g. if the R project is on a shared drive or is sent by email). As described above, relative file paths are facilitated by use of the here package.\nAn example of a relative file path constructed with here() is below. We assume the work is in an R project that contains a sub-folder “data” and within that a subfolder “linelists”, in which there is the .xlsx file of interest.\n\nlinelist &lt;- import(here(\"data\", \"linelists\", \"ebola_linelist.xlsx\"))\n\n\n\n“Absolute” file paths\nAbsolute or “full” file paths can be provided to functions like import() but they are “fragile” as they are unique to the user’s specific computer and therefore not recommended.\nBelow is an example of an absolute file path, where in Laura’s computer there is a folder “analysis”, a sub-folder “data” and within that a sub-folder “linelists”, in which there is the .xlsx file of interest.\n\nlinelist &lt;- import(\"C:/Users/Laura/Documents/analysis/data/linelists/ebola_linelist.xlsx\")\n\nA few things to note about absolute file paths:\n\nAvoid using absolute file paths as they will break if the script is run on a different computer\nUse forward slashes (/), as in the example above (note: this is NOT the default for Windows file paths)\n\nFile paths that begin with double slashes (e.g. “//…”) will likely not be recognized by R and will produce an error. Consider moving your work to a “named” or “lettered” drive that begins with a letter (e.g. “J:” or “C:”). See the page on Directory interactions for more details on this issue.\n\nOne scenario where absolute file paths may be appropriate is when you want to import a file from a shared drive that has the same full file path for all users.\nTIP: To quickly convert all \\ to /, highlight the code of interest, use Ctrl+f (in Windows), check the option box for “In selection”, and then use the replace functionality to convert them.\n\n\n\nSelect file manually\nYou can import data manually via one of these methods:\n\nEnvironment RStudio Pane, click “Import Dataset”, and select the type of data\nClick File / Import Dataset / (select the type of data)\n\nTo hard-code manual selection, use the base R command file.choose() (leaving the parentheses empty) to trigger appearance of a pop-up window that allows the user to manually select the file from their computer. For example:\n\n\n# Manual selection of a file. When this command is run, a POP-UP window will appear. \n# The file path selected will be supplied to the import() command.\n\nmy_data &lt;- import(file.choose())\n\nTIP: The pop-up window may appear BEHIND your RStudio window.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#import-data",
    "href": "new_pages/importing.html#import-data",
    "title": "7  Import and export",
    "section": "7.5 Import data",
    "text": "7.5 Import data\nTo use import() to import a dataset is quite simple. Simply provide the path to the file (including the file name and file extension) in quotes. If using here() to build the file path, follow the instructions above. Below are a few examples:\nImporting a csv file that is located in your “working directory” or in the R project root folder:\n\nlinelist &lt;- import(\"linelist_cleaned.csv\")\n\nImporting the first sheet of an Excel workbook that is located in “data” and “linelists” sub-folders of the R project (the file path built using here()):\n\nlinelist &lt;- import(here(\"data\", \"linelists\", \"linelist_cleaned.xlsx\"))\n\nImporting a data frame (a .rds file) using an absolute file path:\n\nlinelist &lt;- import(\"C:/Users/Laura/Documents/tuberculosis/data/linelists/linelist_cleaned.rds\")\n\n\nSpecific Excel sheets\nBy default, if you provide an Excel workbook (.xlsx) to import(), the workbook’s first sheet will be imported. If you want to import a specific sheet, include the sheet name to the which = argument. For example:\n\nmy_data &lt;- import(\"my_excel_file.xlsx\", which = \"Sheetname\")\n\nIf using the here() method to provide a relative pathway to import(), you can still indicate a specific sheet by adding the which = argument after the closing parentheses of the here() function.\n\n# Demonstration: importing a specific Excel sheet when using relative pathways with the 'here' package\nlinelist_raw &lt;- import(here(\"data\", \"linelist.xlsx\"), which = \"Sheet1\")`  \n\nTo export a data frame from R to a specific Excel sheet and have the rest of the Excel workbook remain unchanged, you will have to import, edit, and export with an alternative package catered to this purpose such as openxlsx. See more information in the page on Directory interactions or at this github page.\nIf your Excel workbook is .xlsb (binary format Excel workbook) you may not be able to import it using rio. Consider re-saving it as .xlsx, or using a package like readxlsb which is built for this purpose.\n\n\n\nMissing values\nYou may want to designate which value(s) in your dataset should be considered as missing. As explained in the page on Missing data, the value in R for missing data is NA, but perhaps the dataset you want to import uses 99, “Missing”, or just empty character space “” instead.\nUse the na = argument for import() and provide the value(s) within quotes (even if they are numbers). You can specify multiple values by including them within a vector, using c() as shown below.\nHere, the value “99” in the imported dataset is considered missing and converted to NA in R.\n\nlinelist &lt;- import(here(\"data\", \"my_linelist.xlsx\"), na = \"99\")\n\nHere, any of the values “Missing”, “” (empty cell), or ” ” (single space) in the imported dataset are converted to NA in R.\n\nlinelist &lt;- import(here(\"data\", \"my_linelist.csv\"), na = c(\"Missing\", \"\", \" \"))\n\n\n\n\nSkip rows\nSometimes, you may want to avoid importing a row of data. You can do this with the argument skip = if using import() from rio on a .xlsx or .csv file. Provide the number of rows you want to skip.\n\nlinelist_raw &lt;- import(\"linelist_raw.xlsx\", skip = 1)  # does not import header row\n\nUnfortunately skip = only accepts one integer value, not a range (e.g. “2:10” does not work). To skip import of specific rows that are not consecutive from the top, consider importing multiple times and using bind_rows() from dplyr. See the example below of skipping only row 2.\n\n\nManage a second header row\nSometimes, your data may have a second row, for example if it is a “data dictionary” row as shown below. This situation can be problematic because it can result in all columns being imported as class “character”.\nBelow is an example of this kind of dataset (with the first row being the data dictionary).\n\n\n\n\n\n\n\nRemove the second header row\nTo drop the second header row, you will likely need to import the data twice.\n\nImport the data in order to store the correct column names\n\nImport the data again, skipping the first two rows (header and second rows)\n\nBind the correct names onto the reduced dataframe\n\nThe exact argument used to bind the correct column names depends on the type of data file (.csv, .tsv, .xlsx, etc.). This is because rio is using a different function for the different file types (see table above).\nFor Excel files: (col_names =)\n\n# import first time; store the column names\nlinelist_raw_names &lt;- import(\"linelist_raw.xlsx\") %&gt;% names()  # save true column names\n\n# import second time; skip row 2, and assign column names to argument col_names =\nlinelist_raw &lt;- import(\"linelist_raw.xlsx\",\n                       skip = 2,\n                       col_names = linelist_raw_names\n                       ) \n\nFor CSV files: (col.names =)\n\n# import first time; sotre column names\nlinelist_raw_names &lt;- import(\"linelist_raw.csv\") %&gt;% names() # save true column names\n\n# note argument for csv files is 'col.names = '\nlinelist_raw &lt;- import(\"linelist_raw.csv\",\n                       skip = 2,\n                       col.names = linelist_raw_names\n                       ) \n\nBackup option - changing column names as a separate command\n\n# assign/overwrite headers using the base 'colnames()' function\ncolnames(linelist_raw) &lt;- linelist_raw_names\n\n\n\nMake a data dictionary\nBonus! If you do have a second row that is a data dictionary, you can easily create a proper data dictionary from it. This tip is adapted from this post.\n\ndict &lt;- linelist_2headers %&gt;%             # begin: linelist with dictionary as first row\n  head(1) %&gt;%                             # keep only column names and first dictionary row                \n  pivot_longer(cols = everything(),       # pivot all columns to long format\n               names_to = \"Column\",       # assign new column names\n               values_to = \"Description\")\n\n\n\n\n\n\n\n\n\nCombine the two header rows\nIn some cases when your raw dataset has two header rows (or more specifically, the 2nd row of data is a secondary header), you may want to “combine” them or add the values in the second header row into the first header row.\nThe command below will define the data frame’s column names as the combination (pasting together) of the first (true) headers with the value immediately underneath (in the first row).\n\nnames(my_data) &lt;- paste(names(my_data), my_data[1, ], sep = \"_\")\n\n\n\n\n\nGoogle sheets\nYou can import data from an online Google spreadsheet with the googlesheet4 package and by authenticating your access to the spreadsheet.\n\npacman::p_load(\"googlesheets4\")\n\nBelow, a demo Google sheet is imported and saved. This command may prompt confirmation of authentification of your Google account. Follow prompts and pop-ups in your internet browser to grant Tidyverse API packages permissions to edit, create, and delete your spreadsheets in Google Drive.\nThe sheet below is “viewable for anyone with the link” and you can try to import it.\n\nGsheets_demo &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY/edit#gid=0\")\n\nThe sheet can also be imported using only the sheet ID, a shorter part of the URL:\n\nGsheets_demo &lt;- read_sheet(\"1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY\")\n\nAnother package, googledrive offers useful functions for writing, editing, and deleting Google sheets. For example, using the gs4_create() and sheet_write() functions found in this package.\nHere are some other helpful online tutorials:\nbasic Google sheets importing tutorial\nmore detailed tutorial\ninteraction between the googlesheets4 and tidyverse",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#multiple-files---import-export-split-combine",
    "href": "new_pages/importing.html#multiple-files---import-export-split-combine",
    "title": "7  Import and export",
    "section": "7.6 Multiple files - import, export, split, combine",
    "text": "7.6 Multiple files - import, export, split, combine\nSee the page on Iteration, loops, and lists for examples of how to import and combine multiple files, or multiple Excel workbook files. That page also has examples on how to split a data frame into parts and export each one separately, or as named sheets in an Excel workbook.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#import_github",
    "href": "new_pages/importing.html#import_github",
    "title": "7  Import and export",
    "section": "7.7 Import from Github",
    "text": "7.7 Import from Github\nImporting data directly from Github into R can be very easy or can require a few steps - depending on the file type. Below are some approaches:\n\nCSV files\nIt can be easy to import a .csv file directly from Github into R with an R command.\n\nGo to the Github repo, locate the file of interest, and click on it\n\nClick on the “Raw” button (you will then see the “raw” csv data, as shown below)\n\nCopy the URL (web address)\n\nPlace the URL in quotes within the import() R command\n\n\n\n\n\n\n\n\n\n\n\n\nXLSX files\nYou may not be able to view the “Raw” data for some files (e.g. .xlsx, .rds, .nwk, .shp)\n\nGo to the Github repo, locate the file of interest, and click on it\n\nClick the “Download” button, as shown below\n\nSave the file on your computer, and import it into R\n\n\n\n\n\n\n\n\n\n\n\n\nShapefiles\nShapefiles have many sub-component files, each with a different file extention. One file will have the “.shp” extension, but others may have “.dbf”, “.prj”, etc. To download a shapefile from Github, you will need to download each of the sub-component files individually, and save them in the same folder on your computer. In Github, click on each file individually and download them by clicking on the “Download” button.\nOnce saved to your computer you can import the shapefile as shown in the GIS basics page using st_read() from the sf package. You only need to provide the filepath and name of the “.shp” file - as long as the other related files are within the same folder on your computer.\nBelow, you can see how the shapefile “sle_adm3” consists of many files - each of which must be downloaded from Github.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#manual-data-entry",
    "href": "new_pages/importing.html#manual-data-entry",
    "title": "7  Import and export",
    "section": "7.8 Manual data entry",
    "text": "7.8 Manual data entry\n\nEntry by rows\nUse the tribble function from the tibble package from the tidyverse (online tibble reference).\nNote how column headers start with a tilde (~). Also note that each column must contain only one class of data (character, numeric, etc.). You can use tabs, spacing, and new rows to make the data entry more intuitive and readable. Spaces do not matter between values, but each row is represented by a new line of code. For example:\n\n# create the dataset manually by row\nmanual_entry_rows &lt;- tibble::tribble(\n  ~colA, ~colB,\n  \"a\",   1,\n  \"b\",   2,\n  \"c\",   3\n  )\n\nAnd now we display the new dataset:\n\n\n\n\n\n\n\n\nEntry by columns\nSince a data frame consists of vectors (vertical columns), the base approach to manual dataframe creation in R expects you to define each column and then bind them together. This can be counter-intuitive in epidemiology, as we usually think about our data in rows (as above).\n\n# define each vector (vertical column) separately, each with its own name\nPatientID &lt;- c(235, 452, 778, 111)\nTreatment &lt;- c(\"Yes\", \"No\", \"Yes\", \"Yes\")\nDeath     &lt;- c(1, 0, 1, 0)\n\nCAUTION: All vectors must be the same length (same number of values).\nThe vectors can then be bound together using the function data.frame():\n\n# combine the columns into a data frame, by referencing the vector names\nmanual_entry_cols &lt;- data.frame(PatientID, Treatment, Death)\n\nAnd now we display the new dataset:\n\n\n\n\n\n\n\n\nPasting from clipboard\nIf you copy data from elsewhere and have it on your clipboard, you can try one of the two ways below:\nFrom the clipr package, you can use read_clip_tbl() to import as a data frame, or just just read_clip() to import as a character vector. In both cases, leave the parentheses empty.\n\nlinelist &lt;- clipr::read_clip_tbl()  # imports current clipboard as data frame\nlinelist &lt;- clipr::read_clip()      # imports as character vector\n\nYou can also easily export to your system’s clipboard with clipr. See the section below on Export.\nAlternatively, you can use the the read.table() function from base R with file = \"clipboard\") to import as a data frame:\n\ndf_from_clipboard &lt;- read.table(\n  file = \"clipboard\",  # specify this as \"clipboard\"\n  sep = \"t\",           # separator could be tab, or commas, etc.\n  header=TRUE)         # if there is a header row",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#import-most-recent-file",
    "href": "new_pages/importing.html#import-most-recent-file",
    "title": "7  Import and export",
    "section": "7.9 Import most recent file",
    "text": "7.9 Import most recent file\nOften you may receive daily updates to your datasets. In this case you will want to write code that imports the most recent file. Below we present two ways to approach this:\n\nSelecting the file based on the date in the file name\n\nSelecting the file based on file metadata (last modification)\n\n\nDates in file name\nThis approach depends on three premises:\n\nYou trust the dates in the file names\n\nThe dates are numeric and appear in generally the same format (e.g. year then month then day)\n\nThere are no other numbers in the file name\n\nWe will explain each step, and then show you them combined at the end.\nFirst, use dir() from base R to extract just the file names for each file in the folder of interest. See the page on Directory interactions for more details about dir(). In this example, the folder of interest is the folder “linelists” within the folder “example” within “data” within the R project.\n\nlinelist_filenames &lt;- dir(here(\"data\", \"example\", \"linelists\")) # get file names from folder\nlinelist_filenames                                              # print\n\n[1] \"20201007linelist.csv\"          \"case_linelist_2020-10-02.csv\" \n[3] \"case_linelist_2020-10-03.csv\"  \"case_linelist_2020-10-04.csv\" \n[5] \"case_linelist_2020-10-05.csv\"  \"case_linelist_2020-10-08.xlsx\"\n[7] \"case_linelist20201006.csv\"    \n\n\nOnce you have this vector of names, you can extract the dates from them by applying str_extract() from stringr using this regular expression. It extracts any numbers in the file name (including any other characters in the middle such as dashes or slashes). You can read more about stringr in the Strings and characters page.\n\nlinelist_dates_raw &lt;- stringr::str_extract(linelist_filenames, \"[0-9].*[0-9]\") # extract numbers and any characters in between\nlinelist_dates_raw  # print\n\n[1] \"20201007\"   \"2020-10-02\" \"2020-10-03\" \"2020-10-04\" \"2020-10-05\"\n[6] \"2020-10-08\" \"20201006\"  \n\n\nAssuming the dates are written in generally the same date format (e.g. Year then Month then Day) and the years are 4-digits, you can use lubridate’s flexible conversion functions (ymd(), dmy(), or mdy()) to convert them to dates. For these functions, the dashes, spaces, or slashes do not matter, only the order of the numbers. Read more in the Working with dates page.\n\nlinelist_dates_clean &lt;- lubridate::ymd(linelist_dates_raw)\nlinelist_dates_clean\n\n[1] \"2020-10-07\" \"2020-10-02\" \"2020-10-03\" \"2020-10-04\" \"2020-10-05\"\n[6] \"2020-10-08\" \"2020-10-06\"\n\n\nThe base R function which.max() can then be used to return the index position (e.g. 1st, 2nd, 3rd, …) of the maximum date value. The latest file is correctly identified as the 6th file - “case_linelist_2020-10-08.xlsx”.\n\nindex_latest_file &lt;- which.max(linelist_dates_clean)\nindex_latest_file\n\n[1] 6\n\n\nIf we condense all these commands, the complete code could look like below. Note that the . in the last line is a placeholder for the piped object at that point in the pipe sequence. At that point the value is simply the number 6. This is placed in double brackets to extract the 6th element of the vector of file names produced by dir().\n\n# load packages\npacman::p_load(\n  tidyverse,         # data management\n  stringr,           # work with strings/characters\n  lubridate,         # work with dates\n  rio,               # import / export\n  here,              # relative file paths\n  fs)                # directory interactions\n\n# extract the file name of latest file\nlatest_file &lt;- dir(here(\"data\", \"example\", \"linelists\")) %&gt;%  # file names from \"linelists\" sub-folder          \n  str_extract(\"[0-9].*[0-9]\") %&gt;%                  # pull out dates (numbers)\n  ymd() %&gt;%                                        # convert numbers to dates (assuming year-month-day format)\n  which.max() %&gt;%                                  # get index of max date (latest file)\n  dir(here(\"data\", \"example\", \"linelists\"))[[.]]              # return the filename of latest linelist\n\nlatest_file  # print name of latest file\n\n[1] \"case_linelist_2020-10-08.xlsx\"\n\n\nYou can now use this name to finish the relative file path, with here():\n\nhere(\"data\", \"example\", \"linelists\", latest_file) \n\nAnd you can now import the latest file:\n\n# import\nimport(here(\"data\", \"example\", \"linelists\", latest_file)) # import \n\n\n\nUse the file info\nIf your files do not have dates in their names (or you do not trust those dates), you can try to extract the last modification date from the file metadata. Use functions from the package fs to examine the metadata information for each file, which includes the last modification time and the file path.\nBelow, we provide the folder of interest to fs’s dir_info(). In this case, the folder of interest is in the R project in the folder “data”, the sub-folder “example”, and its sub-folder “linelists”. The result is a data frame with one line per file and columns for modification_time, path, etc. You can see a visual example of this in the page on Directory interactions.\nWe can sort this data frame of files by the column modification_time, and then keep only the top/latest row (file) with base R’s head(). Then we can extract the file path of this latest file only with the dplyr function pull() on the column path. Finally we can pass this file path to import(). The imported file is saved as latest_file.\n\nlatest_file &lt;- dir_info(here(\"data\", \"example\", \"linelists\")) %&gt;%  # collect file info on all files in directory\n  arrange(desc(modification_time)) %&gt;%      # sort by modification time\n  head(1) %&gt;%                               # keep only the top (latest) file\n  pull(path) %&gt;%                            # extract only the file path\n  import()                                  # import the file",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#import_api",
    "href": "new_pages/importing.html#import_api",
    "title": "7  Import and export",
    "section": "7.10 APIs",
    "text": "7.10 APIs\nAn “Automated Programming Interface” (API) can be used to directly request data from a website. APIs are a set of rules that allow one software application to interact with another. The client (you) sends a “request” and receives a “response” containing content. The R packages httr and jsonlite can facilitate this process.\nEach API-enabled website will have its own documentation and specifics to become familiar with. Some sites are publicly available and can be accessed by anyone. Others, such as platforms with user IDs and credentials, require authentication to access their data.\nNeedless to say, it is necessary to have an internet connection to import data via API. We will briefly give examples of use of APIs to import data, and link you to further resources.\nNote: recall that data may be posted* on a website without an API, which may be easier to retrieve. For example a posted CSV file may be accessible simply by providing the site URL to import() as described in the section on importing from Github.*\n\nHTTP request\nThe API exchange is most commonly done through an HTTP request. HTTP is Hypertext Transfer Protocol, and is the underlying format of a request/response between a client and a server. The exact input and output may vary depending on the type of API but the process is the same - a “Request” (often HTTP Request) from the user, often containing a query, followed by a “Response”, containing status information about the request and possibly the requested content.\nHere are a few components of an HTTP request:\n\nThe URL of the API endpoint\n\nThe “Method” (or “Verb”)\n\nHeaders\n\nBody\n\nThe HTTP request “method” is the action your want to perform. The two most common HTTP methods are GET and POST but others could include PUT, DELETE, PATCH, etc. When importing data into R it is most likely that you will use GET.\nAfter your request, your computer will receive a “response” in a format similar to what you sent, including URL, HTTP status (Status 200 is what you want!), file type, size, and the desired content. You will then need to parse this response and turn it into a workable data frame within your R environment.\n\n\nPackages\nThe httr package works well for handling HTTP requests in R. It requires little prior knowledge of Web APIs and can be used by people less familiar with software development terminology. In addition, if the HTTP response is .json, you can use jsonlite to parse the response.\n\n# load packages\npacman::p_load(httr, jsonlite, tidyverse)\n\n\n\nPublicly-available data\nBelow is an example of an HTTP request, borrowed from a tutorial from the Trafford Data Lab. This site has several other resources to learn and API exercises.\nScenario: We want to import a list of fast food outlets in the city of Trafford, UK. The data can be accessed from the API of the Food Standards Agency, which provides food hygiene rating data for the United Kingdom.\nHere are the parameters for our request:\n\nHTTP verb: GET\n\nAPI endpoint URL: http://api.ratings.food.gov.uk/Establishments\n\nSelected parameters: name, address, longitude, latitude, businessTypeId, ratingKey, localAuthorityId\n\nHeaders: “x-api-version”, 2\n\nData format(s): JSON, XML\n\nDocumentation: http://api.ratings.food.gov.uk/help\n\nThe R code would be as follows:\n\n# prepare the request\npath &lt;- \"http://api.ratings.food.gov.uk/Establishments\"\nrequest &lt;- GET(url = path,\n             query = list(\n               localAuthorityId = 188,\n               BusinessTypeId = 7844,\n               pageNumber = 1,\n               pageSize = 5000),\n             add_headers(\"x-api-version\" = \"2\"))\n\n# check for any server error (\"200\" is good!)\nrequest$status_code\n\n# submit the request, parse the response, and convert to a data frame\nresponse &lt;- content(request, as = \"text\", encoding = \"UTF-8\") %&gt;%\n  fromJSON(flatten = TRUE) %&gt;%\n  pluck(\"establishments\") %&gt;%\n  as_tibble()\n\nYou can now clean and use the response data frame, which contains one row per fast food facility.\n\n\nAuthentication required\nSome APIs require authentication - for you to prove who you are, so you can access restricted data. To import these data, you may need to first use a POST method to provide a username, password, or code. This will return an access token, that can be used for subsequent GET method requests to retrieve the desired data.\nBelow is an example of querying data from Go.Data, which is an outbreak investigation tool. Go.Data uses an API for all interactions between the web front-end and smartphone applications used for data collection. Go.Data is used throughout the world. Because outbreak data are sensitive and you should only be able to access data for your outbreak, authentication is required.\nBelow is some sample R code using httr and jsonlite for connecting to the Go.Data API to import data on contact follow-up from your outbreak.\n\n# set credentials for authorization\nurl &lt;- \"https://godatasampleURL.int/\"           # valid Go.Data instance url\nusername &lt;- \"username\"                          # valid Go.Data username \npassword &lt;- \"password\"                          # valid Go,Data password \noutbreak_id &lt;- \"xxxxxx-xxxx-xxxx-xxxx-xxxxxxx\"  # valid Go.Data outbreak ID\n\n# get access token\nurl_request &lt;- paste0(url,\"api/oauth/token?access_token=123\") # define base URL request\n\n# prepare request\nresponse &lt;- POST(\n  url = url_request,  \n  body = list(\n    username = username,    # use saved username/password from above to authorize                               \n    password = password),                                       \n    encode = \"json\")\n\n# execute request and parse response\ncontent &lt;-\n  content(response, as = \"text\") %&gt;%\n  fromJSON(flatten = TRUE) %&gt;%          # flatten nested JSON\n  glimpse()\n\n# Save access token from response\naccess_token &lt;- content$access_token    # save access token to allow subsequent API calls below\n\n# import outbreak contacts\n# Use the access token \nresponse_contacts &lt;- GET(\n  paste0(url,\"api/outbreaks/\",outbreak_id,\"/contacts\"),          # GET request\n  add_headers(\n    Authorization = paste(\"Bearer\", access_token, sep = \" \")))\n\njson_contacts &lt;- content(response_contacts, as = \"text\")         # convert to text JSON\n\ncontacts &lt;- as_tibble(fromJSON(json_contacts, flatten = TRUE))   # flatten JSON to tibble\n\nCAUTION: If you are importing large amounts of data from an API requiring authentication, it may time-out. To avoid this, retrieve access_token again before each API GET request and try using filters or limits in the query. \nTIP: The fromJSON() function in the jsonlite package does not fully un-nest the first time it’s executed, so you will likely still have list items in your resulting tibble. You will need to further un-nest for certain variables; depending on how nested your .json is. To view more info on this, view the documentation for the jsonlite package, such as the flatten() function. \nFor more details, View documentation on LoopBack Explorer, the Contact Tracing page or API tips on Go.Data Github repository\nYou can read more about the httr package here\nThis section was also informed by this tutorial and this tutorial.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#export",
    "href": "new_pages/importing.html#export",
    "title": "7  Import and export",
    "section": "7.11 Export",
    "text": "7.11 Export\n\nWith rio package\nWith rio, you can use the export() function in a very similar way to import(). First give the name of the R object you want to save (e.g. linelist) and then in quotes put the file path where you want to save the file, including the desired file name and file extension. For example:\nThis saves the data frame linelist as an Excel workbook to the working directory/R project root folder:\n\nexport(linelist, \"my_linelist.xlsx\") # will save to working directory\n\nYou could save the same data frame as a csv file by changing the extension. For example, we also save it to a file path constructed with here():\n\nexport(linelist, here(\"data\", \"clean\", \"my_linelist.csv\"))\n\n\n\nTo clipboard\nTo export a data frame to your computer’s “clipboard” (to then paste into another software like Excel, Google Spreadsheets, etc.) you can use write_clip() from the clipr package.\n\n# export the linelist data frame to your system's clipboard\nclipr::write_clip(linelist)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#import_rds",
    "href": "new_pages/importing.html#import_rds",
    "title": "7  Import and export",
    "section": "7.12 RDS files",
    "text": "7.12 RDS files\nAlong with .csv, .xlsx, etc, you can also export/save R data frames as .rds files. This is a file format specific to R, and is very useful if you know you will work with the exported data again in R.\nThe classes of columns are stored, so you don’t have do to cleaning again when it is imported (with an Excel or even a CSV file this can be a headache!). It is also a smaller file, which is useful for export and import if your dataset is large.\nFor example, if you work in an Epidemiology team and need to send files to a GIS team for mapping, and they use R as well, just send them the .rds file! Then all the column classes are retained and they have less work to do.\n\nexport(linelist, here(\"data\", \"clean\", \"my_linelist.rds\"))",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#import_rdata",
    "href": "new_pages/importing.html#import_rdata",
    "title": "7  Import and export",
    "section": "7.13 Rdata files and lists",
    "text": "7.13 Rdata files and lists\n.Rdata files can store multiple R objects - for example multiple data frames, model results, lists, etc. This can be very useful to consolidate or share a lot of your data for a given project.\nIn the below example, multiple R objects are stored within the exported file “my_objects.Rdata”:\n\nrio::export(my_list, my_dataframe, my_vector, \"my_objects.Rdata\")\n\nNote: if you are trying to import a list, use import_list() from rio to import it with the complete original structure and contents.\n\nrio::import_list(\"my_list.Rdata\")",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#saving-plots",
    "href": "new_pages/importing.html#saving-plots",
    "title": "7  Import and export",
    "section": "7.14 Saving plots",
    "text": "7.14 Saving plots\nInstructions on how to save plots, such as those created by ggplot(), are discussed in depth in the ggplot basics page.\nIn brief, run ggsave(\"my_plot_filepath_and_name.png\") after printing your plot. You can either provide a saved plot object to the plot = argument, or only specify the destination file path (with file extension) to save the most recently-displayed plot. You can also control the width =, height =, units =, and dpi =.\nHow to save a network graph, such as a transmission tree, is addressed in the page on Transmission chains.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/importing.html#resources",
    "href": "new_pages/importing.html#resources",
    "title": "7  Import and export",
    "section": "7.15 Resources",
    "text": "7.15 Resources\nThe R Data Import/Export Manual\nR 4 Data Science chapter on data import\nggsave() documentation\nBelow is a table, taken from the rio online vignette. For each type of data it shows: the expected file extension, the package rio uses to import or export the data, and whether this functionality is included in the default installed version of rio.\n\n\n\n\n\n\n\n\n\n\nFormat\nTypical Extension\nImport Package\nExport Package\nInstalled by Default\n\n\n\n\nComma-separated data\n.csv\ndata.table fread()\ndata.table\nYes\n\n\nPipe-separated data\n.psv\ndata.table fread()\ndata.table\nYes\n\n\nTab-separated data\n.tsv\ndata.table fread()\ndata.table\nYes\n\n\nSAS\n.sas7bdat\nhaven\nhaven\nYes\n\n\nSPSS\n.sav\nhaven\nhaven\nYes\n\n\nStata\n.dta\nhaven\nhaven\nYes\n\n\nSAS\nXPORT\n.xpt\nhaven\nhaven\n\n\nSPSS Portable\n.por\nhaven\n\nYes\n\n\nExcel\n.xls\nreadxl\n\nYes\n\n\nExcel\n.xlsx\nreadxl\nopenxlsx\nYes\n\n\nR syntax\n.R\nbase\nbase\nYes\n\n\nSaved R objects\n.RData, .rda\nbase\nbase\nYes\n\n\nSerialized R objects\n.rds\nbase\nbase\nYes\n\n\nEpiinfo\n.rec\nforeign\n\nYes\n\n\nMinitab\n.mtp\nforeign\n\nYes\n\n\nSystat\n.syd\nforeign\n\nYes\n\n\n“XBASE”\ndatabase files\n.dbf\nforeign\nforeign\n\n\nWeka Attribute-Relation File Format\n.arff\nforeign\nforeign\nYes\n\n\nData Interchange Format\n.dif\nutils\n\nYes\n\n\nFortran data\nno recognized extension\nutils\n\nYes\n\n\nFixed-width format data\n.fwf\nutils\nutils\nYes\n\n\ngzip comma-separated data\n.csv.gz\nutils\nutils\nYes\n\n\nCSVY (CSV + YAML metadata header)\n.csvy\ncsvy\ncsvy\nNo\n\n\nEViews\n.wf1\nhexView\n\nNo\n\n\nFeather R/Python interchange format\n.feather\nfeather\nfeather\nNo\n\n\nFast Storage\n.fst\nfst\nfst\nNo\n\n\nJSON\n.json\njsonlite\njsonlite\nNo\n\n\nMatlab\n.mat\nrmatio\nrmatio\nNo\n\n\nOpenDocument Spreadsheet\n.ods\nreadODS\nreadODS\nNo\n\n\nHTML Tables\n.html\nxml2\nxml2\nNo\n\n\nShallow XML documents\n.xml\nxml2\nxml2\nNo\n\n\nYAML\n.yml\nyaml\nyaml\nNo\n\n\nClipboard default is tsv\n\nclipr\nclipr\nNo",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Import and export</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html",
    "href": "new_pages/cleaning.html",
    "title": "8  Cleaning data and core functions",
    "section": "",
    "text": "Core functions\nThis handbook emphasizes use of the functions from the tidyverse family of R packages. The essential R functions demonstrated in this page are listed below.\nMany of these functions belong to the dplyr R package, which provides “verb” functions to solve data manipulation challenges (the name is a reference to a “data frame-plier. dplyr is part of the tidyverse family of R packages (which also includes ggplot2, tidyr, stringr, tibble, purrr, magrittr, and forcats among others).\nIf you want to see how these functions compare to Stata or SAS commands, see the page on Transition to R.\nYou may encounter an alternative data management framework from the data.table R package with operators like := and frequent use of brackets [ ]. This approach and syntax is briefly explained in the Data Table page.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#cleaning-pipeline",
    "href": "new_pages/cleaning.html#cleaning-pipeline",
    "title": "8  Cleaning data and core functions",
    "section": "8.1 Cleaning pipeline",
    "text": "8.1 Cleaning pipeline\nThis page proceeds through typical cleaning steps, adding them sequentially to a cleaning pipe chain.\nIn epidemiological analysis and data processing, cleaning steps are often performed sequentially, linked together. In R, this often manifests as a cleaning “pipeline”, where the raw dataset is passed or “piped” from one cleaning step to another.\nSuch chains utilize dplyr “verb” functions and the magrittr pipe operator %&gt;%. This pipe begins with the “raw” data (“linelist_raw.xlsx”) and ends with a “clean” R data frame (linelist) that can be used, saved, exported, etc.\nIn a cleaning pipeline the order of the steps is important. Cleaning steps might include:\n\nImporting of data.\n\nColumn names cleaned or changed.\n\nDe-duplication.\n\nColumn creation and transformation (e.g. re-coding or standardising values).\n\nRows filtered or added.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#load-packages",
    "href": "new_pages/cleaning.html#load-packages",
    "title": "8  Cleaning data and core functions",
    "section": "8.2 Load packages",
    "text": "8.2 Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,        # importing data  \n  here,       # relative file pathways  \n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  matchmaker, # dictionary-based cleaning\n  epikit,     # age_categories() function\n  tidyverse   # data management and visualization\n)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#import-data",
    "href": "new_pages/cleaning.html#import-data",
    "title": "8  Cleaning data and core functions",
    "section": "8.3 Import data",
    "text": "8.3 Import data\n\nImport\nHere we import the “raw” case linelist Excel file using the import() function from the package rio. The rio package flexibly handles many types of files (e.g. .xlsx, .csv, .tsv, .rds. See the page on Import and export for more information and tips on unusual situations (e.g. skipping rows, setting missing values, importing Google sheets, etc).\nIf you want to follow along, click to download the “raw” linelist (as .xlsx file).\nIf your dataset is large and takes a long time to import, it can be useful to have the import command be separate from the pipe chain and the “raw” saved as a distinct file. This also allows easy comparison between the original and cleaned versions.\nBelow we import the raw Excel file and save it as the data frame linelist_raw. We assume the file is located in your working directory or R project root, and so no sub-folders are specified in the file path.\n\nlinelist_raw &lt;- import(\"linelist_raw.xlsx\")\n\nYou can view the first 50 rows of the the data frame below. Note: the base R function head(n) allow you to view just the first n rows in the R console.\n\n\n\n\n\n\n\n\nReview\nYou can use the function skim() from the package skimr to get an overview of the entire dataframe (see page on Descriptive tables for more info). Columns are summarised by class/type such as character, numeric. Note: “POSIXct” is a type of raw date class (see Working with dates).\n\nskimr::skim(linelist_raw)\n\n\n\n\nData summary\n\n\nName\nlinelist_raw\n\n\nNumber of rows\n6611\n\n\nNumber of columns\n28\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n17\n\n\nnumeric\n8\n\n\nPOSIXct\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncase_id\n137\n0.98\n6\n6\n0\n5888\n0\n\n\ndate onset\n293\n0.96\n10\n10\n0\n580\n0\n\n\noutcome\n1500\n0.77\n5\n7\n0\n2\n0\n\n\ngender\n324\n0.95\n1\n1\n0\n2\n0\n\n\nhospital\n1512\n0.77\n5\n36\n0\n13\n0\n\n\ninfector\n2323\n0.65\n6\n6\n0\n2697\n0\n\n\nsource\n2323\n0.65\n5\n7\n0\n2\n0\n\n\nage\n107\n0.98\n1\n2\n0\n75\n0\n\n\nage_unit\n7\n1.00\n5\n6\n0\n2\n0\n\n\nfever\n258\n0.96\n2\n3\n0\n2\n0\n\n\nchills\n258\n0.96\n2\n3\n0\n2\n0\n\n\ncough\n258\n0.96\n2\n3\n0\n2\n0\n\n\naches\n258\n0.96\n2\n3\n0\n2\n0\n\n\nvomit\n258\n0.96\n2\n3\n0\n2\n0\n\n\ntime_admission\n844\n0.87\n5\n5\n0\n1091\n0\n\n\nmerged_header\n0\n1.00\n1\n1\n0\n1\n0\n\n\n…28\n0\n1.00\n1\n1\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\ngeneration\n7\n1.00\n16.60\n5.71\n0.00\n13.00\n16.00\n20.00\n37.00\n\n\nlon\n7\n1.00\n-13.23\n0.02\n-13.27\n-13.25\n-13.23\n-13.22\n-13.21\n\n\nlat\n7\n1.00\n8.47\n0.01\n8.45\n8.46\n8.47\n8.48\n8.49\n\n\nrow_num\n0\n1.00\n3240.91\n1857.83\n1.00\n1647.50\n3241.00\n4836.50\n6481.00\n\n\nwt_kg\n7\n1.00\n52.69\n18.59\n-11.00\n41.00\n54.00\n66.00\n111.00\n\n\nht_cm\n7\n1.00\n125.25\n49.57\n4.00\n91.00\n130.00\n159.00\n295.00\n\n\nct_blood\n7\n1.00\n21.26\n1.67\n16.00\n20.00\n22.00\n22.00\n26.00\n\n\ntemp\n158\n0.98\n38.60\n0.95\n35.20\n38.30\n38.80\n39.20\n40.80\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ninfection date\n2322\n0.65\n2012-04-09\n2015-04-27\n2014-10-04\n538\n\n\nhosp date\n7\n1.00\n2012-04-20\n2015-04-30\n2014-10-15\n570\n\n\ndate_of_outcome\n1068\n0.84\n2012-05-14\n2015-06-04\n2014-10-26\n575",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#column-names",
    "href": "new_pages/cleaning.html#column-names",
    "title": "8  Cleaning data and core functions",
    "section": "8.4 Column names",
    "text": "8.4 Column names\nIn R, column names are the “header” or “top” value of a column. They are used to refer to columns in the code, and serve as a default label in figures.\nOther statistical software such as SAS and STATA use “labels” that co-exist as longer printed versions of the shorter column names. While R does offer the possibility of adding column labels to the data, this is not emphasized in most practice. To make column names “printer-friendly” for figures, one typically adjusts their display within the plotting commands that create the outputs (e.g. axis or legend titles of a plot, or column headers in a printed table - see the scales section of the ggplot tips page and Tables for presentation pages). If you want to assign column labels in the data, read more online here and here.\nAs R column names are used very often, so they must have “clean” syntax. We suggest the following:\n\nShort names.\nNo spaces (replace with underscores _ ).\nNo unusual characters (&, #, &lt;, &gt;, …).\n\nSimilar style nomenclature (e.g. all date columns named like date_onset, date_report, date_death…).\n\nThe columns names of linelist_raw are printed below using names() from base R. We can see that initially:\n\nSome names contain spaces (e.g. infection date).\n\nDifferent naming patterns are used for dates (date onset vs. infection date).\n\nThere must have been a merged header across the two last columns in the .xlsx. We know this because the name of two merged columns (“merged_header”) was assigned by R to the first column, and the second column was assigned a placeholder name “…28” (as it was then empty and is the 28th column).\n\n\nnames(linelist_raw)\n\n [1] \"case_id\"         \"generation\"      \"infection date\"  \"date onset\"     \n [5] \"hosp date\"       \"date_of_outcome\" \"outcome\"         \"gender\"         \n [9] \"hospital\"        \"lon\"             \"lat\"             \"infector\"       \n[13] \"source\"          \"age\"             \"age_unit\"        \"row_num\"        \n[17] \"wt_kg\"           \"ht_cm\"           \"ct_blood\"        \"fever\"          \n[21] \"chills\"          \"cough\"           \"aches\"           \"vomit\"          \n[25] \"temp\"            \"time_admission\"  \"merged_header\"   \"...28\"          \n\n\nNOTE: To reference a column name that includes spaces, surround the name with back-ticks, for example: linelist$`infection date`. note that on your keyboard, the back-tick (`) is different from the single quotation mark (’).\n\nAutomatic cleaning\nThe function clean_names() from the package janitor standardizes column names and makes them unique by doing the following:\n\nConverts all names to consist of only underscores, numbers, and letters.\n\nAccented characters are transliterated to ASCII (e.g. german o with umlaut becomes “o”, spanish “enye” becomes “n”).\n\nCapitalization preference for the new column names can be specified using the case = argument (“snake” is default, alternatives include “sentence”, “title”, “small_camel”…).\n\nYou can specify specific name replacements by providing a vector to the replace = argument (e.g. replace = c(onset = \"date_of_onset\")).\n\nHere is an online vignette.\n\nBelow, the cleaning pipeline begins by using clean_names() on the raw linelist.\n\n# pipe the raw dataset through the function clean_names(), assign result as \"linelist\"  \nlinelist &lt;- linelist_raw %&gt;% \n  janitor::clean_names()\n\n# see the new column names\nnames(linelist)\n\n [1] \"case_id\"         \"generation\"      \"infection_date\"  \"date_onset\"     \n [5] \"hosp_date\"       \"date_of_outcome\" \"outcome\"         \"gender\"         \n [9] \"hospital\"        \"lon\"             \"lat\"             \"infector\"       \n[13] \"source\"          \"age\"             \"age_unit\"        \"row_num\"        \n[17] \"wt_kg\"           \"ht_cm\"           \"ct_blood\"        \"fever\"          \n[21] \"chills\"          \"cough\"           \"aches\"           \"vomit\"          \n[25] \"temp\"            \"time_admission\"  \"merged_header\"   \"x28\"            \n\n\nNOTE: The last column name “…28” was changed to “x28”.\n\n\nManual name cleaning\nRe-naming columns manually is often necessary, even after the standardization step above. Below, re-naming is performed using the rename() function from the dplyr package, as part of a pipe chain. rename() uses the style NEW = OLD - the new column name is given before the old column name.\nBelow, a re-naming command is added to the cleaning pipeline. Spaces have been added strategically to align code for easier reading.\n\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome)\n\nNow you can see that the columns names have been changed:\n\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"hospital\"            \n[10] \"lon\"                  \"lat\"                  \"infector\"            \n[13] \"source\"               \"age\"                  \"age_unit\"            \n[16] \"row_num\"              \"wt_kg\"                \"ht_cm\"               \n[19] \"ct_blood\"             \"fever\"                \"chills\"              \n[22] \"cough\"                \"aches\"                \"vomit\"               \n[25] \"temp\"                 \"time_admission\"       \"merged_header\"       \n[28] \"x28\"                 \n\n\n\nRename by column position\nYou can also rename by column position, instead of column name, for example:\n\nrename(newNameForFirstColumn  = 1,\n       newNameForSecondColumn = 2)\n\n\n\nRename via select() and summarise()\nAs a shortcut, you can also rename columns within the dplyr select() and summarise() functions. select() is used to keep only certain columns (and is covered later in this page). summarise() is covered in the Grouping data and Descriptive tables pages. These functions also uses the format new_name = old_name. Here is an example:\n\nlinelist_raw %&gt;% \n  select(# NEW name             # OLD name\n         date_infection       = `infection date`,    # rename and KEEP ONLY these columns\n         date_hospitalisation = `hosp date`)\n\n\n\n\nOther challenges\n\nEmpty Excel column names\nR cannot have dataset columns that do not have column names (headers). So, if you import an Excel dataset with data but no column headers, R will fill-in the headers with names like “…1” or “…2”. The number represents the column number (e.g. if the 4th column in the dataset has no header, then R will name it “…4”).\nYou can clean these names manually by referencing their position number (see example above), or their assigned name (linelist_raw$...1).\n\n\nMerged Excel column names and cells\nMerged cells in an Excel file are a common occurrence when receiving data. As explained in Transition to R, merged cells can be nice for human reading of data, but are not “tidy data” and cause many problems for machine reading of data. R cannot accommodate merged cells.\nRemind people doing data entry that human-readable data is not the same as machine-readable data. Strive to train users about the principles of tidy data. If at all possible, try to change procedures so that data arrive in a tidy format without merged cells.\n\nEach variable must have its own column.\n\nEach observation must have its own row.\n\nEach value must have its own cell.\n\nWhen using rio’s import() function, the value in a merged cell will be assigned to the first cell and subsequent cells will be empty.\nOne solution to deal with merged cells is to import the data with the function readWorkbook() from the package openxlsx. Set the argument fillMergedCells = TRUE. This gives the value in a merged cell to all cells within the merge range.\n\nlinelist_raw &lt;- openxlsx::readWorkbook(\"linelist_raw.xlsx\", fillMergedCells = TRUE)\n\nDANGER: If column names are merged with readWorkbook(), you will end up with duplicate column names, which you will need to fix manually - R does not work well with duplicate column names! You can re-name them by referencing their position (e.g. column 5), as explained in the section on manual column name cleaning.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#select-or-re-order-columns",
    "href": "new_pages/cleaning.html#select-or-re-order-columns",
    "title": "8  Cleaning data and core functions",
    "section": "8.5 Select or re-order columns",
    "text": "8.5 Select or re-order columns\nUse select() from dplyr to select the columns you want to retain, and to specify their order in the data frame.\nCAUTION: In the examples below, the linelist data frame is modified with select() and displayed, but not saved. This is for demonstration purposes. The modified column names are printed by piping the data frame to names().\nHere are ALL the column names in the linelist at this point in the cleaning pipe chain:\n\nnames(linelist)\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"hospital\"            \n[10] \"lon\"                  \"lat\"                  \"infector\"            \n[13] \"source\"               \"age\"                  \"age_unit\"            \n[16] \"row_num\"              \"wt_kg\"                \"ht_cm\"               \n[19] \"ct_blood\"             \"fever\"                \"chills\"              \n[22] \"cough\"                \"aches\"                \"vomit\"               \n[25] \"temp\"                 \"time_admission\"       \"merged_header\"       \n[28] \"x28\"                 \n\n\n\nKeep columns\nSelect only the columns you want to remain\nPut their names in the select() command, with no quotation marks. They will appear in the data frame in the order you provide. Note that if you include a column that does not exist, R will return an error (see use of any_of() below if you want no error in this situation).\n\n# linelist dataset is piped through select() command, and names() prints just the column names\nlinelist %&gt;% \n  select(case_id, date_onset, date_hospitalisation, fever) %&gt;% \n  names()  # display the column names\n\n[1] \"case_id\"              \"date_onset\"           \"date_hospitalisation\"\n[4] \"fever\"               \n\n\n\n\n“tidyselect” helper functions\nThese helper functions exist to make it easy to specify columns to keep, discard, or transform. They are from the package tidyselect, which is included in tidyverse and underlies how columns are selected in dplyr functions.\nFor example, if you want to re-order the columns, everything() is a useful function to signify “all other columns not yet mentioned”. The command below moves columns date_onset and date_hospitalisation to the beginning (left) of the dataset, but keeps all the other columns afterward. Note that everything() is written with empty parentheses:\n\n# move date_onset and date_hospitalisation to beginning\nlinelist %&gt;% \n  select(date_onset, date_hospitalisation, everything()) %&gt;% \n  names()\n\n [1] \"date_onset\"           \"date_hospitalisation\" \"case_id\"             \n [4] \"generation\"           \"date_infection\"       \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"hospital\"            \n[10] \"lon\"                  \"lat\"                  \"infector\"            \n[13] \"source\"               \"age\"                  \"age_unit\"            \n[16] \"row_num\"              \"wt_kg\"                \"ht_cm\"               \n[19] \"ct_blood\"             \"fever\"                \"chills\"              \n[22] \"cough\"                \"aches\"                \"vomit\"               \n[25] \"temp\"                 \"time_admission\"       \"merged_header\"       \n[28] \"x28\"                 \n\n\nHere are other “tidyselect” helper functions that also work within dplyr functions like select(), across(), and summarise():\n\neverything() - all other columns not mentioned\n\nlast_col() - the last column\n\nwhere() - applies a function to all columns and selects those which are TRUE\n\ncontains() - columns containing a character string\n\nexample: select(contains(\"time\"))\n\n\nstarts_with() - matches to a specified prefix\n\nexample: select(starts_with(\"date_\"))\n\n\nends_with() - matches to a specified suffix\n\nexample: select(ends_with(\"_post\"))\n\n\nmatches() - to apply a regular expression (regex)\n\nexample: select(matches(\"[pt]al\"))\n\n\nnum_range() - a numerical range like x01, x02, x03\n\nany_of() - matches IF column exists but returns no error if it is not found\n\nexample: select(any_of(date_onset, date_death, cardiac_arrest))\n\n\nIn addition, use normal operators such as c() to list several columns, : for consecutive columns, ! for opposite, & for AND, and | for OR.\nUse where() to specify logical criteria for columns. If providing a function inside where(), do not include the function’s empty parentheses. The command below selects columns that are class Numeric.\n\n# select columns that are class Numeric\nlinelist %&gt;% \n  select(where(is.numeric)) %&gt;% \n  names()\n\n[1] \"generation\" \"lon\"        \"lat\"        \"row_num\"    \"wt_kg\"     \n[6] \"ht_cm\"      \"ct_blood\"   \"temp\"      \n\n\nUse contains() to select only columns in which the column name contains a specified character string. ends_with() and starts_with() provide more nuance.\n\n# select columns containing certain characters\nlinelist %&gt;% \n  select(contains(\"date\")) %&gt;% \n  names()\n\n[1] \"date_infection\"       \"date_onset\"           \"date_hospitalisation\"\n[4] \"date_outcome\"        \n\n\nThe function matches() works similarly to contains() but can be provided a regular expression (see page on Characters and strings), such as multiple strings separated by OR bars within the parentheses:\n\n# searched for multiple character matches\nlinelist %&gt;% \n  select(matches(\"onset|hosp|fev\")) %&gt;%   # note the OR symbol \"|\"\n  names()\n\n[1] \"date_onset\"           \"date_hospitalisation\" \"hospital\"            \n[4] \"fever\"               \n\n\nCAUTION: If a column name that you specifically provide does not exist in the data, it can return an error and stop your code. Consider using any_of() to cite columns that may or may not exist, especially useful in negative (remove) selections.\nOnly one of these columns exists, but no error is produced and the code continues without stopping your cleaning chain.\n\nlinelist %&gt;% \n  select(any_of(c(\"date_onset\", \"village_origin\", \"village_detection\", \"village_residence\", \"village_travel\"))) %&gt;% \n  names()\n\n[1] \"date_onset\"\n\n\n\n\nRemove columns\nIndicate which columns to remove by placing a minus symbol “-” in front of the column name (e.g. select(-outcome)), or a vector of column names (as below). All other columns will be retained.\n\nlinelist %&gt;% \n  select(-c(date_onset, fever:vomit)) %&gt;% # remove date_onset and all columns from fever to vomit\n  names()\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_hospitalisation\" \"date_outcome\"         \"outcome\"             \n [7] \"gender\"               \"hospital\"             \"lon\"                 \n[10] \"lat\"                  \"infector\"             \"source\"              \n[13] \"age\"                  \"age_unit\"             \"row_num\"             \n[16] \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \n[19] \"temp\"                 \"time_admission\"       \"merged_header\"       \n[22] \"x28\"                 \n\n\nYou can also remove a column using base R syntax, by defining it as NULL. For example:\n\nlinelist$date_onset &lt;- NULL   # deletes column with base R syntax \n\n\n\nStandalone\nselect() can also be used as an independent command (not in a pipe chain). In this case, the first argument is the original dataframe to be operated upon.\n\n# Create a new linelist with id and age-related columns\nlinelist_age &lt;- select(linelist, case_id, contains(\"age\"))\n\n# display the column names\nnames(linelist_age)\n\n[1] \"case_id\"  \"age\"      \"age_unit\"\n\n\n\nAdd to the pipe chain\nIn the linelist_raw, there are a few columns we do not need: row_num, merged_header, and x28. We remove them with a select() command in the cleaning pipe chain:\n\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %&gt;% \n    \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    #####################################################\n\n    # remove column\n    select(-c(row_num, merged_header, x28))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#deduplication",
    "href": "new_pages/cleaning.html#deduplication",
    "title": "8  Cleaning data and core functions",
    "section": "8.6 Deduplication",
    "text": "8.6 Deduplication\nSee the handbook page on De-duplication for extensive options on how to de-duplicate data. Only a very simple row de-duplication example is presented here.\nThe package dplyr offers the distinct() function. This function examines every row and reduce the data frame to only the unique rows. That is, it removes rows that are 100% duplicates.\nWhen evaluating duplicate rows, it takes into account a range of columns - by default it considers all columns. As shown in the de-duplication page, you can adjust this column range so that the uniqueness of rows is only evaluated in regards to certain columns.\nIn this simple example, we just add the empty command distinct() to the pipe chain. This ensures there are no rows that are 100% duplicates of other rows (evaluated across all columns).\nWe begin with nrow(linelist) rows in linelist.\n\nlinelist &lt;- linelist %&gt;% \n  distinct()\n\nAfter de-duplication there are nrow(linelist) rows. Any removed rows would have been 100% duplicates of other rows.\nBelow, the distinct() command is added to the cleaning pipe chain:\n\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %&gt;% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %&gt;% \n  \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    #####################################################\n    \n    # de-duplicate\n    distinct()",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#column-creation-and-transformation",
    "href": "new_pages/cleaning.html#column-creation-and-transformation",
    "title": "8  Cleaning data and core functions",
    "section": "8.7 Column creation and transformation",
    "text": "8.7 Column creation and transformation\nWe recommend using the dplyr function mutate() to add a new column, or to modify an existing one.\nBelow is an example of creating a new column with mutate(). The syntax is: mutate(new_column_name = value or transformation)\nIn Stata, this is similar to the command generate, but R’s mutate() can also be used to modify an existing column.\n\nNew columns\nThe most basic mutate() command to create a new column might look like this. It creates a new column new_col where the value in every row is 10.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(new_col = 10)\n\nYou can also reference values in other columns, to perform calculations. Below, a new column bmi is created to hold the Body Mass Index (BMI) for each case - as calculated using the formula BMI = kg/m^2, using column ht_cm and column wt_kg.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(bmi = wt_kg / (ht_cm/100)^2)\n\nIf creating multiple new columns, separate each with a comma and new line. Below are examples of new columns, including ones that consist of values from other columns combined using str_glue() from the stringr package (see page on Characters and strings.\n\nnew_col_demo &lt;- linelist %&gt;%                       \n  mutate(\n    new_var_dup    = case_id,             # new column = duplicate/copy another existing column\n    new_var_static = 7,                   # new column = all values the same\n    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables\n    new_var_paste  = stringr::str_glue(\"{hospital} on ({date_hospitalisation})\") # new column = pasting together values from other columns\n    ) %&gt;% \n  select(case_id, hospital, date_hospitalisation, contains(\"new\"))        # show only new columns, for demonstration purposes\n\nReview the new columns. For demonstration purposes, only the new columns and the columns used to create them are shown:\n\n\n\n\n\n\nTIP: A variation on mutate() is the function transmute(). This function adds a new column just like mutate(), but also drops/removes all other columns that you do not mention within its parentheses.\n\n# HIDDEN FROM READER\n# removes new demo columns created above\n# linelist &lt;- linelist %&gt;% \n#   select(-contains(\"new_var\"))\n\n\n\nConvert column class\nColumns containing values that are dates, numbers, or logical values (TRUE/FALSE) will only behave as expected if they are correctly classified. There is a difference between “2” of class character and 2 of class numeric!\nThere are ways to set column class during the import commands, but this is often cumbersome. See the R Basics section on object classes to learn more about converting the class of objects and columns.\nFirst, let’s run some checks on important columns to see if they are the correct class. We also saw this in the beginning when we ran skim().\nCurrently, the class of the age column is character. To perform quantitative analyses, we need these numbers to be recognized as numeric!\n\nclass(linelist$age)\n\n[1] \"character\"\n\n\nThe class of the date_onset column is also character! To perform analyses, these dates must be recognized as dates!\n\nclass(linelist$date_onset)\n\n[1] \"character\"\n\n\nTo resolve this, use the ability of mutate() to re-define a column with a transformation. We define the column as itself, but converted to a different class. Here is a basic example, converting or ensuring that the column age is class Numeric:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(age = as.numeric(age))\n\nIn a similar way, you can use as.character() and as.logical(). To convert to class Factor, you can use factor() from base R or as_factor() from forcats. Read more about this in the Factors page.\nYou must be careful when converting to class Date. Several methods are explained on the page Working with dates. Typically, the raw date values must all be in the same format for conversion to work correctly (e.g “MM/DD/YYYY”, or “DD MM YYYY”). After converting to class Date, check your data to confirm that each value was converted correctly.\n\n\nGrouped data\nIf your data frame is already grouped (see page on Grouping data), mutate() may behave differently than if the data frame is not grouped. Any summarizing functions, like mean(), median(), max(), etc. will calculate by group, not by all the rows.\n\n# age normalized to mean of ALL rows\nlinelist %&gt;% \n  mutate(age_norm = age / mean(age, na.rm=T))\n\n# age normalized to mean of hospital group\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  mutate(age_norm = age / mean(age, na.rm=T))\n\nRead more about using mutate () on grouped dataframes in this tidyverse mutate documentation.\n\n\nTransform multiple columns\nOften to write concise code you want to apply the same transformation to multiple columns at once. A transformation can be applied to multiple columns at once using the across() function from the package dplyr (also contained within tidyverse package). across() can be used with any dplyr function, but is commonly used within select(), mutate(), filter(), or summarise(). See how it is applied to summarise() in the page on Descriptive tables.\nSpecify the columns to the argument .cols = and the function(s) to apply to .fns =. Any additional arguments to provide to the .fns function can be included after a comma, still within across().\n\nacross() column selection\nSpecify the columns to the argument .cols =. You can name them individually, or use “tidyselect” helper functions. Specify the function to .fns =. Note that using the function mode demonstrated below, the function is written without its parentheses ( ).\nHere the transformation as.character() is applied to specific columns named within across().\n\nlinelist &lt;- linelist %&gt;% \n  mutate(across(.cols = c(temp, ht_cm, wt_kg), .fns = as.character))\n\nThe “tidyselect” helper functions are available to assist you in specifying columns. They are detailed above in the section on Selecting and re-ordering columns, and they include: everything(), last_col(), where(), starts_with(), ends_with(), contains(), matches(), num_range() and any_of().\nHere is an example of how one would change all columns to character class:\n\n#to change all columns to character class\nlinelist &lt;- linelist %&gt;% \n  mutate(across(.cols = everything(), .fns = as.character))\n\nConvert to character all columns where the name contains the string “date” (note the placement of commas and parentheses):\n\n#to change all columns to character class\nlinelist &lt;- linelist %&gt;% \n  mutate(across(.cols = contains(\"date\"), .fns = as.character))\n\nBelow, an example of mutating the columns that are currently class POSIXct (a raw datetime class that shows timestamps) - in other words, where the function is.POSIXct() evaluates to TRUE. Then we want to apply the function as.Date() to these columns to convert them to a normal class Date.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(across(.cols = where(is.POSIXct), .fns = as.Date))\n\n\nNote that within across() we also use the function where() as is.POSIXct is evaluating to either TRUE or FALSE.\n\nNote that is.POSIXct() is from the package lubridate. Other similar “is” functions like is.character(), is.numeric(), and is.logical() are from base R\n\n\n\nacross() functions\nYou can read the documentation with ?across for details on how to provide functions to across(). A few summary points: there are several ways to specify the function(s) to perform on a column and you can even define your own functions:\n\nYou can provide the function name alone (e.g. mean or as.character)\n\nYou can provide the function in purrr-style (e.g. ~ mean(.x, na.rm = TRUE)) (see this page)\n\nYou can specify multiple functions by providing a list (e.g. list(mean = mean, n_miss = ~ sum(is.na(.x))).\n\nIf you provide multiple functions, multiple transformed columns will be returned per input column, with unique names in the format col_fn. You can adjust how the new columns are named with the .names = argument using glue syntax (see page on Characters and strings) where {.col} and {.fn} are shorthand for the input column and function.\n\n\nHere are a few online resources on using across(): creator Hadley Wickham’s thoughts/rationale\n\n\n\ncoalesce()\nThis dplyr function finds the first non-missing value at each position. It “fills-in” missing values with the first available value in an order you specify.\nHere is an example outside the context of a data frame: Let us say you have two vectors, one containing the patient’s village of detection and another containing the patient’s village of residence. You can use coalesce to pick the first non-missing value for each index:\n\nvillage_detection &lt;- c(\"a\", \"b\", NA,  NA)\nvillage_residence &lt;- c(\"a\", \"c\", \"a\", \"d\")\n\nvillage &lt;- coalesce(village_detection, village_residence)\nvillage    # print\n\n[1] \"a\" \"b\" \"a\" \"d\"\n\n\nThis works the same if you provide data frame columns: for each row, the function will assign the new column value with the first non-missing value in the columns you provided (in order provided).\n\nlinelist &lt;- linelist %&gt;% \n  mutate(village = coalesce(village_detection, village_residence))\n\nThis is an example of a “row-wise” operation. For more complicated row-wise calculations, see the section below on Row-wise calculations.\n\n\nCumulative math\nIf you want a column to reflect the cumulative sum/mean/min/max etc as assessed down the rows of a dataframe to that point, use the following functions:\ncumsum() returns the cumulative sum, as shown below:\n\nsum(c(2,4,15,10))     # returns only one number\n\n[1] 31\n\ncumsum(c(2,4,15,10))  # returns the cumulative sum at each step\n\n[1]  2  6 21 31\n\n\nThis can be used in a dataframe when making a new column. For example, to calculate the cumulative number of cases per day in an outbreak, consider code like this:\n\ncumulative_case_counts &lt;- linelist %&gt;%  # begin with case linelist\n  count(date_onset) %&gt;%                 # count of rows per day, as column 'n'   \n  mutate(cumulative_cases = cumsum(n))  # new column, of the cumulative sum at each row\n\nBelow are the first 10 rows:\n\nhead(cumulative_case_counts, 10)\n\n   date_onset n cumulative_cases\n1  2012-04-15 1                1\n2  2012-05-05 1                2\n3  2012-05-08 1                3\n4  2012-05-31 1                4\n5  2012-06-02 1                5\n6  2012-06-07 1                6\n7  2012-06-14 1                7\n8  2012-06-21 1                8\n9  2012-06-24 1                9\n10 2012-06-25 1               10\n\n\nSee the page on Epidemic curves for how to plot cumulative incidence with the epicurve.\nSee also:\ncumsum(), cummean(), cummin(), cummax(), cumany(), cumall()\n\n\nUsing base R\nTo define a new column (or re-define a column) using base R, write the name of data frame, connected with $, to the new column (or the column to be modified). Use the assignment operator &lt;- to define the new value(s). Remember that when using base R you must specify the data frame name before the column name every time (e.g. dataframe$column). Here is an example of creating the bmi column using base R:\n\nlinelist$bmi = linelist$wt_kg / (linelist$ht_cm / 100) ^ 2)\n\n\n\nAdd to pipe chain\nBelow, a new column is added to the pipe chain and some classes are converted.\n\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %&gt;% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %&gt;% \n  \n    # de-duplicate\n    distinct() %&gt;% \n  \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    ###################################################\n    # add new column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %&gt;% \n  \n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#re-code-values",
    "href": "new_pages/cleaning.html#re-code-values",
    "title": "8  Cleaning data and core functions",
    "section": "8.8 Re-code values",
    "text": "8.8 Re-code values\nHere are a few scenarios where you need to re-code (change) values:\n\nto edit one specific value (e.g. one date with an incorrect year or format)\n\nto reconcile values not spelled the same\nto create a new column of categorical values\n\nto create a new column of numeric categories (e.g. age categories)\n\n\nSpecific values\nTo change values manually you can use the recode() function within the mutate() function.\nImagine there is a nonsensical date in the data (e.g. “2014-14-15”): you could fix the date manually in the raw source data, or, you could write the change into the cleaning pipeline via mutate() and recode(). The latter is more transparent and reproducible to anyone else seeking to understand or repeat your analysis.\n\n# fix incorrect values                   # old value       # new value\nlinelist &lt;- linelist %&gt;% \n  mutate(date_onset = recode(date_onset, \"2014-14-15\" = \"2014-04-15\"))\n\nThe mutate() line above can be read as: “mutate the column date_onset to equal the column date_onset re-coded so that OLD VALUE is changed to NEW VALUE”. Note that this pattern (OLD = NEW) for recode() is the opposite of most R patterns (new = old). The R development community is working on revising this.\nHere is another example re-coding multiple values within one column.\nIn linelist the values in the column “hospital” must be cleaned. There are several different spellings and many missing values.\n\ntable(linelist$hospital, useNA = \"always\")  # print table of all unique values, including missing  \n\n\n                     Central Hopital                     Central Hospital \n                                  11                                  457 \n                          Hospital A                           Hospital B \n                                 290                                  289 \n                    Military Hopital                    Military Hospital \n                                  32                                  798 \n                    Mitylira Hopital                    Mitylira Hospital \n                                   1                                   79 \n                               Other                         Port Hopital \n                                 907                                   48 \n                       Port Hospital St. Mark's Maternity Hospital (SMMH) \n                                1756                                  417 \n  St. Marks Maternity Hopital (SMMH)                                 &lt;NA&gt; \n                                  11                                 1512 \n\n\nThe recode() command below re-defines the column “hospital” as the current column “hospital”, but with the specified recode changes. Don’t forget commas after each!\n\nlinelist &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital,\n                     # for reference: OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      ))\n\nNow we see the spellings in the hospital column have been corrected and consolidated:\n\ntable(linelist$hospital, useNA = \"always\")\n\n\n                    Central Hospital                           Hospital A \n                                 468                                  290 \n                          Hospital B                    Military Hospital \n                                 289                                  910 \n                               Other                        Port Hospital \n                                 907                                 1804 \nSt. Mark's Maternity Hospital (SMMH)                                 &lt;NA&gt; \n                                 428                                 1512 \n\n\nTIP: The number of spaces before and after an equals sign does not matter. Make your code easier to read by aligning the = for all or most rows. Also, consider adding a hashed comment row to clarify for future readers which side is OLD and which side is NEW. \nTIP: Sometimes a blank character value exists in a dataset (not recognized as R’s value for missing - NA). You can reference this value with two quotation marks with no space inbetween (““).\n\n\nBy logic\nBelow we demonstrate how to re-code values in a column using logic and conditions:\n\nUsing replace(), ifelse() and if_else() for simple logic\nUsing case_when() for more complex logic\n\n\n\nSimple logic\n\nreplace()\nTo re-code with simple logical criteria, you can use replace() within mutate(). replace() is a function from base R. Use a logic condition to specify the rows to change . The general syntax is:\nmutate(col_to_change = replace(col_to_change, criteria for rows, new value)).\nOne common situation to use replace() is changing just one value in one row, using an unique row identifier. Below, the gender is changed to “Female” in the row where the column case_id is “2195”.\n\n# Example: change gender of one specific observation to \"Female\" \nlinelist &lt;- linelist %&gt;% \n  mutate(gender = replace(gender, case_id == \"2195\", \"Female\"))\n\nThe equivalent command using base R syntax and indexing brackets [ ] is below. It reads as “Change the value of the dataframe linelist‘s column gender (for the rows where linelist’s column case_id has the value ’2195’) to ‘Female’”.\n\nlinelist$gender[linelist$case_id == \"2195\"] &lt;- \"Female\"\n\n\n\nifelse() and if_else()\nAnother tool for simple logic is ifelse() and its partner if_else(). However, in most cases for re-coding it is more clear to use case_when() (detailed below). These “if else” commands are simplified versions of an if and else programming statement. The general syntax is:\nifelse(condition, value to return if condition evaluates to TRUE, value to return if condition evaluates to FALSE)\nBelow, the column source_known is defined. Its value in a given row is set to “known” if the row’s value in column source is not missing. If the value in source is missing, then the value in source_known is set to “unknown”.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(source_known = ifelse(!is.na(source), \"known\", \"unknown\"))\n\nif_else() is a special version from dplyr that handles dates. Note that if the ‘true’ value is a date, the ‘false’ value must also qualify a date, hence using the special value NA_real_ instead of just NA.\n\n# Create a date of death column, which is NA if patient has not died.\nlinelist &lt;- linelist %&gt;% \n  mutate(date_death = if_else(outcome == \"Death\", date_outcome, NA_real_))\n\nAvoid stringing together many ifelse commands… use case_when() instead! case_when() is much easier to read and you’ll make fewer errors.\n\n\n\n\n\n\n\n\n\nOutside of the context of a data frame, if you want to have an object used in your code switch its value, consider using switch() from base R.\n\n\n\nComplex logic\nUse dplyr’s case_when() if you are re-coding into many new groups, or if you need to use complex logic statements to re-code values. This function evaluates every row in the data frame, assess whether the rows meets specified criteria, and assigns the correct new value.\ncase_when() commands consist of statements that have a Right-Hand Side (RHS) and a Left-Hand Side (LHS) separated by a “tilde” ~. The logic criteria are in the left side and the pursuant values are in the right side of each statement. Statements are separated by commas.\nFor example, here we utilize the columns age and age_unit to create a column age_years:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(age_years = case_when(\n       age_unit == \"years\"  ~ age,       # if age unit is years\n       age_unit == \"months\" ~ age/12,    # if age unit is months, divide age by 12\n       is.na(age_unit)      ~ age))      # if age unit is missing, assume years\n                                         # any other circumstance, assign NA (missing)\n\nAs each row in the data is evaluated, the criteria are applied/evaluated in the order the case_when() statements are written - from top-to-bottom. If the top criteria evaluates to TRUE for a given row, the RHS value is assigned, and the remaining criteria are not even tested for that row in the data. Thus, it is best to write the most specific criteria first, and the most general last. A data row that does not meet any of the RHS criteria will be assigned NA.\nSometimes, you may with to write a final statement that assigns a value for all other scenarios not described by one of the previous lines. To do this, place TRUE on the left-side, which will capture any row that did not meet any of the previous criteria. The right-side of this statement could be assigned a value like “check me!” or missing.\nBelow is another example of case_when() used to create a new column with the patient classification, according to a case definition for confirmed and suspect cases:\n\nlinelist &lt;- linelist %&gt;% \n     mutate(case_status = case_when(\n          \n          # if patient had lab test and it is positive,\n          # then they are marked as a confirmed case \n          ct_blood &lt; 20                   ~ \"Confirmed\",\n          \n          # given that a patient does not have a positive lab result,\n          # if patient has a \"source\" (epidemiological link) AND has fever, \n          # then they are marked as a suspect case\n          !is.na(source) & fever == \"yes\" ~ \"Suspect\",\n          \n          # any other patient not addressed above \n          # is marked for follow up\n          TRUE                            ~ \"To investigate\"))\n\nDANGER: Values on the right-side must all be the same class - either numeric, character, date, logical, etc. To assign missing (NA), you may need to use special variations of NA such as NA_character_, NA_real_ (for numeric or POSIX), and as.Date(NA). Read more in Working with dates.\n\n\nMissing values\nBelow are special functions for handling missing values in the context of data cleaning.\nSee the page on Missing data for more detailed tips on identifying and handling missing values. For example, the is.na() function which logically tests for missingness.\nreplace_na()\nTo change missing values (NA) to a specific value, such as “Missing”, use the dplyr function replace_na() within mutate(). Note that this is used in the same manner as recode above - the name of the variable must be repeated within replace_na().\n\nlinelist &lt;- linelist %&gt;% \n  mutate(hospital = replace_na(hospital, \"Missing\"))\n\nfct_explicit_na()\nThis is a function from the forcats package. The forcats package handles columns of class Factor. Factors are R’s way to handle ordered values such as c(\"First\", \"Second\", \"Third\") or to set the order that values (e.g. hospitals) appear in tables and plots. See the page on Factors.\nIf your data are class Factor and you try to convert NA to “Missing” by using replace_na(), you will get this error: invalid factor level, NA generated. You have tried to add “Missing” as a value, when it was not defined as a possible level of the factor, and it was rejected.\nThe easiest way to solve this is to use the forcats function fct_explicit_na() which converts a column to class factor, and converts NA values to the character “(Missing)”.\n\nlinelist %&gt;% \n  mutate(hospital = fct_explicit_na(hospital))\n\nA slower alternative would be to add the factor level using fct_expand() and then convert the missing values.\nna_if()\nTo convert a specific value to NA, use dplyr’s na_if(). The command below performs the opposite operation of replace_na(). In the example below, any values of “Missing” in the column hospital are converted to NA.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(hospital = na_if(hospital, \"Missing\"))\n\nNote: na_if() cannot be used for logic criteria (e.g. “all values &gt; 99”) - use replace() or case_when() for this:\n\n# Convert temperatures above 40 to NA \nlinelist &lt;- linelist %&gt;% \n  mutate(temp = replace(temp, temp &gt; 40, NA))\n\n# Convert onset dates earlier than 1 Jan 2000 to missing\nlinelist &lt;- linelist %&gt;% \n  mutate(date_onset = replace(date_onset, date_onset &gt; as.Date(\"2000-01-01\"), NA))\n\n\n\nCleaning dictionary\nUse the R package matchmaker and its function match_df() to clean a data frame with a cleaning dictionary.\n\nCreate a cleaning dictionary with 3 columns:\n\nA “from” column (the incorrect value)\n\nA “to” column (the correct value)\n\nA column specifying the column for the changes to be applied (or “.global” to apply to all columns)\n\n\nNote: .global dictionary entries will be overridden by column-specific dictionary entries.\n\n\n\n\n\n\n\n\n\n\nImport the dictionary file into R. This example can be downloaded via instructions on the Download handbook and data page.\n\n\ncleaning_dict &lt;- import(\"cleaning_dict.csv\")\n\n\nPipe the raw linelist to match_df(), specifying to dictionary = the cleaning dictionary data frame. The from = argument should be the name of the dictionary column which contains the “old” values, the by = argument should be dictionary column which contains the corresponding “new” values, and the third column lists the column in which to make the change. Use .global in the by = column to apply a change across all columns. A fourth dictionary column order can be used to specify factor order of new values.\n\nRead more details in the package documentation by running ?match_df. Note this function can take a long time to run for a large dataset.\n\nlinelist &lt;- linelist %&gt;%     # provide or pipe your dataset\n     matchmaker::match_df(\n          dictionary = cleaning_dict,  # name of your dictionary\n          from = \"from\",               # column with values to be replaced (default is col 1)\n          to = \"to\",                   # column with final values (default is col 2)\n          by = \"col\"                   # column with column names (default is col 3)\n  )\n\nNow scroll to the right to see how values have changed - particularly gender (lowercase to uppercase), and all the symptoms columns have been transformed from yes/no to 1/0.\n\n\n\n\n\n\nNote that your column names in the cleaning dictionary must correspond to the names at this point in your cleaning script. See this online reference for the linelist package for more details.\n\nAdd to pipe chain\nBelow, some new columns and column transformations are added to the pipe chain.\n\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %&gt;% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %&gt;% \n  \n    # de-duplicate\n    distinct() %&gt;% \n  \n    # add column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %&gt;%     \n\n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age)) %&gt;% \n    \n    # add column: delay to hospitalisation\n    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %&gt;% \n    \n   # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n   ###################################################\n\n    # clean values of hospital column\n    mutate(hospital = recode(hospital,\n                      # OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      )) %&gt;% \n    \n    mutate(hospital = replace_na(hospital, \"Missing\")) %&gt;% \n\n    # create age_years column (from age and age_unit)\n    mutate(age_years = case_when(\n          age_unit == \"years\" ~ age,\n          age_unit == \"months\" ~ age/12,\n          is.na(age_unit) ~ age,\n          TRUE ~ NA_real_))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#num_cats",
    "href": "new_pages/cleaning.html#num_cats",
    "title": "8  Cleaning data and core functions",
    "section": "8.9 Numeric categories",
    "text": "8.9 Numeric categories\nHere we describe some special approaches for creating categories from numerical columns. Common examples include age categories, groups of lab values, etc. Here we will discuss:\n\nage_categories(), from the epikit package\n\ncut(), from base R\n\ncase_when()\n\nquantile breaks with quantile() and ntile()\n\n\nReview distribution\nFor this example we will create an age_cat column using the age_years column.\n\n#check the class of the linelist variable age\nclass(linelist$age_years)\n\n[1] \"numeric\"\n\n\nFirst, examine the distribution of your data, to make appropriate cut-points. See the page on ggplot basics.\n\n# examine the distribution\nhist(linelist$age_years)\n\n\n\n\n\n\n\n\n\nsummary(linelist$age_years, na.rm=T)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.04   23.00   84.00     107 \n\n\nCAUTION: Sometimes, numeric variables will import as class “character”. This occurs if there are non-numeric characters in some of the values, for example an entry of “2 months” for age, or (depending on your R locale settings) if a comma is used in the decimals place (e.g. “4,5” to mean four and one half years)..\n\n\n\nage_categories()\nWith the epikit package, you can use the age_categories() function to easily categorize and label numeric columns (note: this function can be applied to non-age numeric variables too). As a bonum, the output column is automatically an ordered factor.\nHere are the required inputs:\n\nA numeric vector (column)\n\nThe breakers = argument - provide a numeric vector of break points for the new groups\n\nFirst, the simplest example:\n\n# Simple example\n################\npacman::p_load(epikit)                    # load package\n\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    age_cat = age_categories(             # create new column\n      age_years,                            # numeric column to make groups from\n      breakers = c(0, 5, 10, 15, 20,        # break points\n                   30, 40, 50, 60, 70)))\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")\n\n\n  0-4   5-9 10-14 15-19 20-29 30-39 40-49 50-59 60-69   70+  &lt;NA&gt; \n 1227  1223  1048   827  1216   597   251    78    27     7   107 \n\n\nThe break values you specify are by default the lower bounds - that is, they are included in the “higher” group / the groups are “open” on the lower/left side. As shown below, you can add 1 to each break value to achieve groups that are open at the top/right.\n\n# Include upper ends for the same categories\n############################################\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    age_cat = age_categories(\n      age_years, \n      breakers = c(0, 6, 11, 16, 21, 31, 41, 51, 61, 71)))\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")\n\n\n  0-5  6-10 11-15 16-20 21-30 31-40 41-50 51-60 61-70   71+  &lt;NA&gt; \n 1469  1195  1040   770  1149   547   231    70    24     6   107 \n\n\nYou can adjust how the labels are displayed with separator =. The default is “-”\nYou can adjust how the top numbers are handled, with the ceiling = arguemnt. To set an upper cut-off set ceiling = TRUE. In this use, the highest break value provided is a “ceiling” and a category “XX+” is not created. Any values above highest break value (or to upper =, if defined) are categorized as NA. Below is an example with ceiling = TRUE, so that there is no category of XX+ and values above 70 (the highest break value) are assigned as NA.\n\n# With ceiling set to TRUE\n##########################\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    age_cat = age_categories(\n      age_years, \n      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70),\n      ceiling = TRUE)) # 70 is ceiling, all above become NA\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")\n\n\n  0-4   5-9 10-14 15-19 20-29 30-39 40-49 50-59 60-70  &lt;NA&gt; \n 1227  1223  1048   827  1216   597   251    78    28   113 \n\n\nAlternatively, instead of breakers =, you can provide all of lower =, upper =, and by =:\n\nlower = The lowest number you want considered - default is 0\n\nupper = The highest number you want considered\n\nby = The number of years between groups\n\n\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    age_cat = age_categories(\n      age_years, \n      lower = 0,\n      upper = 100,\n      by = 10))\n\n# show table\ntable(linelist$age_cat, useNA = \"always\")\n\n\n  0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-79 80-89 90-99  100+  &lt;NA&gt; \n 2450  1875  1216   597   251    78    27     6     1     0     0   107 \n\n\nSee the function’s Help page for more details (enter ?age_categories in the R console).\n\n\n\ncut()\ncut() is a base R alternative to age_categories(), but I think you will see why age_categories() was developed to simplify this process. Some notable differences from age_categories() are:\n\nYou do not need to install/load another package\n\nYou can specify whether groups are open/closed on the right/left\n\nYou must provide accurate labels yourself\n\nIf you want 0 included in the lowest group you must specify this\n\nThe basic syntax within cut() is to first provide the numeric column to be cut (age_years), and then the breaks argument, which is a numeric vector c() of break points. Using cut(), the resulting column is an ordered factor.\nBy default, the categorization occurs so that the right/upper side is “open” and inclusive (and the left/lower side is “closed” or exclusive). This is the opposite behavior from the age_categories() function. The default labels use the notation “(A, B]”, which means A is not included but B is. Reverse this behavior by providing the right = TRUE argument.\nThus, by default, “0” values are excluded from the lowest group, and categorized as NA! “0” values could be infants coded as age 0 so be careful! To change this, add the argument include.lowest = TRUE so that any “0” values will be included in the lowest group. The automatically-generated label for the lowest category will then be “[A],B]”. Note that if you include the include.lowest = TRUE argument and right = TRUE, the extreme inclusion will now apply to the highest break point value and category, not the lowest.\nYou can provide a vector of customized labels using the labels = argument. As these are manually written, be very careful to ensure they are accurate! Check your work using cross-tabulation, as described below.\nAn example of cut() applied to age_years to make the new variable age_cat is below:\n\n# Create new variable, by cutting the numeric age variable\n# lower break is excluded but upper break is included in each category\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    age_cat = cut(\n      age_years,\n      breaks = c(0, 5, 10, 15, 20,\n                 30, 50, 70, 100),\n      include.lowest = TRUE         # include 0 in lowest group\n      ))\n\n# tabulate the number of observations per group\ntable(linelist$age_cat, useNA = \"always\")\n\n\n   [0,5]   (5,10]  (10,15]  (15,20]  (20,30]  (30,50]  (50,70] (70,100] \n    1469     1195     1040      770     1149      778       94        6 \n    &lt;NA&gt; \n     107 \n\n\nCheck your work!!! Verify that each age value was assigned to the correct category by cross-tabulating the numeric and category columns. Examine assignment of boundary values (e.g. 15, if neighboring categories are 10-15 and 16-20).\n\n# Cross tabulation of the numeric and category columns. \ntable(\"Numeric Values\" = linelist$age_years,   # names specified in table for clarity.\n      \"Categories\"     = linelist$age_cat,\n      useNA = \"always\")                        # don't forget to examine NA values\n\n                    Categories\nNumeric Values       [0,5] (5,10] (10,15] (15,20] (20,30] (30,50] (50,70]\n  0                    136      0       0       0       0       0       0\n  0.0833333333333333     1      0       0       0       0       0       0\n  0.25                   2      0       0       0       0       0       0\n  0.333333333333333      6      0       0       0       0       0       0\n  0.416666666666667      1      0       0       0       0       0       0\n  0.5                    6      0       0       0       0       0       0\n  0.583333333333333      3      0       0       0       0       0       0\n  0.666666666666667      3      0       0       0       0       0       0\n  0.75                   3      0       0       0       0       0       0\n  0.833333333333333      1      0       0       0       0       0       0\n  0.916666666666667      1      0       0       0       0       0       0\n  1                    275      0       0       0       0       0       0\n  1.5                    2      0       0       0       0       0       0\n  2                    308      0       0       0       0       0       0\n  3                    246      0       0       0       0       0       0\n  4                    233      0       0       0       0       0       0\n  5                    242      0       0       0       0       0       0\n  6                      0    241       0       0       0       0       0\n  7                      0    256       0       0       0       0       0\n  8                      0    239       0       0       0       0       0\n  9                      0    245       0       0       0       0       0\n  10                     0    214       0       0       0       0       0\n  11                     0      0     220       0       0       0       0\n  12                     0      0     224       0       0       0       0\n  13                     0      0     191       0       0       0       0\n  14                     0      0     199       0       0       0       0\n  15                     0      0     206       0       0       0       0\n  16                     0      0       0     186       0       0       0\n  17                     0      0       0     164       0       0       0\n  18                     0      0       0     141       0       0       0\n  19                     0      0       0     130       0       0       0\n  20                     0      0       0     149       0       0       0\n  21                     0      0       0       0     158       0       0\n  22                     0      0       0       0     149       0       0\n  23                     0      0       0       0     125       0       0\n  24                     0      0       0       0     144       0       0\n  25                     0      0       0       0     107       0       0\n  26                     0      0       0       0     100       0       0\n  27                     0      0       0       0     117       0       0\n  28                     0      0       0       0      85       0       0\n  29                     0      0       0       0      82       0       0\n  30                     0      0       0       0      82       0       0\n  31                     0      0       0       0       0      68       0\n  32                     0      0       0       0       0      84       0\n  33                     0      0       0       0       0      78       0\n  34                     0      0       0       0       0      58       0\n  35                     0      0       0       0       0      58       0\n  36                     0      0       0       0       0      33       0\n  37                     0      0       0       0       0      46       0\n  38                     0      0       0       0       0      45       0\n  39                     0      0       0       0       0      45       0\n  40                     0      0       0       0       0      32       0\n  41                     0      0       0       0       0      34       0\n  42                     0      0       0       0       0      26       0\n  43                     0      0       0       0       0      31       0\n  44                     0      0       0       0       0      24       0\n  45                     0      0       0       0       0      27       0\n  46                     0      0       0       0       0      25       0\n  47                     0      0       0       0       0      16       0\n  48                     0      0       0       0       0      21       0\n  49                     0      0       0       0       0      15       0\n  50                     0      0       0       0       0      12       0\n  51                     0      0       0       0       0       0      13\n  52                     0      0       0       0       0       0       7\n  53                     0      0       0       0       0       0       4\n  54                     0      0       0       0       0       0       6\n  55                     0      0       0       0       0       0       9\n  56                     0      0       0       0       0       0       7\n  57                     0      0       0       0       0       0       9\n  58                     0      0       0       0       0       0       6\n  59                     0      0       0       0       0       0       5\n  60                     0      0       0       0       0       0       4\n  61                     0      0       0       0       0       0       2\n  62                     0      0       0       0       0       0       1\n  63                     0      0       0       0       0       0       5\n  64                     0      0       0       0       0       0       1\n  65                     0      0       0       0       0       0       5\n  66                     0      0       0       0       0       0       3\n  67                     0      0       0       0       0       0       2\n  68                     0      0       0       0       0       0       1\n  69                     0      0       0       0       0       0       3\n  70                     0      0       0       0       0       0       1\n  72                     0      0       0       0       0       0       0\n  73                     0      0       0       0       0       0       0\n  76                     0      0       0       0       0       0       0\n  84                     0      0       0       0       0       0       0\n  &lt;NA&gt;                   0      0       0       0       0       0       0\n                    Categories\nNumeric Values       (70,100] &lt;NA&gt;\n  0                         0    0\n  0.0833333333333333        0    0\n  0.25                      0    0\n  0.333333333333333         0    0\n  0.416666666666667         0    0\n  0.5                       0    0\n  0.583333333333333         0    0\n  0.666666666666667         0    0\n  0.75                      0    0\n  0.833333333333333         0    0\n  0.916666666666667         0    0\n  1                         0    0\n  1.5                       0    0\n  2                         0    0\n  3                         0    0\n  4                         0    0\n  5                         0    0\n  6                         0    0\n  7                         0    0\n  8                         0    0\n  9                         0    0\n  10                        0    0\n  11                        0    0\n  12                        0    0\n  13                        0    0\n  14                        0    0\n  15                        0    0\n  16                        0    0\n  17                        0    0\n  18                        0    0\n  19                        0    0\n  20                        0    0\n  21                        0    0\n  22                        0    0\n  23                        0    0\n  24                        0    0\n  25                        0    0\n  26                        0    0\n  27                        0    0\n  28                        0    0\n  29                        0    0\n  30                        0    0\n  31                        0    0\n  32                        0    0\n  33                        0    0\n  34                        0    0\n  35                        0    0\n  36                        0    0\n  37                        0    0\n  38                        0    0\n  39                        0    0\n  40                        0    0\n  41                        0    0\n  42                        0    0\n  43                        0    0\n  44                        0    0\n  45                        0    0\n  46                        0    0\n  47                        0    0\n  48                        0    0\n  49                        0    0\n  50                        0    0\n  51                        0    0\n  52                        0    0\n  53                        0    0\n  54                        0    0\n  55                        0    0\n  56                        0    0\n  57                        0    0\n  58                        0    0\n  59                        0    0\n  60                        0    0\n  61                        0    0\n  62                        0    0\n  63                        0    0\n  64                        0    0\n  65                        0    0\n  66                        0    0\n  67                        0    0\n  68                        0    0\n  69                        0    0\n  70                        0    0\n  72                        1    0\n  73                        3    0\n  76                        1    0\n  84                        1    0\n  &lt;NA&gt;                      0  107\n\n\nRe-labeling NA values\nYou may want to assign NA values a label such as “Missing”. Because the new column is class Factor (restricted values), you cannot simply mutate it with replace_na(), as this value will be rejected. Instead, use fct_explicit_na() from forcats as explained in the Factors page.\n\nlinelist &lt;- linelist %&gt;% \n  \n  # cut() creates age_cat, automatically of class Factor      \n  mutate(age_cat = cut(\n    age_years,\n    breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          \n    right = FALSE,\n    include.lowest = TRUE,        \n    labels = c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-29\", \"30-49\", \"50-69\", \"70-100\")),\n         \n    # make missing values explicit\n    age_cat = fct_explicit_na(\n      age_cat,\n      na_level = \"Missing age\")  # you can specify the label\n  )    \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `age_cat = fct_explicit_na(age_cat, na_level = \"Missing age\")`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\n\n# table to view counts\ntable(linelist$age_cat, useNA = \"always\")\n\n\n        0-4         5-9       10-14       15-19       20-29       30-49 \n       1227        1223        1048         827        1216         848 \n      50-69      70-100 Missing age        &lt;NA&gt; \n        105           7         107           0 \n\n\nQuickly make breaks and labels\nFor a fast way to make breaks and label vectors, use something like below. See the R basics page for references on seq() and rep().\n\n# Make break points from 0 to 90 by 5\nage_seq = seq(from = 0, to = 90, by = 5)\nage_seq\n\n# Make labels for the above categories, assuming default cut() settings\nage_labels = paste0(age_seq + 1, \"-\", age_seq + 5)\nage_labels\n\n# check that both vectors are the same length\nlength(age_seq) == length(age_labels)\n\nRead more about cut() in its Help page by entering ?cut in the R console.\n\n\nQuantile breaks\nIn common understanding, “quantiles” or “percentiles” typically refer to a value below which a proportion of values fall. For example, the 95th percentile of ages in linelist would be the age below which 95% of the age fall.\nHowever in common speech, “quartiles” and “deciles” can also refer to the groups of data as equally divided into 4, or 10 groups (note there will be one more break point than group).\nTo get quantile break points, you can use quantile() from the stats package from base R. You provide a numeric vector (e.g. a column in a dataset) and vector of numeric probability values ranging from 0 to 1.0. The break points are returned as a numeric vector. Explore the details of the statistical methodologies by entering ?quantile.\n\nIf your input numeric vector has any missing values it is best to set na.rm = TRUE\n\nSet names = FALSE to get an un-named numeric vector\n\n\nquantile(linelist$age_years,               # specify numeric vector to work on\n  probs = c(0, .25, .50, .75, .90, .95),   # specify the percentiles you want\n  na.rm = TRUE)                            # ignore missing values \n\n 0% 25% 50% 75% 90% 95% \n  0   6  13  23  33  41 \n\n\nYou can use the results of quantile() as break points in age_categories() or cut(). Below we create a new column deciles using cut() where the breaks are defined using quantiles() on age_years. Below, we display the results using tabyl() from janitor so you can see the percentages (see the Descriptive tables page). Note how they are not exactly 10% in each group.\n\nlinelist %&gt;%                                # begin with linelist\n  mutate(deciles = cut(age_years,           # create new column decile as cut() on column age_years\n    breaks = quantile(                      # define cut breaks using quantile()\n      age_years,                               # operate on age_years\n      probs = seq(0, 1, by = 0.1),             # 0.0 to 1.0 by 0.1\n      na.rm = TRUE),                           # ignore missing values\n    include.lowest = TRUE)) %&gt;%             # for cut() include age 0\n  janitor::tabyl(deciles)                   # pipe to table to display\n\n deciles   n    percent valid_percent\n   [0,2] 748 0.11319613    0.11505922\n   (2,5] 721 0.10911017    0.11090601\n   (5,7] 497 0.07521186    0.07644978\n  (7,10] 698 0.10562954    0.10736810\n (10,13] 635 0.09609564    0.09767728\n (13,17] 755 0.11425545    0.11613598\n (17,21] 578 0.08746973    0.08890940\n (21,26] 625 0.09458232    0.09613906\n (26,33] 596 0.09019370    0.09167820\n (33,84] 648 0.09806295    0.09967697\n    &lt;NA&gt; 107 0.01619249            NA\n\n\n\n\nEvenly-sized groups\nAnother tool to make numeric groups is the the dplyr function ntile(), which attempts to break your data into n evenly-sized groups - but be aware that unlike with quantile() the same value could appear in more than one group. Provide the numeric vector and then the number of groups. The values in the new column created is just group “numbers” (e.g. 1 to 10), not the range of values themselves as when using cut().\n\n# make groups with ntile()\nntile_data &lt;- linelist %&gt;% \n  mutate(even_groups = ntile(age_years, 10))\n\n# make table of counts and proportions by group\nntile_table &lt;- ntile_data %&gt;% \n  janitor::tabyl(even_groups)\n  \n# attach min/max values to demonstrate ranges\nntile_ranges &lt;- ntile_data %&gt;% \n  group_by(even_groups) %&gt;% \n  summarise(\n    min = min(age_years, na.rm=T),\n    max = max(age_years, na.rm=T)\n  )\n\nWarning: There were 2 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `min = min(age_years, na.rm = T)`.\nℹ In group 11: `even_groups = NA`.\nCaused by warning in `min()`:\n! no non-missing arguments to min; returning Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n# combine and print - note that values are present in multiple groups\nleft_join(ntile_table, ntile_ranges, by = \"even_groups\")\n\n even_groups   n    percent valid_percent min  max\n           1 651 0.09851695    0.10013844   0    2\n           2 650 0.09836562    0.09998462   2    5\n           3 650 0.09836562    0.09998462   5    7\n           4 650 0.09836562    0.09998462   7   10\n           5 650 0.09836562    0.09998462  10   13\n           6 650 0.09836562    0.09998462  13   17\n           7 650 0.09836562    0.09998462  17   21\n           8 650 0.09836562    0.09998462  21   26\n           9 650 0.09836562    0.09998462  26   33\n          10 650 0.09836562    0.09998462  33   84\n          NA 107 0.01619249            NA Inf -Inf\n\n\n\n\n\ncase_when()\nIt is possible to use the dplyr function case_when() to create categories from a numeric column, but it is easier to use age_categories() from epikit or cut() because these will create an ordered factor automatically.\nIf using case_when(), please review the proper use as described earlier in the Re-code values section of this page. Also be aware that all right-hand side values must be of the same class. Thus, if you want NA on the right-side you should either write “Missing” or use the special NA value NA_character_.\n\n\nAdd to pipe chain\nBelow, code to create two categorical age columns is added to the cleaning pipe chain:\n\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %&gt;% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %&gt;% \n  \n    # de-duplicate\n    distinct() %&gt;% \n\n    # add column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %&gt;%     \n\n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age)) %&gt;% \n    \n    # add column: delay to hospitalisation\n    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %&gt;% \n    \n    # clean values of hospital column\n    mutate(hospital = recode(hospital,\n                      # OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      )) %&gt;% \n    \n    mutate(hospital = replace_na(hospital, \"Missing\")) %&gt;% \n\n    # create age_years column (from age and age_unit)\n    mutate(age_years = case_when(\n          age_unit == \"years\" ~ age,\n          age_unit == \"months\" ~ age/12,\n          is.na(age_unit) ~ age)) %&gt;% \n  \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    ###################################################   \n    mutate(\n          # age categories: custom\n          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),\n        \n          # age categories: 0 to 85 by 5s\n          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5)))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#add-rows",
    "href": "new_pages/cleaning.html#add-rows",
    "title": "8  Cleaning data and core functions",
    "section": "8.10 Add rows",
    "text": "8.10 Add rows\n\nOne-by-one\nAdding rows one-by-one manually is tedious but can be done with add_row() from dplyr. Remember that each column must contain values of only one class (either character, numeric, logical, etc.). So adding a row requires nuance to maintain this.\n\nlinelist &lt;- linelist %&gt;% \n  add_row(row_num = 666,\n          case_id = \"abc\",\n          generation = 4,\n          `infection date` = as.Date(\"2020-10-10\"),\n          .before = 2)\n\nUse .before and .after. to specify the placement of the row you want to add. .before = 3 will put the new row before the current 3rd row. The default behavior is to add the row to the end. Columns not specified will be left empty (NA).\nThe new row number may look strange (“…23”) but the row numbers in the pre-existing rows have changed. So if using the command twice, examine/test the insertion carefully.\nIf a class you provide is off you will see an error like this:\nError: Can't combine ..1$infection date &lt;date&gt; and ..2$infection date &lt;character&gt;.\n(when inserting a row with a date value, remember to wrap the date in the function as.Date() like as.Date(\"2020-10-10\")).\n\n\nBind rows\nTo combine datasets together by binding the rows of one dataframe to the bottom of another data frame, you can use bind_rows() from dplyr. This is explained in more detail in the page Joining data.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#filter-rows",
    "href": "new_pages/cleaning.html#filter-rows",
    "title": "8  Cleaning data and core functions",
    "section": "8.11 Filter rows",
    "text": "8.11 Filter rows\nA typical cleaning step after you have cleaned the columns and re-coded values is to filter the data frame for specific rows using the dplyr verb filter().\nWithin filter(), specify the logic that must be TRUE for a row in the dataset to be kept. Below we show how to filter rows based on simple and complex logical conditions.\n\n\nSimple filter\nThis simple example re-defines the dataframe linelist as itself, having filtered the rows to meet a logical condition. Only the rows where the logical statement within the parentheses evaluates to TRUE are kept.\nIn this example, the logical statement is gender == \"f\", which is asking whether the value in the column gender is equal to “f” (case sensitive).\nBefore the filter is applied, the number of rows in linelist is nrow(linelist).\n\nlinelist &lt;- linelist %&gt;% \n  filter(gender == \"f\")   # keep only rows where gender is equal to \"f\"\n\nAfter the filter is applied, the number of rows in linelist is linelist %&gt;% filter(gender == \"f\") %&gt;% nrow().\n\n\nFilter out missing values\nIt is fairly common to want to filter out rows that have missing values. Resist the urge to write filter(!is.na(column) & !is.na(column)) and instead use the tidyr function that is custom-built for this purpose: drop_na(). If run with empty parentheses, it removes rows with any missing values. Alternatively, you can provide names of specific columns to be evaluated for missingness, or use the “tidyselect” helper functions described above.\n\nlinelist %&gt;% \n  drop_na(case_id, age_years)  # drop rows with missing values for case_id or age_years\n\nSee the page on Missing data for many techniques to analyse and manage missingness in your data.\n\n\nFilter by row number\nIn a data frame or tibble, each row will usually have a “row number” that (when seen in R Viewer) appears to the left of the first column. It is not itself a true column in the data, but it can be used in a filter() statement.\nTo filter based on “row number”, you can use the dplyr function row_number() with open parentheses as part of a logical filtering statement. Often you will use the %in% operator and a range of numbers as part of that logical statement, as shown below. To see the first N rows, you can also use the special dplyr function head().\n\n# View first 100 rows\nlinelist %&gt;% head(100)     # or use tail() to see the n last rows\n\n# Show row 5 only\nlinelist %&gt;% filter(row_number() == 5)\n\n# View rows 2 through 20, and three specific columns\nlinelist %&gt;% filter(row_number() %in% 2:20) %&gt;% select(date_onset, outcome, age)\n\nYou can also convert the row numbers to a true column by piping your data frame to the tibble function rownames_to_column() (do not put anything in the parentheses).\n\n\n\nComplex filter\nMore complex logical statements can be constructed using parentheses ( ), OR |, negate !, %in%, and AND & operators. An example is below:\nNote: You can use the ! operator in front of a logical criteria to negate it. For example, !is.na(column) evaluates to true if the column value is not missing. Likewise !column %in% c(\"a\", \"b\", \"c\") evaluates to true if the column value is not in the vector.\n\nExamine the data\nBelow is a simple one-line command to create a histogram of onset dates. See that a second smaller outbreak from 2012-2013 is also included in this raw dataset. For our analyses, we want to remove entries from this earlier outbreak.\n\nhist(linelist$date_onset, breaks = 50)\n\n\n\n\n\n\n\n\n\n\nHow filters handle missing numeric and date values\nCan we just filter by date_onset to rows after June 2013? Caution! Applying the code filter(date_onset &gt; as.Date(\"2013-06-01\"))) would remove any rows in the later epidemic with a missing date of onset!\nDANGER: Filtering to greater than (&gt;) or less than (&lt;) a date or number can remove any rows with missing values (NA)! This is because NA is treated as infinitely large and small.\n(See the page on Working with dates for more information on working with dates and the package lubridate)\n\n\nDesign the filter\nExamine a cross-tabulation to make sure we exclude only the correct rows:\n\ntable(Hospital  = linelist$hospital,                     # hospital name\n      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset\n      useNA     = \"always\")                              # show missing values\n\n                                      YearOnset\nHospital                               2012 2013 2014 2015 &lt;NA&gt;\n  Central Hospital                        0    0  351   99   18\n  Hospital A                            229   46    0    0   15\n  Hospital B                            227   47    0    0   15\n  Military Hospital                       0    0  676  200   34\n  Missing                                 0    0 1117  318   77\n  Other                                   0    0  684  177   46\n  Port Hospital                           9    1 1372  347   75\n  St. Mark's Maternity Hospital (SMMH)    0    0  322   93   13\n  &lt;NA&gt;                                    0    0    0    0    0\n\n\nWhat other criteria can we filter on to remove the first outbreak (in 2012 & 2013) from the dataset? We see that:\n\nThe first epidemic in 2012 & 2013 occurred at Hospital A, Hospital B, and that there were also 10 cases at Port Hospital.\n\nHospitals A & B did not have cases in the second epidemic, but Port Hospital did.\n\nWe want to exclude:\n\nThe nrow(linelist %&gt;% filter(hospital %in% c(\"Hospital A\", \"Hospital B\") | date_onset &lt; as.Date(\"2013-06-01\"))) rows with onset in 2012 and 2013 at either hospital A, B, or Port:\n\nExclude nrow(linelist %&gt;% filter(date_onset &lt; as.Date(\"2013-06-01\"))) rows with onset in 2012 and 2013\nExclude nrow(linelist %&gt;% filter(hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset))) rows from Hospitals A & B with missing onset dates\n\nDo not exclude nrow(linelist %&gt;% filter(!hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset))) other rows with missing onset dates.\n\n\nWe start with a linelist of nrow(linelist)`. Here is our filter statement:\n\nlinelist &lt;- linelist %&gt;% \n  # keep rows where onset is after 1 June 2013 OR where onset is missing and it was a hospital OTHER than Hospital A or B\n  filter(date_onset &gt; as.Date(\"2013-06-01\") | (is.na(date_onset) & !hospital %in% c(\"Hospital A\", \"Hospital B\")))\n\nnrow(linelist)\n\n[1] 6019\n\n\nWhen we re-make the cross-tabulation, we see that Hospitals A & B are removed completely, and the 10 Port Hospital cases from 2012 & 2013 are removed, and all other values are the same - just as we wanted.\n\ntable(Hospital  = linelist$hospital,                     # hospital name\n      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset\n      useNA     = \"always\")                              # show missing values\n\n                                      YearOnset\nHospital                               2014 2015 &lt;NA&gt;\n  Central Hospital                      351   99   18\n  Military Hospital                     676  200   34\n  Missing                              1117  318   77\n  Other                                 684  177   46\n  Port Hospital                        1372  347   75\n  St. Mark's Maternity Hospital (SMMH)  322   93   13\n  &lt;NA&gt;                                    0    0    0\n\n\nMultiple statements can be included within one filter command (separated by commas), or you can always pipe to a separate filter() command for clarity.\nNote: some readers may notice that it would be easier to just filter by date_hospitalisation because it is 100% complete with no missing values. This is true. But date_onset is used for purposes of demonstrating a complex filter.\n\n\n\nStandalone\nFiltering can also be done as a stand-alone command (not part of a pipe chain). Like other dplyr verbs, in this case the first argument must be the dataset itself.\n\n# dataframe &lt;- filter(dataframe, condition(s) for rows to keep)\n\nlinelist &lt;- filter(linelist, !is.na(case_id))\n\nYou can also use base R to subset using square brackets which reflect the [rows, columns] that you want to retain.\n\n# dataframe &lt;- dataframe[row conditions, column conditions] (blank means keep all)\n\nlinelist &lt;- linelist[!is.na(case_id), ]\n\n\n\nQuickly review records\nOften you want to quickly review a few records, for only a few columns. The base R function View() will print a data frame for viewing in your RStudio.\nView the linelist in RStudio:\n\nView(linelist)\n\nHere are two examples of viewing specific cells (specific rows, and specific columns):\nWith dplyr functions filter() and select():\nWithin View(), pipe the dataset to filter() to keep certain rows, and then to select() to keep certain columns. For example, to review onset and hospitalization dates of 3 specific cases:\n\nView(linelist %&gt;%\n       filter(case_id %in% c(\"11f8ea\", \"76b97a\", \"47a5f5\")) %&gt;%\n       select(date_onset, date_hospitalisation))\n\nYou can achieve the same with base R syntax, using brackets [ ] to subset you want to see.\n\nView(linelist[linelist$case_id %in% c(\"11f8ea\", \"76b97a\", \"47a5f5\"), c(\"date_onset\", \"date_hospitalisation\")])\n\n\nAdd to pipe chain\n\n# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)\n##################################################################################\n\n# begin cleaning pipe chain\n###########################\nlinelist &lt;- linelist_raw %&gt;%\n    \n    # standardize column name syntax\n    janitor::clean_names() %&gt;% \n    \n    # manually re-name columns\n           # NEW name             # OLD name\n    rename(date_infection       = infection_date,\n           date_hospitalisation = hosp_date,\n           date_outcome         = date_of_outcome) %&gt;% \n    \n    # remove column\n    select(-c(row_num, merged_header, x28)) %&gt;% \n  \n    # de-duplicate\n    distinct() %&gt;% \n\n    # add column\n    mutate(bmi = wt_kg / (ht_cm/100)^2) %&gt;%     \n\n    # convert class of columns\n    mutate(across(contains(\"date\"), as.Date), \n           generation = as.numeric(generation),\n           age        = as.numeric(age)) %&gt;% \n    \n    # add column: delay to hospitalisation\n    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %&gt;% \n    \n    # clean values of hospital column\n    mutate(hospital = recode(hospital,\n                      # OLD = NEW\n                      \"Mitylira Hopital\"  = \"Military Hospital\",\n                      \"Mitylira Hospital\" = \"Military Hospital\",\n                      \"Military Hopital\"  = \"Military Hospital\",\n                      \"Port Hopital\"      = \"Port Hospital\",\n                      \"Central Hopital\"   = \"Central Hospital\",\n                      \"other\"             = \"Other\",\n                      \"St. Marks Maternity Hopital (SMMH)\" = \"St. Mark's Maternity Hospital (SMMH)\"\n                      )) %&gt;% \n    \n    mutate(hospital = replace_na(hospital, \"Missing\")) %&gt;% \n\n    # create age_years column (from age and age_unit)\n    mutate(age_years = case_when(\n          age_unit == \"years\" ~ age,\n          age_unit == \"months\" ~ age/12,\n          is.na(age_unit) ~ age)) %&gt;% \n  \n    mutate(\n          # age categories: custom\n          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),\n        \n          # age categories: 0 to 85 by 5s\n          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5))) %&gt;% \n    \n    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED\n    ###################################################\n    filter(\n          # keep only rows where case_id is not missing\n          !is.na(case_id),  \n          \n          # also filter to keep only the second outbreak\n          date_onset &gt; as.Date(\"2013-06-01\") | (is.na(date_onset) & !hospital %in% c(\"Hospital A\", \"Hospital B\")))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#row-wise-calculations",
    "href": "new_pages/cleaning.html#row-wise-calculations",
    "title": "8  Cleaning data and core functions",
    "section": "8.12 Row-wise calculations",
    "text": "8.12 Row-wise calculations\nIf you want to perform a calculation within a row, you can use rowwise() from dplyr. See this online vignette on row-wise calculations. For example, this code applies rowwise() and then creates a new column that sums the number of the specified symptom columns that have value “yes”, for each row in the linelist. The columns are specified within sum() by name within a vector c(). rowwise() is essentially a special kind of group_by(), so it is best to use ungroup() when you are done (page on Grouping data).\n\nlinelist %&gt;%\n  rowwise() %&gt;%\n  mutate(num_symptoms = sum(c(fever, chills, cough, aches, vomit) == \"yes\")) %&gt;% \n  ungroup() %&gt;% \n  select(fever, chills, cough, aches, vomit, num_symptoms) # for display\n\n# A tibble: 5,888 × 6\n   fever chills cough aches vomit num_symptoms\n   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;        &lt;int&gt;\n 1 no    no     yes   no    yes              2\n 2 &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;            NA\n 3 &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;            NA\n 4 no    no     no    no    no               0\n 5 no    no     yes   no    yes              2\n 6 no    no     yes   no    yes              2\n 7 &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;            NA\n 8 no    no     yes   no    yes              2\n 9 no    no     yes   no    yes              2\n10 no    no     yes   no    no               1\n# ℹ 5,878 more rows\n\n\nAs you specify the column to evaluate, you may want to use the “tidyselect” helper functions described in the select() section of this page. You just have to make one adjustment (because you are not using them within a dplyr function like select() or summarise()).\nPut the column-specification criteria within the dplyr function c_across(). This is because c_across (documentation) is designed to work with rowwise() specifically. For example, the following code:\n\nApplies rowwise() so the following operation (sum()) is applied within each row (not summing entire columns)\n\nCreates new column num_NA_dates, defined for each row as the number of columns (with name containing “date”) for which is.na() evaluated to TRUE (they are missing data).\n\nungroup() to remove the effects of rowwise() for subsequent steps\n\n\nlinelist %&gt;%\n  rowwise() %&gt;%\n  mutate(num_NA_dates = sum(is.na(c_across(contains(\"date\"))))) %&gt;% \n  ungroup() %&gt;% \n  select(num_NA_dates, contains(\"date\")) # for display\n\n# A tibble: 5,888 × 5\n   num_NA_dates date_infection date_onset date_hospitalisation date_outcome\n          &lt;int&gt; &lt;date&gt;         &lt;date&gt;     &lt;date&gt;               &lt;date&gt;      \n 1            1 2014-05-08     2014-05-13 2014-05-15           NA          \n 2            1 NA             2014-05-13 2014-05-14           2014-05-18  \n 3            1 NA             2014-05-16 2014-05-18           2014-05-30  \n 4            1 2014-05-04     2014-05-18 2014-05-20           NA          \n 5            0 2014-05-18     2014-05-21 2014-05-22           2014-05-29  \n 6            0 2014-05-03     2014-05-22 2014-05-23           2014-05-24  \n 7            0 2014-05-22     2014-05-27 2014-05-29           2014-06-01  \n 8            0 2014-05-28     2014-06-02 2014-06-03           2014-06-07  \n 9            1 NA             2014-06-05 2014-06-06           2014-06-18  \n10            1 NA             2014-06-05 2014-06-07           2014-06-09  \n# ℹ 5,878 more rows\n\n\nYou could also provide other functions, such as max() to get the latest or most recent date for each row:\n\nlinelist %&gt;%\n  rowwise() %&gt;%\n  mutate(latest_date = max(c_across(contains(\"date\")), na.rm=T)) %&gt;% \n  ungroup() %&gt;% \n  select(latest_date, contains(\"date\"))  # for display\n\n# A tibble: 5,888 × 5\n   latest_date date_infection date_onset date_hospitalisation date_outcome\n   &lt;date&gt;      &lt;date&gt;         &lt;date&gt;     &lt;date&gt;               &lt;date&gt;      \n 1 2014-05-15  2014-05-08     2014-05-13 2014-05-15           NA          \n 2 2014-05-18  NA             2014-05-13 2014-05-14           2014-05-18  \n 3 2014-05-30  NA             2014-05-16 2014-05-18           2014-05-30  \n 4 2014-05-20  2014-05-04     2014-05-18 2014-05-20           NA          \n 5 2014-05-29  2014-05-18     2014-05-21 2014-05-22           2014-05-29  \n 6 2014-05-24  2014-05-03     2014-05-22 2014-05-23           2014-05-24  \n 7 2014-06-01  2014-05-22     2014-05-27 2014-05-29           2014-06-01  \n 8 2014-06-07  2014-05-28     2014-06-02 2014-06-03           2014-06-07  \n 9 2014-06-18  NA             2014-06-05 2014-06-06           2014-06-18  \n10 2014-06-09  NA             2014-06-05 2014-06-07           2014-06-09  \n# ℹ 5,878 more rows",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/cleaning.html#arrange-and-sort",
    "href": "new_pages/cleaning.html#arrange-and-sort",
    "title": "8  Cleaning data and core functions",
    "section": "8.13 Arrange and sort",
    "text": "8.13 Arrange and sort\nUse the dplyr function arrange() to sort or order the rows by column values.\nSimple list the columns in the order they should be sorted on. Specify .by_group = TRUE if you want the sorting to to first occur by any groupings applied to the data (see page on Grouping data).\nBy default, column will be sorted in “ascending” order (which applies to numeric and also to character columns). You can sort a variable in “descending” order by wrapping it with desc().\nSorting data with arrange() is particularly useful when making Tables for presentation, using slice() to take the “top” rows per group, or setting factor level order by order of appearance.\nFor example, to sort the our linelist rows by hospital, then by date_onset in descending order, we would use:\n\nlinelist %&gt;% \n   arrange(hospital, desc(date_onset))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Cleaning data and core functions</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html",
    "href": "new_pages/dates.html",
    "title": "9  Working with dates",
    "section": "",
    "text": "9.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#preparation",
    "href": "new_pages/dates.html#preparation",
    "title": "9  Working with dates",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for this page. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\n# Checks if package is installed, installs if necessary, and loads package for current session\n\npacman::p_load(\n  lubridate,  # general package for handling and converting dates  \n  parsedate,  # has function to \"guess\" messy dates\n  aweek,      # another option for converting dates to weeks, and weeks to dates\n  zoo,        # additional date/time functions\n  here,       # file management\n  rio,        # data import/export\n  tidyverse)  # data management and visualization  \n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow along step-by-step, see instruction in the Download handbook and data page. We assume the file is in the working directory so no sub-folders are specified in this file path.\n\nlinelist &lt;- import(\"linelist_cleaned.xlsx\")",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#current-date",
    "href": "new_pages/dates.html#current-date",
    "title": "9  Working with dates",
    "section": "9.2 Current date",
    "text": "9.2 Current date\nYou can get the current “system” date or system datetime of your computer by doing the following with base R.\n\n# get the system date - this is a DATE class\nSys.Date()\n\n[1] \"2024-06-19\"\n\n# get the system time - this is a DATETIME class\nSys.time()\n\n[1] \"2024-06-19 12:27:19 CEST\"\n\n\nWith the lubridate package these can also be returned with today() and now(), respectively. date() returns the current date and time with weekday and month names.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#convert-to-date",
    "href": "new_pages/dates.html#convert-to-date",
    "title": "9  Working with dates",
    "section": "9.3 Convert to Date",
    "text": "9.3 Convert to Date\nAfter importing a dataset into R, date column values may look like “1989/12/30”, “05/06/2014”, or “13 Jan 2020”. In these cases, R is likely still treating these values as Character values. R must be told that these values are dates… and what the format of the date is (which part is Day, which is Month, which is Year, etc).\nOnce told, R converts these values to class Date. In the background, R will store the dates as numbers (the number of days from its “origin” date 1 Jan 1970). You will not interface with the date number often, but this allows for R to treat dates as continuous variables and to allow special operations such as calculating the distance between dates.\nBy default, values of class Date in R are displayed as YYYY-MM-DD. Later in this section we will discuss how to change the display of date values.\nBelow we present two approaches to converting a column from character values to class Date.\nTIP: You can check the current class of a column with base R function class(), like class(linelist$date_onset).\n\nbase R\nas.Date() is the standard, base R function to convert an object or column to class Date (note capitalization of “D”).\nUse of as.Date() requires that:\n\nYou specify the existing format of the raw character date or the origin date if supplying dates as numbers (see section on Excel dates)\n\nIf used on a character column, all date values must have the same exact format (if this is not the case, try parse_date() from the parsedate package)\n\nFirst, check the class of your column with class() from base R. If you are unsure or confused about the class of your data (e.g. you see “POSIXct”, etc.) it can be easiest to first convert the column to class Character with as.character(), and then convert it to class Date.\nSecond, within the as.Date() function, use the format = argument to tell R the current format of the character date components - which characters refer to the month, the day, and the year, and how they are separated. If your values are already in one of R’s standard date formats (“YYYY-MM-DD” or “YYYY/MM/DD”) the format = argument is not necessary.\nTo format =, provide a character string (in quotes) that represents the current date format using the special “strptime” abbreviations below. For example, if your character dates are currently in the format “DD/MM/YYYY”, like “24/04/1968”, then you would use format = \"%d/%m/%Y\" to convert the values into dates. Putting the format in quotation marks is necessary. And don’t forget any slashes or dashes!\n\n# Convert to class date\nlinelist &lt;- linelist %&gt;% \n  mutate(date_onset = as.Date(date_of_onset, format = \"%d/%m/%Y\"))\n\nMost of the strptime abbreviations are listed below. You can see the complete list by running ?strptime.\n%d = Day number of month (5, 17, 28, etc.)\n%j = Day number of the year (Julian day 001-366)\n%a = Abbreviated weekday (Mon, Tue, Wed, etc.)\n%A = Full weekday (Monday, Tuesday, etc.) %w = Weekday number (0-6, Sunday is 0)\n%u = Weekday number (1-7, Monday is 1)\n%W = Week number (00-53, Monday is week start)\n%U = Week number (01-53, Sunday is week start)\n%m = Month number (e.g. 01, 02, 03, 04)\n%b = Abbreviated month (Jan, Feb, etc.)\n%B = Full month (January, February, etc.)\n%y = 2-digit year (e.g. 89)\n%Y = 4-digit year (e.g. 1989)\n%h = hours (24-hr clock)\n%m = minutes\n%s = seconds %z = offset from GMT\n%Z = Time zone (character)\nTIP: The format = argument of as.Date() is not telling R the format you want the dates to be, but rather how to identify the date parts as they are before you run the command.\nTIP: Be sure that in the format = argument you use the date-part separator (e.g. /, -, or space) that is present in your dates.\nOnce the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.\n\n\nlubridate\nConverting character objects to dates can be made easier by using the lubridate package. This is a tidyverse package designed to make working with dates and times more simple and consistent than in base R. For these reasons, lubridate is often considered the gold-standard package for dates and time, and is recommended whenever working with them.\nThe lubridate package provides several different helper functions designed to convert character objects to dates in an intuitive, and more lenient way than specifying the format in as.Date(). These functions are specific to the rough date format, but allow for a variety of separators, and synonyms for dates (e.g. 01 vs Jan vs January) - they are named after abbreviations of date formats.\n\n# install/load lubridate \npacman::p_load(lubridate)\n\nThe ymd() function flexibly converts date values supplied as year, then month, then day.\n\n# read date in year-month-day format\nymd(\"2020-10-11\")\n\n[1] \"2020-10-11\"\n\nymd(\"20201011\")\n\n[1] \"2020-10-11\"\n\n\nThe mdy() function flexibly converts date values supplied as month, then day, then year.\n\n# read date in month-day-year format\nmdy(\"10/11/2020\")\n\n[1] \"2020-10-11\"\n\nmdy(\"Oct 11 20\")\n\n[1] \"2020-10-11\"\n\n\nThe dmy() function flexibly converts date values supplied as day, then month, then year.\n\n# read date in day-month-year format\ndmy(\"11 10 2020\")\n\n[1] \"2020-10-11\"\n\ndmy(\"11 October 2020\")\n\n[1] \"2020-10-11\"\n\n\n\n\n\n\nIf using piping, the conversion of a character column to dates with lubridate might look like this:\n\nlinelist &lt;- linelist %&gt;%\n  mutate(date_onset = lubridate::dmy(date_onset))\n\nOnce complete, you can run class() to verify the class of the column\n\n# Check the class of the column\nclass(linelist$date_onset)  \n\nOnce the values are in class Date, R will by default display them in the standard format, which is YYYY-MM-DD.\nNote that the above functions work best with 4-digit years. 2-digit years can produce unexpected results, as lubridate attempts to guess the century.\nTo convert a 2-digit year into a 4-digit year (all in the same century) you can convert to class character and then combine the existing digits with a pre-fix using str_glue() from the stringr package (see Characters and strings). Then convert to date.\n\ntwo_digit_years &lt;- c(\"15\", \"15\", \"16\", \"17\")\nstr_glue(\"20{two_digit_years}\")\n\n2015\n2015\n2016\n2017\n\n\n\n\nCombine columns\nYou can use the lubridate functions make_date() and make_datetime() to combine multiple numeric columns into one date column. For example if you have numeric columns onset_day, onset_month, and onset_year in the data frame linelist:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(onset_date = make_date(year = onset_year, month = onset_month, day = onset_day))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#excel-dates",
    "href": "new_pages/dates.html#excel-dates",
    "title": "9  Working with dates",
    "section": "9.4 Excel dates",
    "text": "9.4 Excel dates\nIn the background, most software store dates as numbers. R stores dates from an origin of 1st January, 1970. Thus, if you run as.numeric(as.Date(\"1970-01-01)) you will get 0.\nMicrosoft Excel stores dates with an origin of either December 30, 1899 (Windows) or January 1, 1904 (Mac), depending on your operating system. See this Microsoft guidance for more information.\nExcel dates often import into R as these numeric values instead of as characters. If the dataset you imported from Excel shows dates as numbers or characters like “41369”… use as.Date() (or lubridate’s as_date() function) to convert, but instead of supplying a “format” as above, supply the Excel origin date to the argument origin =.\nThis will not work if the Excel date is stored in R as a character type, so be sure to ensure the number is class Numeric!\nNOTE: You should provide the origin date in R’s default date format (“YYYY-MM-DD”).\n\n# An example of providing the Excel 'origin date' when converting Excel number dates\ndata_cleaned &lt;- data %&gt;% \n  mutate(date_onset = as.numeric(date_onset)) %&gt;%   # ensure class is numeric\n  mutate(date_onset = as.Date(date_onset, origin = \"1899-12-30\")) # convert to date using Excel origin",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#messy-dates",
    "href": "new_pages/dates.html#messy-dates",
    "title": "9  Working with dates",
    "section": "9.5 Messy dates",
    "text": "9.5 Messy dates\nThe function parse_date() from the parsedate package attempts to read a “messy” date column containing dates in many different formats and convert the dates to a standard format. You can read more online about parse_date().\nFor example parse_date() would see a vector of the following character dates “03 Jan 2018”, “07/03/1982”, and “08/20/85” and convert them to class Date as: 2018-01-03, 1982-03-07, and 1985-08-20.\n\nparsedate::parse_date(c(\"03 January 2018\",\n                        \"07/03/1982\",\n                        \"08/20/85\"))\n\n[1] \"2018-01-03 UTC\" \"1982-07-03 UTC\" \"1985-08-20 UTC\"\n\n\n\n# An example using parse_date() on the column date_onset\nlinelist &lt;- linelist %&gt;%      \n  mutate(date_onset = parse_date(date_onset))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#working-with-date-time-class",
    "href": "new_pages/dates.html#working-with-date-time-class",
    "title": "9  Working with dates",
    "section": "9.6 Working with date-time class",
    "text": "9.6 Working with date-time class\nAs previously mentioned, R also supports a datetime class - a column that contains date and time information. As with the Date class, these often need to be converted from character objects to datetime objects.\n\nConvert dates with times\nA standard datetime object is formatted with the date first, which is followed by a time component - for example 01 Jan 2020, 16:30. As with dates, there are many ways this can be formatted, and there are numerous levels of precision (hours, minutes, seconds) that can be supplied.\nLuckily, lubridate helper functions also exist to help convert these strings to datetime objects. These functions are extensions of the date helper functions, with _h (only hours supplied), _hm (hours and minutes supplied), or _hms (hours, minutes, and seconds supplied) appended to the end (e.g. dmy_hms()). These can be used as shown:\nConvert datetime with only hours to datetime object\n\nymd_h(\"2020-01-01 16hrs\")\n\n[1] \"2020-01-01 16:00:00 UTC\"\n\nymd_h(\"2020-01-01 4PM\")\n\n[1] \"2020-01-01 16:00:00 UTC\"\n\n\nConvert datetime with hours and minutes to datetime object\n\ndmy_hm(\"01 January 2020 16:20\")\n\n[1] \"2020-01-01 16:20:00 UTC\"\n\n\nConvert datetime with hours, minutes, and seconds to datetime object\n\nmdy_hms(\"01 January 2020, 16:20:40\")\n\n[1] \"2020-01-20 16:20:40 UTC\"\n\n\nYou can supply time zone but it is ignored. See section later in this page on time zones.\n\nmdy_hms(\"01 January 2020, 16:20:40 PST\")\n\n[1] \"2020-01-20 16:20:40 UTC\"\n\n\nWhen working with a data frame, time and date columns can be combined to create a datetime column using str_glue() from stringr package and an appropriate lubridate function. See the page on Characters and strings for details on stringr.\nIn this example, the linelist data frame has a column in format “hours:minutes”. To convert this to a datetime we follow a few steps:\n\nCreate a “clean” time of admission column with missing values filled-in with the column median. We do this because lubridate won’t operate on missing values. Combine it with the column date_hospitalisation, and then use the function ymd_hm() to convert.\n\n\n# packages\npacman::p_load(tidyverse, lubridate, stringr)\n\n# time_admission is a column in hours:minutes\nlinelist &lt;- linelist %&gt;%\n  \n  # when time of admission is not given, assign the median admission time\n  mutate(\n    time_admission_clean = ifelse(\n      is.na(time_admission),         # if time is missing\n      median(time_admission),        # assign the median\n      time_admission                 # if not missing keep as is\n  ) %&gt;%\n  \n    # use str_glue() to combine date and time columns to create one character column\n    # and then use ymd_hm() to convert it to datetime\n  mutate(\n    date_time_of_admission = str_glue(\"{date_hospitalisation} {time_admission_clean}\") %&gt;% \n      ymd_hm()\n  )\n\n\n\nConvert times alone\nIf your data contain only a character time (hours and minutes), you can convert and manipulate them as times using strptime() from base R. For example, to get the difference between two of these times:\n\n# raw character times\ntime1 &lt;- \"13:45\" \ntime2 &lt;- \"15:20\"\n\n# Times converted to a datetime class\ntime1_clean &lt;- strptime(time1, format = \"%H:%M\")\ntime2_clean &lt;- strptime(time2, format = \"%H:%M\")\n\n# Difference is of class \"difftime\" by default, here converted to numeric hours \nas.numeric(time2_clean - time1_clean)   # difference in hours\n\n[1] 1.583333\n\n\nNote however that without a date value provided, it assumes the date is today. To combine a string date and a string time together see how to use stringr in the section just above. Read more about strptime() here.\nTo convert single-digit numbers to double-digits (e.g. to “pad” hours or minutes with leading zeros to achieve 2 digits), see this “Pad length” section of the Characters and strings page.\n\n\nExtract time\nYou can extract elements of a time with hour(), minute(), or second() from lubridate.\nHere is an example of extracting the hour, and then classifing by part of the day. We begin with the column time_admission, which is class Character in format “HH:MM”. First, the strptime() is used as described above to convert the characters to datetime class. Then, the hour is extracted with hour(), returning a number from 0-24. Finally, a column time_period is created using logic with case_when() to classify rows into Morning/Afternoon/Evening/Night based on their hour of admission.\n\nlinelist &lt;- linelist %&gt;%\n  mutate(hour_admit = hour(strptime(time_admission, format = \"%H:%M\"))) %&gt;%\n  mutate(time_period = case_when(\n    hour_admit &gt; 06 & hour_admit &lt; 12 ~ \"Morning\",\n    hour_admit &gt;= 12 & hour_admit &lt; 17 ~ \"Afternoon\",\n    hour_admit &gt;= 17 & hour_admit &lt; 21 ~ \"Evening\",\n    hour_admit &gt;=21 | hour_admit &lt;= 6 ~ \"Night\"))\n\nTo learn more about case_when() see the page on Cleaning data and core functions.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#working-with-dates",
    "href": "new_pages/dates.html#working-with-dates",
    "title": "9  Working with dates",
    "section": "9.7 Working with dates",
    "text": "9.7 Working with dates\nlubridate can also be used for a variety of other functions, such as extracting aspects of a date/datetime, performing date arithmetic, or calculating date intervals\nHere we define a date to use for the examples:\n\n# create object of class Date\nexample_date &lt;- ymd(\"2020-03-01\")\n\n\nExtract date components\nYou can extract common aspects such as month, day, weekday:\n\nmonth(example_date)  # month number\n\n[1] 3\n\nday(example_date)    # day (number) of the month\n\n[1] 1\n\nwday(example_date)   # day number of the week (1-7)\n\n[1] 1\n\n\nYou can also extract time components from a datetime object or column. This can be useful if you want to view the distribution of admission times.\n\nexample_datetime &lt;- ymd_hm(\"2020-03-01 14:45\")\n\nhour(example_datetime)     # extract hour\nminute(example_datetime)   # extract minute\nsecond(example_datetime)   # extract second\n\nThere are several options to retrieve weeks. See the section on Epidemiological weeks below.\nNote that if you are seeking to display a date a certain way (e.g. “Jan 2020” or “Thursday 20 March” or “Week 20, 1977”) you can do this more flexibly as described in the section on Date display.\n\n\nDate math\nYou can add certain numbers of days or weeks using their respective function from lubridate.\n\n# add 3 days to this date\nexample_date + days(3)\n\n[1] \"2020-03-04\"\n\n# add 7 weeks and subtract two days from this date\nexample_date + weeks(7) - days(2)\n\n[1] \"2020-04-17\"\n\n\n\n\nDate intervals\nThe difference between dates can be calculated by:\n\nEnsure both dates are of class date\n\nUse subtraction to return the “difftime” difference between the two dates\n\nIf necessary, convert the result to numeric class to perform subsequent mathematical calculations\n\nBelow the interval between two dates is calculated and displayed. You can find intervals by using the subtraction “minus” symbol on values that are class Date. Note, however that the class of the returned value is “difftime” as displayed below, and must be converted to numeric.\n\n# find the interval between this date and Feb 20 2020 \noutput &lt;- example_date - ymd(\"2020-02-20\")\noutput    # print\n\nTime difference of 10 days\n\nclass(output)\n\n[1] \"difftime\"\n\n\nTo do subsequent operations on a “difftime”, convert it to numeric with as.numeric().\nThis can all be brought together to work with data - for example:\n\npacman::p_load(lubridate, tidyverse)   # load packages\n\nlinelist &lt;- linelist %&gt;%\n  \n  # convert date of onset from character to date objects by specifying dmy format\n  mutate(date_onset = dmy(date_onset),\n         date_hospitalisation = dmy(date_hospitalisation)) %&gt;%\n  \n  # filter out all cases without onset in march\n  filter(month(date_onset) == 3) %&gt;%\n    \n  # find the difference in days between onset and hospitalisation\n  mutate(days_onset_to_hosp = date_hospitalisation - date_of_onset)\n\nIn a data frame context, if either of the above dates is missing, the operation will fail for that row. This will result in an NA instead of a numeric value. When using this column for calculations, be sure to set the na.rm = argument to TRUE. For example:\n\n# calculate the median number of days to hospitalisation for all cases where data are available\nmedian(linelist_delay$days_onset_to_hosp, na.rm = T)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#date-display",
    "href": "new_pages/dates.html#date-display",
    "title": "9  Working with dates",
    "section": "9.8 Date display",
    "text": "9.8 Date display\nOnce dates are the correct class, you often want them to display differently, for example to display as “Monday 05 January” instead of “2018-01-05”. You may also want to adjust the display in order to then group rows by the date elements displayed - for example to group by month-year.\n\nformat()\nAdjust date display with the base R function format(). This function accepts a character string (in quotes) specifying the desired output format in the “%” strptime abbreviations (the same syntax as used in as.Date()). Below are most of the common abbreviations.\nNote: using format() will convert the values to class Character, so this is generally used towards the end of an analysis or for display purposes only! You can see the complete list by running ?strptime.\n%d = Day number of month (5, 17, 28, etc.)\n%j = Day number of the year (Julian day 001-366)\n%a = Abbreviated weekday (Mon, Tue, Wed, etc.)\n%A = Full weekday (Monday, Tuesday, etc.)\n%w = Weekday number (0-6, Sunday is 0)\n%u = Weekday number (1-7, Monday is 1)\n%W = Week number (00-53, Monday is week start)\n%U = Week number (01-53, Sunday is week start)\n%m = Month number (e.g. 01, 02, 03, 04)\n%b = Abbreviated month (Jan, Feb, etc.)\n%B = Full month (January, February, etc.)\n%y = 2-digit year (e.g. 89)\n%Y = 4-digit year (e.g. 1989)\n%h = hours (24-hr clock)\n%m = minutes\n%s = seconds\n%z = offset from GMT\n%Z = Time zone (character)\nAn example of formatting today’s date:\n\n# today's date, with formatting\nformat(Sys.Date(), format = \"%d %B %Y\")\n\n[1] \"19 June 2024\"\n\n# easy way to get full date and time (default formatting)\ndate()\n\n[1] \"Wed Jun 19 12:27:20 2024\"\n\n# formatted combined date, time, and time zone using str_glue() function\nstr_glue(\"{format(Sys.Date(), format = '%A, %B %d %Y, %z  %Z, ')}{format(Sys.time(), format = '%H:%M:%S')}\")\n\nWednesday, June 19 2024, +0000  UTC, 12:27:20\n\n# Using format to display weeks\nformat(Sys.Date(), \"%Y Week %W\")\n\n[1] \"2024 Week 25\"\n\n\nNote that if using str_glue(), be aware of that within the expected double quotes ” you should only use single quotes (as above).\n\n\nMonth-Year\nTo convert a Date column to Month-year format, we suggest you use the function as.yearmon() from the zoo package. This converts the date to class “yearmon” and retains the proper ordering. In contrast, using format(column, \"%Y %B\") will convert to class Character and will order the values alphabetically (incorrectly).\nBelow, a new column yearmonth is created from the column date_onset, using the as.yearmon() function. The default (correct) ordering of the resulting values are shown in the table.\n\n# create new column \ntest_zoo &lt;- linelist %&gt;% \n     mutate(yearmonth = zoo::as.yearmon(date_onset))\n\n# print table\ntable(test_zoo$yearmon)\n\n\nApr 2014 May 2014 Jun 2014 Jul 2014 Aug 2014 Sep 2014 Oct 2014 Nov 2014 \n       7       64      100      226      528     1070     1112      763 \nDec 2014 Jan 2015 Feb 2015 Mar 2015 Apr 2015 \n     562      431      306      277      186 \n\n\nIn contrast, you can see how only using format() does achieve the desired display format, but not the correct ordering.\n\n# create new column\ntest_format &lt;- linelist %&gt;% \n     mutate(yearmonth = format(date_onset, \"%b %Y\"))\n\n# print table\ntable(test_format$yearmon)\n\n\nApr 2014 Apr 2015 Aug 2014 Dec 2014 Feb 2015 Jan 2015 Jul 2014 Jun 2014 \n       7      186      528      562      306      431      226      100 \nMar 2015 May 2014 Nov 2014 Oct 2014 Sep 2014 \n     277       64      763     1112     1070 \n\n\nNote: if you are working within a ggplot() and want to adjust how dates are displayed only, it may be sufficient to provide a strptime format to the date_labels = argument in scale_x_date() - you can use \"%b %Y\" or \"%Y %b\". See the ggplot tips page.\nzoo also offers the function as.yearqtr(), and you can use scale_x_yearmon() when using ggplot().",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#dates_epi_wks",
    "href": "new_pages/dates.html#dates_epi_wks",
    "title": "9  Working with dates",
    "section": "9.9 Epidemiological weeks",
    "text": "9.9 Epidemiological weeks\n\nlubridate\nSee the page on Grouping data for more extensive examples of grouping data by date. Below we briefly describe grouping data by weeks.\nWe generally recommend using the floor_date() function from lubridate, with the argument unit = \"week\". This rounds the date down to the “start” of the week, as defined by the argument week_start =. The default week start is 1 (for Mondays) but you can specify any day of the week as the start (e.g. 7 for Sundays). floor_date() is versitile and can be used to round down to other time units by setting unit = to “second”, “minute”, “hour”, “day”, “month”, or “year”.\nThe returned value is the start date of the week, in Date class. Date class is useful when plotting the data, as it will be easily recognized and ordered correctly by ggplot().\nIf you are only interested in adjusting dates to display by week in a plot, see the section in this page on Date display. For example when plotting an epicurve you can format the date display by providing the desired strptime “%” nomenclature. For example, use “%Y-%W” or “%Y-%U” to return the year and week number (given Monday or Sunday week start, respectively).\n\n\nWeekly counts\nSee the page on Grouping data for a thorough explanation of grouping data with count(), group_by(), and summarise(). A brief example is below.\n\nCreate a new ‘week’ column with mutate(), using floor_date() with unit = \"week\"\n\nGet counts of rows (cases) per week with count(); filter out any cases with missing date\n\nFinish with complete() from tidyr to ensure that all weeks appear in the data - even those with no rows/cases. By default the count values for any “new” rows are NA, but you can make them 0 with the fill = argument, which expects a named list (below, n is the name of the counts column).\n\n\n# Make aggregated dataset of weekly case counts\nweekly_counts &lt;- linelist %&gt;% \n  drop_na(date_onset) %&gt;%             # remove cases missing onset date\n  mutate(weekly_cases = floor_date(   # make new column, week of onset\n    date_onset,\n    unit = \"week\")) %&gt;%            \n  count(weekly_cases) %&gt;%           # group data by week and count rows per group (creates column 'n')\n  tidyr::complete(                  # ensure all weeks are present, even those with no cases reported\n    weekly_cases = seq.Date(          # re-define the \"weekly_cases\" column as a complete sequence,\n      from = min(weekly_cases),       # from the minimum date\n      to = max(weekly_cases),         # to the maxiumum date\n      by = \"week\"),                   # by weeks\n    fill = list(n = 0))             # fill-in NAs in the n counts column with 0\n\nHere are the first rows of the resulting data frame:\n\n\n\n\n\n\n\n\nEpiweek alternatives\nNote that lubridate also has functions week(), epiweek(), and isoweek(), each of which has slightly different start dates and other nuances. Generally speaking though, floor_date() should be all that you need. Read the details for these functions by entering ?week into the console or reading the documentation here.\nYou might consider using the package aweek to set epidemiological weeks. You can read more about it on the RECON website. It has the functions date2week() and week2date() in which you can set the week start day with week_start = \"Monday\". This package is easiest if you want “week”-style outputs (e.g. “2020-W12”). Another advantage of aweek is that when date2week() is applied to a date column, the returned column (week format) is automatically of class Factor and includes levels for all weeks in the time span (this avoids the extra step of complete() described above). However, aweek does not have the functionality to round dates to other time units such as months, years, etc.\nAnother alternative for time series which also works well to show a a “week” format (“2020 W12”) is yearweek() from the package tsibble, as demonstrated in the page on Time series and outbreak detection.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#converting-datestime-zones",
    "href": "new_pages/dates.html#converting-datestime-zones",
    "title": "9  Working with dates",
    "section": "9.10 Converting dates/time zones",
    "text": "9.10 Converting dates/time zones\nWhen data is present in different time time zones, it can often be important to standardise this data in a unified time zone. This can present a further challenge, as the time zone component of data must be coded manually in most cases.\nIn R, each datetime object has a timezone component. By default, all datetime objects will carry the local time zone for the computer being used - this is generally specific to a location rather than a named timezone, as time zones will often change in locations due to daylight savings time. It is not possible to accurately compensate for time zones without a time component of a date, as the event a date column represents cannot be attributed to a specific time, and therefore time shifts measured in hours cannot be reasonably accounted for.\nTo deal with time zones, there are a number of helper functions in lubridate that can be used to change the time zone of a datetime object from the local time zone to a different time zone. Time zones are set by attributing a valid tz database time zone to the datetime object. A list of these can be found here - if the location you are using data from is not on this list, nearby large cities in the time zone are available and serve the same purpose.\nhttps://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n\n# assign the current time to a column\ntime_now &lt;- Sys.time()\ntime_now\n\n[1] \"2024-06-19 12:27:21 CEST\"\n\n# use with_tz() to assign a new timezone to the column, while CHANGING the clock time\ntime_london_real &lt;- with_tz(time_now, \"Europe/London\")\n\n# use force_tz() to assign a new timezone to the column, while KEEPING the clock time\ntime_london_local &lt;- force_tz(time_now, \"Europe/London\")\n\n\n# note that as long as the computer that was used to run this code is NOT set to London time,\n# there will be a difference in the times \n# (the number of hours difference from the computers time zone to london)\ntime_london_real - time_london_local\n\nTime difference of -1 hours\n\n\nThis may seem largely abstract, and is often not needed if the user isn’t working across time zones.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#lagging-and-leading-calculations",
    "href": "new_pages/dates.html#lagging-and-leading-calculations",
    "title": "9  Working with dates",
    "section": "9.11 Lagging and leading calculations",
    "text": "9.11 Lagging and leading calculations\nlead() and lag() are functions from the dplyr package which help find previous (lagged) or subsequent (leading) values in a vector - typically a numeric or date vector. This is useful when doing calculations of change/difference between time units.\nLet’s say you want to calculate the difference in cases between a current week and the previous one. The data are initially provided in weekly counts as shown below.\n\n\n\n\n\n\nWhen using lag() or lead() the order of rows in the dataframe is very important! - pay attention to whether your dates/numbers are ascending or descending\nFirst, create a new column containing the value of the previous (lagged) week.\n\nControl the number of units back/forward with n = (must be a non-negative integer)\n\nUse default = to define the value placed in non-existing rows (e.g. the first row for which there is no lagged value). By default this is NA.\n\nUse order_by = TRUE if your the rows are not ordered by your reference column\n\n\ncounts &lt;- counts %&gt;% \n  mutate(cases_prev_wk = lag(cases_wk, n = 1))\n\n\n\n\n\n\n\nNext, create a new column which is the difference between the two cases columns:\n\ncounts &lt;- counts %&gt;% \n  mutate(cases_prev_wk = lag(cases_wk, n = 1),\n         case_diff = cases_wk - cases_prev_wk)\n\n\n\n\n\n\n\nYou can read more about lead() and lag() in the documentation here or by entering ?lag in your console.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/dates.html#resources",
    "href": "new_pages/dates.html#resources",
    "title": "9  Working with dates",
    "section": "9.12 Resources",
    "text": "9.12 Resources\nlubridate tidyverse page\nlubridate RStudio cheatsheet\nR for Data Science page on dates and times\nOnline tutorial Date formats",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Working with dates</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html",
    "href": "new_pages/characters_strings.html",
    "title": "10  Characters and strings",
    "section": "",
    "text": "10.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#preparation",
    "href": "new_pages/characters_strings.html#preparation",
    "title": "10  Characters and strings",
    "section": "",
    "text": "Load packages\nInstall or load the stringr and other tidyverse packages.\n\n# install/load packages\npacman::p_load(\n  stringr,    # many functions for handling strings\n  tidyverse,  # for optional data manipulation\n  tools)      # alternative for converting to title case\n\n\n\nImport data\nIn this page we will occassionally reference the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export(importing.qmd) page for details).\n\n# import case linelist \nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#unite-split-and-arrange",
    "href": "new_pages/characters_strings.html#unite-split-and-arrange",
    "title": "10  Characters and strings",
    "section": "10.2 Unite, split, and arrange",
    "text": "10.2 Unite, split, and arrange\nThis section covers:\n\nUsing str_c(), str_glue(), and unite() to combine strings\n\nUsing str_order() to arrange strings\n\nUsing str_split() and separate() to split strings\n\n\n\nCombine strings\nTo combine or concatenate multiple strings into one string, we suggest using str_c from stringr. If you have distinct character values to combine, simply provide them as unique arguments, separated by commas.\n\nstr_c(\"String1\", \"String2\", \"String3\")\n\n[1] \"String1String2String3\"\n\n\nThe argument sep = inserts a character value between each of the arguments you provided (e.g. inserting a comma, space, or newline \"\\n\")\n\nstr_c(\"String1\", \"String2\", \"String3\", sep = \", \")\n\n[1] \"String1, String2, String3\"\n\n\nThe argument collapse = is relevant if you are inputting multiple vectors as arguments to str_c(). It is used to separate the elements of what would be an output vector, such that the output vector only has one long character element.\nThe example below shows the combination of two vectors into one (first names and last names). Another similar example might be jurisdictions and their case counts. In this example:\n\nThe sep = value appears between each first and last name\n\nThe collapse = value appears between each person\n\n\nfirst_names &lt;- c(\"abdul\", \"fahruk\", \"janice\") \nlast_names  &lt;- c(\"hussein\", \"akinleye\", \"okeke\")\n\n# sep displays between the respective input strings, while collapse displays between the elements produced\nstr_c(first_names, last_names, sep = \" \", collapse = \";  \")\n\n[1] \"abdul hussein;  fahruk akinleye;  janice okeke\"\n\n\nNote: Depending on your desired display context, when printing such a combined string with newlines, you may need to wrap the whole phrase in cat() for the newlines to print properly:\n\n# For newlines to print correctly, the phrase may need to be wrapped in cat()\ncat(str_c(first_names, last_names, sep = \" \", collapse = \";\\n\"))\n\nabdul hussein;\nfahruk akinleye;\njanice okeke\n\n\n\n\n\nDynamic strings\nUse str_glue() to insert dynamic R code into a string. This is a very useful function for creating dynamic plot captions, as demonstrated below.\n\nAll content goes between double quotation marks str_glue(\"\")\n\nAny dynamic code or references to pre-defined values are placed within curly brackets {} within the double quotation marks. There can be many curly brackets in the same str_glue() command.\n\nTo display character quotes ’’, use single quotes within the surrounding double quotes (e.g. when providing date format - see example below)\n\nTip: You can use \\n to force a new line\n\nTip: You use format() to adjust date display, and use Sys.Date() to display the current date\n\nA simple example, of a dynamic plot caption:\n\nstr_glue(\"Data include {nrow(linelist)} cases and are current to {format(Sys.Date(), '%d %b %Y')}.\")\n\nData include 5888 cases and are current to 19 Jun 2024.\n\n\nAn alternative format is to use placeholders within the brackets and define the code in separate arguments at the end of the str_glue() function, as below. This can improve code readability if the text is long.\n\nstr_glue(\"Linelist as of {current_date}.\\nLast case hospitalized on {last_hospital}.\\n{n_missing_onset} cases are missing date of onset and not shown\",\n         current_date = format(Sys.Date(), '%d %b %Y'),\n         last_hospital = format(as.Date(max(linelist$date_hospitalisation, na.rm=T)), '%d %b %Y'),\n         n_missing_onset = nrow(linelist %&gt;% filter(is.na(date_onset)))\n         )\n\nLinelist as of 19 Jun 2024.\nLast case hospitalized on 30 Apr 2015.\n256 cases are missing date of onset and not shown\n\n\nPulling from a data frame\nSometimes, it is useful to pull data from a data frame and have it pasted together in sequence. Below is an example data frame. We will use it to to make a summary statement about the jurisdictions and the new and total case counts.\n\n# make case data frame\ncase_table &lt;- data.frame(\n  zone        = c(\"Zone 1\", \"Zone 2\", \"Zone 3\", \"Zone 4\", \"Zone 5\"),\n  new_cases   = c(3, 0, 7, 0, 15),\n  total_cases = c(40, 4, 25, 10, 103)\n  )\n\n\n\n\n\n\n\nUse str_glue_data(), which is specially made for taking data from data frame rows:\n\ncase_table %&gt;% \n  str_glue_data(\"{zone}: {new_cases} ({total_cases} total cases)\")\n\nZone 1: 3 (40 total cases)\nZone 2: 0 (4 total cases)\nZone 3: 7 (25 total cases)\nZone 4: 0 (10 total cases)\nZone 5: 15 (103 total cases)\n\n\nCombine strings across rows\nIf you are trying to “roll-up” values in a data frame column, e.g. combine values from multiple rows into just one row by pasting them together with a separator, see the section of the De-duplication page on “rolling-up” values.\nData frame to one line\nYou can make the statement appear in one line using str_c() (specifying the data frame and column names), and providing sep = and collapse = arguments.\n\nstr_c(case_table$zone, case_table$new_cases, sep = \" = \", collapse = \";  \")\n\n[1] \"Zone 1 = 3;  Zone 2 = 0;  Zone 3 = 7;  Zone 4 = 0;  Zone 5 = 15\"\n\n\nYou could add the pre-fix text “New Cases:” to the beginning of the statement by wrapping with a separate str_c() (if “New Cases:” was within the original str_c() it would appear multiple times).\n\nstr_c(\"New Cases: \", str_c(case_table$zone, case_table$new_cases, sep = \" = \", collapse = \";  \"))\n\n[1] \"New Cases: Zone 1 = 3;  Zone 2 = 0;  Zone 3 = 7;  Zone 4 = 0;  Zone 5 = 15\"\n\n\n\n\nUnite columns\nWithin a data frame, bringing together character values from multiple columns can be achieved with unite() from tidyr. This is the opposite of separate().\nProvide the name of the new united column. Then provide the names of the columns you wish to unite.\n\nBy default, the separator used in the united column is underscore _, but this can be changed with the sep = argument.\n\nremove = removes the input columns from the data frame (TRUE by default)\n\nna.rm = removes missing values while uniting (FALSE by default)\n\nBelow, we define a mini-data frame to demonstrate with:\n\ndf &lt;- data.frame(\n  case_ID = c(1:6),\n  symptoms  = c(\"jaundice, fever, chills\",     # patient 1\n                \"chills, aches, pains\",        # patient 2 \n                \"fever\",                       # patient 3\n                \"vomiting, diarrhoea\",         # patient 4\n                \"bleeding from gums, fever\",   # patient 5\n                \"rapid pulse, headache\"),      # patient 6\n  outcome = c(\"Recover\", \"Death\", \"Death\", \"Recover\", \"Recover\", \"Recover\"))\n\n\ndf_split &lt;- separate(df, symptoms, into = c(\"sym_1\", \"sym_2\", \"sym_3\"), extra = \"merge\")\n\nWarning: Expected 3 pieces. Missing pieces filled with `NA` in 2 rows [3, 4].\n\n\nHere is the example data frame:\n\n\n\n\n\n\nBelow, we unite the three symptom columns:\n\ndf_split %&gt;% \n  unite(\n    col = \"all_symptoms\",         # name of the new united column\n    c(\"sym_1\", \"sym_2\", \"sym_3\"), # columns to unite\n    sep = \", \",                   # separator to use in united column\n    remove = TRUE,                # if TRUE, removes input cols from the data frame\n    na.rm = TRUE                  # if TRUE, missing values are removed before uniting\n  )\n\n  case_ID                all_symptoms outcome\n1       1     jaundice, fever, chills Recover\n2       2        chills, aches, pains   Death\n3       3                       fever   Death\n4       4         vomiting, diarrhoea Recover\n5       5 bleeding, from, gums, fever Recover\n6       6      rapid, pulse, headache Recover\n\n\n\n\n\nSplit\nTo split a string based on a pattern, use str_split(). It evaluates the string(s) and returns a list of character vectors consisting of the newly-split values.\nThe simple example below evaluates one string and splits it into three. By default it returns an object of class list with one element (a character vector) for each string initially provided. If simplify = TRUE it returns a character matrix.\nIn this example, one string is provided, and the function returns a list with one element - a character vector with three values.\n\nstr_split(string = \"jaundice, fever, chills\",\n          pattern = \",\")\n\n[[1]]\n[1] \"jaundice\" \" fever\"   \" chills\" \n\n\nIf the output is saved, you can then access the nth split value with bracket syntax. To access a specific value you can use syntax like this: the_returned_object[[1]][2], which would access the second value from the first evaluated string (“fever”). See the R basics page for more detail on accessing elements.\n\npt1_symptoms &lt;- str_split(\"jaundice, fever, chills\", \",\")\n\npt1_symptoms[[1]][2]  # extracts 2nd value from 1st (and only) element of the list\n\n[1] \" fever\"\n\n\nIf multiple strings are provided by str_split(), there will be more than one element in the returned list.\n\nsymptoms &lt;- c(\"jaundice, fever, chills\",     # patient 1\n              \"chills, aches, pains\",        # patient 2 \n              \"fever\",                       # patient 3\n              \"vomiting, diarrhoea\",         # patient 4\n              \"bleeding from gums, fever\",   # patient 5\n              \"rapid pulse, headache\")       # patient 6\n\nstr_split(symptoms, \",\")                     # split each patient's symptoms\n\n[[1]]\n[1] \"jaundice\" \" fever\"   \" chills\" \n\n[[2]]\n[1] \"chills\" \" aches\" \" pains\"\n\n[[3]]\n[1] \"fever\"\n\n[[4]]\n[1] \"vomiting\"   \" diarrhoea\"\n\n[[5]]\n[1] \"bleeding from gums\" \" fever\"            \n\n[[6]]\n[1] \"rapid pulse\" \" headache\"  \n\n\nTo return a “character matrix” instead, which may be useful if creating data frame columns, set the argument simplify = TRUE as shown below:\n\nstr_split(symptoms, \",\", simplify = TRUE)\n\n     [,1]                 [,2]         [,3]     \n[1,] \"jaundice\"           \" fever\"     \" chills\"\n[2,] \"chills\"             \" aches\"     \" pains\" \n[3,] \"fever\"              \"\"           \"\"       \n[4,] \"vomiting\"           \" diarrhoea\" \"\"       \n[5,] \"bleeding from gums\" \" fever\"     \"\"       \n[6,] \"rapid pulse\"        \" headache\"  \"\"       \n\n\nYou can also adjust the number of splits to create with the n = argument. For example, this restricts the number of splits to 2. Any further commas remain within the second values.\n\nstr_split(symptoms, \",\", simplify = TRUE, n = 2)\n\n     [,1]                 [,2]            \n[1,] \"jaundice\"           \" fever, chills\"\n[2,] \"chills\"             \" aches, pains\" \n[3,] \"fever\"              \"\"              \n[4,] \"vomiting\"           \" diarrhoea\"    \n[5,] \"bleeding from gums\" \" fever\"        \n[6,] \"rapid pulse\"        \" headache\"     \n\n\nNote - the same outputs can be achieved with str_split_fixed(), in which you do not give the simplify argument, but must instead designate the number of columns (n).\n\nstr_split_fixed(symptoms, \",\", n = 2)\n\n\n\nSplit columns\nIf you are trying to split data frame column, it is best to use the separate() function from dplyr. It is used to split one character column into other columns.\nLet’s say we have a simple data frame df (defined and united in the unite section) containing a case_ID column, one character column with many symptoms, and one outcome column. Our goal is to separate the symptoms column into many columns - each one containing one symptom.\n\n\n\n\n\n\nAssuming the data are piped into separate(), first provide the column to be separated. Then provide into = as a vector c( ) containing the new columns names, as shown below.\n\nsep = the separator, can be a character, or a number (interpreted as the character position to split at)\nremove = FALSE by default, removes the input column\n\nconvert = FALSE by default, will cause string “NA”s to become NA\n\nextra = this controls what happens if there are more values created by the separation than new columns named.\n\nextra = \"warn\" means you will see a warning but it will drop excess values (the default)\n\nextra = \"drop\" means the excess values will be dropped with no warning\n\nextra = \"merge\" will only split to the number of new columns listed in into - this setting will preserve all your data\n\n\nAn example with extra = \"merge\" is below - no data is lost. Two new columns are defined but any third symptoms are left in the second new column:\n\n# third symptoms combined into second new column\ndf %&gt;% \n  separate(symptoms, into = c(\"sym_1\", \"sym_2\"), sep=\",\", extra = \"merge\")\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [3].\n\n\n  case_ID              sym_1          sym_2 outcome\n1       1           jaundice  fever, chills Recover\n2       2             chills   aches, pains   Death\n3       3              fever           &lt;NA&gt;   Death\n4       4           vomiting      diarrhoea Recover\n5       5 bleeding from gums          fever Recover\n6       6        rapid pulse       headache Recover\n\n\nWhen the default extra = \"drop\" is used below, a warning is given but the third symptoms are lost:\n\n# third symptoms are lost\ndf %&gt;% \n  separate(symptoms, into = c(\"sym_1\", \"sym_2\"), sep=\",\")\n\nWarning: Expected 2 pieces. Additional pieces discarded in 2 rows [1, 2].\n\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [3].\n\n\n  case_ID              sym_1      sym_2 outcome\n1       1           jaundice      fever Recover\n2       2             chills      aches   Death\n3       3              fever       &lt;NA&gt;   Death\n4       4           vomiting  diarrhoea Recover\n5       5 bleeding from gums      fever Recover\n6       6        rapid pulse   headache Recover\n\n\nCAUTION: If you do not provide enough into values for the new columns, your data may be truncated.\n\n\n\nArrange alphabetically\nSeveral strings can be sorted by alphabetical order. str_order() returns the order, while str_sort() returns the strings in that order.\n\n# strings\nhealth_zones &lt;- c(\"Alba\", \"Takota\", \"Delta\")\n\n# return the alphabetical order\nstr_order(health_zones)\n\n[1] 1 3 2\n\n# return the strings in alphabetical order\nstr_sort(health_zones)\n\n[1] \"Alba\"   \"Delta\"  \"Takota\"\n\n\nTo use a different alphabet, add the argument locale =. See the full list of locales by entering stringi::stri_locale_list() in the R console.\n\n\n\nbase R functions\nIt is common to see base R functions paste() and paste0(), which concatenate vectors after converting all parts to character. They act similarly to str_c() but the syntax is arguably more complicated - in the parentheses each part is separated by a comma. The parts are either character text (in quotes) or pre-defined code objects (no quotes). For example:\n\nn_beds &lt;- 10\nn_masks &lt;- 20\n\npaste0(\"Regional hospital needs \", n_beds, \" beds and \", n_masks, \" masks.\")\n\n[1] \"Regional hospital needs 10 beds and 20 masks.\"\n\n\nsep = and collapse = arguments can be specified. paste() is simply paste0() with a default sep = \" \" (one space).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#clean-and-standardise",
    "href": "new_pages/characters_strings.html#clean-and-standardise",
    "title": "10  Characters and strings",
    "section": "10.3 Clean and standardise",
    "text": "10.3 Clean and standardise\n\n\nChange case\nOften one must alter the case/capitalization of a string value, for example names of jursidictions. Use str_to_upper(), str_to_lower(), and str_to_title(), from stringr, as shown below:\n\nstr_to_upper(\"California\")\n\n[1] \"CALIFORNIA\"\n\nstr_to_lower(\"California\")\n\n[1] \"california\"\n\n\nUsing *base** R, the above can also be achieved with toupper(), tolower().\nTitle case\nTransforming the string so each word is capitalized can be achieved with str_to_title():\n\nstr_to_title(\"go to the US state of california \")\n\n[1] \"Go To The Us State Of California \"\n\n\nUse toTitleCase() from the tools package to achieve more nuanced capitalization (words like “to”, “the”, and “of” are not capitalized).\n\ntools::toTitleCase(\"This is the US state of california\")\n\n[1] \"This is the US State of California\"\n\n\nYou can also use str_to_sentence(), which capitalizes only the first letter of the string.\n\nstr_to_sentence(\"the patient must be transported\")\n\n[1] \"The patient must be transported\"\n\n\n\n\nPad length\nUse str_pad() to add characters to a string, to a minimum length. By default spaces are added, but you can also pad with other characters using the pad = argument.\n\n# ICD codes of differing length\nICD_codes &lt;- c(\"R10.13\",\n               \"R10.819\",\n               \"R17\")\n\n# ICD codes padded to 7 characters on the right side\nstr_pad(ICD_codes, 7, \"right\")\n\n[1] \"R10.13 \" \"R10.819\" \"R17    \"\n\n# Pad with periods instead of spaces\nstr_pad(ICD_codes, 7, \"right\", pad = \".\")\n\n[1] \"R10.13.\" \"R10.819\" \"R17....\"\n\n\nFor example, to pad numbers with leading zeros (such as for hours or minutes), you can pad the number to minimum length of 2 with pad = \"0\".\n\n# Add leading zeros to two digits (e.g. for times minutes/hours)\nstr_pad(\"4\", 2, pad = \"0\") \n\n[1] \"04\"\n\n# example using a numeric column named \"hours\"\n# hours &lt;- str_pad(hours, 2, pad = \"0\")\n\n\n\nTruncate\nstr_trunc() sets a maximum length for each string. If a string exceeds this length, it is truncated (shortened) and an ellipsis (…) is included to indicate that the string was previously longer. Note that the ellipsis is counted in the length. The ellipsis characters can be changed with the argument ellipsis =. The optional side = argument specifies which where the ellipsis will appear within the truncated string (“left”, “right”, or “center”).\n\noriginal &lt;- \"Symptom onset on 4/3/2020 with vomiting\"\nstr_trunc(original, 10, \"center\")\n\n[1] \"Symp...ing\"\n\n\n\n\nStandardize length\nUse str_trunc() to set a maximum length, and then use str_pad() to expand the very short strings to that truncated length. In the example below, 6 is set as the maximum length (one value is truncated), and then one very short value is padded to achieve length of 6.\n\n# ICD codes of differing length\nICD_codes   &lt;- c(\"R10.13\",\n                 \"R10.819\",\n                 \"R17\")\n\n# truncate to maximum length of 6\nICD_codes_2 &lt;- str_trunc(ICD_codes, 6)\nICD_codes_2\n\n[1] \"R10.13\" \"R10...\" \"R17\"   \n\n# expand to minimum length of 6\nICD_codes_3 &lt;- str_pad(ICD_codes_2, 6, \"right\")\nICD_codes_3\n\n[1] \"R10.13\" \"R10...\" \"R17   \"\n\n\n\n\nRemove leading/trailing whitespace\nUse str_trim() to remove spaces, newlines (\\n) or tabs (\\t) on sides of a string input. Add \"right\" \"left\", or \"both\" to the command to specify which side to trim (e.g. str_trim(x, \"right\").\n\n# ID numbers with excess spaces on right\nIDs &lt;- c(\"provA_1852  \", # two excess spaces\n         \"provA_2345\",   # zero excess spaces\n         \"provA_9460 \")  # one excess space\n\n# IDs trimmed to remove excess spaces on right side only\nstr_trim(IDs)\n\n[1] \"provA_1852\" \"provA_2345\" \"provA_9460\"\n\n\n\n\nRemove repeated whitespace within\nUse str_squish() to remove repeated spaces that appear inside a string. For example, to convert double spaces into single spaces. It also removes spaces, newlines, or tabs on the outside of the string like str_trim().\n\n# original contains excess spaces within string\nstr_squish(\"  Pt requires   IV saline\\n\") \n\n[1] \"Pt requires IV saline\"\n\n\nEnter ?str_trim, ?str_pad in your R console to see further details.\n\n\nWrap into paragraphs\nUse str_wrap() to wrap a long unstructured text into a structured paragraph with fixed line length. Provide the ideal character length for each line, and it applies an algorithm to insert newlines (\\n) within the paragraph, as seen in the example below.\n\npt_course &lt;- \"Symptom onset 1/4/2020 vomiting chills fever. Pt saw traditional healer in home village on 2/4/2020. On 5/4/2020 pt symptoms worsened and was admitted to Lumta clinic. Sample was taken and pt was transported to regional hospital on 6/4/2020. Pt died at regional hospital on 7/4/2020.\"\n\nstr_wrap(pt_course, 40)\n\n[1] \"Symptom onset 1/4/2020 vomiting chills\\nfever. Pt saw traditional healer in\\nhome village on 2/4/2020. On 5/4/2020\\npt symptoms worsened and was admitted\\nto Lumta clinic. Sample was taken and pt\\nwas transported to regional hospital on\\n6/4/2020. Pt died at regional hospital\\non 7/4/2020.\"\n\n\nThe base function cat() can be wrapped around the above command in order to print the output, displaying the new lines added.\n\ncat(str_wrap(pt_course, 40))\n\nSymptom onset 1/4/2020 vomiting chills\nfever. Pt saw traditional healer in\nhome village on 2/4/2020. On 5/4/2020\npt symptoms worsened and was admitted\nto Lumta clinic. Sample was taken and pt\nwas transported to regional hospital on\n6/4/2020. Pt died at regional hospital\non 7/4/2020.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#handle-by-position",
    "href": "new_pages/characters_strings.html#handle-by-position",
    "title": "10  Characters and strings",
    "section": "10.4 Handle by position",
    "text": "10.4 Handle by position\n\nExtract by character position\nUse str_sub() to return only a part of a string. The function takes three main arguments:\n\nthe character vector(s)\n\nstart position\n\nend position\n\nA few notes on position numbers:\n\nIf a position number is positive, the position is counted starting from the left end of the string.\n\nIf a position number is negative, it is counted starting from the right end of the string.\n\nPosition numbers are inclusive.\n\nPositions extending beyond the string will be truncated (removed).\n\nBelow are some examples applied to the string “pneumonia”:\n\n# start and end third from left (3rd letter from left)\nstr_sub(\"pneumonia\", 3, 3)\n\n[1] \"e\"\n\n# 0 is not present\nstr_sub(\"pneumonia\", 0, 0)\n\n[1] \"\"\n\n# 6th from left, to the 1st from right\nstr_sub(\"pneumonia\", 6, -1)\n\n[1] \"onia\"\n\n# 5th from right, to the 2nd from right\nstr_sub(\"pneumonia\", -5, -2)\n\n[1] \"moni\"\n\n# 4th from left to a position outside the string\nstr_sub(\"pneumonia\", 4, 15)\n\n[1] \"umonia\"\n\n\n\n\nExtract by word position\nTo extract the nth ‘word’, use word(), also from stringr. Provide the string(s), then the first word position to extract, and the last word position to extract.\nBy default, the separator between ‘words’ is assumed to be a space, unless otherwise indicated with sep = (e.g. sep = \"_\" when words are separated by underscores.\n\n# strings to evaluate\nchief_complaints &lt;- c(\"I just got out of the hospital 2 days ago, but still can barely breathe.\",\n                      \"My stomach hurts\",\n                      \"Severe ear pain\")\n\n# extract 1st to 3rd words of each string\nword(chief_complaints, start = 1, end = 3, sep = \" \")\n\n[1] \"I just got\"       \"My stomach hurts\" \"Severe ear pain\" \n\n\n\n\nReplace by character position\nstr_sub() paired with the assignment operator (&lt;-) can be used to modify a part of a string:\n\nword &lt;- \"pneumonia\"\n\n# convert the third and fourth characters to X \nstr_sub(word, 3, 4) &lt;- \"XX\"\n\n# print\nword\n\n[1] \"pnXXmonia\"\n\n\nAn example applied to multiple strings (e.g. a column). Note the expansion in length of “HIV”.\n\nwords &lt;- c(\"pneumonia\", \"tubercolosis\", \"HIV\")\n\n# convert the third and fourth characters to X \nstr_sub(words, 3, 4) &lt;- \"XX\"\n\nwords\n\n[1] \"pnXXmonia\"    \"tuXXrcolosis\" \"HIXX\"        \n\n\n\n\nEvaluate length\n\nstr_length(\"abc\")\n\n[1] 3\n\n\nAlternatively, use nchar() from base R",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#patterns",
    "href": "new_pages/characters_strings.html#patterns",
    "title": "10  Characters and strings",
    "section": "10.5 Patterns",
    "text": "10.5 Patterns\nMany stringr functions work to detect, locate, extract, match, replace, and split based on a specified pattern.\n\n\nDetect a pattern\nUse str_detect() as below to detect presence/absence of a pattern within a string. First provide the string or vector to search in (string =), and then the pattern to look for (pattern =). Note that by default the search is case sensitive!\n\nstr_detect(string = \"primary school teacher\", pattern = \"teach\")\n\n[1] TRUE\n\n\nThe argument negate = can be included and set to TRUE if you want to know if the pattern is NOT present.\n\nstr_detect(string = \"primary school teacher\", pattern = \"teach\", negate = TRUE)\n\n[1] FALSE\n\n\nTo ignore case/capitalization, wrap the pattern within regex(), and within regex() add the argument ignore_case = TRUE (or T as shorthand).\n\nstr_detect(string = \"Teacher\", pattern = regex(\"teach\", ignore_case = T))\n\n[1] TRUE\n\n\nWhen str_detect() is applied to a character vector or a data frame column, it will return TRUE or FALSE for each of the values.\n\n# a vector/column of occupations \noccupations &lt;- c(\"field laborer\",\n                 \"university professor\",\n                 \"primary school teacher & tutor\",\n                 \"tutor\",\n                 \"nurse at regional hospital\",\n                 \"lineworker at Amberdeen Fish Factory\",\n                 \"physican\",\n                 \"cardiologist\",\n                 \"office worker\",\n                 \"food service\")\n\n# Detect presence of pattern \"teach\" in each string - output is vector of TRUE/FALSE\nstr_detect(occupations, \"teach\")\n\n [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nIf you need to count the TRUEs, simply sum() the output. This counts the number TRUE.\n\nsum(str_detect(occupations, \"teach\"))\n\n[1] 1\n\n\nTo search inclusive of multiple terms, include them separated by OR bars (|) within the pattern = argument, as shown below:\n\nsum(str_detect(string = occupations, pattern = \"teach|professor|tutor\"))\n\n[1] 3\n\n\nIf you need to build a long list of search terms, you can combine them using str_c() and sep = |, then define this is a character object, and then reference the vector later more succinctly. The example below includes possible occupation search terms for front-line medical providers.\n\n# search terms\noccupation_med_frontline &lt;- str_c(\"medical\", \"medicine\", \"hcw\", \"healthcare\", \"home care\", \"home health\",\n                                \"surgeon\", \"doctor\", \"doc\", \"physician\", \"surgery\", \"peds\", \"pediatrician\",\n                               \"intensivist\", \"cardiologist\", \"coroner\", \"nurse\", \"nursing\", \"rn\", \"lpn\",\n                               \"cna\", \"pa\", \"physician assistant\", \"mental health\",\n                               \"emergency department technician\", \"resp therapist\", \"respiratory\",\n                                \"phlebotomist\", \"pharmacy\", \"pharmacist\", \"hospital\", \"snf\", \"rehabilitation\",\n                               \"rehab\", \"activity\", \"elderly\", \"subacute\", \"sub acute\",\n                                \"clinic\", \"post acute\", \"therapist\", \"extended care\",\n                                \"dental\", \"dential\", \"dentist\", sep = \"|\")\n\noccupation_med_frontline\n\n[1] \"medical|medicine|hcw|healthcare|home care|home health|surgeon|doctor|doc|physician|surgery|peds|pediatrician|intensivist|cardiologist|coroner|nurse|nursing|rn|lpn|cna|pa|physician assistant|mental health|emergency department technician|resp therapist|respiratory|phlebotomist|pharmacy|pharmacist|hospital|snf|rehabilitation|rehab|activity|elderly|subacute|sub acute|clinic|post acute|therapist|extended care|dental|dential|dentist\"\n\n\nThis command returns the number of occupations which contain any one of the search terms for front-line medical providers (occupation_med_frontline):\n\nsum(str_detect(string = occupations, pattern = occupation_med_frontline))\n\n[1] 2\n\n\nBase R string search functions\nThe base function grepl() works similarly to str_detect(), in that it searches for matches to a pattern and returns a logical vector. The basic syntax is grepl(pattern, strings_to_search, ignore.case = FALSE, ...). One advantage is that the ignore.case argument is easier to write (there is no need to involve the regex() function).\nLikewise, the base functions sub() and gsub() act similarly to str_replace(). Their basic syntax is: gsub(pattern, replacement, strings_to_search, ignore.case = FALSE). sub() will replace the first instance of the pattern, whereas gsub() will replace all instances of the pattern.\n\nConvert commas to periods\nHere is an example of using gsub() to convert commas to periods in a vector of numbers. This could be useful if your data come from parts of the world other than the United States or Great Britain.\nThe inner gsub() which acts first on lengths is converting any periods to no space ““. The period character”.” has to be “escaped” with two slashes to actually signify a period, because “.” in regex means “any character”. Then, the result (with only commas) is passed to the outer gsub() in which commas are replaced by periods.\n\nlengths &lt;- c(\"2.454,56\", \"1,2\", \"6.096,5\")\n\nas.numeric(gsub(pattern = \",\",                # find commas     \n                replacement = \".\",            # replace with periods\n                x = gsub(\"\\\\.\", \"\", lengths)  # vector with other periods removed (periods escaped)\n                )\n           )                                  # convert outcome to numeric\n\n\n\n\nReplace all\nUse str_replace_all() as a “find and replace” tool. First, provide the strings to be evaluated to string =, then the pattern to be replaced to pattern =, and then the replacement value to replacement =. The example below replaces all instances of “dead” with “deceased”. Note, this IS case sensitive.\n\noutcome &lt;- c(\"Karl: dead\",\n            \"Samantha: dead\",\n            \"Marco: not dead\")\n\nstr_replace_all(string = outcome, pattern = \"dead\", replacement = \"deceased\")\n\n[1] \"Karl: deceased\"      \"Samantha: deceased\"  \"Marco: not deceased\"\n\n\nNotes:\n\nTo replace a pattern with NA, use str_replace_na().\n\nThe function str_replace() replaces only the first instance of the pattern within each evaluated string.\n\n\n\n\nDetect within logic\nWithin case_when()\nstr_detect() is often used within case_when() (from dplyr). Let’s say occupations is a column in the linelist. The mutate() below creates a new column called is_educator by using conditional logic via case_when(). See the page on data cleaning to learn more about case_when().\n\ndf &lt;- df %&gt;% \n  mutate(is_educator = case_when(\n    # term search within occupation, not case sensitive\n    str_detect(occupations,\n               regex(\"teach|prof|tutor|university\",\n                     ignore_case = TRUE))              ~ \"Educator\",\n    # all others\n    TRUE                                               ~ \"Not an educator\"))\n\nAs a reminder, it may be important to add exclusion criteria to the conditional logic (negate = F):\n\ndf &lt;- df %&gt;% \n  # value in new column is_educator is based on conditional logic\n  mutate(is_educator = case_when(\n    \n    # occupation column must meet 2 criteria to be assigned \"Educator\":\n    # it must have a search term AND NOT any exclusion term\n    \n    # Must have a search term\n    str_detect(occupations,\n               regex(\"teach|prof|tutor|university\", ignore_case = T)) &              \n    \n    # AND must NOT have an exclusion term\n    str_detect(occupations,\n               regex(\"admin\", ignore_case = T),\n               negate = TRUE                        ~ \"Educator\"\n    \n    # All rows not meeting above criteria\n    TRUE                                            ~ \"Not an educator\"))\n\n\n\n\nLocate pattern position\nTo locate the first position of a pattern, use str_locate(). It outputs a start and end position.\n\nstr_locate(\"I wish\", \"sh\")\n\n     start end\n[1,]     5   6\n\n\nLike other str functions, there is an “_all” version (str_locate_all()) which will return the positions of all instances of the pattern within each string. This outputs as a list.\n\nphrases &lt;- c(\"I wish\", \"I hope\", \"he hopes\", \"He hopes\")\n\nstr_locate(phrases, \"h\" )     # position of *first* instance of the pattern\n\n     start end\n[1,]     6   6\n[2,]     3   3\n[3,]     1   1\n[4,]     4   4\n\nstr_locate_all(phrases, \"h\" ) # position of *every* instance of the pattern\n\n[[1]]\n     start end\n[1,]     6   6\n\n[[2]]\n     start end\n[1,]     3   3\n\n[[3]]\n     start end\n[1,]     1   1\n[2,]     4   4\n\n[[4]]\n     start end\n[1,]     4   4\n\n\n\n\n\nExtract a match\nstr_extract_all() returns the matching patterns themselves, which is most useful when you have offered several patterns via “OR” conditions. For example, looking in the string vector of occupations (see previous tab) for either “teach”, “prof”, or “tutor”.\nstr_extract_all() returns a list which contains all matches for each evaluated string. See below how occupation 3 has two pattern matches within it.\n\nstr_extract_all(occupations, \"teach|prof|tutor\")\n\n[[1]]\ncharacter(0)\n\n[[2]]\n[1] \"prof\"\n\n[[3]]\n[1] \"teach\" \"tutor\"\n\n[[4]]\n[1] \"tutor\"\n\n[[5]]\ncharacter(0)\n\n[[6]]\ncharacter(0)\n\n[[7]]\ncharacter(0)\n\n[[8]]\ncharacter(0)\n\n[[9]]\ncharacter(0)\n\n[[10]]\ncharacter(0)\n\n\nstr_extract() extracts only the first match in each evaluated string, producing a character vector with one element for each evaluated string. It returns NA where there was no match. The NAs can be removed by wrapping the returned vector with na.exclude(). Note how the second of occupation 3’s matches is not shown.\n\nstr_extract(occupations, \"teach|prof|tutor\")\n\n [1] NA      \"prof\"  \"teach\" \"tutor\" NA      NA      NA      NA      NA     \n[10] NA     \n\n\n\n\n\nSubset and count\nAligned functions include str_subset() and str_count().\nstr_subset() returns the actual values which contained the pattern:\n\nstr_subset(occupations, \"teach|prof|tutor\")\n\n[1] \"university professor\"           \"primary school teacher & tutor\"\n[3] \"tutor\"                         \n\n\nstr_count() returns a vector of numbers: the number of times a search term appears in each evaluated value.\n\nstr_count(occupations, regex(\"teach|prof|tutor\", ignore_case = TRUE))\n\n [1] 0 1 2 1 0 0 0 0 0 0\n\n\n\n\n\nRegex groups\nUNDER CONSTRUCTION",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#special-characters",
    "href": "new_pages/characters_strings.html#special-characters",
    "title": "10  Characters and strings",
    "section": "10.6 Special characters",
    "text": "10.6 Special characters\nBackslash \\ as escape\nThe backslash \\ is used to “escape” the meaning of the next character. This way, a backslash can be used to have a quote mark display within other quote marks (\\\") - the middle quote mark will not “break” the surrounding quote marks.\nNote - thus, if you want to display a backslash, you must escape it’s meaning with another backslash. So you must write two backslashes \\\\ to display one.\nSpecial characters\n\n\n\n\n\n\n\nSpecial character\nRepresents\n\n\n\n\n\"\\\\\"\nbackslash\n\n\n\"\\n\"\na new line (newline)\n\n\n\"\\\"\"\ndouble-quote within double quotes\n\n\n'\\''\nsingle-quote within single quotes\n\n\n\"\\“| grave accent”| carriage return“| tab”| vertical tab“`\nbackspace\n\n\n\nRun ?\"'\" in the R Console to display a complete list of these special characters (it will appear in the RStudio Help pane).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#regular-expressions-regex",
    "href": "new_pages/characters_strings.html#regular-expressions-regex",
    "title": "10  Characters and strings",
    "section": "10.7 Regular expressions (regex)",
    "text": "10.7 Regular expressions (regex)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#regex-and-special-characters",
    "href": "new_pages/characters_strings.html#regex-and-special-characters",
    "title": "10  Characters and strings",
    "section": "10.8 Regex and special characters",
    "text": "10.8 Regex and special characters\nRegular expressions, or “regex”, is a concise language for describing patterns in strings. If you are not familiar with it, a regular expression can look like an alien language. Here we try to de-mystify this language a little bit.\nMuch of this section is adapted from this tutorial and this cheatsheet. We selectively adapt here knowing that this handbook might be viewed by people without internet access to view the other tutorials.\nA regular expression is often applied to extract specific patterns from “unstructured” text - for example medical notes, chief complaints, patient history, or other free text columns in a data frame\nThere are four basic tools one can use to create a basic regular expression:\n\nCharacter sets\n\nMeta characters\n\nQuantifiers\n\nGroups\n\nCharacter sets\nCharacter sets, are a way of expressing listing options for a character match, within brackets. So any a match will be triggered if any of the characters within the brackets are found in the string. For example, to look for vowels one could use this character set: “[aeiou]”. Some other common character sets are:\n\n\n\n\n\n\n\nCharacter set\nMatches for\n\n\n\n\n\"[A-Z]\"\nany single capital letter\n\n\n\"[a-z]\"\nany single lowercase letter\n\n\n\"[0-9]\"\nany digit\n\n\n[:alnum:]\nany alphanumeric character\n\n\n[:digit:]\nany numeric digit\n\n\n[:alpha:]\nany letter (upper or lowercase)\n\n\n[:upper:]\nany uppercase letter\n\n\n[:lower:]\nany lowercase letter\n\n\n\nCharacter sets can be combined within one bracket (no spaces!), such as \"[A-Za-z]\" (any upper or lowercase letter), or another example \"[t-z0-5]\" (lowercase t through z OR number 0 through 5).\nMeta characters\nMeta characters are shorthand for character sets. Some of the important ones are listed below:\n\n\n\n\n\n\n\nMeta character\nRepresents\n\n\n\n\n\"\\\\s\"\na single space\n\n\n\"\\\\w\"\nany single alphanumeric character (A-Z, a-z, or 0-9)\n\n\n\"\\\\d\"\nany single numeric digit (0-9)\n\n\n\nQuantifiers\nTypically you do not want to search for a match on only one character. Quantifiers allow you to designate the length of letters/numbers to allow for the match.\nQuantifiers are numbers written within curly brackets { } after the character they are quantifying, for example,\n\n\"A{2}\" will return instances of two capital A letters.\n\n\"A{2,4}\" will return instances of between two and four capital A letters (do not put spaces!).\n\n\"A{2,}\" will return instances of two or more capital A letters.\n\n\"A+\" will return instances of one or more capital A letters (group extended until a different character is encountered).\n\nPrecede with an * asterisk to return zero or more matches (useful if you are not sure the pattern is present)\n\nUsing the + plus symbol as a quantifier, the match will occur until a different character is encountered. For example, this expression will return all words (alpha characters: \"[A-Za-z]+\"\n\n# test string for quantifiers\ntest &lt;- \"A-AA-AAA-AAAA\"\n\nWhen a quantifier of {2} is used, only pairs of consecutive A’s are returned. Two pairs are identified within AAAA.\n\nstr_extract_all(test, \"A{2}\")\n\n[[1]]\n[1] \"AA\" \"AA\" \"AA\" \"AA\"\n\n\nWhen a quantifier of {2,4} is used, groups of consecutive A’s that are two to four in length are returned.\n\nstr_extract_all(test, \"A{2,4}\")\n\n[[1]]\n[1] \"AA\"   \"AAA\"  \"AAAA\"\n\n\nWith the quantifier +, groups of one or more are returned:\n\nstr_extract_all(test, \"A+\")\n\n[[1]]\n[1] \"A\"    \"AA\"   \"AAA\"  \"AAAA\"\n\n\nRelative position\nThese express requirements for what precedes or follows a pattern. For example, to extract sentences, “two numbers that are followed by a period” (\"\"). (?&lt;=\\.)\\s(?=[A-Z])\n\nstr_extract_all(test, \"\")\n\n[[1]]\n [1] \"A\" \"-\" \"A\" \"A\" \"-\" \"A\" \"A\" \"A\" \"-\" \"A\" \"A\" \"A\" \"A\"\n\n\n\n\n\n\n\n\n\nPosition statement\nMatches to\n\n\n\n\n\"(?&lt;=b)a\"\n“a” that is preceded by a “b”\n\n\n\"(?&lt;!b)a\"\n“a” that is NOT preceded by a “b”\n\n\n\"a(?=b)\"\n“a” that is followed by a “b”\n\n\n\"a(?!b)\"\n“a” that is NOT followed by a “b”\n\n\n\nGroups\nCapturing groups in your regular expression is a way to have a more organized output upon extraction.\nRegex examples\nBelow is a free text for the examples. We will try to extract useful information from it using a regular expression search term.\n\npt_note &lt;- \"Patient arrived at Broward Hospital emergency ward at 18:00 on 6/12/2005. Patient presented with radiating abdominal pain from LR quadrant. Patient skin was pale, cool, and clammy. Patient temperature was 99.8 degrees farinheit. Patient pulse rate was 100 bpm and thready. Respiratory rate was 29 per minute.\"\n\nThis expression matches to all words (any character until hitting non-character such as a space):\n\nstr_extract_all(pt_note, \"[A-Za-z]+\")\n\n[[1]]\n [1] \"Patient\"     \"arrived\"     \"at\"          \"Broward\"     \"Hospital\"   \n [6] \"emergency\"   \"ward\"        \"at\"          \"on\"          \"Patient\"    \n[11] \"presented\"   \"with\"        \"radiating\"   \"abdominal\"   \"pain\"       \n[16] \"from\"        \"LR\"          \"quadrant\"    \"Patient\"     \"skin\"       \n[21] \"was\"         \"pale\"        \"cool\"        \"and\"         \"clammy\"     \n[26] \"Patient\"     \"temperature\" \"was\"         \"degrees\"     \"farinheit\"  \n[31] \"Patient\"     \"pulse\"       \"rate\"        \"was\"         \"bpm\"        \n[36] \"and\"         \"thready\"     \"Respiratory\" \"rate\"        \"was\"        \n[41] \"per\"         \"minute\"     \n\n\nThe expression \"[0-9]{1,2}\" matches to consecutive numbers that are 1 or 2 digits in length. It could also be written \"\\\\d{1,2}\", or \"[:digit:]{1,2}\".\n\nstr_extract_all(pt_note, \"[0-9]{1,2}\")\n\n[[1]]\n [1] \"18\" \"00\" \"6\"  \"12\" \"20\" \"05\" \"99\" \"8\"  \"10\" \"0\"  \"29\"\n\n\n\n\n\n\nYou can view a useful list of regex expressions and tips on page 2 of this cheatsheet\nAlso see this tutorial.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/characters_strings.html#resources",
    "href": "new_pages/characters_strings.html#resources",
    "title": "10  Characters and strings",
    "section": "10.9 Resources",
    "text": "10.9 Resources\nA reference sheet for stringr functions can be found here\nA vignette on stringr can be found here",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Characters and strings</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html",
    "href": "new_pages/factors.html",
    "title": "11  Factors",
    "section": "",
    "text": "11.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#preparation",
    "href": "new_pages/factors.html#preparation",
    "title": "11  Factors",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,           # import/export\n  here,          # filepaths\n  lubridate,     # working with dates\n  forcats,       # factors\n  aweek,         # create epiweeks with automatic factor levels\n  janitor,       # tables\n  tidyverse      # data mgmt and viz\n  )\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\n\n\nWarning: The `trust` argument of `import()` should be explicit for serialization formats\nas of rio 1.0.3.\nℹ Missing `trust` will be set to FALSE by default for RDS in 2.0.0.\nℹ The deprecated feature was likely used in the rio package.\n  Please report the issue at &lt;https://github.com/gesistsa/rio/issues&gt;.\n\n\n\n# import your dataset\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\n\n\nNew categorical variable\nFor demonstration in this page we will use a common scenario - the creation of a new categorical variable.\nNote that if you convert a numeric column to class factor, you will not be able to calculate numeric statistics on it.\n\nCreate column\nWe use the existing column days_onset_hosp (days from symptom onset to hospital admission) and create a new column delay_cat by classifying each row into one of several categories. We do this with the dplyr function case_when(), which sequentially applies logical criteria (right-side) to each row and returns the corresponding left-side value for the new column delay_cat. Read more about case_when() in Cleaning data and core functions.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(delay_cat = case_when(\n    # criteria                                   # new value if TRUE\n    days_onset_hosp &lt; 2                        ~ \"&lt;2 days\",\n    days_onset_hosp &gt;= 2 & days_onset_hosp &lt; 5 ~ \"2-5 days\",\n    days_onset_hosp &gt;= 5                       ~ \"&gt;5 days\",\n    is.na(days_onset_hosp)                     ~ NA_character_,\n    TRUE                                       ~ \"Check me\"))  \n\n\n\nDefault value order\nAs created with case_when(), the new column delay_cat is a categorical column of class Character - not yet a factor. Thus, in a frequency table, we see that the unique values appear in a default alpha-numeric order - an order that does not make much intuitive sense:\n\ntable(linelist$delay_cat, useNA = \"always\")\n\n\n &lt;2 days  &gt;5 days 2-5 days     &lt;NA&gt; \n    2990      602     2040      256 \n\n\nLikewise, if we make a bar plot, the values also appear in this order on the x-axis (see the ggplot basics page for more on ggplot2 - the most common visualization package in R).\n\nggplot(data = linelist) +\n  geom_bar(mapping = aes(x = delay_cat))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#convert-to-factor",
    "href": "new_pages/factors.html#convert-to-factor",
    "title": "11  Factors",
    "section": "11.2 Convert to factor",
    "text": "11.2 Convert to factor\nTo convert a character or numeric column to class factor, you can use any function from the forcats package (many are detailed below). They will convert to class factor and then also perform or allow certain ordering of the levels - for example using fct_relevel() lets you manually specify the level order. The function as_factor() simply converts the class without any further capabilities.\nThe base R function factor() converts a column to factor and allows you to manually specify the order of the levels, as a character vector to its levels = argument.\nBelow we use mutate() and fct_relevel() to convert the column delay_cat from class character to class factor. The column delay_cat is created in the Preparation section above.\n\nlinelist &lt;- linelist %&gt;%\n  mutate(delay_cat = fct_relevel(delay_cat))\n\nThe unique “values” in this column are now considered “levels” of the factor. The levels have an order, which can be printed with the base R function levels(), or alternatively viewed in a count table via table() from base R or tabyl() from janitor. By default, the order of the levels will be alpha-numeric, as before. Note that NA is not a factor level.\n\nlevels(linelist$delay_cat)\n\n[1] \"&lt;2 days\"  \"&gt;5 days\"  \"2-5 days\"\n\n\nThe function fct_relevel() has the additional utility of allowing you to manually specify the level order. Simply write the level values in order, in quotation marks, separated by commas, as shown below. Note that the spelling must exactly match the values. If you want to create levels that do not exist in the data, use fct_expand() instead).\n\nlinelist &lt;- linelist %&gt;%\n  mutate(delay_cat = fct_relevel(delay_cat, \"&lt;2 days\", \"2-5 days\", \"&gt;5 days\"))\n\nWe can now see that the levels are ordered, as specified in the previous command, in a sensible order.\n\nlevels(linelist$delay_cat)\n\n[1] \"&lt;2 days\"  \"2-5 days\" \"&gt;5 days\" \n\n\nNow the plot order makes more intuitive sense as well.\n\nggplot(data = linelist) +\n  geom_bar(mapping = aes(x = delay_cat))",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#add-or-drop-levels",
    "href": "new_pages/factors.html#add-or-drop-levels",
    "title": "11  Factors",
    "section": "11.3 Add or drop levels",
    "text": "11.3 Add or drop levels\n\nAdd\nIf you need to add levels to a factor, you can do this with fct_expand(). Just write the column name followed by the new levels (separated by commas). By tabulating the values, we can see the new levels and the zero counts. You can use table() from base R, or tabyl() from janitor:\n\nlinelist %&gt;% \n  mutate(delay_cat = fct_expand(delay_cat, \"Not admitted to hospital\", \"Transfer to other jurisdiction\")) %&gt;% \n  tabyl(delay_cat)   # print table\n\n                      delay_cat    n    percent valid_percent\n                        &lt;2 days 2990 0.50781250     0.5308949\n                       2-5 days 2040 0.34646739     0.3622159\n                        &gt;5 days  602 0.10224185     0.1068892\n       Not admitted to hospital    0 0.00000000     0.0000000\n Transfer to other jurisdiction    0 0.00000000     0.0000000\n                           &lt;NA&gt;  256 0.04347826            NA\n\n\nNote: there is a special forcats function to easily add missing values (NA) as a level. See the section on Missing values below.\n\n\nDrop\nIf you use fct_drop(), the “unused” levels with zero counts will be dropped from the set of levels. The levels we added above (“Not admitted to a hospital”) exists as a level but no rows actually have those values. So they will be dropped by applying fct_drop() to our factor column:\n\nlinelist %&gt;% \n  mutate(delay_cat = fct_drop(delay_cat)) %&gt;% \n  tabyl(delay_cat)\n\n delay_cat    n    percent valid_percent\n   &lt;2 days 2990 0.50781250     0.5308949\n  2-5 days 2040 0.34646739     0.3622159\n   &gt;5 days  602 0.10224185     0.1068892\n      &lt;NA&gt;  256 0.04347826            NA",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#fct_adjust",
    "href": "new_pages/factors.html#fct_adjust",
    "title": "11  Factors",
    "section": "11.4 Adjust level order",
    "text": "11.4 Adjust level order\nThe package forcats offers useful functions to easily adjust the order of a factor’s levels (after a column been defined as class factor):\nThese functions can be applied to a factor column in two contexts:\n\nTo the column in the data frame, as usual, so the transformation is available for any subsequent use of the data.\n\nInside of a plot, so that the change is applied only within the plot.\n\n\nManually\nThis function is used to manually order the factor levels. If used on a non-factor column, the column will first be converted to class factor.\nWithin the parentheses first provide the factor column name, then provide either:\n\nAll the levels in the desired order (as a character vector c()), or,\n\nOne level and it’s corrected placement using the after = argument.\n\nHere is an example of redefining the column delay_cat (which is already class Factor) and specifying all the desired order of levels.\n\n# re-define level order\nlinelist &lt;- linelist %&gt;% \n  mutate(delay_cat = fct_relevel(delay_cat, c(\"&lt;2 days\", \"2-5 days\", \"&gt;5 days\")))\n\nIf you only want to move one level, you can specify it to fct_relevel() alone and give a number to the after = argument to indicate where in the order it should be. For example, the command below shifts “&lt;2 days” to the second position:\n\n# re-define level order\nlinelist %&gt;% \n  mutate(delay_cat = fct_relevel(delay_cat, \"&lt;2 days\", after = 1)) %&gt;% \n  tabyl(delay_cat)\n\n\n\nWithin a plot\nThe forcats commands can be used to set the level order in the data frame, or only within a plot. By using the command to “wrap around” the column name within the ggplot() plotting command, you can reverse/relevel/etc. the transformation will only apply within that plot.\nBelow, two plots are created with ggplot() (see the ggplot basics page). In the first, the delay_cat column is mapped to the x-axis of the plot, with it’s default level order as in the data linelist. In the second example it is wrapped within fct_relevel() and the order is changed in the plot.\n\n# Alpha-numeric default order - no adjustment within ggplot\nggplot(data = linelist) +\n    geom_bar(mapping = aes(x = delay_cat))\n\n# Factor level order adjusted within ggplot\nggplot(data = linelist) +\n  geom_bar(mapping = aes(x = fct_relevel(delay_cat, c(\"&lt;2 days\", \"2-5 days\", \"&gt;5 days\"))))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that default x-axis title is now quite complicated - you can overwrite this title with the ggplot2 labs() argument.\n\n\nReverse\nIt is rather common that you want to reverse the level order. Simply wrap the factor with fct_rev().\nNote that if you want to reverse only a plot legend but not the actual factor levels, you can do that with guides() (see ggplot tips).\n\n\nBy frequency\nTo order by frequency that the value appears in the data, use fct_infreq(). Any missing values (NA) will automatically be included at the end, unless they are converted to an explicit level (see this section). You can reverse the order by further wrapping with fct_rev().\nThis function can be used within a ggplot(), as shown below.\n\n# ordered by frequency\nggplot(data = linelist, aes(x = fct_infreq(delay_cat))) +\n  geom_bar() +\n  labs(x = \"Delay onset to admission (days)\",\n       title = \"Ordered by frequency\")\n\n# reversed frequency\nggplot(data = linelist, aes(x = fct_rev(fct_infreq(delay_cat)))) +\n  geom_bar() +\n  labs(x = \"Delay onset to admission (days)\",\n       title = \"Reverse of order by frequency\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy appearance\nUse fct_inorder() to set the level order to match the order of appearance in the data, starting from the first row. This can be useful if you first carefully arrange() the data in the data frame, and then use this to set the factor order.\n\n\nBy summary statistic of another column\nYou can use fct_reorder() to order the levels of one column by a summary statistic of another column. Visually, this can result in pleasing plots where the bars/points ascend or descend steadily across the plot.\nIn the examples below, the x-axis is delay_cat, and the y-axis is numeric column ct_blood (cycle-threshold value). Box plots show the CT value distribution by delay_cat group. We want to order the box plots in ascending order by the group median CT value.\nIn the first example below, the default order alpha-numeric level order is used. You can see the box plot heights are jumbled and not in any particular order. In the second example, the delay_cat column (mapped to the x-axis) has been wrapped in fct_reorder(), the column ct_blood is given as the second argument, and “median” is given as the third argument (you could also use “max”, “mean”, “min”, etc). Thus, the order of the levels of delay_cat will now reflect ascending median CT values of each delay_cat group’s median CT value. This is reflected in the second plot - the box plots have been re-arranged to ascend. Note how NA (missing) will appear at the end, unless converted to an explicit level.\n\n# boxplots ordered by original factor levels\nggplot(data = linelist) +\n  geom_boxplot(\n    aes(x = delay_cat,\n        y = ct_blood, \n        fill = delay_cat)) +\n  labs(x = \"Delay onset to admission (days)\",\n       title = \"Ordered by original alpha-numeric levels\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n# boxplots ordered by median CT value\nggplot(data = linelist) +\n  geom_boxplot(\n    aes(x = fct_reorder(delay_cat, ct_blood, \"median\"),\n        y = ct_blood,\n        fill = delay_cat)) +\n  labs(x = \"Delay onset to admission (days)\",\n       title = \"Ordered by median CT value in group\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote in this example above there are no steps required prior to the ggplot() call - the grouping and calculations are all done internally to the ggplot command.\n\n\nBy “end” value\nUse fct_reorder2() for grouped line plots. It orders the levels (and therefore the legend) to align with the vertical ordering of the lines at the “end” of the plot. Technically speaking, it “orders by the y-values associated with the largest x values.”\nFor example, if you have lines showing case counts by hospital over time, you can apply fct_reorder2() to the color = argument within aes(), such that the vertical order of hospitals appearing in the legend aligns with the order of lines at the terminal end of the plot. Read more in the online documentation.\n\nepidemic_data &lt;- linelist %&gt;%         # begin with the linelist   \n    filter(date_onset &lt; as.Date(\"2014-09-21\")) %&gt;%    # cut-off date, for visual clarity\n    count(                                            # get case counts per week and by hospital\n      epiweek = lubridate::floor_date(date_onset, \"week\"),  \n      hospital                                            \n    ) \n  \nggplot(data = epidemic_data) +                       # start plot\n  geom_line(                                        # make lines\n    aes(\n      x = epiweek,                                  # x-axis epiweek\n      y = n,                                        # height is number of cases per week\n      color = fct_reorder2(hospital, epiweek, n))) + # data grouped and colored by hospital, with factor order by height at end of plot\n  labs(title = \"Factor levels (and legend display) by line height at end of plot\",\n       color = \"Hospital\")                          # change legend title",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#fct_missing",
    "href": "new_pages/factors.html#fct_missing",
    "title": "11  Factors",
    "section": "11.5 Missing values",
    "text": "11.5 Missing values\nIf you have NA values in your factor column, you can easily convert them to a named level such as “Missing” with fct_explicit_na(). The NA values are converted to “(Missing)” at the end of the level order by default. You can adjust the level name with the argument na_level =.\nBelow, this opertation is performed on the column delay_cat and a table is printed with tabyl() with NA converted to “Missing delay”.\n\nlinelist %&gt;% \n  mutate(delay_cat = fct_explicit_na(delay_cat, na_level = \"Missing delay\")) %&gt;% \n  tabyl(delay_cat)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `delay_cat = fct_explicit_na(delay_cat, na_level = \"Missing\n  delay\")`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\n\n\n     delay_cat    n    percent\n      2-5 days 2040 0.34646739\n       &lt;2 days 2990 0.50781250\n       &gt;5 days  602 0.10224185\n Missing delay  256 0.04347826",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#combine-levels",
    "href": "new_pages/factors.html#combine-levels",
    "title": "11  Factors",
    "section": "11.6 Combine levels",
    "text": "11.6 Combine levels\n\nManually\nYou can adjust the level displays manually manually with fct_recode(). This is like the dplyr function recode() (see Cleaning data and core functions), but it allows the creation of new factor levels. If you use the simple recode() on a factor, new re-coded values will be rejected unless they have already been set as permissible levels.\nThis tool can also be used to “combine” levels, by assigning multiple levels the same re-coded value. Just be careful to not lose information! Consider doing these combining steps in a new column (not over-writing the existing column).\nDANGER: fct_recode() has a different syntax than recode(). recode() uses OLD = NEW, whereas fct_recode() uses NEW = OLD. \nThe current levels of delay_cat are:\n\nlevels(linelist$delay_cat)\n\n[1] \"&lt;2 days\"  \"2-5 days\" \"&gt;5 days\" \n\n\nThe new levels are created using syntax fct_recode(column, \"new\" = \"old\", \"new\" = \"old\", \"new\" = \"old\") and printed:\n\nlinelist %&gt;% \n  mutate(delay_cat = fct_recode(\n    delay_cat,\n    \"Less than 2 days\" = \"&lt;2 days\",\n    \"2 to 5 days\"      = \"2-5 days\",\n    \"More than 5 days\" = \"&gt;5 days\")) %&gt;% \n  tabyl(delay_cat)\n\n        delay_cat    n    percent valid_percent\n Less than 2 days 2990 0.50781250     0.5308949\n      2 to 5 days 2040 0.34646739     0.3622159\n More than 5 days  602 0.10224185     0.1068892\n             &lt;NA&gt;  256 0.04347826            NA\n\n\nHere they are manually combined with fct_recode(). Note there is no error raised at the creation of a new level “Less than 5 days”.\n\nlinelist %&gt;% \n  mutate(delay_cat = fct_recode(\n    delay_cat,\n    \"Less than 5 days\" = \"&lt;2 days\",\n    \"Less than 5 days\" = \"2-5 days\",\n    \"More than 5 days\" = \"&gt;5 days\")) %&gt;% \n  tabyl(delay_cat)\n\n        delay_cat    n    percent valid_percent\n Less than 5 days 5030 0.85427989     0.8931108\n More than 5 days  602 0.10224185     0.1068892\n             &lt;NA&gt;  256 0.04347826            NA\n\n\n\n\nReduce into “Other”\nYou can use fct_other() to manually assign factor levels to an “Other” level. Below, all levels in the column hospital, aside from “Port Hospital” and “Central Hospital”, are combined into “Other”. You can provide a vector to either keep =, or drop =. You can change the display of the “Other” level with other_level =.\n\nlinelist %&gt;%    \n  mutate(hospital = fct_other(                      # adjust levels\n    hospital,\n    keep = c(\"Port Hospital\", \"Central Hospital\"),  # keep these separate\n    other_level = \"Other Hospital\")) %&gt;%            # All others as \"Other Hospital\"\n  tabyl(hospital)                                   # print table\n\n         hospital    n    percent\n Central Hospital  454 0.07710598\n    Port Hospital 1762 0.29925272\n   Other Hospital 3672 0.62364130\n\n\n\n\nReduce by frequency\nYou can combine the least-frequent factor levels automatically using fct_lump().\nTo “lump” together many low-frequency levels into an “Other” group, do one of the following:\n\nSet n = as the number of groups you want to keep. The n most-frequent levels will be kept, and all others will combine into “Other”.\n\nSet prop = as the threshold frequency proportion for levels above which you want to keep. All other values will combine into “Other”.\n\nYou can change the display of the “Other” level with other_level =. Below, all but the two most-frequent hospitals are combined into “Other Hospital”.\n\nlinelist %&gt;%    \n  mutate(hospital = fct_lump(                      # adjust levels\n    hospital,\n    n = 2,                                          # keep top 2 levels\n    other_level = \"Other Hospital\")) %&gt;%            # all others as \"Other Hospital\"\n  tabyl(hospital)                                   # print table\n\n       hospital    n   percent\n        Missing 1469 0.2494905\n  Port Hospital 1762 0.2992527\n Other Hospital 2657 0.4512568",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#show-all-levels",
    "href": "new_pages/factors.html#show-all-levels",
    "title": "11  Factors",
    "section": "11.7 Show all levels",
    "text": "11.7 Show all levels\nOne benefit of using factors is to standardise the appearance of plot legends and tables, regardless of which values are actually present in a dataset.\nIf you are preparing many figures (e.g. for multiple jurisdictions) you will want the legends and tables to appear identically even with varying levels of data completion or data composition.\n\nIn plots\nIn a ggplot() figure, simply add the argument drop = FALSE in the relevant scale_xxxx() function. All factor levels will be displayed, regardless of whether they are present in the data. If your factor column levels are displayed using fill =, then in scale_fill_discrete() you include drop = FALSE, as shown below. If your levels are displayed with x = (to the x-axis) color = or size = you would provide this to scale_color_discrete() or scale_size_discrete() accordingly.\nThis example is a stacked bar plot of age category, by hospital. Adding scale_fill_discrete(drop = FALSE) ensures that all age groups appear in the legend, even if not present in the data.\n\nggplot(data = linelist) +\n  geom_bar(mapping = aes(x = hospital, fill = age_cat)) +\n  scale_fill_discrete(drop = FALSE) +                        # show all age groups in the legend, even those not present\n  labs(\n    title = \"All age groups will appear in legend, even if not present in data\")\n\n\n\n\n\n\n\n\n\n\nIn tables\nBoth the base R table() and tabyl() from janitor will show all factor levels (even unused levels).\nIf you use count() or summarise() from dplyr to make a table, add the argument .drop = FALSE to include counts for all factor levels even those unused.\nRead more in the Descriptive tables page, or at the scale_discrete documentation, or the count() documentation. You can see another example in the Contact tracing page.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#epiweeks",
    "href": "new_pages/factors.html#epiweeks",
    "title": "11  Factors",
    "section": "11.8 Epiweeks",
    "text": "11.8 Epiweeks\nPlease see the extensive discussion of how to create epidemiological weeks in the Grouping data page.\nPlease also see the Working with dates page for tips on how to create and format epidemiological weeks.\n\nEpiweeks in a plot\nIf your goal is to create epiweeks to display in a plot, you can do this simply with lubridate’s floor_date(), as explained in the Grouping data page. The values returned will be of class Date with format YYYY-MM-DD. If you use this column in a plot, the dates will naturally order correctly, and you do not need to worry about levels or converting to class Factor. See the ggplot() histogram of onset dates below.\nIn this approach, you can adjust the display of the dates on an axis with scale_x_date(). See the page on Epidemic curves for more information. You can specify a “strptime” display format to the date_labels = argument of scale_x_date(). These formats use “%” placeholders and are covered in the Working with dates page. Use “%Y” to represent a 4-digit year, and either “%W” or “%U” to represent the week number (Monday or Sunday weeks respectively).\n\nlinelist %&gt;% \n  mutate(epiweek_date = floor_date(date_onset, \"week\")) %&gt;%  # create week column\n  ggplot() +                                                  # begin ggplot\n  geom_histogram(mapping = aes(x = epiweek_date)) +           # histogram of date of onset\n  scale_x_date(date_labels = \"%Y-W%W\")                       # adjust disply of dates to be YYYY-WWw\n\n\n\n\n\n\n\n\n\n\nEpiweeks in the data\nHowever, if your purpose in factoring is not to plot, you can approach this one of two ways:\n\nFor fine control over the display, convert the lubridate epiweek column (YYYY-MM-DD) to the desired display format (YYYY-WWw) within the data frame itself, and then convert it to class Factor.\n\nFirst, use format() from base R to convert the date display from YYYY-MM-DD to YYYY-Www display (see the Working with dates page). In this process the class will be converted to character. Then, convert from character to class Factor with factor().\n\nlinelist &lt;- linelist %&gt;% \n  mutate(epiweek_date = floor_date(date_onset, \"week\"),       # create epiweeks (YYYY-MM-DD)\n         epiweek_formatted = format(epiweek_date, \"%Y-W%W\"),  # Convert to display (YYYY-WWw)\n         epiweek_formatted = factor(epiweek_formatted))       # Convert to factor\n\n# Display levels\nlevels(linelist$epiweek_formatted)\n\n [1] \"2014-W13\" \"2014-W14\" \"2014-W15\" \"2014-W16\" \"2014-W17\" \"2014-W18\"\n [7] \"2014-W19\" \"2014-W20\" \"2014-W21\" \"2014-W22\" \"2014-W23\" \"2014-W24\"\n[13] \"2014-W25\" \"2014-W26\" \"2014-W27\" \"2014-W28\" \"2014-W29\" \"2014-W30\"\n[19] \"2014-W31\" \"2014-W32\" \"2014-W33\" \"2014-W34\" \"2014-W35\" \"2014-W36\"\n[25] \"2014-W37\" \"2014-W38\" \"2014-W39\" \"2014-W40\" \"2014-W41\" \"2014-W42\"\n[31] \"2014-W43\" \"2014-W44\" \"2014-W45\" \"2014-W46\" \"2014-W47\" \"2014-W48\"\n[37] \"2014-W49\" \"2014-W50\" \"2014-W51\" \"2015-W00\" \"2015-W01\" \"2015-W02\"\n[43] \"2015-W03\" \"2015-W04\" \"2015-W05\" \"2015-W06\" \"2015-W07\" \"2015-W08\"\n[49] \"2015-W09\" \"2015-W10\" \"2015-W11\" \"2015-W12\" \"2015-W13\" \"2015-W14\"\n[55] \"2015-W15\" \"2015-W16\"\n\n\nDANGER: If you place the weeks ahead of the years (“Www-YYYY”) (“%W-%Y”), the default alpha-numeric level ordering will be incorrect (e.g. 01-2015 will be before 35-2014). You could need to manually adjust the order, which would be a long painful process.\n\nFor fast default display, use the aweek package and it’s function date2week(). You can set the week_start = day, and if you set factor = TRUE then the output column is an ordered factor. As a bonus, the factor includes levels for all possible weeks in the span - even if there are no cases that week.\n\n\ndf &lt;- linelist %&gt;% \n  mutate(epiweek = date2week(date_onset, week_start = \"Monday\", factor = TRUE))\n\nlevels(df$epiweek)\n\nSee the Working with dates page for more information about aweek. It also offers the reverse function week2date().",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/factors.html#resources",
    "href": "new_pages/factors.html#resources",
    "title": "11  Factors",
    "section": "11.9 Resources",
    "text": "11.9 Resources\nR for Data Science page on factors\naweek package vignette",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "new_pages/pivoting.html",
    "href": "new_pages/pivoting.html",
    "title": "12  Pivoting data",
    "section": "",
    "text": "12.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pivoting data</span>"
    ]
  },
  {
    "objectID": "new_pages/pivoting.html#preparation",
    "href": "new_pages/pivoting.html#preparation",
    "title": "12  Pivoting data",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  kableExtra,   # Build and manipulate complex tables\n  tidyverse)    # data management + ggplot2 graphics\n\n\n\nImport data\n\n\nMalaria count data\nIn this page, we will use a fictional dataset of daily malaria cases, by facility and age group. If you want to follow along, click here to download (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n\nWarning: The `trust` argument of `import()` should be explicit for serialization formats\nas of rio 1.0.3.\nℹ Missing `trust` will be set to FALSE by default for RDS in 2.0.0.\nℹ The deprecated feature was likely used in the rio package.\n  Please report the issue at &lt;https://github.com/gesistsa/rio/issues&gt;.\n\n\n\n# Import data\ncount_data &lt;- import(\"malaria_facility_count_data.rds\")\n\nThe first 50 rows are displayed below.\n\n\n\n\n\n\n\n\nLinelist case data\nIn the later part of this page, we will also use the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\n\n# import your dataset\nlinelist &lt;- import(\"linelist_cleaned.rds\")",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pivoting data</span>"
    ]
  },
  {
    "objectID": "new_pages/pivoting.html#wide-to-long",
    "href": "new_pages/pivoting.html#wide-to-long",
    "title": "12  Pivoting data",
    "section": "12.2 Wide-to-long",
    "text": "12.2 Wide-to-long\n\n\n\n\n\n\n\n\n\n\n\n“Wide” format\nData are often entered and stored in a “wide” format - where a subject’s characteristics or responses are stored in a single row. While this may be useful for presentation, it is not ideal for some types of analysis.\nLet us take the count_data dataset imported in the Preparation section above as an example. You can see that each row represents a “facility-day”. The actual case counts (the right-most columns) are stored in a “wide” format such that the information for every age group on a given facility-day is stored in a single row.\n\n\n\n\n\n\nEach observation in this dataset refers to the malaria counts at one of 65 facilities on a given date, ranging from count_data$data_date %&gt;% min() to count_data$data_date %&gt;% max(). These facilities are located in one Province (North) and four District (Spring, Bolo, Dingo, and Barnard). The dataset provides the overall counts of malaria, as well as age-specific counts in each of three age groups - &lt;4 years, 5-14 years, and 15 years and older.\n“Wide” data like this are not adhering to “tidy data” standards, because the column headers do not actually represent “variables” - they represent values of a hypothetical “age group” variable.\nThis format can be useful for presenting the information in a table, or for entering data (e.g. in Excel) from case report forms. However, in the analysis stage, these data typically should be transformed to a “longer” format more aligned with “tidy data” standards. The plotting R package ggplot2 in particular works best when data are in a “long” format.\nVisualising the total malaria counts over time poses no difficulty with the data in it’s current format:\n\nggplot(count_data) +\n  geom_col(aes(x = data_date, y = malaria_tot), width = 1)\n\n\n\n\n\n\n\n\nHowever, what if we wanted to display the relative contributions of each age group to this total count? In this case, we need to ensure that the variable of interest (age group), appears in the dataset in a single column that can be passed to {ggplot2}’s “mapping aesthetics” aes() argument.\n\n\n\npivot_longer()\nThe tidyr function pivot_longer() makes data “longer”. tidyr is part of the tidyverse of R packages.\nIt accepts a range of columns to transform (specified to cols =). Therefore, it can operate on only a part of a dataset. This is useful for the malaria data, as we only want to pivot the case count columns.\nIn this process, you will end up with two “new” columns - one with the categories (the former column names), and one with the corresponding values (e.g. case counts). You can accept the default names for these new columns, or you can specify your own to names_to = and values_to = respectively.\nLet’s see pivot_longer() in action.\n\n\nStandard pivoting\nWe want to use tidyr’s pivot_longer() function to convert the “wide” data to a “long” format. Specifically, to convert the four numeric columns with data on malaria counts to two new columns: one which holds the age groups and one which holds the corresponding values.\n\ndf_long &lt;- count_data %&gt;% \n  pivot_longer(\n    cols = c(`malaria_rdt_0-4`, `malaria_rdt_5-14`, `malaria_rdt_15`, `malaria_tot`)\n  )\n\ndf_long\n\nNotice that the newly created data frame (df_long) has more rows (12,152 vs 3,038); it has become longer. In fact, it is precisely four times as long, because each row in the original dataset now represents four rows in df_long, one for each of the malaria count observations (&lt;4y, 5-14y, 15y+, and total).\nIn addition to becoming longer, the new dataset has fewer columns (8 vs 10), as the data previously stored in four columns (those beginning with the prefix malaria_) is now stored in two.\nSince the names of these four columns all begin with the prefix malaria_, we could have made use of the handy “tidyselect” function starts_with() to achieve the same result (see the page Cleaning data and core functions for more of these helper functions).\n\n# provide column with a tidyselect helper function\ncount_data %&gt;% \n  pivot_longer(\n    cols = starts_with(\"malaria_\")\n  )\n\n# A tibble: 12,152 × 8\n   location_name data_date  submitted_date Province District newid name    value\n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;int&gt;\n 1 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    11\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    12\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    23\n 4 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    46\n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    11\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    10\n 7 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…     5\n 8 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    26\n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malari…     8\n10 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malari…     5\n# ℹ 12,142 more rows\n\n\nor by position:\n\n# provide columns by position\ncount_data %&gt;% \n  pivot_longer(\n    cols = 6:9\n  )\n\nor by named range:\n\n# provide range of consecutive columns\ncount_data %&gt;% \n  pivot_longer(\n    cols = `malaria_rdt_0-4`:malaria_tot\n  )\n\nThese two new columns are given the default names of name and value, but we can override these defaults to provide more meaningful names, which can help remember what is stored within, using the names_to and values_to arguments. Let’s use the names age_group and counts:\n\ndf_long &lt;- \n  count_data %&gt;% \n  pivot_longer(\n    cols = starts_with(\"malaria_\"),\n    names_to = \"age_group\",\n    values_to = \"counts\"\n  )\n\ndf_long\n\n# A tibble: 12,152 × 8\n   location_name data_date  submitted_date Province District newid age_group    \n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;        \n 1 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 4 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_tot  \n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 7 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 8 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_tot  \n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malaria_rdt_…\n10 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malaria_rdt_…\n# ℹ 12,142 more rows\n# ℹ 1 more variable: counts &lt;int&gt;\n\n\nWe can now pass this new dataset to {ggplot2}, and map the new column count to the y-axis and new column age_group to the fill = argument (the column internal color). This will display the malaria counts in a stacked bar chart, by age group:\n\nggplot(data = df_long) +\n  geom_col(\n    mapping = aes(x = data_date, y = counts, fill = age_group),\n    width = 1\n  )\n\n\n\n\n\n\n\n\nExamine this new plot, and compare it with the plot we created earlier - what has gone wrong?\nWe have encountered a common problem when wrangling surveillance data - we have also included the total counts from the malaria_tot column, so the magnitude of each bar in the plot is twice as high as it should be.\nWe can handle this in a number of ways. We could simply filter these totals from the dataset before we pass it to ggplot():\n\ndf_long %&gt;% \n  filter(age_group != \"malaria_tot\") %&gt;% \n  ggplot() +\n  geom_col(\n    aes(x = data_date, y = counts, fill = age_group),\n    width = 1\n  )\n\n\n\n\n\n\n\n\nAlternatively, we could have excluded this variable when we ran pivot_longer(), thereby maintaining it in the dataset as a separate variable. See how its values “expand” to fill the new rows.\n\ncount_data %&gt;% \n  pivot_longer(\n    cols = `malaria_rdt_0-4`:malaria_rdt_15,   # does not include the totals column\n    names_to = \"age_group\",\n    values_to = \"counts\"\n  )\n\n# A tibble: 9,114 × 9\n   location_name data_date  submitted_date Province District malaria_tot newid\n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;          &lt;int&gt; &lt;int&gt;\n 1 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 4 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 7 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n 8 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n10 Facility 4    2020-08-11 2020-08-12     North    Bolo              49     4\n# ℹ 9,104 more rows\n# ℹ 2 more variables: age_group &lt;chr&gt;, counts &lt;int&gt;\n\n\n\n\nPivoting data of multiple classes\nThe above example works well in situations in which all the columns you want to “pivot longer” are of the same class (character, numeric, logical…).\nHowever, there will be many cases when, as a field epidemiologist, you will be working with data that was prepared by non-specialists and which follow their own non-standard logic - as Hadley Wickham noted (referencing Tolstoy) in his seminal article on Tidy Data principles: “Like families, tidy datasets are all alike but every messy dataset is messy in its own way.”\nOne particularly common problem you will encounter will be the need to pivot columns that contain different classes of data. This pivot will result in storing these different data types in a single column, which is not a good situation. There are various approaches one can take to separate out the mess this creates, but there is an important step you can take using pivot_longer() to avoid creating such a situation yourself.\nTake a situation in which there have been a series of observations at different time steps for each of three items A, B and C. Examples of such items could be individuals (e.g. contacts of an Ebola case being traced each day for 21 days) or remote village health posts being monitored once per year to ensure they are still functional. Let’s use the contact tracing example. Imagine that the data are stored as follows:\n\n\n\n\n\n\nAs can be seen, the data are a bit complicated. Each row stores information about one item, but with the time series running further and further away to the right as time progresses. Moreover, the column classes alternate between date and character values.\nOne particularly bad example of this encountered by this author involved cholera surveillance data, in which 8 new columns of observations were added each day over the course of 4 years. Simply opening the Excel file in which these data were stored took &gt;10 minuntes on my laptop!\nIn order to work with these data, we need to transform the data frame to long format, but keeping the separation between a date column and a character (status) column, for each observation for each item. If we don’t, we might end up with a mixture of variable types in a single column (a very big “no-no” when it comes to data management and tidy data):\n\ndf %&gt;% \n  pivot_longer(\n    cols = -id,\n    names_to = c(\"observation\")\n  )\n\n# A tibble: 18 × 3\n   id    observation value     \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1 A     obs1_date   2021-04-23\n 2 A     obs1_status Healthy   \n 3 A     obs2_date   2021-04-24\n 4 A     obs2_status Healthy   \n 5 A     obs3_date   2021-04-25\n 6 A     obs3_status Unwell    \n 7 B     obs1_date   2021-04-23\n 8 B     obs1_status Healthy   \n 9 B     obs2_date   2021-04-24\n10 B     obs2_status Healthy   \n11 B     obs3_date   2021-04-25\n12 B     obs3_status Healthy   \n13 C     obs1_date   2021-04-23\n14 C     obs1_status Missing   \n15 C     obs2_date   2021-04-24\n16 C     obs2_status Healthy   \n17 C     obs3_date   2021-04-25\n18 C     obs3_status Healthy   \n\n\nAbove, our pivot has merged dates and characters into a single value column. R will react by converting the entire column to class character, and the utility of the dates is lost.\nTo prevent this situation, we can take advantage of the syntax structure of the original column names. There is a common naming structure, with the observation number, an underscore, and then either “status” or “date”. We can leverage this syntax to keep these two data types in separate columns after the pivot.\nWe do this by:\n\nProviding a character vector to the names_to = argument, with the second item being (\".value\" ). This special term indicates that the pivoted columns will be split based on a character in their name.\n\nYou must also provide the “splitting” character to the names_sep = argument. In this case, it is the underscore “_“.\n\nThus, the naming and split of new columns is based around the underscore in the existing variable names.\n\ndf_long &lt;- \n  df %&gt;% \n  pivot_longer(\n    cols = -id,\n    names_to = c(\"observation\", \".value\"),\n    names_sep = \"_\"\n  )\n\ndf_long\n\n# A tibble: 9 × 4\n  id    observation date       status \n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;  \n1 A     obs1        2021-04-23 Healthy\n2 A     obs2        2021-04-24 Healthy\n3 A     obs3        2021-04-25 Unwell \n4 B     obs1        2021-04-23 Healthy\n5 B     obs2        2021-04-24 Healthy\n6 B     obs3        2021-04-25 Healthy\n7 C     obs1        2021-04-23 Missing\n8 C     obs2        2021-04-24 Healthy\n9 C     obs3        2021-04-25 Healthy\n\n\nFinishing touches:\nNote that the date column is currently in character class - we can easily convert this into it’s proper date class using the mutate() and as_date() functions described in the Working with dates page.\nWe may also want to convert the observation column to a numeric format by dropping the “obs” prefix and converting to numeric. We can do this with str_remove_all() from the stringr package (see the Characters and strings page).\n\ndf_long &lt;- \n  df_long %&gt;% \n  mutate(\n    date = date %&gt;% lubridate::as_date(),\n    observation = \n      observation %&gt;% \n      str_remove_all(\"obs\") %&gt;% \n      as.numeric()\n  )\n\ndf_long\n\n# A tibble: 9 × 4\n  id    observation date       status \n  &lt;chr&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;  \n1 A               1 2021-04-23 Healthy\n2 A               2 2021-04-24 Healthy\n3 A               3 2021-04-25 Unwell \n4 B               1 2021-04-23 Healthy\n5 B               2 2021-04-24 Healthy\n6 B               3 2021-04-25 Healthy\n7 C               1 2021-04-23 Missing\n8 C               2 2021-04-24 Healthy\n9 C               3 2021-04-25 Healthy\n\n\nAnd now, we can start to work with the data in this format, e.g. by plotting a descriptive heat tile:\n\nggplot(data = df_long, mapping = aes(x = date, y = id, fill = status)) +\n  geom_tile(colour = \"black\") +\n  scale_fill_manual(\n    values = \n      c(\"Healthy\" = \"lightgreen\", \n        \"Unwell\" = \"red\", \n        \"Missing\" = \"orange\")\n  )",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pivoting data</span>"
    ]
  },
  {
    "objectID": "new_pages/pivoting.html#long-to-wide",
    "href": "new_pages/pivoting.html#long-to-wide",
    "title": "12  Pivoting data",
    "section": "12.3 Long-to-wide",
    "text": "12.3 Long-to-wide\n\n\n\n\n\n\n\n\n\nIn some instances, we may wish to convert a dataset to a wider format. For this, we can use the pivot_wider() function.\nA typical use-case is when we want to transform the results of an analysis into a format which is more digestible for the reader (such as a Table for presentation). Usually, this involves transforming a dataset in which information for one subject is are spread over multiple rows into a format in which that information is stored in a single row.\n\nData\nFor this section of the page, we will use the case linelist (see the Preparation section), which contains one row per case.\nHere are the first 50 rows:\n\n\n\n\n\n\nSuppose that we want to know the counts of individuals in the different age groups, by gender:\n\ndf_wide &lt;- \n  linelist %&gt;% \n  count(age_cat, gender)\n\ndf_wide\n\n   age_cat gender   n\n1      0-4      f 640\n2      0-4      m 416\n3      0-4   &lt;NA&gt;  39\n4      5-9      f 641\n5      5-9      m 412\n6      5-9   &lt;NA&gt;  42\n7    10-14      f 518\n8    10-14      m 383\n9    10-14   &lt;NA&gt;  40\n10   15-19      f 359\n11   15-19      m 364\n12   15-19   &lt;NA&gt;  20\n13   20-29      f 468\n14   20-29      m 575\n15   20-29   &lt;NA&gt;  30\n16   30-49      f 179\n17   30-49      m 557\n18   30-49   &lt;NA&gt;  18\n19   50-69      f   2\n20   50-69      m  91\n21   50-69   &lt;NA&gt;   2\n22     70+      m   5\n23     70+   &lt;NA&gt;   1\n24    &lt;NA&gt;   &lt;NA&gt;  86\n\n\nThis gives us a long dataset that is great for producing visualisations in ggplot2, but not ideal for presentation in a table:\n\nggplot(df_wide) +\n  geom_col(aes(x = age_cat, y = n, fill = gender))\n\n\n\n\n\n\n\n\n\n\nPivot wider\nTherefore, we can use pivot_wider() to transform the data into a better format for inclusion as tables in our reports.\nThe argument names_from specifies the column from which to generate the new column names, while the argument values_from specifies the column from which to take the values to populate the cells. The argument id_cols = is optional, but can be provided a vector of column names that should not be pivoted, and will thus identify each row.\n\ntable_wide &lt;- \n  df_wide %&gt;% \n  pivot_wider(\n    id_cols = age_cat,\n    names_from = gender,\n    values_from = n\n  )\n\ntable_wide\n\n# A tibble: 9 × 4\n  age_cat     f     m  `NA`\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 0-4       640   416    39\n2 5-9       641   412    42\n3 10-14     518   383    40\n4 15-19     359   364    20\n5 20-29     468   575    30\n6 30-49     179   557    18\n7 50-69       2    91     2\n8 70+        NA     5     1\n9 &lt;NA&gt;       NA    NA    86\n\n\nThis table is much more reader-friendly, and therefore better for inclusion in our reports. You can convert into a pretty table with several packages including flextable and knitr. This process is elaborated in the page Tables for presentation.\n\ntable_wide %&gt;% \n  janitor::adorn_totals(c(\"row\", \"col\")) %&gt;% # adds row and column totals\n  knitr::kable() %&gt;% \n  kableExtra::row_spec(row = 10, bold = TRUE) %&gt;% \n  kableExtra::column_spec(column = 5, bold = TRUE) \n\n\n\n\nage_cat\nf\nm\nNA\nTotal\n\n\n\n\n0-4\n640\n416\n39\n1095\n\n\n5-9\n641\n412\n42\n1095\n\n\n10-14\n518\n383\n40\n941\n\n\n15-19\n359\n364\n20\n743\n\n\n20-29\n468\n575\n30\n1073\n\n\n30-49\n179\n557\n18\n754\n\n\n50-69\n2\n91\n2\n95\n\n\n70+\nNA\n5\n1\n6\n\n\nNA\nNA\nNA\n86\n86\n\n\nTotal\n2807\n2803\n278\n5888",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pivoting data</span>"
    ]
  },
  {
    "objectID": "new_pages/pivoting.html#fill",
    "href": "new_pages/pivoting.html#fill",
    "title": "12  Pivoting data",
    "section": "12.4 Fill",
    "text": "12.4 Fill\nIn some situations after a pivot, and more commonly after a bind, we are left with gaps in some cells that we would like to fill.\n\n\nData\nFor example, take two datasets, each with observations for the measurement number, the name of the facility, and the case count at that time. However, the second dataset also has a variable Year.\n\ndf1 &lt;- \n  tibble::tribble(\n       ~Measurement, ~Facility, ~Cases,\n                  1,  \"Hosp 1\",     66,\n                  2,  \"Hosp 1\",     26,\n                  3,  \"Hosp 1\",      8,\n                  1,  \"Hosp 2\",     71,\n                  2,  \"Hosp 2\",     62,\n                  3,  \"Hosp 2\",     70,\n                  1,  \"Hosp 3\",     47,\n                  2,  \"Hosp 3\",     70,\n                  3,  \"Hosp 3\",     38,\n       )\n\ndf1 \n\n# A tibble: 9 × 3\n  Measurement Facility Cases\n        &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1           1 Hosp 1      66\n2           2 Hosp 1      26\n3           3 Hosp 1       8\n4           1 Hosp 2      71\n5           2 Hosp 2      62\n6           3 Hosp 2      70\n7           1 Hosp 3      47\n8           2 Hosp 3      70\n9           3 Hosp 3      38\n\ndf2 &lt;- \n  tibble::tribble(\n    ~Year, ~Measurement, ~Facility, ~Cases,\n     2000,            1,  \"Hosp 4\",     82,\n     2001,            2,  \"Hosp 4\",     87,\n     2002,            3,  \"Hosp 4\",     46\n  )\n\ndf2\n\n# A tibble: 3 × 4\n   Year Measurement Facility Cases\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1  2000           1 Hosp 4      82\n2  2001           2 Hosp 4      87\n3  2002           3 Hosp 4      46\n\n\nWhen we perform a bind_rows() to join the two datasets together, the Year variable is filled with NA for those rows where there was no prior information (i.e. the first dataset):\n\ndf_combined &lt;- \n  bind_rows(df1, df2) %&gt;% \n  arrange(Measurement, Facility)\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 1      66    NA\n 2           1 Hosp 2      71    NA\n 3           1 Hosp 3      47    NA\n 4           1 Hosp 4      82  2000\n 5           2 Hosp 1      26    NA\n 6           2 Hosp 2      62    NA\n 7           2 Hosp 3      70    NA\n 8           2 Hosp 4      87  2001\n 9           3 Hosp 1       8    NA\n10           3 Hosp 2      70    NA\n11           3 Hosp 3      38    NA\n12           3 Hosp 4      46  2002\n\n\n\n\n\nfill()\nIn this case, Year is a useful variable to include, particularly if we want to explore trends over time. Therefore, we use fill() to fill in those empty cells, by specifying the column to fill and the direction (in this case up):\n\ndf_combined %&gt;% \n  fill(Year, .direction = \"up\")\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 1      66  2000\n 2           1 Hosp 2      71  2000\n 3           1 Hosp 3      47  2000\n 4           1 Hosp 4      82  2000\n 5           2 Hosp 1      26  2001\n 6           2 Hosp 2      62  2001\n 7           2 Hosp 3      70  2001\n 8           2 Hosp 4      87  2001\n 9           3 Hosp 1       8  2002\n10           3 Hosp 2      70  2002\n11           3 Hosp 3      38  2002\n12           3 Hosp 4      46  2002\n\n\nAlternatively, we can rearrange the data so that we would need to fill in a downward direction:\n\ndf_combined &lt;- \n  df_combined %&gt;% \n  arrange(Measurement, desc(Facility))\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 4      82  2000\n 2           1 Hosp 3      47    NA\n 3           1 Hosp 2      71    NA\n 4           1 Hosp 1      66    NA\n 5           2 Hosp 4      87  2001\n 6           2 Hosp 3      70    NA\n 7           2 Hosp 2      62    NA\n 8           2 Hosp 1      26    NA\n 9           3 Hosp 4      46  2002\n10           3 Hosp 3      38    NA\n11           3 Hosp 2      70    NA\n12           3 Hosp 1       8    NA\n\ndf_combined &lt;- \n  df_combined %&gt;% \n  fill(Year, .direction = \"down\")\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 4      82  2000\n 2           1 Hosp 3      47  2000\n 3           1 Hosp 2      71  2000\n 4           1 Hosp 1      66  2000\n 5           2 Hosp 4      87  2001\n 6           2 Hosp 3      70  2001\n 7           2 Hosp 2      62  2001\n 8           2 Hosp 1      26  2001\n 9           3 Hosp 4      46  2002\n10           3 Hosp 3      38  2002\n11           3 Hosp 2      70  2002\n12           3 Hosp 1       8  2002\n\n\nWe now have a useful dataset for plotting:\n\nggplot(df_combined) +\n  aes(Year, Cases, fill = Facility) +\n  geom_col()\n\n\n\n\n\n\n\n\nBut less useful for presenting in a table, so let’s practice converting this long, untidy dataframe into a wider, tidy dataframe:\n\ndf_combined %&gt;% \n  pivot_wider(\n    id_cols = c(Measurement, Facility),\n    names_from = \"Year\",\n    values_from = \"Cases\"\n  ) %&gt;% \n  arrange(Facility) %&gt;% \n  janitor::adorn_totals(c(\"row\", \"col\")) %&gt;% \n  knitr::kable() %&gt;% \n  kableExtra::row_spec(row = 5, bold = TRUE) %&gt;% \n  kableExtra::column_spec(column = 5, bold = TRUE) \n\n\n\n\nMeasurement\nFacility\n2000\n2001\n2002\nTotal\n\n\n\n\n1\nHosp 1\n66\nNA\nNA\n66\n\n\n2\nHosp 1\nNA\n26\nNA\n26\n\n\n3\nHosp 1\nNA\nNA\n8\n8\n\n\n1\nHosp 2\n71\nNA\nNA\n71\n\n\n2\nHosp 2\nNA\n62\nNA\n62\n\n\n3\nHosp 2\nNA\nNA\n70\n70\n\n\n1\nHosp 3\n47\nNA\nNA\n47\n\n\n2\nHosp 3\nNA\n70\nNA\n70\n\n\n3\nHosp 3\nNA\nNA\n38\n38\n\n\n1\nHosp 4\n82\nNA\nNA\n82\n\n\n2\nHosp 4\nNA\n87\nNA\n87\n\n\n3\nHosp 4\nNA\nNA\n46\n46\n\n\nTotal\n-\n266\n245\n162\n673\n\n\n\n\n\n\n\nN.B. In this case, we had to specify to only include the three variables Facility, Year, and Cases as the additional variable Measurement would interfere with the creation of the table:\n\ndf_combined %&gt;% \n  pivot_wider(\n    names_from = \"Year\",\n    values_from = \"Cases\"\n  ) %&gt;% \n  knitr::kable()\n\n\n\n\nMeasurement\nFacility\n2000\n2001\n2002\n\n\n\n\n1\nHosp 4\n82\nNA\nNA\n\n\n1\nHosp 3\n47\nNA\nNA\n\n\n1\nHosp 2\n71\nNA\nNA\n\n\n1\nHosp 1\n66\nNA\nNA\n\n\n2\nHosp 4\nNA\n87\nNA\n\n\n2\nHosp 3\nNA\n70\nNA\n\n\n2\nHosp 2\nNA\n62\nNA\n\n\n2\nHosp 1\nNA\n26\nNA\n\n\n3\nHosp 4\nNA\nNA\n46\n\n\n3\nHosp 3\nNA\nNA\n38\n\n\n3\nHosp 2\nNA\nNA\n70\n\n\n3\nHosp 1\nNA\nNA\n8",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pivoting data</span>"
    ]
  },
  {
    "objectID": "new_pages/pivoting.html#resources",
    "href": "new_pages/pivoting.html#resources",
    "title": "12  Pivoting data",
    "section": "12.5 Resources",
    "text": "12.5 Resources\nHere is a helpful tutorial",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pivoting data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html",
    "href": "new_pages/grouping.html",
    "title": "13  Grouping data",
    "section": "",
    "text": "13.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#preparation",
    "href": "new_pages/grouping.html#preparation",
    "title": "13  Grouping data",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,       # to import data\n  here,      # to locate files\n  tidyverse, # to clean, handle, and plot the data (includes dplyr)\n  janitor)   # adding total rows and columns\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). The dataset is imported using the import() function from the rio package. See the page on Import and export for various ways to import data.\n\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of linelist:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#grouping",
    "href": "new_pages/grouping.html#grouping",
    "title": "13  Grouping data",
    "section": "13.2 Grouping",
    "text": "13.2 Grouping\nThe function group_by() from dplyr groups the rows by the unique values in the column specified to it. If multiple columns are specified, rows are grouped by the unique combinations of values across the columns. Each unique value (or combination of values) constitutes a group. Subsequent changes to the dataset or calculations can then be performed within the context of each group.\nFor example, the command below takes the linelist and groups the rows by unique values in the column outcome, saving the output as a new data frame ll_by_outcome. The grouping column(s) are placed inside the parentheses of the function group_by().\n\nll_by_outcome &lt;- linelist %&gt;% \n  group_by(outcome)\n\nNote that there is no perceptible change to the dataset after running group_by(), until another dplyr verb such as mutate(), summarise(), or arrange() is applied on the “grouped” data frame.\nYou can however “see” the groupings by printing the data frame. When you print a grouped data frame, you will see it has been transformed into a tibble class object which, when printed, displays which groupings have been applied and how many groups there are - written just above the header row.\n\n# print to see which groups are active\nll_by_outcome\n\n# A tibble: 5,888 × 30\n# Groups:   outcome [3]\n   case_id generation date_infection date_onset date_hospitalisation\n   &lt;chr&gt;        &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;     &lt;date&gt;              \n 1 5fe599           4 2014-05-08     2014-05-13 2014-05-15          \n 2 8689b7           4 NA             2014-05-13 2014-05-14          \n 3 11f8ea           2 NA             2014-05-16 2014-05-18          \n 4 b8812a           3 2014-05-04     2014-05-18 2014-05-20          \n 5 893f25           3 2014-05-18     2014-05-21 2014-05-22          \n 6 be99c8           3 2014-05-03     2014-05-22 2014-05-23          \n 7 07e3e8           4 2014-05-22     2014-05-27 2014-05-29          \n 8 369449           4 2014-05-28     2014-06-02 2014-06-03          \n 9 f393b4           4 NA             2014-06-05 2014-06-06          \n10 1389ca           4 NA             2014-06-05 2014-06-07          \n# ℹ 5,878 more rows\n# ℹ 25 more variables: date_outcome &lt;date&gt;, outcome &lt;chr&gt;, gender &lt;chr&gt;,\n#   age &lt;dbl&gt;, age_unit &lt;chr&gt;, age_years &lt;dbl&gt;, age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;,\n#   hospital &lt;chr&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, infector &lt;chr&gt;, source &lt;chr&gt;,\n#   wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;, ct_blood &lt;dbl&gt;, fever &lt;chr&gt;, chills &lt;chr&gt;,\n#   cough &lt;chr&gt;, aches &lt;chr&gt;, vomit &lt;chr&gt;, temp &lt;dbl&gt;, time_admission &lt;chr&gt;,\n#   bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\n\nUnique groups\nThe groups created reflect each unique combination of values across the grouping columns.\nTo see the groups and the number of rows in each group, pass the grouped data to tally(). To see just the unique groups without counts you can pass to group_keys().\nSee below that there are three unique values in the grouping column outcome: “Death”, “Recover”, and NA. See that there were nrow(linelist %&gt;% filter(outcome == \"Death\")) deaths, nrow(linelist %&gt;% filter(outcome == \"Recover\")) recoveries, and nrow(linelist %&gt;% filter(is.na(outcome))) with no outcome recorded.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  tally()\n\n# A tibble: 3 × 2\n  outcome     n\n  &lt;chr&gt;   &lt;int&gt;\n1 Death    2582\n2 Recover  1983\n3 &lt;NA&gt;     1323\n\n\nYou can group by more than one column. Below, the data frame is grouped by outcome and gender, and then tallied. Note how each unique combination of outcome and gender is registered as its own group - including missing values for either column.\n\nlinelist %&gt;% \n  group_by(outcome, gender) %&gt;% \n  tally()\n\n# A tibble: 9 × 3\n# Groups:   outcome [3]\n  outcome gender     n\n  &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt;\n1 Death   f       1227\n2 Death   m       1228\n3 Death   &lt;NA&gt;     127\n4 Recover f        953\n5 Recover m        950\n6 Recover &lt;NA&gt;      80\n7 &lt;NA&gt;    f        627\n8 &lt;NA&gt;    m        625\n9 &lt;NA&gt;    &lt;NA&gt;      71\n\n\n\n\nNew columns\nYou can also create a new grouping column within the group_by() statement. This is equivalent to calling mutate() before the group_by(). For a quick tabulation this style can be handy, but for more clarity in your code consider creating this column in its own mutate() step and then piping to group_by().\n\n# group dat based on a binary column created *within* the group_by() command\nlinelist %&gt;% \n  group_by(\n    age_class = ifelse(age &gt;= 18, \"adult\", \"child\")) %&gt;% \n  tally(sort = T)\n\n# A tibble: 3 × 2\n  age_class     n\n  &lt;chr&gt;     &lt;int&gt;\n1 child      3618\n2 adult      2184\n3 &lt;NA&gt;         86\n\n\n\n\nAdd/drop grouping columns\nBy default, if you run group_by() on data that are already grouped, the old groups will be removed and the new one(s) will apply. If you want to add new groups to the existing ones, include the argument .add = TRUE.\n\n# Grouped by outcome\nby_outcome &lt;- linelist %&gt;% \n  group_by(outcome)\n\n# Add grouping by gender in addition\nby_outcome_gender &lt;- by_outcome %&gt;% \n  group_by(gender, .add = TRUE)\n\n** Keep all groups**\nIf you group on a column of class factor there may be levels of the factor that are not currently present in the data. If you group on this column, by default those non-present levels are dropped and not included as groups. To change this so that all levels appear as groups (even if not present in the data), set .drop = FALSE in your group_by() command.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#un-group",
    "href": "new_pages/grouping.html#un-group",
    "title": "13  Grouping data",
    "section": "13.3 Un-group",
    "text": "13.3 Un-group\nData that have been grouped will remain grouped until specifically ungrouped via ungroup(). If you forget to ungroup, it can lead to incorrect calculations! Below is an example of removing all groupings:\n\nlinelist %&gt;% \n  group_by(outcome, gender) %&gt;% \n  tally() %&gt;% \n  ungroup()\n\nYou can also remove grouping for only specific columns, by placing the column name inside ungroup().\n\nlinelist %&gt;% \n  group_by(outcome, gender) %&gt;% \n  tally() %&gt;% \n  ungroup(gender) # remove the grouping by gender, leave grouping by outcome\n\nNOTE: The verb count() automatically ungroups the data after counting.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#group_summarise",
    "href": "new_pages/grouping.html#group_summarise",
    "title": "13  Grouping data",
    "section": "13.4 Summarise",
    "text": "13.4 Summarise\nSee the dplyr section of the Descriptive tables page for a detailed description of how to produce summary tables with summarise(). Here we briefly address how its behavior changes when applied to grouped data.\nThe dplyr function summarise() (or summarize()) takes a data frame and converts it into a new summary data frame, with columns containing summary statistics that you define. On an ungrouped data frame, the summary statistics will be calculated from all rows. Applying summarise() to grouped data produces those summary statistics for each group.\nThe syntax of summarise() is such that you provide the name(s) of the new summary column(s), an equals sign, and then a statistical function to apply to the data, as shown below. For example, min(), max(), median(), or sd(). Within the statistical function, list the column to be operated on and any relevant argument (e.g. na.rm = TRUE). You can use sum() to count the number of rows that meet a logical criteria (with double equals ==).\nBelow is an example of summarise() applied without grouped data. The statistics returned are produced from the entire dataset.\n\n# summary statistics on ungrouped linelist\nlinelist %&gt;% \n  summarise(\n    n_cases  = n(),\n    mean_age = mean(age_years, na.rm=T),\n    max_age  = max(age_years, na.rm=T),\n    min_age  = min(age_years, na.rm=T),\n    n_males  = sum(gender == \"m\", na.rm=T))\n\n  n_cases mean_age max_age min_age n_males\n1    5888 16.01831      84       0    2803\n\n\nIn contrast, below is the same summarise() statement applied to grouped data. The statistics are calculated for each outcome group. Note how grouping columns will carry over into the new data frame.\n\n# summary statistics on grouped linelist\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(\n    n_cases  = n(),\n    mean_age = mean(age_years, na.rm=T),\n    max_age  = max(age_years, na.rm=T),\n    min_age  = min(age_years, na.rm=T),\n    n_males    = sum(gender == \"m\", na.rm=T))\n\n# A tibble: 3 × 6\n  outcome n_cases mean_age max_age min_age n_males\n  &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n1 Death      2582     15.9      76       0    1228\n2 Recover    1983     16.1      84       0     950\n3 &lt;NA&gt;       1323     16.2      69       0     625\n\n\nTIP: The summarise function works with both UK and US spelling - summarise() and summarize() call the same function.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#counts-and-tallies",
    "href": "new_pages/grouping.html#counts-and-tallies",
    "title": "13  Grouping data",
    "section": "13.5 Counts and tallies",
    "text": "13.5 Counts and tallies\ncount() and tally() provide similar functionality but are different. Read more about the distinction between tally() and count() here\n\ntally()\ntally() is shorthand for summarise(n = n()), and does not group data. Thus, to achieve grouped tallys it must follow a group_by() command. You can add sort = TRUE to see the largest groups first.\n\nlinelist %&gt;% \n  tally()\n\n     n\n1 5888\n\n\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  tally(sort = TRUE)\n\n# A tibble: 3 × 2\n  outcome     n\n  &lt;chr&gt;   &lt;int&gt;\n1 Death    2582\n2 Recover  1983\n3 &lt;NA&gt;     1323\n\n\n\n\ncount()\nIn contrast, count() does the following:\n\napplies group_by() on the specified column(s)\n\napplies summarise() and returns column n with the number of rows per group\n\napplies ungroup()\n\n\nlinelist %&gt;% \n  count(outcome)\n\n  outcome    n\n1   Death 2582\n2 Recover 1983\n3    &lt;NA&gt; 1323\n\n\nJust like with group_by() you can create a new column within the count() command:\n\nlinelist %&gt;% \n  count(age_class = ifelse(age &gt;= 18, \"adult\", \"child\"), sort = T)\n\n  age_class    n\n1     child 3618\n2     adult 2184\n3      &lt;NA&gt;   86\n\n\ncount() can be called multiple times, with the functionality “rolling up”. For example, to summarise the number of hospitals present for each gender, run the following. Note, the name of the final column is changed from default “n” for clarity (with name  =).\n\nlinelist %&gt;% \n  # produce counts by unique outcome-gender groups\n  count(gender, hospital) %&gt;% \n  # gather rows by gender (3) and count number of hospitals per gender (6)\n  count(gender, name = \"hospitals per gender\" ) \n\n  gender hospitals per gender\n1      f                    6\n2      m                    6\n3   &lt;NA&gt;                    6\n\n\n\n\nAdd counts\nIn contrast to count() and summarise(), you can use add_count() to add a new column n with the counts of rows per group while retaining all the other data frame columns.\nThis means that a group’s count number, in the new column n, will be printed in each row of the group. For demonstration purposes, we add this column and then re-arrange the columns for easier viewing. See the section below on filter on group size for another example.\n\nlinelist %&gt;% \n  as_tibble() %&gt;%                   # convert to tibble for nicer printing \n  add_count(hospital) %&gt;%           # add column n with counts by hospital\n  select(hospital, n, everything()) # re-arrange for demo purposes\n\n# A tibble: 5,888 × 31\n   hospital                       n case_id generation date_infection date_onset\n   &lt;chr&gt;                      &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;    \n 1 Other                        885 5fe599           4 2014-05-08     2014-05-13\n 2 Missing                     1469 8689b7           4 NA             2014-05-13\n 3 St. Mark's Maternity Hosp…   422 11f8ea           2 NA             2014-05-16\n 4 Port Hospital               1762 b8812a           3 2014-05-04     2014-05-18\n 5 Military Hospital            896 893f25           3 2014-05-18     2014-05-21\n 6 Port Hospital               1762 be99c8           3 2014-05-03     2014-05-22\n 7 Missing                     1469 07e3e8           4 2014-05-22     2014-05-27\n 8 Missing                     1469 369449           4 2014-05-28     2014-06-02\n 9 Missing                     1469 f393b4           4 NA             2014-06-05\n10 Missing                     1469 1389ca           4 NA             2014-06-05\n# ℹ 5,878 more rows\n# ℹ 25 more variables: date_hospitalisation &lt;date&gt;, date_outcome &lt;date&gt;,\n#   outcome &lt;chr&gt;, gender &lt;chr&gt;, age &lt;dbl&gt;, age_unit &lt;chr&gt;, age_years &lt;dbl&gt;,\n#   age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, infector &lt;chr&gt;,\n#   source &lt;chr&gt;, wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;, ct_blood &lt;dbl&gt;, fever &lt;chr&gt;,\n#   chills &lt;chr&gt;, cough &lt;chr&gt;, aches &lt;chr&gt;, vomit &lt;chr&gt;, temp &lt;dbl&gt;,\n#   time_admission &lt;chr&gt;, bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\n\n\nAdd totals\nTo easily add total sum rows or columns after using tally() or count(), see the janitor section of the Descriptive tables page. This package offers functions like adorn_totals() and adorn_percentages() to add totals and convert to show percentages. Below is a brief example:\n\nlinelist %&gt;%                                  # case linelist\n  tabyl(age_cat, gender) %&gt;%                  # cross-tabulate counts of two columns\n  adorn_totals(where = \"row\") %&gt;%             # add a total row\n  adorn_percentages(denominator = \"col\") %&gt;%  # convert to proportions with column denominator\n  adorn_pct_formatting() %&gt;%                  # convert proportions to percents\n  adorn_ns(position = \"front\") %&gt;%            # display as: \"count (percent)\"\n  adorn_title(                                # adjust titles\n    row_name = \"Age Category\",\n    col_name = \"Gender\")\n\n                      Gender                            \n Age Category              f              m          NA_\n          0-4   640  (22.8%)   416  (14.8%)  39  (14.0%)\n          5-9   641  (22.8%)   412  (14.7%)  42  (15.1%)\n        10-14   518  (18.5%)   383  (13.7%)  40  (14.4%)\n        15-19   359  (12.8%)   364  (13.0%)  20   (7.2%)\n        20-29   468  (16.7%)   575  (20.5%)  30  (10.8%)\n        30-49   179   (6.4%)   557  (19.9%)  18   (6.5%)\n        50-69     2   (0.1%)    91   (3.2%)   2   (0.7%)\n          70+     0   (0.0%)     5   (0.2%)   1   (0.4%)\n         &lt;NA&gt;     0   (0.0%)     0   (0.0%)  86  (30.9%)\n        Total 2,807 (100.0%) 2,803 (100.0%) 278 (100.0%)\n\n\nTo add more complex totals rows that involve summary statistics other than sums, see this section of the Descriptive Tables page.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#grouping-by-date",
    "href": "new_pages/grouping.html#grouping-by-date",
    "title": "13  Grouping data",
    "section": "13.6 Grouping by date",
    "text": "13.6 Grouping by date\nWhen grouping data by date, you must have (or create) a column for the date unit of interest - for example “day”, “epiweek”, “month”, etc. You can make this column using floor_date() from lubridate, as explained in the Epidemiological weeks section of the Working with dates page. Once you have this column, you can use count() from dplyr to group the rows by those unique date values and achieve aggregate counts.\nOne additional step common for date situations, is to “fill-in” any dates in the sequence that are not present in the data. Use complete() from tidyr so that the aggregated date series is complete including all possible date units within the range. Without this step, a week with no cases reported might not appear in your data!\nWithin complete() you re-define your date column as a sequence of dates seq.Date() from the minimum to the maximum - thus the dates are expanded. By default, the case count values in any new “expanded” rows will be NA. You can set them to 0 using the fill = argument of complete(), which expects a named list (if your counts column is named n, provide fill = list(n = 0). See ?complete for details and the Working with dates page for an example.\n\nLinelist cases into days\nHere is an example of grouping cases into days without using complete(). Note the first rows skip over dates with no cases.\n\ndaily_counts &lt;- linelist %&gt;% \n  drop_na(date_onset) %&gt;%        # remove that were missing date_onset\n  count(date_onset)              # count number of rows per unique date\n\n\n\n\n\n\n\nBelow we add the complete() command to ensure every day in the range is represented.\n\ndaily_counts &lt;- linelist %&gt;% \n  drop_na(date_onset) %&gt;%                 # remove case missing date_onset\n  count(date_onset) %&gt;%                   # count number of rows per unique date\n  complete(                               # ensure all days appear even if no cases\n    date_onset = seq.Date(                # re-define date colume as daily sequence of dates\n      from = min(date_onset, na.rm=T), \n      to = max(date_onset, na.rm=T),\n      by = \"day\"),\n    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) \n\n\n\n\n\n\n\n\n\nLinelist cases into weeks\nThe same principle can be applied for weeks. First create a new column that is the week of the case using floor_date() with unit = \"week\". Then, use count() as above to achieve weekly case counts. Finish with complete() to ensure that all weeks are represented, even if they contain no cases.\n\n# Make dataset of weekly case counts\nweekly_counts &lt;- linelist %&gt;% \n  drop_na(date_onset) %&gt;%                 # remove cases missing date_onset\n  mutate(week = lubridate::floor_date(date_onset, unit = \"week\")) %&gt;%  # new column of week of onset\n  count(week) %&gt;%                         # group data by week and count rows per group\n  complete(                               # ensure all days appear even if no cases\n    week = seq.Date(                      # re-define date colume as daily sequence of dates\n      from = min(week, na.rm=T), \n      to = max(week, na.rm=T),\n      by = \"week\"),\n    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) \n\nHere are the first 50 rows of the resulting data frame:\n\n\n\n\n\n\n\n\nLinelist cases into months\nTo aggregate cases into months, again use floor_date() from the lubridate package, but with the argument unit = \"months\". This rounds each date down to the 1st of its month. The output will be class Date. Note that in the complete() step we also use by = \"months\".\n\n# Make dataset of monthly case counts\nmonthly_counts &lt;- linelist %&gt;% \n  drop_na(date_onset) %&gt;% \n  mutate(month = lubridate::floor_date(date_onset, unit = \"months\")) %&gt;%  # new column, 1st of month of onset\n  count(month) %&gt;%                          # count cases by month\n  complete(\n    month = seq.Date(\n      min(month, na.rm=T),     # include all months with no cases reported\n      max(month, na.rm=T),\n      by=\"month\"),\n    fill = list(n = 0))\n\n\n\n\n\n\n\n\n\nDaily counts into weeks\nTo aggregate daily counts into weekly counts, use floor_date() as above. However, use group_by() and summarize() instead of count() because you need to sum() daily case counts instead of just counting the number of rows per week.\n\nDaily counts into months\nTo aggregate daily counts into months counts, use floor_date() with unit = \"month\" as above. However, use group_by() and summarize() instead of count() because you need to sum() daily case counts instead of just counting the number of rows per month.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#arranging-grouped-data",
    "href": "new_pages/grouping.html#arranging-grouped-data",
    "title": "13  Grouping data",
    "section": "13.7 Arranging grouped data",
    "text": "13.7 Arranging grouped data\nUsing the dplyr verb arrange() to order the rows in a data frame behaves the same when the data are grouped, unless you set the argument .by_group =TRUE. In this case the rows are ordered first by the grouping columns and then by any other columns you specify to arrange().",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#filter-on-grouped-data",
    "href": "new_pages/grouping.html#filter-on-grouped-data",
    "title": "13  Grouping data",
    "section": "13.8 Filter on grouped data",
    "text": "13.8 Filter on grouped data\n\nfilter()\nWhen applied in conjunction with functions that evaluate the data frame (like max(), min(), mean()), these functions will now be applied to the groups. For example, if you want to filter and keep rows where patients are above the median age, this will now apply per group - filtering to keep rows above the group’s median age.\n\n\nSlice rows per group\nThe dplyr function slice(), which filters rows based on their position in the data, can also be applied per group. Remember to account for sorting the data within each group to get the desired “slice”.\nFor example, to retrieve only the latest 5 admissions from each hospital:\n\nGroup the linelist by column hospital\n\nArrange the records from latest to earliest date_hospitalisation within each hospital group\n\nSlice to retrieve the first 5 rows from each hospital\n\n\nlinelist %&gt;%\n  group_by(hospital) %&gt;%\n  arrange(hospital, date_hospitalisation) %&gt;%\n  slice_head(n = 5) %&gt;% \n  arrange(hospital) %&gt;%                            # for display\n  select(case_id, hospital, date_hospitalisation)  # for display\n\n# A tibble: 30 × 3\n# Groups:   hospital [6]\n   case_id hospital          date_hospitalisation\n   &lt;chr&gt;   &lt;chr&gt;             &lt;date&gt;              \n 1 20b688  Central Hospital  2014-05-06          \n 2 d58402  Central Hospital  2014-05-10          \n 3 b8f2fd  Central Hospital  2014-05-13          \n 4 acf422  Central Hospital  2014-05-28          \n 5 275cc7  Central Hospital  2014-05-28          \n 6 d1fafd  Military Hospital 2014-04-17          \n 7 974bc1  Military Hospital 2014-05-13          \n 8 6a9004  Military Hospital 2014-05-13          \n 9 09e386  Military Hospital 2014-05-14          \n10 865581  Military Hospital 2014-05-15          \n# ℹ 20 more rows\n\n\nslice_head() - selects n rows from the top\nslice_tail() - selects n rows from the end\nslice_sample() - randomly selects n rows\nslice_min() - selects n rows with highest values in order_by = column, use with_ties = TRUE to keep ties\nslice_max() - selects n rows with lowest values in order_by = column, use with_ties = TRUE to keep ties\nSee the De-duplication page for more examples and detail on slice().\n\n\nFilter on group size\nThe function add_count() adds a column n to the original data giving the number of rows in that row’s group.\nShown below, add_count() is applied to the column hospital, so the values in the new column n reflect the number of rows in that row’s hospital group. Note how values in column n are repeated. In the example below, the column name n could be changed using name = within add_count(). For demonstration purposes we re-arrange the columns with select().\n\nlinelist %&gt;% \n  as_tibble() %&gt;% \n  add_count(hospital) %&gt;%          # add \"number of rows admitted to same hospital as this row\" \n  select(hospital, n, everything())\n\n# A tibble: 5,888 × 31\n   hospital                       n case_id generation date_infection date_onset\n   &lt;chr&gt;                      &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;    \n 1 Other                        885 5fe599           4 2014-05-08     2014-05-13\n 2 Missing                     1469 8689b7           4 NA             2014-05-13\n 3 St. Mark's Maternity Hosp…   422 11f8ea           2 NA             2014-05-16\n 4 Port Hospital               1762 b8812a           3 2014-05-04     2014-05-18\n 5 Military Hospital            896 893f25           3 2014-05-18     2014-05-21\n 6 Port Hospital               1762 be99c8           3 2014-05-03     2014-05-22\n 7 Missing                     1469 07e3e8           4 2014-05-22     2014-05-27\n 8 Missing                     1469 369449           4 2014-05-28     2014-06-02\n 9 Missing                     1469 f393b4           4 NA             2014-06-05\n10 Missing                     1469 1389ca           4 NA             2014-06-05\n# ℹ 5,878 more rows\n# ℹ 25 more variables: date_hospitalisation &lt;date&gt;, date_outcome &lt;date&gt;,\n#   outcome &lt;chr&gt;, gender &lt;chr&gt;, age &lt;dbl&gt;, age_unit &lt;chr&gt;, age_years &lt;dbl&gt;,\n#   age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, infector &lt;chr&gt;,\n#   source &lt;chr&gt;, wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;, ct_blood &lt;dbl&gt;, fever &lt;chr&gt;,\n#   chills &lt;chr&gt;, cough &lt;chr&gt;, aches &lt;chr&gt;, vomit &lt;chr&gt;, temp &lt;dbl&gt;,\n#   time_admission &lt;chr&gt;, bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\nIt then becomes easy to filter for case rows who were hospitalized at a “small” hospital, say, a hospital that admitted fewer than 500 patients:\n\nlinelist %&gt;% \n  add_count(hospital) %&gt;% \n  filter(n &lt; 500)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#mutate-on-grouped-data",
    "href": "new_pages/grouping.html#mutate-on-grouped-data",
    "title": "13  Grouping data",
    "section": "13.9 Mutate on grouped data",
    "text": "13.9 Mutate on grouped data\nTo retain all columns and rows (not summarise) and add a new column containing group statistics, use mutate() after group_by() instead of summarise().\nThis is useful if you want group statistics in the original dataset with all other columns present - e.g. for calculations that compare one row to its group.\nFor example, this code below calculates the difference between a row’s delay-to-admission and the median delay for their hospital. The steps are:\n\nGroup the data by hospital\n\nUse the column days_onset_hosp (delay to hospitalisation) to create a new column containing the mean delay at the hospital of that row\n\nCalculate the difference between the two columns\n\nWe select() only certain columns to display, for demonstration purposes.\n\nlinelist %&gt;% \n  # group data by hospital (no change to linelist yet)\n  group_by(hospital) %&gt;% \n  \n  # new columns\n  mutate(\n    # mean days to admission per hospital (rounded to 1 decimal)\n    group_delay_admit = round(mean(days_onset_hosp, na.rm=T), 1),\n    \n    # difference between row's delay and mean delay at their hospital (rounded to 1 decimal)\n    diff_to_group     = round(days_onset_hosp - group_delay_admit, 1)) %&gt;%\n  \n  # select certain rows only - for demonstration/viewing purposes\n  select(case_id, hospital, days_onset_hosp, group_delay_admit, diff_to_group)\n\n# A tibble: 5,888 × 5\n# Groups:   hospital [6]\n   case_id hospital              days_onset_hosp group_delay_admit diff_to_group\n   &lt;chr&gt;   &lt;chr&gt;                           &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 5fe599  Other                               2               2             0  \n 2 8689b7  Missing                             1               2.1          -1.1\n 3 11f8ea  St. Mark's Maternity…               2               2.1          -0.1\n 4 b8812a  Port Hospital                       2               2.1          -0.1\n 5 893f25  Military Hospital                   1               2.1          -1.1\n 6 be99c8  Port Hospital                       1               2.1          -1.1\n 7 07e3e8  Missing                             2               2.1          -0.1\n 8 369449  Missing                             1               2.1          -1.1\n 9 f393b4  Missing                             1               2.1          -1.1\n10 1389ca  Missing                             2               2.1          -0.1\n# ℹ 5,878 more rows",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#select-on-grouped-data",
    "href": "new_pages/grouping.html#select-on-grouped-data",
    "title": "13  Grouping data",
    "section": "13.10 Select on grouped data",
    "text": "13.10 Select on grouped data\nThe verb select() works on grouped data, but the grouping columns are always included (even if not mentioned in select()). If you do not want these grouping columns, use ungroup() first.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/grouping.html#resources",
    "href": "new_pages/grouping.html#resources",
    "title": "13  Grouping data",
    "section": "13.11 Resources",
    "text": "13.11 Resources\nHere are some useful resources for more information:\nYou can perform any summary function on grouped data; see the RStudio data transformation cheat sheet\nThe Data Carpentry page on dplyr\nThe tidyverse reference pages on group_by() and grouping\nThis page on Data manipulation\nSummarize with conditions in dplyr",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping data</span>"
    ]
  },
  {
    "objectID": "new_pages/joining_matching.html",
    "href": "new_pages/joining_matching.html",
    "title": "14  Joining data",
    "section": "",
    "text": "14.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "new_pages/joining_matching.html#preparation",
    "href": "new_pages/joining_matching.html#preparation",
    "title": "14  Joining data",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,            # import and export\n  here,           # locate files \n  tidyverse,      # data management and visualisation\n  RecordLinkage,  # probabilistic matches\n  fastLink        # probabilistic matches\n)\n\n\n\nImport data\nTo begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import case linelist \nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.\n\n\n\n\n\n\n\n\n\nExample datasets\nIn the joining section below, we will use the following datasets:\n\nA “miniature” version of the case linelist, containing only the columns case_id, date_onset, and hospital, and only the first 10 rows\n\nA separate data frame named hosp_info, which contains more details about each hospital\n\nIn the section on probabilistic matching, we will use two different small datasets. The code to create those datasets is given in that section.\n\n“Miniature” case linelist\nBelow is the the miniature case linelist, which contains only 10 rows and only columns case_id, date_onset, and hospital.\n\nlinelist_mini &lt;- linelist %&gt;%                 # start with original linelist\n  select(case_id, date_onset, hospital) %&gt;%   # select columns\n  head(10)                                    # only take the first 10 rows\n\n\n\n\n\n\n\n\n\nHospital information data frame\nBelow is the code to create a separate data frame with additional information about seven hospitals (the catchment population, and the level of care available). Note that the name “Military Hospital” belongs to two different hospitals - one a primary level serving 10000 residents and the other a secondary level serving 50280 residents.\n\n# Make the hospital information data frame\nhosp_info = data.frame(\n  hosp_name     = c(\"central hospital\", \"military\", \"military\", \"port\", \"St. Mark's\", \"ignace\", \"sisters\"),\n  catchment_pop = c(1950280, 40500, 10000, 50280, 12000, 5000, 4200),\n  level         = c(\"Tertiary\", \"Secondary\", \"Primary\", \"Secondary\", \"Secondary\", \"Primary\", \"Primary\")\n)\n\nHere is this data frame:\n\n\n\n\n\n\n\n\n\n\nPre-cleaning\nTraditional joins (non-probabilistic) are case-sensitive and require exact character matches between values in the two data frames. To demonstrate some of the cleaning steps you might need to do before initiating a join, we will clean and align the linelist_mini and hosp_info datasets now.\nIdentify differences\nWe need the values of the hosp_name column in the hosp_info data frame to match the values of the hospital column in the linelist_mini data frame.\nHere are the values in the linelist_mini data frame, printed with the base R function unique():\n\nunique(linelist_mini$hospital)\n\n[1] \"Other\"                               \n[2] \"Missing\"                             \n[3] \"St. Mark's Maternity Hospital (SMMH)\"\n[4] \"Port Hospital\"                       \n[5] \"Military Hospital\"                   \n\n\nand here are the values in the hosp_info data frame:\n\nunique(hosp_info$hosp_name)\n\n[1] \"central hospital\" \"military\"         \"port\"             \"St. Mark's\"      \n[5] \"ignace\"           \"sisters\"         \n\n\nYou can see that while some of the hospitals exist in both data frames, there are many differences in spelling.\nAlign values\nWe begin by cleaning the values in the hosp_info data frame. As explained in the Cleaning data and core functions page, we can re-code values with logical criteria using dplyr’s case_when() function. For the four hospitals that exist in both data frames we change the values to align with the values in linelist_mini. The other hospitals we leave the values as they are (TRUE ~ hosp_name).\nCAUTION: Typically when cleaning one should create a new column (e.g. hosp_name_clean), but for ease of demonstration we show modification of the old column\n\nhosp_info &lt;- hosp_info %&gt;% \n  mutate(\n    hosp_name = case_when(\n      # criteria                         # new value\n      hosp_name == \"military\"          ~ \"Military Hospital\",\n      hosp_name == \"port\"              ~ \"Port Hospital\",\n      hosp_name == \"St. Mark's\"        ~ \"St. Mark's Maternity Hospital (SMMH)\",\n      hosp_name == \"central hospital\"  ~ \"Central Hospital\",\n      TRUE                             ~ hosp_name\n      )\n    )\n\nThe hospital names that appear in both data frames are aligned. There are two hospitals in hosp_info that are not present in linelist_mini - we will deal with these later, in the join.\n\nunique(hosp_info$hosp_name)\n\n[1] \"Central Hospital\"                    \n[2] \"Military Hospital\"                   \n[3] \"Port Hospital\"                       \n[4] \"St. Mark's Maternity Hospital (SMMH)\"\n[5] \"ignace\"                              \n[6] \"sisters\"                             \n\n\nPrior to a join, it is often easiest to convert a column to all lowercase or all uppercase. If you need to convert all values in a column to UPPER or lower case, use mutate() and wrap the column with one of these functions from stringr, as shown in the page on Characters and strings.\nstr_to_upper()\nstr_to_upper()\nstr_to_title()",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "new_pages/joining_matching.html#dplyr-joins",
    "href": "new_pages/joining_matching.html#dplyr-joins",
    "title": "14  Joining data",
    "section": "14.2 dplyr joins",
    "text": "14.2 dplyr joins\nThe dplyr package offers several different join functions. dplyr is included in the tidyverse package. These join functions are described below, with simple use cases.\nMany thanks to https://github.com/gadenbuie for the informative gifs!\n\n\nGeneral syntax\nThe join commands can be run as standalone commands to join two data frames into a new object, or they can be used within a pipe chain (%&gt;%) to merge one data frame into another as it is being cleaned or otherwise modified.\nIn the example below, the function left_join() is used as a standalone command to create the a new joined_data data frame. The inputs are data frames 1 and 2 (df1 and df2). The first data frame listed is the baseline data frame, and the second one listed is joined to it.\nThe third argument by = is where you specify the columns in each data frame that will be used to aligns the rows in the two data frames. If the names of these columns are different, provide them within a c() vector as shown below, where the rows are matched on the basis of common values between the column ID in df1 and the column identifier in df2.\n\n# Join based on common values between column \"ID\" (first data frame) and column \"identifier\" (second data frame)\njoined_data &lt;- left_join(df1, df2, by = c(\"ID\" = \"identifier\"))\n\nIf the by columns in both data frames have the exact same name, you can just provide this one name, within quotes.\n\n# Joint based on common values in column \"ID\" in both data frames\njoined_data &lt;- left_join(df1, df2, by = \"ID\")\n\nIf you are joining the data frames based on common values across multiple fields, list these fields within the c() vector. This example joins rows if the values in three columns in each dataset align exactly.\n\n# join based on same first name, last name, and age\njoined_data &lt;- left_join(df1, df2, by = c(\"name\" = \"firstname\", \"surname\" = \"lastname\", \"Age\" = \"age\"))\n\nThe join commands can also be run within a pipe chain. This will modify the data frame being piped.\nIn the example below, df1 is is passed through the pipes, df2 is joined to it, and df is thus modified and re-defined.\n\ndf1 &lt;- df1 %&gt;%\n  filter(date_onset &lt; as.Date(\"2020-03-05\")) %&gt;% # miscellaneous cleaning \n  left_join(df2, by = c(\"ID\" = \"identifier\"))    # join df2 to df1\n\nCAUTION: Joins are case-specific! Therefore it is useful to convert all values to lowercase or uppercase prior to joining. See the page on characters/strings.\n\n\n\nLeft and right joins\nA left or right join is commonly used to add information to a data frame - new information is added only to rows that already existed in the baseline data frame. These are common joins in epidemiological work as they are used to add information from one dataset into another.\nIn using these joins, the written order of the data frames in the command is important*.\n\nIn a left join, the first data frame written is the baseline\n\nIn a right join, the second data frame written is the baseline\n\nAll rows of the baseline data frame are kept. Information in the other (secondary) data frame is joined to the baseline data frame only if there is a match via the identifier column(s). In addition:\n\nRows in the secondary data frame that do not match are dropped.\n\nIf there are many baseline rows that match to one row in the secondary data frame (many-to-one), the secondary information is added to each matching baseline row.\n\nIf a baseline row matches to multiple rows in the secondary data frame (one-to-many), all combinations are given, meaning new rows may be added to your returned data frame!\n\nAnimated examples of left and right joins (image source)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample\nBelow is the output of a left_join() of hosp_info (secondary data frame, view here) into linelist_mini (baseline data frame, view here). The original linelist_mini has nrow(linelist_mini) rows. The modified linelist_mini is displayed. Note the following:\n\nTwo new columns, catchment_pop and level have been added on the left side of linelist_mini\n\nAll original rows of the baseline data frame linelist_mini are kept\n\nAny original rows of linelist_mini for “Military Hospital” are duplicated because it matched to two rows in the secondary data frame, so both combinations are returned\n\nThe join identifier column of the secondary dataset (hosp_name) has disappeared because it is redundant with the identifier column in the primary dataset (hospital)\n\nWhen a baseline row did not match to any secondary row (e.g. when hospital is “Other” or “Missing”), NA (blank) fills in the columns from the secondary data frame\n\nRows in the secondary data frame with no match to the baseline data frame (“sisters” and “ignace” hospitals) were dropped\n\n\nlinelist_mini %&gt;% \n  left_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in left_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n“Should I use a right join, or a left join?”\nTo answer the above question, ask yourself “which data frame should retain all of its rows?” - use this one as the baseline. A left join keep all the rows in the first data frame written in the command, whereas a right join keeps all the rows in the second data frame.\nThe two commands below achieve the same output - 10 rows of hosp_info joined into a linelist_mini baseline, but they use different joins. The result is that the column order will differ based on whether hosp_info arrives from the right (in the left join) or arrives from the left (in the right join). The order of the rows may also shift accordingly. But both of these consequences can be subsequently addressed, using select() to re-order columns or arrange() to sort rows.\n\n# The two commands below achieve the same data, but with differently ordered rows and columns\nleft_join(linelist_mini, hosp_info, by = c(\"hospital\" = \"hosp_name\"))\nright_join(hosp_info, linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\nHere is the result of hosp_info into linelist_mini via a left join (new columns incoming from the right)\n\n\nWarning in left_join(linelist_mini, hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\nHere is the result of hosp_info into linelist_mini via a right join (new columns incoming from the left)\n\n\nWarning in right_join(hosp_info, linelist_mini, by = c(hosp_name = \"hospital\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 4 of `x` matches multiple rows in `y`.\nℹ Row 5 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\nAlso consider whether your use-case is within a pipe chain (%&gt;%). If the dataset in the pipes is the baseline, you will likely use a left join to add data to it.\n\n\n\n\nFull join\nA full join is the most inclusive of the joins - it returns all rows from both data frames.\nIf there are any rows present in one and not the other (where no match was found), the data frame will include them and become longer. NA missing values are used to fill-in any gaps created. As you join, watch the number of columns and rows carefully to troubleshoot case-sensitivity and exact character matches.\nThe “baseline” data frame is the one written first in the command. Adjustment of this will not impact which records are returned by the join, but it can impact the resulting column order, row order, and which identifier columns are retained.\n\n\n\n\n\n\n\n\n\nAnimated example of a full join (image source)\nExample\nBelow is the output of a full_join() of hosp_info (originally nrow(hosp_info), view here) into linelist_mini (originally nrow(linelist_mini), view here). Note the following:\n\nAll baseline rows are kept (linelist_mini)\n\nRows in the secondary that do not match to the baseline are kept (“ignace” and “sisters”), with values in the corresponding baseline columns case_id and onset filled in with missing values\n\nLikewise, rows in the baseline data frame that do not match to the secondary (“Other” and “Missing”) are kept, with secondary columns catchment_pop and level filled-in with missing values\n\nIn the case of one-to-many or many-to-one matches (e.g. rows for “Military Hospital”), all possible combinations are returned (lengthening the final data frame)\n\nOnly the identifier column from the baseline is kept (hospital)\n\n\nlinelist_mini %&gt;% \n  full_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in full_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\nInner join\nAn inner join is the most restrictive of the joins - it returns only rows with matches across both data frames.\nThis means that the number of rows in the baseline data frame may actually reduce. Adjustment of which data frame is the “baseline” (written first in the function) will not impact which rows are returned, but it will impact the column order, row order, and which identifier columns are retained.\n\n\n\n\n\n\n\n\n\nAnimated example of an inner join (image source)\nExample\nBelow is the output of an inner_join() of linelist_mini (baseline) with hosp_info (secondary). Note the following:\n\nBaseline rows with no match to the secondary data are removed (rows where hospital is “Missing” or “Other”)\n\nLikewise, rows from the secondary data frame that had no match in the baseline are removed (rows where hosp_name is “sisters” or “ignace”)\n\nOnly the identifier column from the baseline is kept (hospital)\n\n\nlinelist_mini %&gt;% \n  inner_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in inner_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\nSemi join\nA semi join is a “filtering join” which uses another dataset not to add rows or columns, but to perform filtering.\nA semi-join keeps all observations in the baseline data frame that have a match in the secondary data frame (but does not add new columns nor duplicate any rows for multiple matches). Read more about these “filtering” joins here.\n\n\n\n\n\n\n\n\n\nAnimated example of a semi join (image source)\nAs an example, the below code returns rows from the hosp_info data frame that have matches in linelist_mini based on hospital name.\n\nhosp_info %&gt;% \n  semi_join(linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\n                             hosp_name catchment_pop     level\n1                    Military Hospital         40500 Secondary\n2                    Military Hospital         10000   Primary\n3                        Port Hospital         50280 Secondary\n4 St. Mark's Maternity Hospital (SMMH)         12000 Secondary\n\n\n\n\n\nAnti join\nThe anti join is another “filtering join” that returns rows in the baseline data frame that do not have a match in the secondary data frame.\nRead more about filtering joins here.\nCommon scenarios for an anti-join include identifying records not present in another data frame, troubleshooting spelling in a join (reviewing records that should have matched), and examining records that were excluded after another join.\nAs with right_join() and left_join(), the baseline data frame (listed first) is important. The returned rows are from the baseline data frame only. Notice in the gif below that row in the secondary data frame (purple row 4) is not returned even though it does not match with the baseline.\n\n\n\n\n\n\n\n\n\nAnimated example of an anti join (image source)\n\nSimple anti_join() example\nFor a simple example, let’s find the hosp_info hospitals that do not have any cases present in linelist_mini. We list hosp_info first, as the baseline data frame. The hospitals which are not present in linelist_mini are returned.\n\nhosp_info %&gt;% \n  anti_join(linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\n\n\n\n\n\n\n\n\nComplex anti_join() example\nFor another example, let us say we ran an inner_join() between linelist_mini and hosp_info. This returns only a subset of the original linelist_mini records, as some are not present in hosp_info.\n\nlinelist_mini %&gt;% \n  inner_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in inner_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\nTo review the linelist_mini records that were excluded during the inner join, we can run an anti-join with the same settings (linelist_mini as the baseline).\n\nlinelist_mini %&gt;% \n  anti_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\n\n\n\n\nTo see the hosp_info records that were excluded in the inner join, we could also run an anti-join with hosp_info as the baseline data frame.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "new_pages/joining_matching.html#probabalistic-matching",
    "href": "new_pages/joining_matching.html#probabalistic-matching",
    "title": "14  Joining data",
    "section": "14.3 Probabalistic matching",
    "text": "14.3 Probabalistic matching\nIf you do not have a unique identifier common across datasets to join on, consider using a probabilistic matching algorithm. This would find matches between records based on similarity (e.g. Jaro–Winkler string distance, or numeric distance). Below is a simple example using the package fastLink .\nLoad packages\n\npacman::p_load(\n  tidyverse,      # data manipulation and visualization\n  fastLink        # record matching\n  )\n\nHere are two small example datasets that we will use to demonstrate the probabilistic matching (cases and test_results):\nHere is the code used to make the datasets:\n\n# make datasets\n\ncases &lt;- tribble(\n  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,\n  \"M\",     \"Amir\",      NA,          \"Khan\",       1989,  11,   22,   \"River\",\n  \"M\",     \"Anthony\",   \"B.\",        \"Smith\",      1970, 09, 19,      \"River\", \n  \"F\",     \"Marialisa\", \"Contreras\", \"Rodrigues\",  1972, 04, 15,      \"River\",\n  \"F\",     \"Elizabeth\", \"Casteel\",   \"Chase\",      1954, 03, 03,      \"City\",\n  \"M\",     \"Jose\",      \"Sanchez\",   \"Lopez\",      1996, 01, 06,      \"City\",\n  \"F\",     \"Cassidy\",   \"Jones\",      \"Davis\",     1980, 07, 20,      \"City\",\n  \"M\",     \"Michael\",   \"Murphy\",     \"O'Calaghan\",1969, 04, 12,      \"Rural\", \n  \"M\",     \"Oliver\",    \"Laurent\",    \"De Bordow\" , 1971, 02, 04,     \"River\",\n  \"F\",      \"Blessing\",  NA,          \"Adebayo\",   1955,  02, 14,     \"Rural\"\n)\n\nresults &lt;- tribble(\n  ~gender,  ~first,     ~middle,     ~last,          ~yr, ~mon, ~day, ~district, ~result,\n  \"M\",      \"Amir\",     NA,          \"Khan\",         1989, 11,   22,  \"River\", \"positive\",\n  \"M\",      \"Tony\",   \"B\",         \"Smith\",          1970, 09,   19,  \"River\", \"positive\",\n  \"F\",      \"Maria\",    \"Contreras\", \"Rodriguez\",    1972, 04,   15,  \"Cty\",   \"negative\",\n  \"F\",      \"Betty\",    \"Castel\",   \"Chase\",        1954,  03,   30,  \"City\",  \"positive\",\n  \"F\",      \"Andrea\",   NA,          \"Kumaraswamy\",  2001, 01,   05,  \"Rural\", \"positive\",      \n  \"F\",      \"Caroline\", NA,          \"Wang\",         1988, 12,   11,  \"Rural\", \"negative\",\n  \"F\",      \"Trang\",    NA,          \"Nguyen\",       1981, 06,   10,  \"Rural\", \"positive\",\n  \"M\",      \"Olivier\" , \"Laurent\",   \"De Bordeaux\",  NA,   NA,   NA,  \"River\", \"positive\",\n  \"M\",      \"Mike\",     \"Murphy\",    \"O'Callaghan\",  1969, 04,   12,  \"Rural\", \"negative\",\n  \"F\",      \"Cassidy\",  \"Jones\",     \"Davis\",        1980, 07,   02,  \"City\",  \"positive\",\n  \"M\",      \"Mohammad\", NA,          \"Ali\",          1942, 01,   17,  \"City\",  \"negative\",\n  NA,       \"Jose\",     \"Sanchez\",   \"Lopez\",        1995, 01,   06,  \"City\",  \"negative\",\n  \"M\",      \"Abubakar\", NA,          \"Abullahi\",     1960, 01,   01,  \"River\", \"positive\",\n  \"F\",      \"Maria\",    \"Salinas\",   \"Contreras\",    1955, 03,   03,  \"River\", \"positive\"\n  )\n\nThe cases dataset has 9 records of patients who are awaiting test results.\n\n\n\n\n\n\nThe test_results dataset has 14 records and contains the column result, which we want to add to the records in cases based on probabilistic matching of records.\n\n\n\n\n\n\n\nProbabilistic matching\nThe fastLink() function from the fastLink package can be used to apply a matching algorithm. Here is the basic information. You can read more detail by entering ?fastLink in your console.\n\nDefine the two data frames for comparison to arguments dfA = and dfB =\n\nIn varnames = give all column names to be used for matching. They must all exist in both dfA and dfB.\n\nIn stringdist.match = give columns from those in varnames to be evaluated on string “distance”.\n\nIn numeric.match = give columns from those in varnames to be evaluated on numeric distance.\n\nMissing values are ignored\n\nBy default, each row in either data frame is matched to at most one row in the other data frame. If you want to see all the evaluated matches, set dedupe.matches = FALSE. The deduplication is done using Winkler’s linear assignment solution.\n\nTip: split one date column into three separate numeric columns using day(), month(), and year() from lubridate package\nThe default threshold for matches is 0.94 (threshold.match =) but you can adjust it higher or lower. If you define the threshold, consider that higher thresholds could yield more false-negatives (rows that do not match which actually should match) and likewise a lower threshold could yield more false-positive matches.\nBelow, the data are matched on string distance across the name and district columns, and on numeric distance for year, month, and day of birth. A match threshold of 95% probability is set.\n\nfl_output &lt;- fastLink::fastLink(\n  dfA = cases,\n  dfB = results,\n  varnames = c(\"gender\", \"first\", \"middle\", \"last\", \"yr\", \"mon\", \"day\", \"district\"),\n  stringdist.match = c(\"first\", \"middle\", \"last\", \"district\"),\n  numeric.match = c(\"yr\", \"mon\", \"day\"),\n  threshold.match = 0.95)\n\n\n==================== \nfastLink(): Fast Probabilistic Record Linkage\n==================== \n\nIf you set return.all to FALSE, you will not be able to calculate a confusion table as a summary statistic.\nCalculating matches for each variable.\nGetting counts for parameter estimation.\n    Parallelizing calculation using OpenMP. 1 threads out of 8 are used.\nRunning the EM algorithm.\nGetting the indices of estimated matches.\n    Parallelizing calculation using OpenMP. 1 threads out of 8 are used.\nDeduping the estimated matches.\nGetting the match patterns for each estimated match.\n\n\nReview matches\nWe defined the object returned from fastLink() as fl_output. It is of class list, and it actually contains several data frames within it, detailing the results of the matching. One of these data frames is matches, which contains the most likely matches across cases and results. You can access this “matches” data frame with fl_output$matches. Below, it is saved as my_matches for ease of accessing later.\nWhen my_matches is printed, you see two column vectors: the pairs of row numbers/indices (also called “rownames”) in cases (“inds.a”) and in results (“inds.b”) representing the best matches. If a row number from a datafrane is missing, then no match was found in the other data frame at the specified match threshold.\n\n# print matches\nmy_matches &lt;- fl_output$matches\nmy_matches\n\n  inds.a inds.b\n1      1      1\n2      2      2\n3      3      3\n4      4      4\n5      8      8\n6      7      9\n7      6     10\n8      5     12\n\n\nThings to note:\n\nMatches occurred despite slight differences in name spelling and dates of birth:\n\n“Tony B. Smith” matched to “Anthony B Smith”\n\n“Maria Rodriguez” matched to “Marialisa Rodrigues”\n\n“Betty Chase” matched to “Elizabeth Chase”\n\n“Olivier Laurent De Bordeaux” matched to “Oliver Laurent De Bordow” (missing date of birth ignored)\n\n\nOne row from cases (for “Blessing Adebayo”, row 9) had no good match in results, so it is not present in my_matches.\n\nJoin based on the probabilistic matches\nTo use these matches to join results to cases, one strategy is:\n\nUse left_join() to join my_matches to cases (matching rownames in cases to “inds.a” in my_matches)\n\nThen use another left_join() to join results to cases (matching the newly-acquired “inds.b” in cases to rownames in results)\n\nBefore the joins, we should clean the three data frames:\n\nBoth dfA and dfB should have their row numbers (“rowname”) converted to a proper column.\n\nBoth the columns in my_matches are converted to class character, so they can be joined to the character rownames\n\n\n# Clean data prior to joining\n#############################\n\n# convert cases rownames to a column \ncases_clean &lt;- cases %&gt;% rownames_to_column()\n\n# convert test_results rownames to a column\nresults_clean &lt;- results %&gt;% rownames_to_column()  \n\n# convert all columns in matches dataset to character, so they can be joined to the rownames\nmatches_clean &lt;- my_matches %&gt;%\n  mutate(across(everything(), as.character))\n\n\n\n# Join matches to dfA, then add dfB\n###################################\n# column \"inds.b\" is added to dfA\ncomplete &lt;- left_join(cases_clean, matches_clean, by = c(\"rowname\" = \"inds.a\"))\n\n# column(s) from dfB are added \ncomplete &lt;- left_join(complete, results_clean, by = c(\"inds.b\" = \"rowname\"))\n\nAs performed using the code above, the resulting data frame complete will contain all columns from both cases and results. Many will be appended with suffixes “.x” and “.y”, because the column names would otherwise be duplicated.\n\n\n\n\n\n\nAlternatively, to achieve only the “original” 9 records in cases with the new column(s) from results, use select() on results before the joins, so that it contains only rownames and the columns that you want to add to cases (e.g. the column result).\n\ncases_clean &lt;- cases %&gt;% rownames_to_column()\n\nresults_clean &lt;- results %&gt;%\n  rownames_to_column() %&gt;% \n  select(rowname, result)    # select only certain columns \n\nmatches_clean &lt;- my_matches %&gt;%\n  mutate(across(everything(), as.character))\n\n# joins\ncomplete &lt;- left_join(cases_clean, matches_clean, by = c(\"rowname\" = \"inds.a\"))\ncomplete &lt;- left_join(complete, results_clean, by = c(\"inds.b\" = \"rowname\"))\n\n\n\n\n\n\n\nIf you want to subset either dataset to only the rows that matched, you can use the codes below:\n\ncases_matched &lt;- cases[my_matches$inds.a,]  # Rows in cases that matched to a row in results\nresults_matched &lt;- results[my_matches$inds.b,]  # Rows in results that matched to a row in cases\n\nOr, to see only the rows that did not match:\n\ncases_not_matched &lt;- cases[!rownames(cases) %in% my_matches$inds.a,]  # Rows in cases that did NOT match to a row in results\nresults_not_matched &lt;- results[!rownames(results) %in% my_matches$inds.b,]  # Rows in results that did NOT match to a row in cases\n\n\n\nProbabilistic deduplication\nProbabilistic matching can be used to deduplicate a dataset as well. See the page on deduplication for other methods of deduplication.\nHere we began with the cases dataset, but are now calling it cases_dup, as it has 2 additional rows that could be duplicates of previous rows: See “Tony” with “Anthony”, and “Marialisa Rodrigues” with “Maria Rodriguez”.\n\n\n\n\n\n\nRun fastLink() like before, but compare the cases_dup data frame to itself. When the two data frames provided are identical, the function assumes you want to de-duplicate. Note we do not specify stringdist.match = or numeric.match = as we did previously.\n\n## Run fastLink on the same dataset\ndedupe_output &lt;- fastLink(\n  dfA = cases_dup,\n  dfB = cases_dup,\n  varnames = c(\"gender\", \"first\", \"middle\", \"last\", \"yr\", \"mon\", \"day\", \"district\")\n)\n\n\n==================== \nfastLink(): Fast Probabilistic Record Linkage\n==================== \n\nIf you set return.all to FALSE, you will not be able to calculate a confusion table as a summary statistic.\ndfA and dfB are identical, assuming deduplication of a single data set.\nSetting return.all to FALSE.\n\nCalculating matches for each variable.\nGetting counts for parameter estimation.\n    Parallelizing calculation using OpenMP. 1 threads out of 8 are used.\nRunning the EM algorithm.\nGetting the indices of estimated matches.\n    Parallelizing calculation using OpenMP. 1 threads out of 8 are used.\nCalculating the posterior for each pair of matched observations.\nGetting the match patterns for each estimated match.\n\n\nNow, you can review the potential duplicates with getMatches(). Provide the data frame as both dfA = and dfB =, and provide the output of the fastLink() function as fl.out =. fl.out must be of class fastLink.dedupe, or in other words, the result of fastLink().\n\n## Run getMatches()\ncases_dedupe &lt;- getMatches(\n  dfA = cases_dup,\n  dfB = cases_dup,\n  fl.out = dedupe_output)\n\nSee the right-most column, which indicates the duplicate IDs - the final two rows are identified as being likely duplicates of rows 2 and 3.\n\n\n\n\n\n\nTo return the row numbers of rows which are likely duplicates, you can count the number of rows per unique value in the dedupe.ids column, and then filter to keep only those with more than one row. In this case this leaves rows 2 and 3.\n\ncases_dedupe %&gt;% \n  count(dedupe.ids) %&gt;% \n  filter(n &gt; 1)\n\n  dedupe.ids n\n1          2 2\n2          3 2\n\n\nTo inspect the whole rows of the likely duplicates, put the row number in this command:\n\n# displays row 2 and all likely duplicates of it\ncases_dedupe[cases_dedupe$dedupe.ids == 2,]   \n\n   gender   first middle  last   yr mon day district dedupe.ids\n2       M Anthony     B. Smith 1970   9  19    River          2\n10      M    Tony     B. Smith 1970   9  19    River          2",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "new_pages/joining_matching.html#binding-and-aligning",
    "href": "new_pages/joining_matching.html#binding-and-aligning",
    "title": "14  Joining data",
    "section": "14.4 Binding and aligning",
    "text": "14.4 Binding and aligning\nAnother method of combining two data frames is “binding” them together. You can also think of this as “appending” or “adding” rows or columns.\nThis section will also discuss how to “align” the order of rows of one data frame to the order in another data frame. This topic is discussed below in the section on Binding columns.\n\nBind rows\nTo bind rows of one data frame to the bottom of another data frame, use bind_rows() from dplyr. It is very inclusive, so any column present in either data frame will be included in the output. A few notes:\n\nUnlike the base R version row.bind(), dplyr’s bind_rows() does not require that the order of columns be the same in both data frames. As long as the column names are spelled identically, it will align them correctly.\n\nYou can optionally specify the argument .id =. Provide a character column name. This will produce a new column that serves to identify which data frame each row originally came from.\n\nYou can use bind_rows() on a list of similarly-structured data frames to combine them into one data frame. See an example in the Iteration, loops, and lists page involving the import of multiple linelists with purrr.\n\nOne common example of row binding is to bind a “total” row onto a descriptive table made with dplyr’s summarise() function. Below we create a table of case counts and median CT values by hospital with a total row.\nThe function summarise() is used on data grouped by hospital to return a summary data frame by hospital. But the function summarise() does not automatically produce a “totals” row, so we create it by summarising the data again, but with the data not grouped by hospital. This produces a second data frame of just one row. We can then bind these data frames together to achieve the final table.\nSee other worked examples like this in the Descriptive tables and Tables for presentation pages.\n\n# Create core table\n###################\nhosp_summary &lt;- linelist %&gt;% \n  group_by(hospital) %&gt;%                        # Group data by hospital\n  summarise(                                    # Create new summary columns of indicators of interest\n    cases = n(),                                  # Number of rows per hospital-outcome group     \n    ct_value_med = median(ct_blood, na.rm=T))     # median CT value per group\n\nHere is the hosp_summary data frame:\n\n\n\n\n\n\nCreate a data frame with the “total” statistics (not grouped by hospital). This will return just one row.\n\n# create totals\n###############\ntotals &lt;- linelist %&gt;% \n  summarise(\n    cases = n(),                               # Number of rows for whole dataset     \n    ct_value_med = median(ct_blood, na.rm=T))  # Median CT for whole dataset\n\nAnd below is that totals data frame. Note how there are only two columns. These columns are also in hosp_summary, but there is one column in hosp_summary that is not in totals (hospital).\n\n\n\n\n\n\nNow we can bind the rows together with bind_rows().\n\n# Bind data frames together\ncombined &lt;- bind_rows(hosp_summary, totals)\n\nNow we can view the result. See how in the final row, an empty NA value fills in for the column hospital that was not in hosp_summary. As explained in the Tables for presentation page, you could “fill-in” this cell with “Total” using replace_na().\n\n\n\n\n\n\n\n\nBind columns\nThere is a similar dplyr function bind_cols() which you can use to combine two data frames sideways. Note that rows are matched to each other by position (not like a join above) - for example the 12th row in each data frame will be aligned.\nFor an example, we bind several summary tables together. In order to do this, we also demonstrate how to re-arrange the order of rows in one data frame to match the order in another data frame, with match().\nHere we define case_info as a summary data frame of linelist cases, by hospital, with the number of cases and the number of deaths.\n\n# Case information\ncase_info &lt;- linelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    cases = n(),\n    deaths = sum(outcome == \"Death\", na.rm=T)\n  )\n\n\n\n\n\n\n\nAnd let’s say that here is a different data frame contact_fu containing information on the percent of exposed contacts investigated and “followed-up”, again by hospital.\n\ncontact_fu &lt;- data.frame(\n  hospital = c(\"St. Mark's Maternity Hospital (SMMH)\", \"Military Hospital\", \"Missing\", \"Central Hospital\", \"Port Hospital\", \"Other\"),\n  investigated = c(\"80%\", \"82%\", NA, \"78%\", \"64%\", \"55%\"),\n  per_fu = c(\"60%\", \"25%\", NA, \"20%\", \"75%\", \"80%\")\n)\n\n\n\n\n\n\n\nNote that the hospitals are the same, but are in different orders in each data frame. The easiest solution would be to use a left_join() on the hospital column, but you could also use bind_cols() with one extra step.\n\nUse match() to align ordering\nBecause the row orders are different, a simple bind_cols() command would result in a mis-match of data. To fix this we can use match() from base R to align the rows of a data frame in the same order as in another. We assume for this approach that there are no duplicate values in either data frame.\nWhen we use match(), the syntax is match(TARGET ORDER VECTOR, DATA FRAME COLUMN TO CHANGE), where the first argument is the desired order (either a stand-alone vector, or in this case a column in a data frame), and the second argument is the data frame column in the data frame that will be re-ordered. The output of match() is a vector of numbers representing the correct position ordering. You can read more with ?match.\n\nmatch(case_info$hospital, contact_fu$hospital)\n\n[1] 4 2 3 6 5 1\n\n\nYou can use this numeric vector to re-order the data frame - place it within subset brackets [ ] before the comma. Read more about base R bracket subset syntax in the R basics page. The command below creates a new data frame, defined as the old one in which the rows are ordered in the numeric vector above.\n\ncontact_fu_aligned &lt;- contact_fu[match(case_info$hospital, contact_fu$hospital),]\n\n\n\n\n\n\n\nNow we can bind the data frame columns together, with the correct row order. Note that some columns are duplicated and will require cleaning with rename(). Read more aboout bind_rows() here.\n\nbind_cols(case_info, contact_fu)\n\nNew names:\n• `hospital` -&gt; `hospital...1`\n• `hospital` -&gt; `hospital...4`\n\n\n# A tibble: 6 × 6\n  hospital...1                     cases deaths hospital...4 investigated per_fu\n  &lt;chr&gt;                            &lt;int&gt;  &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; \n1 Central Hospital                   454    193 St. Mark's … 80%          60%   \n2 Military Hospital                  896    399 Military Ho… 82%          25%   \n3 Missing                           1469    611 Missing      &lt;NA&gt;         &lt;NA&gt;  \n4 Other                              885    395 Central Hos… 78%          20%   \n5 Port Hospital                     1762    785 Port Hospit… 64%          75%   \n6 St. Mark's Maternity Hospital (…   422    199 Other        55%          80%   \n\n\nA base R alternative to bind_cols is cbind(), which performs the same operation.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "new_pages/joining_matching.html#resources",
    "href": "new_pages/joining_matching.html#resources",
    "title": "14  Joining data",
    "section": "14.5 Resources",
    "text": "14.5 Resources\nThe tidyverse page on joins\nThe R for Data Science page on relational data\nTh tidyverse page on dplyr on binding\nA vignette on fastLink at the package’s Github page\nPublication describing methodology of fastLink\nPublication describing RecordLinkage package",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Joining data</span>"
    ]
  },
  {
    "objectID": "new_pages/deduplication.html",
    "href": "new_pages/deduplication.html",
    "title": "15  De-duplication",
    "section": "",
    "text": "15.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>De-duplication</span>"
    ]
  },
  {
    "objectID": "new_pages/deduplication.html#preparation",
    "href": "new_pages/deduplication.html#preparation",
    "title": "15  De-duplication",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  tidyverse,   # deduplication, grouping, and slicing functions\n  janitor,     # function for reviewing duplicates\n  stringr)      # for string searches, can be used in \"rolling-up\" values\n\n\n\nImport data\nFor demonstration, we will use an example dataset that is created with the R code below.\nThe data are records of COVID-19 phone encounters, including encounters with contacts and with cases. The columns include recordID (computer-generated), personID, name, date of encounter, time of encounter, the purpose of the encounter (either to interview as a case or as a contact), and symptoms_ever (whether the person in that encounter reported ever having symptoms).\nHere is the code to create the obs dataset:\n\nobs &lt;- data.frame(\n  recordID  = c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18),\n  personID  = c(1,1,2,2,3,2,4,5,6,7,2,1,3,3,4,5,5,7,8),\n  name      = c(\"adam\", \"adam\", \"amrish\", \"amrish\", \"mariah\", \"amrish\", \"nikhil\", \"brian\", \"smita\", \"raquel\", \"amrish\",\n                \"adam\", \"mariah\", \"mariah\", \"nikhil\", \"brian\", \"brian\", \"raquel\", \"natalie\"),\n  date      = c(\"1/1/2020\", \"1/1/2020\", \"2/1/2020\", \"2/1/2020\", \"5/1/2020\", \"5/1/2020\", \"5/1/2020\", \"5/1/2020\", \"5/1/2020\",\"5/1/2020\", \"2/1/2020\",\n                \"5/1/2020\", \"6/1/2020\", \"6/1/2020\", \"6/1/2020\", \"6/1/2020\", \"7/1/2020\", \"7/1/2020\", \"7/1/2020\"),\n  time      = c(\"09:00\", \"09:00\", \"14:20\", \"14:20\", \"12:00\", \"16:10\", \"13:01\", \"15:20\", \"14:20\", \"12:30\", \"10:24\",\n                \"09:40\", \"07:25\", \"08:32\", \"15:36\", \"15:31\", \"07:59\", \"11:13\", \"17:12\"),\n  encounter = c(1,1,1,1,1,3,1,1,1,1,2,\n                2,2,3,2,2,3,2,1),\n  purpose   = c(\"contact\", \"contact\", \"contact\", \"contact\", \"case\", \"case\", \"contact\", \"contact\", \"contact\", \"contact\", \"contact\",\n                \"case\", \"contact\", \"contact\", \"contact\", \"contact\", \"case\", \"contact\", \"case\"),\n  symptoms_ever = c(NA, NA, \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", NA, \"Yes\",\n                    \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"No\",\"No\", \"No\")) %&gt;% \n  mutate(date = as.Date(date, format = \"%d/%m/%Y\"))\n\n\nHere is the data frame\nUse the filter boxes along the top to review the encounters for each person.\n\n\n\n\n\n\nA few things to note as you review the data:\n\nThe first two records are 100% complete duplicates including duplicate recordID (must be a computer glitch!)\n\nThe second two rows are duplicates, in all columns except for recordID\n\nSeveral people had multiple phone encounters, at various dates and times, and as contacts and/or cases\n\nAt each encounter, the person was asked if they had ever had symptoms, and some of this information is missing.\n\nAnd here is a quick summary of the people and the purposes of their encounters, using tabyl() from janitor:\n\nobs %&gt;% \n  tabyl(name, purpose)\n\n    name case contact\n    adam    1       2\n  amrish    1       3\n   brian    1       2\n  mariah    1       2\n natalie    1       0\n  nikhil    0       2\n  raquel    0       2\n   smita    0       1",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>De-duplication</span>"
    ]
  },
  {
    "objectID": "new_pages/deduplication.html#deduplication",
    "href": "new_pages/deduplication.html#deduplication",
    "title": "15  De-duplication",
    "section": "15.2 Deduplication",
    "text": "15.2 Deduplication\nThis section describes how to review and remove duplicate rows in a data frame. It also show how to handle duplicate elements in a vector.\n\n\nExamine duplicate rows\nTo quickly review rows that have duplicates, you can use get_dupes() from the janitor package. By default, all columns are considered when duplicates are evaluated - rows returned by the function are 100% duplicates considering the values in all columns.\nIn the obs data frame, the first two rows are 100% duplicates - they have the same value in every column (including the recordID column, which is supposed to be unique - it must be some computer glitch). The returned data frame automatically includes a new column dupe_count on the right side, showing the number of rows with that combination of duplicate values.\n\n# 100% duplicates across all columns\nobs %&gt;% \n  janitor::get_dupes()\n\n\n\n\n\n\n\nSee the original data\nHowever, if we choose to ignore recordID, the 3rd and 4th rows rows are also duplicates of each other. That is, they have the same values in all columns except for recordID. You can specify specific columns to be ignored in the function using a - minus symbol.\n\n# Duplicates when column recordID is not considered\nobs %&gt;% \n  janitor::get_dupes(-recordID)         # if multiple columns, wrap them in c()\n\n\n\n\n\n\n\nYou can also positively specify the columns to consider. Below, only rows that have the same values in the name and purpose columns are returned. Notice how “amrish” now has dupe_count equal to 3 to reflect his three “contact” encounters.\n*Scroll left for more rows**\n\n# duplicates based on name and purpose columns ONLY\nobs %&gt;% \n  janitor::get_dupes(name, purpose)\n\n\n\n\n\n\n\nSee the original data.\nSee ?get_dupes for more details, or see this online reference\n\n\n\nKeep only unique rows\nTo keep only unique rows of a data frame, use distinct() from dplyr (as demonstrated in the Cleaning data and core functions page). Rows that are duplicates are removed such that only the first of such rows is kept. By default, “first” means the highest rownumber (order of rows top-to-bottom). Only unique rows remain.\nIn the example below, we run distinct() such that the column recordID is excluded from consideration - thus two duplicate rows are removed. The first row (for “adam”) was 100% duplicated and has been removed. Also row 3 (for “amrish”) was a duplicate in every column except recordID (which is not being considered) and so is also removed. The obs dataset n is now nrow(obs)-2, not nrow(obs) rows).\nScroll to the left to see the entire data frame\n\n# added to a chain of pipes (e.g. data cleaning)\nobs %&gt;% \n  distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates)\n           .keep_all = TRUE) \n\n# if outside pipes, include the data as first argument \n# distinct(obs)\n\n\n\n\n\n\n\nCAUTION: If using distinct() on grouped data, the function will apply to each group.\nDeduplicate based on specific columns\nYou can also specify columns to be the basis for de-duplication. In this way, the de-duplication only applies to rows that are duplicates within the specified columns. Unless you set .keep_all = TRUE, all columns not mentioned will be dropped.\nIn the example below, the de-duplication only applies to rows that have identical values for name and purpose columns. Thus, “brian” has only 2 rows instead of 3 - his first “contact” encounter and his only “case” encounter. To adjust so that brian’s latest encounter of each purpose is kept, see the tab on Slicing within groups.\nScroll to the left to see the entire data frame\n\n# added to a chain of pipes (e.g. data cleaning)\nobs %&gt;% \n  distinct(name, purpose, .keep_all = TRUE) %&gt;%  # keep rows unique by name and purpose, retain all columns\n  arrange(name)                                  # arrange for easier viewing\n\n\n\n\n\n\n\nSee the original data.\n\n\n\nDeduplicate elements in a vector\nThe function duplicated() from base R will evaluate a vector (column) and return a logical vector of the same length (TRUE/FALSE). The first time a value appears, it will return FALSE (not a duplicate), and subsequent times that value appears it will return TRUE. Note how NA is treated the same as any other value.\n\nx &lt;- c(1, 1, 2, NA, NA, 4, 5, 4, 4, 1, 2)\nduplicated(x)\n\n [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nTo return only the duplicated elements, you can use brackets to subset the original vector:\n\nx[duplicated(x)]\n\n[1]  1 NA  4  4  1  2\n\n\nTo return only the unique elements, use unique() from base R. To remove NAs from the output, nest na.omit() within unique().\n\nunique(x)           # alternatively, use x[!duplicated(x)]\n\n[1]  1  2 NA  4  5\n\nunique(na.omit(x))  # remove NAs \n\n[1] 1 2 4 5\n\n\n\n\n\nUsing base R\nTo return duplicate rows\nIn base R, you can also see which rows are 100% duplicates in a data frame df with the command duplicated(df) (returns a logical vector of the rows).\nThus, you can also use the base subset [ ] on the data frame to see the duplicated rows with df[duplicated(df),] (don’t forget the comma, meaning that you want to see all columns!).\nTo return unique rows\nSee the notes above. To see the unique rows you add the logical negator ! in front of the duplicated() function:\ndf[!duplicated(df),]\nTo return rows that are duplicates of only certain columns\nSubset the df that is within the duplicated() parentheses, so this function will operate on only certain columns of the df.\nTo specify the columns, provide column numbers or names after a comma (remember, all this is within the duplicated() function).\nBe sure to keep the comma , outside after the duplicated() function as well!\nFor example, to evaluate only columns 2 through 5 for duplicates: df[!duplicated(df[, 2:5]),]\nTo evaluate only columns name and purpose for duplicates: df[!duplicated(df[, c(\"name\", \"purpose)]),]",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>De-duplication</span>"
    ]
  },
  {
    "objectID": "new_pages/deduplication.html#slicing",
    "href": "new_pages/deduplication.html#slicing",
    "title": "15  De-duplication",
    "section": "15.3 Slicing",
    "text": "15.3 Slicing\nTo “slice” a data frame to apply a filter on the rows by row number/position. This becomes particularly useful if you have multiple rows per functional group (e.g. per “person”) and you only want to keep one or some of them.\nThe basic slice() function accepts numbers and returns rows in those positions. If the numbers provided are positive, only they are returned. If negative, those rows are not returned. Numbers must be either all positive or all negative.\n\nobs %&gt;% slice(4)  # return the 4th row\n\n  recordID personID   name       date  time encounter purpose symptoms_ever\n1        3        2 amrish 2020-01-02 14:20         1 contact            No\n\n\n\nobs %&gt;% slice(c(2,4))  # return rows 2 and 4\n\n  recordID personID   name       date  time encounter purpose symptoms_ever\n1        1        1   adam 2020-01-01 09:00         1 contact          &lt;NA&gt;\n2        3        2 amrish 2020-01-02 14:20         1 contact            No\n\n#obs %&gt;% slice(c(2:4))  # return rows 2 through 4\n\nSee the original data.\nThere are several variations: These should be provided with a column and a number of rows to return (to n =).\n\nslice_min() and slice_max() keep only the row(s) with the minimium or maximum value(s) of the specified column. This also works to return the “min” and “max” of ordered factors.\n\nslice_head() and slice_tail() - keep only the first or last row(s).\n\nslice_sample() - keep only a random sample of the rows.\n\n\nobs %&gt;% slice_max(encounter, n = 1)  # return rows with the largest encounter number\n\n  recordID personID   name       date  time encounter purpose symptoms_ever\n1        5        2 amrish 2020-01-05 16:10         3    case           Yes\n2       13        3 mariah 2020-01-06 08:32         3 contact            No\n3       16        5  brian 2020-01-07 07:59         3    case            No\n\n\nUse arguments n = or prop = to specify the number or proportion of rows to keep. If not using the function in a pipe chain, provide the data argument first (e.g. slice(data, n = 2)). See ?slice for more information.\nOther arguments:\n.order_by = used in slice_min() and slice_max() this is a column to order by before slicing.\nwith_ties = TRUE by default, meaning ties are kept.\n.preserve = FALSE by default. If TRUE then the grouping structure is re-calculated after slicing.\nweight_by = Optional, numeric column to weight by (bigger number more likely to get sampled). Also replace = for whether sampling is done with/without replacement.\nTIP: When using slice_max() and slice_min(), be sure to specify/write the n = (e.g. n = 2, not just 2). Otherwise you may get an error Error:…is not empty. \nNOTE: You may encounter the function top_n(), which has been superseded by the slice functions.\n\n\nSlice with groups\nThe slice_*() functions can be very useful if applied to a grouped data frame because the slice operation is performed on each group separately. Use the function group_by() in conjunction with slice() to group the data to take a slice from each group.\nThis is helpful for de-duplication if you have multiple rows per person but only want to keep one of them. You first use group_by() with key columns that are the same per person, and then use a slice function on a column that will differ among the grouped rows.\nIn the example below, to keep only the latest encounter per person, we group the rows by name and then use slice_max() with n = 1 on the date column. Be aware! To apply a function like slice_max() on dates, the date column must be class Date.\nBy default, “ties” (e.g. same date in this scenario) are kept, and we would still get multiple rows for some people (e.g. adam). To avoid this we set with_ties = FALSE. We get back only one row per person.\nCAUTION: If using arrange(), specify .by_group = TRUE to have the data arranged within each group.\nDANGER: If with_ties = FALSE, the first row of a tie is kept. This may be deceptive. See how for Mariah, she has two encounters on her latest date (6 Jan) and the first (earliest) one was kept. Likely, we want to keep her later encounter on that day. See how to “break” these ties in the next example. \n\nobs %&gt;% \n  group_by(name) %&gt;%       # group the rows by 'name'\n  slice_max(date,          # keep row per group with maximum date value \n            n = 1,         # keep only the single highest row \n            with_ties = F) # if there's a tie (of date), take the first row\n\n\n\n\n\n\n\nAbove, for example we can see that only Amrish’s row on 5 Jan was kept, and only Brian’s row on 7 Jan was kept. See the original data.\nBreaking “ties”\nMultiple slice statements can be run to “break ties”. In this case, if a person has multiple encounters on their latest date, the encounter with the latest time is kept (lubridate::hm() is used to convert the character times to a sortable time class).\nNote how now, the one row kept for “Mariah” on 6 Jan is encounter 3 from 08:32, not encounter 2 at 07:25.\n\n# Example of multiple slice statements to \"break ties\"\nobs %&gt;%\n  group_by(name) %&gt;%\n  \n  # FIRST - slice by latest date\n  slice_max(date, n = 1, with_ties = TRUE) %&gt;% \n  \n  # SECOND - if there is a tie, select row with latest time; ties prohibited\n  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE)\n\n\n\n\n\n\n\nIn the example above, it would also have been possible to slice by encounter number, but we showed the slice on date and time for example purposes.\nTIP: To use slice_max() or slice_min() on a “character” column, mutate it to an ordered factor class!\nSee the original data.\n\n\n\nKeep all but mark them\nIf you want to keep all records but mark only some for analysis, consider a two-step approach utilizing a unique recordID/encounter number:\n\nReduce/slice the orginal data frame to only the rows for analysis. Save/retain this reduced data frame.\n\nIn the original data frame, mark rows as appropriate with case_when(), based on whether their record unique identifier (recordID in this example) is present in the reduced data frame.\n\n\n# 1. Define data frame of rows to keep for analysis\nobs_keep &lt;- obs %&gt;%\n  group_by(name) %&gt;%\n  slice_max(encounter, n = 1, with_ties = FALSE) # keep only latest encounter per person\n\n\n# 2. Mark original data frame\nobs_marked &lt;- obs %&gt;%\n\n  # make new dup_record column\n  mutate(dup_record = case_when(\n    \n    # if record is in obs_keep data frame\n    recordID %in% obs_keep$recordID ~ \"For analysis\", \n    \n    # all else marked as \"Ignore\" for analysis purposes\n    TRUE                            ~ \"Ignore\"))\n\n# print\nobs_marked\n\n   recordID personID    name       date  time encounter purpose symptoms_ever\n1         1        1    adam 2020-01-01 09:00         1 contact          &lt;NA&gt;\n2         1        1    adam 2020-01-01 09:00         1 contact          &lt;NA&gt;\n3         2        2  amrish 2020-01-02 14:20         1 contact            No\n4         3        2  amrish 2020-01-02 14:20         1 contact            No\n5         4        3  mariah 2020-01-05 12:00         1    case            No\n6         5        2  amrish 2020-01-05 16:10         3    case           Yes\n7         6        4  nikhil 2020-01-05 13:01         1 contact           Yes\n8         7        5   brian 2020-01-05 15:20         1 contact            No\n9         8        6   smita 2020-01-05 14:20         1 contact           Yes\n10        9        7  raquel 2020-01-05 12:30         1 contact          &lt;NA&gt;\n11       10        2  amrish 2020-01-02 10:24         2 contact           Yes\n12       11        1    adam 2020-01-05 09:40         2    case            No\n13       12        3  mariah 2020-01-06 07:25         2 contact            No\n14       13        3  mariah 2020-01-06 08:32         3 contact            No\n15       14        4  nikhil 2020-01-06 15:36         2 contact           Yes\n16       15        5   brian 2020-01-06 15:31         2 contact           Yes\n17       16        5   brian 2020-01-07 07:59         3    case            No\n18       17        7  raquel 2020-01-07 11:13         2 contact            No\n19       18        8 natalie 2020-01-07 17:12         1    case            No\n     dup_record\n1        Ignore\n2        Ignore\n3        Ignore\n4        Ignore\n5        Ignore\n6  For analysis\n7        Ignore\n8        Ignore\n9  For analysis\n10       Ignore\n11       Ignore\n12 For analysis\n13       Ignore\n14 For analysis\n15 For analysis\n16       Ignore\n17 For analysis\n18 For analysis\n19 For analysis\n\n\n\n\n\n\n\n\nSee the original data.\n\n\n\nCalculate row completeness\nCreate a column that contains a metric for the row’s completeness (non-missingness). This could be helpful when deciding which rows to prioritize over others when de-duplicating/slicing.\nIn this example, “key” columns over which you want to measure completeness are saved in a vector of column names.\nThen the new column key_completeness is created with mutate(). The new value in each row is defined as a calculated fraction: the number of non-missing values in that row among the key columns, divided by the number of key columns.\nThis involves the function rowSums() from base R. Also used is ., which within piping refers to the data frame at that point in the pipe (in this case, it is being subset with brackets []).\n*Scroll to the right to see more rows**\n\n# create a \"key variable completeness\" column\n# this is a *proportion* of the columns designated as \"key_cols\" that have non-missing values\n\nkey_cols = c(\"personID\", \"name\", \"symptoms_ever\")\n\nobs %&gt;% \n  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) \n\n\n\n\n\n\n\nSee the original data.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>De-duplication</span>"
    ]
  },
  {
    "objectID": "new_pages/deduplication.html#str_rollup",
    "href": "new_pages/deduplication.html#str_rollup",
    "title": "15  De-duplication",
    "section": "15.4 Roll-up values",
    "text": "15.4 Roll-up values\nThis section describes:\n\nHow to “roll-up” values from multiple rows into just one row, with some variations\n\nOnce you have “rolled-up” values, how to overwrite/prioritize the values in each cell\n\nThis tab uses the example dataset from the Preparation tab.\n\n\nRoll-up values into one row\nThe code example below uses group_by() and summarise() to group rows by person, and then paste together all unique values within the grouped rows. Thus, you get one summary row per person. A few notes:\n\nA suffix is appended to all new columns (“_roll” in this example)\n\nIf you want to show only unique values per cell, then wrap the na.omit() with unique()\n\nna.omit() removes NA values, but if this is not desired it can be removed paste0(.x)…\n\n\n# \"Roll-up\" values into one row per group (per \"personID\") \ncases_rolled &lt;- obs %&gt;% \n  \n  # create groups by name\n  group_by(personID) %&gt;% \n  \n  # order the rows within each group (e.g. by date)\n  arrange(date, .by_group = TRUE) %&gt;% \n  \n  # For each column, paste together all values within the grouped rows, separated by \";\"\n  summarise(\n    across(everything(),                           # apply to all columns\n           ~paste0(na.omit(.x), collapse = \"; \"))) # function is defined which combines non-NA values\n\nThe result is one row per group (ID), with entries arranged by date and pasted together. Scroll to the left to see more rows\n\n\n\n\n\n\nSee the original data.\nThis variation shows unique values only:\n\n# Variation - show unique values only \ncases_rolled &lt;- obs %&gt;% \n  group_by(personID) %&gt;% \n  arrange(date, .by_group = TRUE) %&gt;% \n  summarise(\n    across(everything(),                                   # apply to all columns\n           ~paste0(unique(na.omit(.x)), collapse = \"; \"))) # function is defined which combines unique non-NA values\n\n\n\n\n\n\n\nThis variation appends a suffix to each column.\nIn this case “_roll” to signify that it has been rolled:\n\n# Variation - suffix added to column names \ncases_rolled &lt;- obs %&gt;% \n  group_by(personID) %&gt;% \n  arrange(date, .by_group = TRUE) %&gt;% \n  summarise(\n    across(everything(),                \n           list(roll = ~paste0(na.omit(.x), collapse = \"; \")))) # _roll is appended to column names\n\n\n\n\n\n\n\n\n\n\nOverwrite values/hierarchy\nIf you then want to evaluate all of the rolled values, and keep only a specific value (e.g. “best” or “maximum” value), you can use mutate() across the desired columns, to implement case_when(), which uses str_detect() from the stringr package to sequentially look for string patterns and overwrite the cell content.\n\n# CLEAN CASES\n#############\ncases_clean &lt;- cases_rolled %&gt;% \n    \n    # clean Yes-No-Unknown vars: replace text with \"highest\" value present in the string\n    mutate(across(c(contains(\"symptoms_ever\")),                     # operates on specified columns (Y/N/U)\n             list(mod = ~case_when(                                 # adds suffix \"_mod\" to new cols; implements case_when()\n               \n               str_detect(.x, \"Yes\")       ~ \"Yes\",                 # if \"Yes\" is detected, then cell value converts to yes\n               str_detect(.x, \"No\")        ~ \"No\",                  # then, if \"No\" is detected, then cell value converts to no\n               str_detect(.x, \"Unknown\")   ~ \"Unknown\",             # then, if \"Unknown\" is detected, then cell value converts to Unknown\n               TRUE                        ~ as.character(.x)))),   # then, if anything else if it kept as is\n      .keep = \"unused\")                                             # old columns removed, leaving only _mod columns\n\nNow you can see in the column symptoms_ever that if the person EVER said “Yes” to symptoms, then only “Yes” is displayed.\n\n\n\n\n\n\nSee the original data.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>De-duplication</span>"
    ]
  },
  {
    "objectID": "new_pages/deduplication.html#probabilistic-de-duplication",
    "href": "new_pages/deduplication.html#probabilistic-de-duplication",
    "title": "15  De-duplication",
    "section": "15.5 Probabilistic de-duplication",
    "text": "15.5 Probabilistic de-duplication\nSometimes, you may want to identify “likely” duplicates based on similarity (e.g. string “distance”) across several columns such as name, age, sex, date of birth, etc. You can apply a probabilistic matching algorithm to identify likely duplicates.\nSee the page on Joining data for an explanation on this method. The section on Probabilistic Matching contains an example of applying these algorithms to compare a data frame to itself, thus performing probabilistic de-duplication.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>De-duplication</span>"
    ]
  },
  {
    "objectID": "new_pages/deduplication.html#resources",
    "href": "new_pages/deduplication.html#resources",
    "title": "15  De-duplication",
    "section": "15.6 Resources",
    "text": "15.6 Resources\nMuch of the information in this page is adapted from these resources and vignettes online:\ndatanovia\ndplyr tidyverse reference\ncran janitor vignette",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>De-duplication</span>"
    ]
  },
  {
    "objectID": "new_pages/iteration.html",
    "href": "new_pages/iteration.html",
    "title": "16  Iteration, loops, and lists",
    "section": "",
    "text": "16.1 Preparation",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Iteration, loops, and lists</span>"
    ]
  },
  {
    "objectID": "new_pages/iteration.html#preparation",
    "href": "new_pages/iteration.html#preparation",
    "title": "16  Iteration, loops, and lists",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n     rio,         # import/export\n     here,        # file locator\n     purrr,       # iteration\n     grates,      # scales in ggplot\n     tidyverse    # data management and visualization\n)\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n\nWarning: The `trust` argument of `import()` should be explicit for serialization formats\nas of rio 1.0.3.\nℹ Missing `trust` will be set to FALSE by default for RDS in 2.0.0.\nℹ The deprecated feature was likely used in the rio package.\n  Please report the issue at &lt;https://github.com/gesistsa/rio/issues&gt;.\n\n\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Iteration, loops, and lists</span>"
    ]
  },
  {
    "objectID": "new_pages/iteration.html#for-loops",
    "href": "new_pages/iteration.html#for-loops",
    "title": "16  Iteration, loops, and lists",
    "section": "16.2 for loops",
    "text": "16.2 for loops\n\nfor loops in R\nfor loops are not emphasized in R, but are common in other programming languages. As a beginner, they can be helpful to learn and practice with because they are easier to “explore”, “de-bug”, and otherwise grasp exactly what is happening for each iteration, especially when you are not yet comfortable writing your own functions.\nYou may move quickly through for loops to iterating with mapped functions with purrr (see section below).\n\n\nCore components\nA for loop has three core parts:\n\nThe sequence of items to iterate through.\n\nThe operations to conduct per item in the sequence.\n\nThe container for the results (optional).\n\nThe basic syntax is: for (item in sequence) {do operations using item}. Note the parentheses and the curly brackets. The results could be printed to console, or stored in a container R object.\nA simple for loop example is below.\n\nfor (num in c(1,2,3,4,5)) {  # the SEQUENCE is defined (numbers 1 to 5) and loop is opened with \"{\"\n  print(num + 2)             # The OPERATIONS (add two to each sequence number and print)\n}                            # The loop is closed with \"}\"                            \n\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n\n                             # There is no \"container\" in this example\n\n\n\nSequence\nThis is the “for” part of a for loop - the operations will run “for” each item in the sequence. The sequence can be a series of values (e.g. names of jurisdictions, diseases, column names, list elements, etc), or it can be a series of consecutive numbers (e.g. 1,2,3,4,5). Each approach has their own utilities, described below.\nThe basic structure of a sequence statement is item in vector.\n\nYou can write any character or word in place of “item” (e.g. “i”, “num”, “hosp”, “district”, etc.). The value of this “item” changes with each iteration of the loop, proceeding through each value in the vector.\n\nThe vector could be of character values, column names, or perhaps a sequence of numbers - these are the values that will change with each iteration. You can use them within the for loop operations using the “item” term.\n\nExample: sequence of character values\nIn this example, a loop is performed for each value in a pre-defined character vector of hospital names.\n\n# make vector of the hospital names\nhospital_names &lt;- unique(linelist$hospital)\nhospital_names # print\n\n[1] \"Other\"                               \n[2] \"Missing\"                             \n[3] \"St. Mark's Maternity Hospital (SMMH)\"\n[4] \"Port Hospital\"                       \n[5] \"Military Hospital\"                   \n[6] \"Central Hospital\"                    \n\n\nWe have chosen the term hosp to represent values from the vector hospital_names. For the first iteration of the loop, the value of hosp will be hospital_names[[1]]. For the second loop it will be hospital_names[[2]]. And so on.\n\n# a 'for loop' with character sequence\n\nfor (hosp in hospital_names){       # sequence\n  \n       # OPERATIONS HERE\n  }\n\nExample: sequence of column names\nThis is a variation on the character sequence above, in which the names of an existing R object are extracted and become the vector. For example, the column names of a data frame. Conveniently, in the operations code of the for loop, the column names can be used to index (subset) their original data frame\nBelow, the sequence is the names() (column names) of the linelist data frame. Our “item” name is col, which will represent each column name as the loops proceeds.\nFor purposes of example, we include operations code inside the for loop, which is run for every value in the sequence. In this code, the sequence values (column names) are used to index (subset) linelist, one-at-a-time. As taught in the R basics page, double branckets [[ ]] are used to subset. The resulting column is passed to is.na(), then to sum() to produce the number of values in the column that are missing. The result is printed to the console - one number for each column.\nA note on indexing with column names - whenever referencing the column itself do not just write “col”! col represents just the character column name! To refer to the entire column you must use the column name as an index on linelist via linelist[[col]].\n\nfor (col in names(linelist)){        # loop runs for each column in linelist; column name represented by \"col\" \n  \n  # Example operations code - print number of missing values in column\n  print(sum(is.na(linelist[[col]])))  # linelist is indexed by current value of \"col\"\n     \n}\n\n[1] 0\n[1] 0\n[1] 2087\n[1] 256\n[1] 0\n[1] 936\n[1] 1323\n[1] 278\n[1] 86\n[1] 0\n[1] 86\n[1] 86\n[1] 86\n[1] 0\n[1] 0\n[1] 0\n[1] 2088\n[1] 2088\n[1] 0\n[1] 0\n[1] 0\n[1] 249\n[1] 249\n[1] 249\n[1] 249\n[1] 249\n[1] 149\n[1] 765\n[1] 0\n[1] 256\n\n\nSequence of numbers\nIn this approach, the sequence is a series of consecutive numbers. Thus, the value of the “item” is not a character value (e.g. “Central Hospital” or “date_onset”) but is a number. This is useful for looping through data frames, as you can use the “item” number inside the for loop to index the data frame by row number.\nFor example, let’s say that you want to loop through every row in your data frame and extract certain information. Your “items” would be numeric row numbers. Often, “items” in this case are written as i.\nThe for loop process could be explained in words as “for every item in a sequence of numbers from 1 to the total number of rows in my data frame, do X”. For the first iteration of the loop, the value of “item” i would be 1. For the second iteration, i would be 2, etc.\nHere is what the sequence looks like in code: for (i in 1:nrow(linelist)) {OPERATIONS CODE} where i represents the “item” and 1:nrow(linelist) produces a sequence of consecutive numbers from 1 through the number of rows in linelist.\n\nfor (i in 1:nrow(linelist)) {  # use on a data frame\n  # OPERATIONS HERE\n}  \n\nIf you want the sequence to be numbers, but you are starting from a vector (not a data frame), use the shortcut seq_along() to return a sequence of numbers for each element in the vector. For example, for (i in seq_along(hospital_names) {OPERATIONS CODE}.\nThe below code actually returns numbers, which would become the value of i in their respective loop.\n\nseq_along(hospital_names)  # use on a named vector\n\n[1] 1 2 3 4 5 6\n\n\nOne advantage of using numbers in the sequence is that is easy to also use the i number to index a container that stores the loop outputs. There is an example of this in the Operations section below.\n\n\nOperations\nThis is code within the curly brackets { } of the for loop. You want this code to run for each “item” in the sequence. Therefore, be careful that every part of your code that changes by the “item” is correctly coded such that it actually changes! E.g. remember to use [[ ]] for indexing.\nIn the example below, we iterate through each row in the linelist. The gender and age values of each row are pasted together and stored in the container character vector cases_demographics. Note how we also use indexing [[i]] to save the loop output to the correct position in the “container” vector.\n\n# create container to store results - a character vector\ncases_demographics &lt;- vector(mode = \"character\", length = nrow(linelist))\n\n# the for loop\nfor (i in 1:nrow(linelist)){\n  \n  # OPERATIONS\n  # extract values from linelist for row i, using brackets for indexing\n  row_gender  &lt;- linelist$gender[[i]]\n  row_age     &lt;- linelist$age_years[[i]]    # don't forget to index!\n     \n  # combine gender-age and store in container vector at indexed location\n  cases_demographics[[i]] &lt;- str_c(row_gender, row_age, sep = \",\") \n\n}  # end for loop\n\n\n# display first 10 rows of container\nhead(cases_demographics, 10)\n\n [1] \"m,2\"  \"f,3\"  \"m,56\" \"f,18\" \"m,3\"  \"f,16\" \"f,16\" \"f,0\"  \"m,61\" \"f,27\"\n\n\n\n\nContainer\nSometimes the results of your for loop will be printed to the console or RStudio Plots pane. Other times, you will want to store the outputs in a “container” for later use. Such a container could be a vector, a data frame, or even a list.\nIt is most efficient to create the container for the results before even beginning the for loop. In practice, this means creating an empty vector, data frame, or list. These can be created with the functions vector() for vectors or lists, or with matrix() and data.frame() for a data frame.\nEmpty vector\nUse vector() and specify the mode = based on the expected class of the objects you will insert - either “double” (to hold numbers), “character”, or “logical”. You should also set the length = in advance. This should be the length of your for loop sequence.\nSay you want to store the median delay-to-admission for each hospital. You would use “double” and set the length to be the number of expected outputs (the number of unique hospitals in the data set).\n\ndelays &lt;- vector(\n  mode = \"double\",                            # we expect to store numbers\n  length = length(unique(linelist$hospital))) # the number of unique hospitals in the dataset\n\nEmpty data frame\nYou can make an empty data frame by specifying the number of rows and columns like this:\n\ndelays &lt;- data.frame(matrix(ncol = 2, nrow = 3))\n\nEmpty list\nYou may want store some plots created by a for loop in a list. A list is like vector, but holds other R objects within it that can be of different classes. Items in a list could be a single number, a dataframe, a vector, and even another list.\nYou actually initialize an empty list using the same vector() command as above, but with mode = \"list\". Specify the length however you wish.\n\nplots &lt;- vector(mode = \"list\", length = 16)\n\n\n\nPrinting\nNote that to print from within a for loop you will likely need to explicitly wrap with the function print().\nIn this example below, the sequence is an explicit character vector, which is used to subset the linelist by hospital. The results are not stored in a container, but rather are printed to console with the print() function.\n\nfor (hosp in hospital_names){ \n     hospital_cases &lt;- linelist %&gt;% filter(hospital == hosp)\n     print(nrow(hospital_cases))\n}\n\n[1] 885\n[1] 1469\n[1] 422\n[1] 1762\n[1] 896\n[1] 454\n\n\n\n\nTesting your for loop\nTo test your loop, you can run a command to make a temporary assignment of the “item”, such as i &lt;- 10 or hosp &lt;- \"Central Hospital\". Do this outside the loop and then run your operations code only (the code within the curly brackets) to see if the expected results are produced.\n\n\nLooping plots\nTo put all three components together (container, sequence, and operations) let’s try to plot an epicurve for each hospital (see page on Epidemic curves).\nWe can make a nice epicurve of all the cases by gender using the incidence2 package as below:\n\n# create 'incidence' object\noutbreak &lt;- incidence2::incidence(   \n     x = linelist,                   # dataframe - complete linelist\n     date_index = \"date_onset\",        # date column\n     interval = \"week\",              # aggregate counts weekly\n     groups = \"gender\")               # group values by gender\n     #na_as_group = TRUE)             # missing gender is own group\n\n# tracer la courbe d'épidémie\nggplot(outbreak, # nom de l'objet d'incidence\n        aes(x = date_index, #aesthetiques et axes\n            y = count, \n            fill = gender), # Fill colour of bars by gender\n       color = \"black\"      # Contour colour of bars\n       ) +  \n     geom_col() + \n     facet_wrap(~gender) +\n     theme_bw() + \n     labs(title = \"Outbreak of all cases\", #titre\n          x = \"Counts\", \n          y = \"Date\", \n          fill = \"Gender\", \n          color = \"Gender\")\n\n\n\n\n\n\n\n\nTo produce a separate plot for each hospital’s cases, we can put this epicurve code within a for loop.\nFirst, we save a named vector of the unique hospital names, hospital_names. The for loop will run once for each of these names: for (hosp in hospital_names). Each iteration of the for loop, the current hospital name from the vector will be represented as hosp for use within the loop.\nWithin the loop operations, you can write R code as normal, but use the “item” (hosp in this case) knowing that its value will be changing. Within this loop:\n\nA filter() is applied to linelist, such that column hospital must equal the current value of hosp.\n\nThe incidence object is created on the filtered linelist.\n\nThe plot for the current hospital is created, with an auto-adjusting title that uses hosp.\n\nThe plot for the current hospital is temporarily saved and then printed.\n\nThe loop then moves onward to repeat with the next hospital in hospital_names.\n\n\n# make vector of the hospital names\nhospital_names &lt;- unique(linelist$hospital)\n\n# for each name (\"hosp\") in hospital_names, create and print the epi curve\nfor (hosp in hospital_names) {\n     \n     # create incidence object specific to the current hospital\n     outbreak_hosp &lt;- incidence2::incidence(\n          x = linelist %&gt;% filter(hospital == hosp),   # linelist is filtered to the current hospital\n          date_index = \"date_onset\",\n          interval = \"week\", \n          groups = \"gender\"#,\n          #na_as_group = TRUE\n     )\n     \n      plot_hosp &lt;- ggplot(outbreak_hosp, # incidence object name\n                         aes(x = date_index, #axes\n                             y = count, \n                             fill = gender), # fill colour by gender\n                         color = \"black\"      # colour of bar contour\n                         ) +  \n          geom_col() + \n          facet_wrap(~gender) +\n          theme_bw() + \n          labs(title = stringr::str_glue(\"Epidemic of cases admitted to {hosp}\"), #title\n               x = \"Counts\", \n               y = \"Date\", \n               fill = \"Gender\", \n               color = \"Gender\")\n     \n     # With older versions of R, remove the # before na_as_group and use this plot command instead.\n    # plot_hosp &lt;- plot(\n#       outbreak_hosp,\n#       fill = \"gender\",\n#       color = \"black\",\n#       title = stringr::str_glue(\"Epidemic of cases admitted to {hosp}\")\n#     )\n     \n     #print the plot for hospitals\n     print(plot_hosp)\n     \n} # end the for loop when it has been run for every hospital in hospital_names \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTracking progress of a loop\nA loop with many iterations can run for many minutes or even hours. Thus, it can be helpful to print the progress to the R console. The if statement below can be placed within the loop operations to print every 100th number. Just adjust it so that i is the “item” in your loop.\n\n# loop with code to print progress every 100 iterations\nfor (i in seq_len(nrow(linelist))){\n\n  # print progress\n  if(i %% 100==0){    # The %% operator is the remainder\n    print(i)\n\n}",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Iteration, loops, and lists</span>"
    ]
  },
  {
    "objectID": "new_pages/iteration.html#iter_purrr",
    "href": "new_pages/iteration.html#iter_purrr",
    "title": "16  Iteration, loops, and lists",
    "section": "16.3 purrr and lists",
    "text": "16.3 purrr and lists\nAnother approach to iterative operations is the purrr package - it is the tidyverse approach to iteration.\nIf you are faced with performing the same task several times, it is probably worth creating a generalised solution that you can use across many inputs. For example, producing plots for multiple jurisdictions, or importing and combining many files.\nThere are also a few other advantages to purrr - you can use it with pipes %&gt;%, it handles errors better than normal for loops, and the syntax is quite clean and simple! If you are using a for loop, you can probably do it more clearly and succinctly with purrr!\nKeep in mind that purrr is a functional programming tool. That is, the operations that are to be iteratively applied are wrapped up into functions. See the Writing functions page to learn how to write your own functions.\npurrr is also almost entirely based around lists and vectors - so think about it as applying a function to each element of that list/vector!\n\nLoad packages\npurrr is part of the tidyverse, so there is no need to install/load a separate package.\n\npacman::p_load(\n     rio,            # import/export\n     here,           # relative filepaths\n     tidyverse,      # data mgmt and viz\n     writexl,        # write Excel file with multiple sheets\n     readxl          # import Excel with multiple sheets\n)\n\n\n\nmap()\nOne core purrr function is map(), which “maps” (applies) a function to each input element of a list/vector you provide.\nThe basic syntax is map(.x = SEQUENCE, .f = FUNCTION, OTHER ARGUMENTS). In a bit more detail:\n\n.x = are the inputs upon which the .f function will be iteratively applied - e.g. a vector of jurisdiction names, columns in a data frame, or a list of data frames.\n\n.f = is the function to apply to each element of the .x input - it could be a function like print() that already exists, or a custom function that you define. The function is often written after a tilde ~ (details below).\n\nA few more notes on syntax:\n\nIf the function needs no further arguments specified, it can be written with no parentheses and no tilde (e.g. .f = mean). To provide arguments that will be the same value for each iteration, provide them within map() but outside the .f = argument, such as the na.rm = T in map(.x = my_list, .f = mean, na.rm=T).\n\nYou can use .x (or simply .) within the .f = function as a placeholder for the .x value of that iteration\n\nUse tilde syntax (~) to have greater control over the function - write the function as normal with parentheses, such as: map(.x = my_list, .f = ~mean(., na.rm = T)). Use this syntax particularly if the value of an argument will change each iteration, or if it is the value .x itself (see examples below)\n\nThe output of using map() is a list - a list is an object class like a vector but whose elements can be of different classes. So, a list produced by map() could contain many data frames, or many vectors, many single values, or even many lists! There are alternative versions of map() explained below that produce other types of outputs (e.g. map_dfr() to produce a data frame, map_chr() to produce character vectors, and map_dbl() to produce numeric vectors).\n\nExample - import and combine Excel sheets\nLet’s demonstrate with a common epidemiologist task: - You want to import an Excel workbook with case data, but the data are split across different named sheets in the workbook. How do you efficiently import and combine the sheets into one data frame?\nLet’s say we are sent the below Excel workbook. Each sheet contains cases from a given hospital.\n\n\n\n\n\n\n\n\n\nHere is one approach that uses map():\n\nmap() the function import() so that it runs for each Excel sheet.\nCombine the imported data frames into one using bind_rows().\n\nAlong the way, preserve the original sheet name for each row, storing this information in a new column in the final data frame.\n\nFirst, we need to extract the sheet names and save them. We provide the Excel workbook’s file path to the function excel_sheets() from the package readxl, which extracts the sheet names. We store them in a character vector called sheet_names.\n\nsheet_names &lt;- readxl::excel_sheets(\"hospital_linelists.xlsx\")\n\nHere are the names:\n\nsheet_names\n\n[1] \"Central Hospital\"              \"Military Hospital\"            \n[3] \"Missing\"                       \"Other\"                        \n[5] \"Port Hospital\"                 \"St. Mark's Maternity Hospital\"\n\n\nNow that we have this vector of names, map() can provide them one-by-one to the function import(). In this example, the sheet_names are .x and import() is the function .f.\nRecall from the Import and export page that when used on Excel workbooks, import() can accept the argument which = specifying the sheet to import. Within the .f function import(), we provide which = .x, whose value will change with each iteration through the vector sheet_names - first “Central Hospital”, then “Military Hospital”, etc.\nOf note - because we have used map(), the data in each Excel sheet will be saved as a separate data frame within a list. We want each of these list elements (data frames) to have a name, so before we pass sheet_names to map() we pass it through set_names() from purrr, which ensures that each list element gets the appropriate name.\nWe save the output list as combined.\n\ncombined &lt;- sheet_names %&gt;% \n  purrr::set_names() %&gt;% \n  map(.f = ~import(\"hospital_linelists.xlsx\", which = .x))\n\nWhen we inspect output, we see that the data from each Excel sheet is saved in the list with a name. This is good, but we are not quite finished.\n\n\n\n\n\n\n\n\n\nLastly, we use the function bind_rows() (from dplyr) which accepts the list of similarly-structured data frames and combines them into one data frame. To create a new column from the list element names, we use the argument .id = and provide it with the desired name for the new column.\nBelow is the whole sequence of commands:\n\nsheet_names &lt;- readxl::excel_sheets(\"hospital_linelists.xlsx\")  # extract sheet names\n \ncombined &lt;- sheet_names %&gt;%                                     # begin with sheet names\n  purrr::set_names() %&gt;%                                        # set their names\n  map(.f = ~import(\"hospital_linelists.xlsx\", which = .x)) %&gt;%  # iterate, import, save in list\n  bind_rows(.id = \"origin_sheet\") # combine list of data frames, preserving origin in new column  \n\nAnd now we have one data frame with a column containing the sheet of origin!\n\n\n\n\n\n\n\n\n\nThere are variations of map() that you should be aware of. For example, map_dfr() returns a data frame, not a list. Thus, we could have used it for the task above and not have had to bind rows. But then we would not have been able to capture which sheet (hospital) each case came from.\nOther variations include map_chr(), map_dbl(). These are very useful functions for two reasons. Firstly. they automatically convert the output of an iterative function into a vector (not a list). Secondly, they can explicitly control the class that the data comes back in - you ensure that your data comes back as a character vector with map_chr(), or numeric vector with map_dbl(). Lets return to these later in the section!\nThe functions map_at() and map_if() are also very useful for iteration - they allow you to specify which elements of a list you should iterate at! These work by simply applying a vector of indexes/names (in the case of map_at()) or a logical test (in the case of map_if()).\nLets use an example where we didn’t want to read the first sheet of hospital data. We use map_at() instead of map(), and specify the .at = argument to c(-1) which means to not use the first element of .x. Alternatively, you can provide a vector of positive numbers, or names, to .at = to specify which elements to use.\n\nsheet_names &lt;- readxl::excel_sheets(\"hospital_linelists.xlsx\")\n\ncombined &lt;- sheet_names %&gt;% \n     purrr::set_names() %&gt;% \n     # exclude the first sheet\n     map_at(.f = ~import( \"hospital_linelists.xlsx\", which = .x),\n            .at = c(-1))\n\nNote that the first sheet name will still appear as an element of the output list - but it is only a single character name (not a data frame). You would need to remove this element before binding rows. We will cover how to remove and modify list elements in a later section.\n\n\n\nSplit dataset and export\nBelow, we give an example of how to split a dataset into parts and then use map() iteration to export each part as a separate Excel sheet, or as a separate CSV file.\n\nSplit dataset\nLet’s say we have the complete case linelist as a data frame, and we now want to create a separate linelist for each hospital and export each as a separate CSV file. Below, we do the following steps:\nUse group_split() (from dplyr) to split the linelist data frame by unique values in column hospital. The output is a list containing one data frame per hospital subset.\n\nlinelist_split &lt;- linelist %&gt;% \n     group_split(hospital)\n\nWe can run View(linelist_split) and see that this list contains 6 data frames (“tibbles”), each representing the cases from one hospital.\n\n\n\n\n\n\n\n\n\nHowever, note that the data frames in the list do not have names by default! We want each to have a name, and then to use that name when saving the CSV file.\nOne approach to extracting the names is to use pull() (from dplyr) to extract the hospital column from each data frame in the list. Then, to be safe, we convert the values to character and then use unique() to get the name for that particular data frame. All of these steps are applied to each data frame via map().\n\nnames(linelist_split) &lt;- linelist_split %&gt;%   # Assign to names of listed data frames \n     # Extract the names by doing the following to each data frame: \n     map(.f = ~pull(.x, hospital)) %&gt;%        # Pull out hospital column\n     map(.f = ~as.character(.x)) %&gt;%          # Convert to character, just in case\n     map(.f = ~unique(.x))                    # Take the unique hospital name\n\nWe can now see that each of the list elements has a name. These names can be accessed via names(linelist_split).\n\n\n\n\n\n\n\n\n\n\nnames(linelist_split)\n\n[1] \"Central Hospital\"                    \n[2] \"Military Hospital\"                   \n[3] \"Missing\"                             \n[4] \"Other\"                               \n[5] \"Port Hospital\"                       \n[6] \"St. Mark's Maternity Hospital (SMMH)\"\n\n\n\nMore than one group_split() column\nIf you wanted to split the linelist by more than one grouping column, such as to produce subset linelist by intersection of hospital AND gender, you will need a different approach to naming the list elements. This involves collecting the unique “group keys” using group_keys() from dplyr - they are returned as a data frame. Then you can combine the group keys into values with unite() as shown below, and assign these conglomerate names to linelist_split.\n\n# split linelist by unique hospital-gender combinations\nlinelist_split &lt;- linelist %&gt;% \n     group_split(hospital, gender)\n\n# extract group_keys() as a dataframe\ngroupings &lt;- linelist %&gt;% \n     group_by(hospital, gender) %&gt;%       \n     group_keys()\n\ngroupings      # show unique groupings \n\n# A tibble: 18 × 2\n   hospital                             gender\n   &lt;chr&gt;                                &lt;chr&gt; \n 1 Central Hospital                     f     \n 2 Central Hospital                     m     \n 3 Central Hospital                     &lt;NA&gt;  \n 4 Military Hospital                    f     \n 5 Military Hospital                    m     \n 6 Military Hospital                    &lt;NA&gt;  \n 7 Missing                              f     \n 8 Missing                              m     \n 9 Missing                              &lt;NA&gt;  \n10 Other                                f     \n11 Other                                m     \n12 Other                                &lt;NA&gt;  \n13 Port Hospital                        f     \n14 Port Hospital                        m     \n15 Port Hospital                        &lt;NA&gt;  \n16 St. Mark's Maternity Hospital (SMMH) f     \n17 St. Mark's Maternity Hospital (SMMH) m     \n18 St. Mark's Maternity Hospital (SMMH) &lt;NA&gt;  \n\n\nNow we combine the groupings together, separated by dashes, and assign them as the names of list elements in linelist_split. This takes some extra lines as we replace NA with “Missing”, use unite() from dplyr to combine the column values together (separated by dashes), and then convert into an un-named vector so it can be used as names of linelist_split.\n\n# Combine into one name value \nnames(linelist_split) &lt;- groupings %&gt;% \n     mutate(across(everything(), replace_na, \"Missing\")) %&gt;%  # replace NA with \"Missing\" in all columns\n     unite(\"combined\", sep = \"-\") %&gt;%                         # Unite all column values into one\n     setNames(NULL) %&gt;% \n     as_vector() %&gt;% \n     as.list()\n\n\n\n\nExport as Excel sheets\nTo export the hospital linelists as an Excel workbook with one linelist per sheet, we can just provide the named list linelist_split to the write_xlsx() function from the writexl package. This has the ability to save one Excel workbook with multiple sheets. The list element names are automatically applied as the sheet names.\n\nlinelist_split %&gt;% \n     writexl::write_xlsx(path = here(\"data\", \"hospital_linelists.xlsx\"))\n\nYou can now open the Excel file and see that each hospital has its own sheet.\n\n\n\n\n\n\n\n\n\n\n\nExport as CSV files\nIt is a bit more complex command, but you can also export each hospital-specific linelist as a separate CSV file, with a file name specific to the hospital.\nAgain we use map(): we take the vector of list element names (shown above) and use map() to iterate through them, applying export() (from the rio package, see Import and export page) on the data frame in the list linelist_split that has that name. We also use the name to create a unique file name. Here is how it works:\n\nWe begin with the vector of character names, passed to map() as .x.\n\nThe .f function is export() , which requires a data frame and a file path to write to.\n\nThe input .x (the hospital name) is used within .f to extract/index that specific element of linelist_split list. This results in only one data frame at a time being provided to export().\n\nFor example, when map() iterates for “Military Hospital”, then linelist_split[[.x]] is actually linelist_split[[\"Military Hospital\"]], thus returning the second element of linelist_split - which is all the cases from Military Hospital.\n\nThe file path provided to export() is dynamic via use of str_glue() (see Characters and strings page):\n\nhere() is used to get the base of the file path and specify the “data” folder (note single quotes to not interrupt the str_glue() double quotes).\n\n\nThen a slash /, and then again the .x which prints the current hospital name to make the file identifiable.\n\nFinally the extension “.csv” which export() uses to create a CSV file.\n\n\nnames(linelist_split) %&gt;%\n     map(.f = ~export(linelist_split[[.x]], file = str_glue(\"{here('data')}/{.x}.csv\")))\n\nNow you can see that each file is saved in the “data” folder of the R Project “Epi_R_handbook”!\n\n\n\n\n\n\n\n\n\n\n\n\nCustom functions\nYou may want to create your own function to provide to map().\nLet’s say we want to create epidemic curves for each hospital’s cases. To do this using purrr, our .f function can be ggplot() and extensions with + as usual. As the output of map() is always a list, the plots are stored in a list. Because they are plots, they can be extracted and plotted with the ggarrange() function from the ggpubr package (documentation).\n\n# load package for plotting elements from list\npacman::p_load(ggpubr)\n\n# map across the vector of 6 hospital \"names\" (created earlier)\n# use the ggplot function specified\n# output is a list with 6 ggplots\n\nhospital_names &lt;- unique(linelist$hospital)\n\nmy_plots &lt;- map(\n  .x = hospital_names,\n  .f = ~ggplot(data = linelist %&gt;% filter(hospital == .x)) +\n                geom_histogram(aes(x = date_onset)) +\n                labs(title = .x)\n)\n\n# print the ggplots (they are stored in a list)\nggarrange(plotlist = my_plots, ncol = 2, nrow = 3)\n\n\n\n\n\n\n\n\nIf this map() code looks too messy, you can achieve the same result by saving your specific ggplot() command as a custom user-defined function, for example we can name it make_epicurve()). This function is then used within the map(). .x will be iteratively replaced by the hospital name, and used as hosp_name in the make_epicurve() function. See the page on Writing functions.\n\n# Create function\nmake_epicurve &lt;- function(hosp_name){\n  \n  ggplot(data = linelist %&gt;% filter(hospital == hosp_name)) +\n    geom_histogram(aes(x = date_onset)) +\n    theme_classic()+\n    labs(title = hosp_name)\n  \n}\n\n\n# mapping\nmy_plots &lt;- map(hospital_names, ~make_epicurve(hosp_name = .x))\n\n# print the ggplots (they are stored in a list)\nggarrange(plotlist = my_plots, ncol = 2, nrow = 3)\n\n\n\nMapping a function across columns\nAnother common use-case is to map a function across many columns. Below, we map() the function t.test() across numeric columns in the data frame linelist, comparing the numeric values by gender.\nRecall from the page on Simple statistical tests that t.test() can take inputs in a formula format, such as t.test(numeric column ~ binary column). In this example, we do the following:\n\nThe numeric columns of interest are selected from linelist - these become the .x inputs to map().\n\nThe function t.test() is supplied as the .f function, which is applied to each numeric column.\n\nWithin the parentheses of t.test():\n\nthe first ~ precedes the .f that map() will iterate over .x.\n\nthe .x represents the current column being supplied to the function t.test().\n\nthe second ~ is part of the t-test equation described above.\n\nthe t.test() function expects a binary column on the right-hand side of the equation. We supply the vector linelist$gender independently and statically (note that it is not included in select()).\n\n\nmap() returns a list, so the output is a list of t-test results - one list element for each numeric column analysed.\n\n# Results are saved as a list\nt.test_results &lt;- linelist %&gt;% \n  select(age, wt_kg, ht_cm, ct_blood, temp) %&gt;%  # keep only some numeric columns to map across\n  map(.f = ~t.test(.x ~ linelist$gender))        # t.test function, with equation NUMERIC ~ CATEGORICAL\n\nHere is what the list t.test_results looks like when opened (Viewed) in RStudio. We have highlighted parts that are important for the examples in this page.\n\nYou can see at the top that the whole list is named t.test_results and has five elements. Those five elements are named age, wt_km, ht_cm, ct_blood, temp after each variable that was used in a t-test with gender from the linelist.\n\nEach of those five elements are themselves lists, with elements within them such as p.value and conf.int. Some of these elements like p.value are single numbers, whereas some such as estimate consist of two or more elements (mean in group f and mean in group m).\n\n\n\n\n\n\n\n\n\n\nNote: Remember that if you want to apply a function to only certain columns in a data frame, you can also simply use mutate() and across(), as explained in the Cleaning data and core functions page. Below is an example of applying as.character() to only the “age” columns. Note the placement of the parentheses and commas.\n\n# convert columns with column name containing \"age\" to class Character\nlinelist &lt;- linelist %&gt;% \n  mutate(across(.cols = contains(\"age\"), .fns = as.character))  \n\n\n\nExtract from lists\nAs map() produces an output of class List, we will spend some time discussing how to extract data from lists using accompanying purrr functions. To demonstrate this, we will use the list t.test_results from the previous section. This is a list of 5 lists - each of the 5 lists contains the results of a t-test between a column from linelist data frame and its binary column gender. See the image in the section above for a visual of the list structure.\n\nNames of elements\nTo extract the names of the elements themselves, simply use names() from base R. In this case, we use names() on t.test_results to return the names of each sub-list, which are the names of the 5 variables that had t-tests performed.\n\nnames(t.test_results)\n\n[1] \"age\"      \"wt_kg\"    \"ht_cm\"    \"ct_blood\" \"temp\"    \n\n\n\n\nElements by name or position\nTo extract list elements by name or by position you can use brackets [[ ]] as described in the R basics page. Below we use double brackets to index the list t.tests_results and display the first element which is the results of the t-test on age.\n\nt.test_results[[1]] # first element by position\n\n\n    Welch Two Sample t-test\n\ndata:  .x by linelist$gender\nt = -21.3, df = 4902.9, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n -7.544409 -6.272675\nsample estimates:\nmean in group f mean in group m \n       12.66085        19.56939 \n\nt.test_results[[1]][\"p.value\"] # return element named \"p.value\" from first element  \n\n$p.value\n[1] 2.350374e-96\n\n\nHowever, below we will demonstrate use of the simple and flexible purrr functions map() and pluck() to achieve the same outcomes.\n\n\npluck()\npluck() pulls out elements by name or by position. For example - to extract the t-test results for age, you can use pluck() like this:\n\nt.test_results %&gt;% \n  pluck(\"age\")        # alternatively, use pluck(1)\n\n\n    Welch Two Sample t-test\n\ndata:  .x by linelist$gender\nt = -21.3, df = 4902.9, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n -7.544409 -6.272675\nsample estimates:\nmean in group f mean in group m \n       12.66085        19.56939 \n\n\nIndex deeper levels by specifying the further levels with commas. The below extracts the element named “p.value” from the list age within the list t.test_results. You can also use numbers instead of character names.\n\nt.test_results %&gt;% \n  pluck(\"age\", \"p.value\")\n\n[1] 2.350374e-96\n\n\nYou can extract such inner elements from all first-level elements by using map() to run the pluck() function across each first-level element. For example, the below code extracts the “p.value” elements from all lists within t.test_results. The list of t-test results is the .x iterated across, pluck() is the .f function being iterated, and the value “p-value” is provided to the function.\n\nt.test_results %&gt;%\n  map(pluck, \"p.value\")   # return every p-value\n\n$age\n[1] 2.350374e-96\n\n$wt_kg\n[1] 2.664367e-182\n\n$ht_cm\n[1] 3.515713e-144\n\n$ct_blood\n[1] 0.4473498\n\n$temp\n[1] 0.5735923\n\n\nAs another alternative, map() offers a shorthand where you can write the element name in quotes, and it will pluck it out. If you use map() the output will be a list, whereas if you use map_chr() it will be a named character vector and if you use map_dbl() it will be a named numeric vector.\n\nt.test_results %&gt;% \n  map_dbl(\"p.value\")   # return p-values as a named numeric vector\n\n          age         wt_kg         ht_cm      ct_blood          temp \n 2.350374e-96 2.664367e-182 3.515713e-144  4.473498e-01  5.735923e-01 \n\n\nYou can read more about pluck() in it’s purrr documentation. It has a sibling function chuck() that will return an error instead of NULL if an element does not exist.\n\n\n\nConvert list to data frame\nThis is a complex topic - see the Resources section for more complete tutorials. Nevertheless, we will demonstrate converting the list of t-test results into a data frame. We will create a data frame with columns for the variable, its p-value, and the means from the two groups (male and female).\nHere are some of the new approaches and functions that will be used:\n\nThe function tibble() will be used to create a tibble (like a data frame).\n\nWe surround the tibble() function with curly brackets { } to prevent the entire t.test_results from being stored as the first tibble column.\n\n\nWithin tibble(), each column is created explicitly, similar to the syntax of mutate():\n\nThe . represents t.test_results.\nTo create a column with the t-test variable names (the names of each list element) we use names() as described above.\n\nTo create a column with the p-values we use map_dbl() as described above to pull the p.value elements and convert them to a numeric vector.\n\n\n\nt.test_results %&gt;% {\n  tibble(\n    variables = names(.),\n    p         = map_dbl(., \"p.value\"))\n  }\n\n# A tibble: 5 × 2\n  variables         p\n  &lt;chr&gt;         &lt;dbl&gt;\n1 age       2.35e- 96\n2 wt_kg     2.66e-182\n3 ht_cm     3.52e-144\n4 ct_blood  4.47e-  1\n5 temp      5.74e-  1\n\n\nBut now let’s add columns containing the means for each group (males and females).\nWe would need to extract the element estimate, but this actually contains two elements within it (mean in group f and mean in group m). So, it cannot be simplified into a vector with map_chr() or map_dbl(). Instead, we use map(), which used within tibble() will create a column of class list within the tibble! Yes, this is possible!\n\nt.test_results %&gt;% \n  {tibble(\n    variables = names(.),\n    p = map_dbl(., \"p.value\"),\n    means = map(., \"estimate\"))}\n\n# A tibble: 5 × 3\n  variables         p means       \n  &lt;chr&gt;         &lt;dbl&gt; &lt;named list&gt;\n1 age       2.35e- 96 &lt;dbl [2]&gt;   \n2 wt_kg     2.66e-182 &lt;dbl [2]&gt;   \n3 ht_cm     3.52e-144 &lt;dbl [2]&gt;   \n4 ct_blood  4.47e-  1 &lt;dbl [2]&gt;   \n5 temp      5.74e-  1 &lt;dbl [2]&gt;   \n\n\nOnce you have this list column, there are several tidyr functions (part of tidyverse) that help you “rectangle” or “un-nest” these “nested list” columns. Read more about them here, or by running vignette(\"rectangle\"). In brief:\n\nunnest_wider() - gives each element of a list-column its own column.\n\nunnest_longer() - gives each element of a list-column its own row.\nhoist() - acts like unnest_wider() but you specify which elements to unnest.\n\nBelow, we pass the tibble to unnest_wider() specifying the tibble’s means column (which is a nested list). The result is that means is replaced by two new columns, each reflecting the two elements that were previously in each means cell.\n\nt.test_results %&gt;% \n  {tibble(\n    variables = names(.),\n    p = map_dbl(., \"p.value\"),\n    means = map(., \"estimate\")\n    )} %&gt;% \n  unnest_wider(means)\n\n# A tibble: 5 × 4\n  variables         p `mean in group f` `mean in group m`\n  &lt;chr&gt;         &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 age       2.35e- 96              12.7              19.6\n2 wt_kg     2.66e-182              45.8              59.6\n3 ht_cm     3.52e-144             109.              142. \n4 ct_blood  4.47e-  1              21.2              21.2\n5 temp      5.74e-  1              38.6              38.6\n\n\n\n\nDiscard, keep, and compact lists\nBecause working with purrr so often involves lists, we will briefly explore some purrr functions to modify lists. See the Resources section for more complete tutorials on purrr functions.\n\nlist_modify() has many uses, one of which can be to remove a list element.\n\nkeep() retains the elements specified to .p =, or where a function supplied to .p = evaluates to TRUE.\n\ndiscard() removes the elements specified to .p, or where a function supplied to .p = evaluates to TRUE.\n\ncompact() removes all empty elements.\n\nHere are some examples using the combined list created in the section above on using map() to import and combine multiple files (it contains 6 case linelist data frames):\nElements can be removed by name with list_modify() and setting the name equal to NULL.\n\ncombined %&gt;% \n  list_modify(\"Central Hospital\" = NULL)   # remove list element by name\n\nYou can also remove elements by criteria, by providing a “predicate” equation to .p = (an equation that evaluates to either TRUE or FALSE). Place a tilde ~ before the function and use .x to represent the list element. Using keep() the list elements that evaluate to TRUE will be kept. Inversely, if using discard() the list elements that evaluate to TRUE will be removed.\n\n# keep only list elements with more than 500 rows\ncombined %&gt;% \n  keep(.p = ~nrow(.x) &gt; 500)  \n\nIn the below example, list elements are discarded if their class are not data frames.\n\n# Discard list elements that are not data frames\ncombined %&gt;% \n  discard(.p = ~class(.x) != \"data.frame\")\n\nYour predicate function can also reference elements/columns within each list item. For example, below, list elements where the mean of column ct_blood is over 25 are discarded.\n\n# keep only list elements where ct_blood column mean is over 25\ncombined %&gt;% \n  discard(.p = ~mean(.x$ct_blood) &gt; 25)  \n\nThis command would remove all empty list elements:\n\n# Remove all empty list elements\ncombined %&gt;% \n  compact()\n\n\n\npmap()\nThe function pmap() from the purrr package allows us to apply map_*() functions over multiple vectors. The “p” in pmap() stands for parallel. It works down a a dataset or list sequentially, carrying out your operation. Note, it does not refer to parallel computing.\nIn pmap() you specify a single dataset, or list that contains all of the vectors, or lists, that you want to supply your function.\nThis can allow you to very quickly carry out calculations with multiple columns of a dataframe, or lists of information.\nFor example, here is a simple dataset of three numbers.\n\ndata_generic &lt;- data.frame(\n     A = c(1, 10, 100),\n     B = c(3, 6, 9),\n     C = c(25, 75, 50)\n)\n\ndata_generic\n\n    A B  C\n1   1 3 25\n2  10 6 75\n3 100 9 50\n\n\nHere we are going to using the function sum() from base R to look at what the sum of each row is.\n\ndata_generic %&gt;%    #Our dataset\n     pmap_dbl(sum)      #The function we want to use\n\n[1]  29  91 159\n\n\nYou can see that the function pmap_dbl has gone through each row of the datasets, and summed the values. While there are other ways of carrying out this operation in this example, such as using rowSums() from base, pmap_*() functions are much quicker. Additionally, pmap_*() allows you to input custom functions, and specify more complicated inputs.\nFor example, here we are going to create a new column to count how many symptoms those in our linelist dataset have.\n\nlinelist_symptom_count &lt;- linelist %&gt;%                                                     #Our dataset\n     mutate(number_symptoms = linelist %&gt;%                       #Creating a new column to count symptoms\n                 select(fever:vomit) %&gt;%                         #Selecting the columns that indicate the presence of symptoms\n                 pmap_int(~sum(c(...) == \"yes\", na.rm = T)))    #Here pmap is looking at each row of symptoms, counting which values are set as \"yes\" and then summing all the values in the row\n\n#Display the results\nlinelist_symptom_count %&gt;%\n     select(fever:vomit, number_symptoms) %&gt;%\n     slice(1:10)\n\n   fever chills cough aches vomit number_symptoms\n1     no     no   yes    no   yes               2\n2   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;               0\n3   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;               0\n4     no     no    no    no    no               0\n5     no     no   yes    no   yes               2\n6     no     no   yes    no   yes               2\n7   &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;               0\n8     no     no   yes    no   yes               2\n9     no     no   yes    no   yes               2\n10    no     no   yes    no    no               1\n\n\nAs another example, here we have written our own custom function, using str_glue, see section on Characters and strings to summarise each patient’s gender, age, date of onset, outcome and the date of outcome:\n\n#Function\nsummarise_function &lt;- function(case_id, gender, age, date_onset, date_outcome, outcome, ...){\n     str_glue(\"Case {case_id} who had the gender {gender} and the age {age}, had symptom onset on {date_onset}, and had the outcome of {outcome} on {date_outcome}.\")\n}\n\n#Run the custom pmap function\nlinelist_summary &lt;- linelist %&gt;%\n     pmap_chr(summarise_function)\n\n#Display only the first 3 for ease of viewing\nlinelist_summary[1:2]\n\n[1] \"Case 5fe599 who had the gender m and the age 2, had symptom onset on 2014-05-13, and had the outcome of NA on NA.\"             \n[2] \"Case 8689b7 who had the gender f and the age 3, had symptom onset on 2014-05-13, and had the outcome of Recover on 2014-05-18.\"\n\n\nNote that here we did not even have to specify which columns to use, as they are the same name in the function, summarise_function() as in the dataset. pmap_*() functions automatically map the column or list names to the function.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Iteration, loops, and lists</span>"
    ]
  },
  {
    "objectID": "new_pages/iteration.html#apply-functions",
    "href": "new_pages/iteration.html#apply-functions",
    "title": "16  Iteration, loops, and lists",
    "section": "16.4 Apply functions",
    "text": "16.4 Apply functions\nThe “apply” family of functions is a base R alternative to purrr for iterative operations. You can read more about them here.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Iteration, loops, and lists</span>"
    ]
  },
  {
    "objectID": "new_pages/iteration.html#resources",
    "href": "new_pages/iteration.html#resources",
    "title": "16  Iteration, loops, and lists",
    "section": "16.5 Resources",
    "text": "16.5 Resources\nfor loops with Data Carpentry\nThe R for Data Science page on iteration\nVignette on write/read Excel files\nA purrr tutorial by jennybc\nAnother purrr tutorial by Rebecca Barter\nA purrr tutorial on map, pmap, and imap\npurrr cheatsheet\npurrr tips and tricks\nkeep and discard",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Iteration, loops, and lists</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html",
    "href": "new_pages/tables_descriptive.html",
    "title": "17  Descriptive tables",
    "section": "",
    "text": "17.1 Preparation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html#preparation",
    "href": "new_pages/tables_descriptive.html#preparation",
    "title": "17  Descriptive tables",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  skimr,        # get overview of data\n  tidyverse,    # data management + ggplot2 graphics \n  gtsummary,    # summary statistics and tests\n  rstatix,      # summary statistics and statistical tests\n  janitor,      # adding totals and percents to tables\n  scales,       # easily convert proportions to percents  \n  flextable     # converting tables to pretty images\n  )\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html#browse-data",
    "href": "new_pages/tables_descriptive.html#browse-data",
    "title": "17  Descriptive tables",
    "section": "17.2 Browse data",
    "text": "17.2 Browse data\n\nskimr package\nBy using the skimr package, you can get a detailed and aesthetically pleasing overview of each of the variables in your dataset. Read more about skimr at its github page.\nBelow, the function skim() is applied to the entire linelist data frame. An overview of the data frame and a summary of every column (by class) is produced.\n\n## get information about each variable in a dataset \nskim(linelist)\n\n\n\n\nData summary\n\n\nName\nlinelist\n\n\nNumber of rows\n5888\n\n\nNumber of columns\n30\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nDate\n4\n\n\nfactor\n2\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncase_id\n0\n1.00\n6\n6\n0\n5888\n0\n\n\noutcome\n1323\n0.78\n5\n7\n0\n2\n0\n\n\ngender\n278\n0.95\n1\n1\n0\n2\n0\n\n\nage_unit\n0\n1.00\n5\n6\n0\n2\n0\n\n\nhospital\n0\n1.00\n5\n36\n0\n6\n0\n\n\ninfector\n2088\n0.65\n6\n6\n0\n2697\n0\n\n\nsource\n2088\n0.65\n5\n7\n0\n2\n0\n\n\nfever\n249\n0.96\n2\n3\n0\n2\n0\n\n\nchills\n249\n0.96\n2\n3\n0\n2\n0\n\n\ncough\n249\n0.96\n2\n3\n0\n2\n0\n\n\naches\n249\n0.96\n2\n3\n0\n2\n0\n\n\nvomit\n249\n0.96\n2\n3\n0\n2\n0\n\n\ntime_admission\n765\n0.87\n5\n5\n0\n1072\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate_infection\n2087\n0.65\n2014-03-19\n2015-04-27\n2014-10-11\n359\n\n\ndate_onset\n256\n0.96\n2014-04-07\n2015-04-30\n2014-10-23\n367\n\n\ndate_hospitalisation\n0\n1.00\n2014-04-17\n2015-04-30\n2014-10-23\n363\n\n\ndate_outcome\n936\n0.84\n2014-04-19\n2015-06-04\n2014-11-01\n371\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nage_cat\n86\n0.99\nFALSE\n8\n0-4: 1095, 5-9: 1095, 20-: 1073, 10-: 941\n\n\nage_cat5\n86\n0.99\nFALSE\n17\n0-4: 1095, 5-9: 1095, 10-: 941, 15-: 743\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\ngeneration\n0\n1.00\n16.56\n5.79\n0.00\n13.00\n16.00\n20.00\n37.00\n\n\nage\n86\n0.99\n16.07\n12.62\n0.00\n6.00\n13.00\n23.00\n84.00\n\n\nage_years\n86\n0.99\n16.02\n12.64\n0.00\n6.00\n13.00\n23.00\n84.00\n\n\nlon\n0\n1.00\n-13.23\n0.02\n-13.27\n-13.25\n-13.23\n-13.22\n-13.21\n\n\nlat\n0\n1.00\n8.47\n0.01\n8.45\n8.46\n8.47\n8.48\n8.49\n\n\nwt_kg\n0\n1.00\n52.64\n18.58\n-11.00\n41.00\n54.00\n66.00\n111.00\n\n\nht_cm\n0\n1.00\n124.96\n49.52\n4.00\n91.00\n129.00\n159.00\n295.00\n\n\nct_blood\n0\n1.00\n21.21\n1.69\n16.00\n20.00\n22.00\n22.00\n26.00\n\n\ntemp\n149\n0.97\n38.56\n0.98\n35.20\n38.20\n38.80\n39.20\n40.80\n\n\nbmi\n0\n1.00\n46.89\n55.39\n-1200.00\n24.56\n32.12\n50.01\n1250.00\n\n\ndays_onset_hosp\n256\n0.96\n2.06\n2.26\n0.00\n1.00\n1.00\n3.00\n22.00\n\n\n\n\n\nYou can also use the summary() function, from base R, to get information about an entire dataset, but this output can be more difficult to read than using skimr. Therefore the output is not shown below, to conserve page space.\n\n## get information about each column in a dataset \nsummary(linelist)\n\n\n\nSummary statistics\nYou can use base R functions to return summary statistics on a numeric column. You can return most of the useful summary statistics for a numeric column using summary(), as below. Note that the data frame name must also be specified as shown below.\n\nsummary(linelist$age_years)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.02   23.00   84.00      86 \n\n\nYou can access and save one specific part of it with index brackets [ ]:\n\nsummary(linelist$age_years)[[2]]            # return only the 2nd element\n\n[1] 6\n\n# equivalent, alternative to above by element name\n# summary(linelist$age_years)[[\"1st Qu.\"]]  \n\nYou can return individual statistics with base R functions like max(), min(), median(), mean(), quantile(), sd(), and range(). See the R basics page for a complete list.\nCAUTION: If your data contain missing values, R wants you to know this and so will return NA unless you specify to the above mathematical functions that you want R to ignore missing values, via the argument na.rm = TRUE.\nYou can use the get_summary_stats() function from rstatix to return summary statistics in a data frame format. This can be helpful for performing subsequent operations or plotting on the numbers. See the Simple statistical tests page for more details on the rstatix package and its functions.\n\nlinelist %&gt;% \n  get_summary_stats(\n    age, wt_kg, ht_cm, ct_blood, temp,  # columns to calculate for\n    type = \"common\")                    # summary stats to return\n\n# A tibble: 5 × 10\n  variable     n   min   max median   iqr  mean     sd    se    ci\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age       5802   0    84     13      17  16.1 12.6   0.166 0.325\n2 wt_kg     5888 -11   111     54      25  52.6 18.6   0.242 0.475\n3 ht_cm     5888   4   295    129      68 125.  49.5   0.645 1.26 \n4 ct_blood  5888  16    26     22       2  21.2  1.69  0.022 0.043\n5 temp      5739  35.2  40.8   38.8     1  38.6  0.977 0.013 0.025",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html#tbl_janitor",
    "href": "new_pages/tables_descriptive.html#tbl_janitor",
    "title": "17  Descriptive tables",
    "section": "17.3 janitor package",
    "text": "17.3 janitor package\nThe janitor packages offers the tabyl() function to produce tabulations and cross-tabulations, which can be “adorned” or modified with helper functions to display percents, proportions, counts, etc.\nBelow, we pipe the linelist data frame to janitor functions and print the result. If desired, you can also save the resulting tables with the assignment operator &lt;-.\n\nSimple tabyl\nThe default use of tabyl() on a specific column produces the unique values, counts, and column-wise “percents” (actually proportions). The proportions may have many digits. You can adjust the number of decimals with adorn_rounding() as described below.\n\nlinelist %&gt;% tabyl(age_cat)\n\n age_cat    n     percent valid_percent\n     0-4 1095 0.185971467   0.188728025\n     5-9 1095 0.185971467   0.188728025\n   10-14  941 0.159816576   0.162185453\n   15-19  743 0.126188859   0.128059290\n   20-29 1073 0.182235054   0.184936229\n   30-49  754 0.128057065   0.129955188\n   50-69   95 0.016134511   0.016373664\n     70+    6 0.001019022   0.001034126\n    &lt;NA&gt;   86 0.014605978            NA\n\n\nAs you can see above, if there are missing values they display in a row labeled &lt;NA&gt;. You can suppress them with show_na = FALSE. If there are no missing values, this row will not appear. If there are missing values, all proportions are given as both raw (denominator inclusive of NA counts) and “valid” (denominator excludes NA counts).\nIf the column is class Factor and only certain levels are present in your data, all levels will still appear in the table. You can suppress this feature by specifying show_missing_levels = FALSE. Read more on the Factors page.\n\n\nCross-tabulation\nCross-tabulation counts are achieved by adding one or more additional columns within tabyl(). Note that now only counts are returned - proportions and percents can be added with additional steps shown below.\n\nlinelist %&gt;% tabyl(age_cat, gender)\n\n age_cat   f   m NA_\n     0-4 640 416  39\n     5-9 641 412  42\n   10-14 518 383  40\n   15-19 359 364  20\n   20-29 468 575  30\n   30-49 179 557  18\n   50-69   2  91   2\n     70+   0   5   1\n    &lt;NA&gt;   0   0  86\n\n\n\n\n“Adorning” the tabyl\nUse janitor’s “adorn” functions to add totals or convert to proportions, percents, or otherwise adjust the display. Often, you will pipe the tabyl through several of these functions.\n\n\n\n\n\n\n\nFunction\nOutcome\n\n\n\n\nadorn_totals()\nAdds totals (where = “row”, “col”, or “both”). Set name = for “Total”.\n\n\nadorn_percentages()\nConvert counts to proportions, with denominator = “row”, “col”, or “all”\n\n\nadorn_pct_formatting()\nConverts proportions to percents. Specify digits =. Remove the “%” symbol with affix_sign = FALSE.\n\n\nadorn_rounding()\nTo round proportions to digits = places. To round percents use adorn_pct_formatting() with digits =.\n\n\nadorn_ns()\nAdd counts to a table of proportions or percents. Indicate position = “rear” to show counts in parentheses, or “front” to put the percents in parentheses.\n\n\nadorn_title()\nAdd string via arguments row_name = and/or col_name =\n\n\n\nBe conscious of the order you apply the above functions. Below are some examples.\nA simple one-way table with percents instead of the default proportions.\n\nlinelist %&gt;%               # case linelist\n  tabyl(age_cat) %&gt;%       # tabulate counts and proportions by age category\n  adorn_pct_formatting()   # convert proportions to percents\n\n age_cat    n percent valid_percent\n     0-4 1095   18.6%         18.9%\n     5-9 1095   18.6%         18.9%\n   10-14  941   16.0%         16.2%\n   15-19  743   12.6%         12.8%\n   20-29 1073   18.2%         18.5%\n   30-49  754   12.8%         13.0%\n   50-69   95    1.6%          1.6%\n     70+    6    0.1%          0.1%\n    &lt;NA&gt;   86    1.5%             -\n\n\nA cross-tabulation with a total row and row percents.\n\nlinelist %&gt;%                                  \n  tabyl(age_cat, gender) %&gt;%                  # counts by age and gender\n  adorn_totals(where = \"row\") %&gt;%             # add total row\n  adorn_percentages(denominator = \"row\") %&gt;%  # convert counts to proportions\n  adorn_pct_formatting(digits = 1)            # convert proportions to percents\n\n age_cat     f     m    NA_\n     0-4 58.4% 38.0%   3.6%\n     5-9 58.5% 37.6%   3.8%\n   10-14 55.0% 40.7%   4.3%\n   15-19 48.3% 49.0%   2.7%\n   20-29 43.6% 53.6%   2.8%\n   30-49 23.7% 73.9%   2.4%\n   50-69  2.1% 95.8%   2.1%\n     70+  0.0% 83.3%  16.7%\n    &lt;NA&gt;  0.0%  0.0% 100.0%\n   Total 47.7% 47.6%   4.7%\n\n\nA cross-tabulation adjusted so that both counts and percents are displayed.\n\nlinelist %&gt;%                                  # case linelist\n  tabyl(age_cat, gender) %&gt;%                  # cross-tabulate counts\n  adorn_totals(where = \"row\") %&gt;%             # add a total row\n  adorn_percentages(denominator = \"col\") %&gt;%  # convert to proportions\n  adorn_pct_formatting() %&gt;%                  # convert to percents\n  adorn_ns(position = \"front\") %&gt;%            # display as: \"count (percent)\"\n  adorn_title(                                # adjust titles\n    row_name = \"Age Category\",\n    col_name = \"Gender\")\n\n                      Gender                            \n Age Category              f              m          NA_\n          0-4   640  (22.8%)   416  (14.8%)  39  (14.0%)\n          5-9   641  (22.8%)   412  (14.7%)  42  (15.1%)\n        10-14   518  (18.5%)   383  (13.7%)  40  (14.4%)\n        15-19   359  (12.8%)   364  (13.0%)  20   (7.2%)\n        20-29   468  (16.7%)   575  (20.5%)  30  (10.8%)\n        30-49   179   (6.4%)   557  (19.9%)  18   (6.5%)\n        50-69     2   (0.1%)    91   (3.2%)   2   (0.7%)\n          70+     0   (0.0%)     5   (0.2%)   1   (0.4%)\n         &lt;NA&gt;     0   (0.0%)     0   (0.0%)  86  (30.9%)\n        Total 2,807 (100.0%) 2,803 (100.0%) 278 (100.0%)\n\n\n\n\nPrinting the tabyl\nBy default, the tabyl will print raw to your R console.\nAlternatively, you can pass the tabyl to flextable or similar package to print as a “pretty” image in the RStudio Viewer, which could be exported as .png, .jpeg, .html, etc. This is discussed in the page Tables for presentation. Note that if printing in this manner and using adorn_titles(), you must specify placement = \"combined\".\n\nlinelist %&gt;%\n  tabyl(age_cat, gender) %&gt;% \n  adorn_totals(where = \"col\") %&gt;% \n  adorn_percentages(denominator = \"col\") %&gt;% \n  adorn_pct_formatting() %&gt;% \n  adorn_ns(position = \"front\") %&gt;% \n  adorn_title(\n    row_name = \"Age Category\",\n    col_name = \"Gender\",\n    placement = \"combined\") %&gt;% # this is necessary to print as image\n  flextable::flextable() %&gt;%    # convert to pretty image\n  flextable::autofit()          # format to one line per row \n\nAge Category/GenderfmNA_Total0-4640 (22.8%)416 (14.8%)39 (14.0%)1,095 (18.6%)5-9641 (22.8%)412 (14.7%)42 (15.1%)1,095 (18.6%)10-14518 (18.5%)383 (13.7%)40 (14.4%)941 (16.0%)15-19359 (12.8%)364 (13.0%)20  (7.2%)743 (12.6%)20-29468 (16.7%)575 (20.5%)30 (10.8%)1,073 (18.2%)30-49179  (6.4%)557 (19.9%)18  (6.5%)754 (12.8%)50-692  (0.1%)91  (3.2%)2  (0.7%)95  (1.6%)70+0  (0.0%)5  (0.2%)1  (0.4%)6  (0.1%)0  (0.0%)0  (0.0%)86 (30.9%)86  (1.5%)\n\n\n\n\nUse on other tables\nYou can use janitor’s adorn_*() functions on other tables, such as those created by summarise() and count() from dplyr, or table() from base R. Simply pipe the table to the desired janitor function. For example:\n\nlinelist %&gt;% \n  count(hospital) %&gt;%   # dplyr function\n  adorn_totals()        # janitor function\n\n                             hospital    n\n                     Central Hospital  454\n                    Military Hospital  896\n                              Missing 1469\n                                Other  885\n                        Port Hospital 1762\n St. Mark's Maternity Hospital (SMMH)  422\n                                Total 5888\n\n\n\n\nSaving the tabyl\nIf you convert the table to a “pretty” image with a package like flextable, you can save it with functions from that package - like save_as_html(), save_as_word(), save_as_ppt(), and save_as_image() from flextable (as discussed more extensively in the Tables for presentation page). Below, the table is saved as a Word document, in which it can be further hand-edited.\n\nlinelist %&gt;%\n  tabyl(age_cat, gender) %&gt;% \n  adorn_totals(where = \"col\") %&gt;% \n  adorn_percentages(denominator = \"col\") %&gt;% \n  adorn_pct_formatting() %&gt;% \n  adorn_ns(position = \"front\") %&gt;% \n  adorn_title(\n    row_name = \"Age Category\",\n    col_name = \"Gender\",\n    placement = \"combined\") %&gt;% \n  flextable::flextable() %&gt;%                     # convert to image\n  flextable::autofit() %&gt;%                       # ensure only one line per row\n  flextable::save_as_docx(path = \"tabyl.docx\")   # save as Word document to filepath\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\nYou can apply statistical tests on tabyls, like chisq.test() or fisher.test() from the stats package, as shown below. Note missing values are not allowed so they are excluded from the tabyl with show_na = FALSE.\n\nage_by_outcome &lt;- linelist %&gt;% \n  tabyl(age_cat, outcome, show_na = FALSE) \n\nchisq.test(age_by_outcome)\n\n\n    Pearson's Chi-squared test\n\ndata:  age_by_outcome\nX-squared = 6.4931, df = 7, p-value = 0.4835\n\n\nSee the page on Simple statistical tests for more code and tips about statistics.\n\n\nOther tips\n\nInclude the argument na.rm = TRUE to exclude missing values from any of the above calculations.\n\nIf applying any adorn_*() helper functions to tables not created by tabyl(), you can specify particular column(s) to apply them to like adorn_percentage(,,,c(cases,deaths)) (specify them to the 4th unnamed argument). The syntax is not simple. Consider using summarise() instead.\n\nYou can read more detail in the janitor page and this tabyl vignette.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html#dplyr-package",
    "href": "new_pages/tables_descriptive.html#dplyr-package",
    "title": "17  Descriptive tables",
    "section": "17.4 dplyr package",
    "text": "17.4 dplyr package\ndplyr is part of the tidyverse packages and is an very common data management tool. Creating tables with dplyr functions summarise() and count() is a useful approach to calculating summary statistics, summarize by group, or pass tables to ggplot().\nsummarise() creates a new, summary data frame. If the data are ungrouped, it will return a one-row dataframe with the specified summary statistics of the entire data frame. If the data are grouped, the new data frame will have one row per group (see Grouping data page).\nWithin the summarise() parentheses, you provide the names of each new summary column followed by an equals sign and a statistical function to apply.\nTIP: The summarise function works with both UK and US spelling (summarise() and summarize()).\n\nGet counts\nThe most simple function to apply within summarise() is n(). Leave the parentheses empty to count the number of rows.\n\nlinelist %&gt;%                 # begin with linelist\n  summarise(n_rows = n())    # return new summary dataframe with column n_rows\n\n  n_rows\n1   5888\n\n\nThis gets more interesting if we have grouped the data beforehand.\n\nlinelist %&gt;% \n  group_by(age_cat) %&gt;%     # group data by unique values in column age_cat\n  summarise(n_rows = n())   # return number of rows *per group*\n\n# A tibble: 9 × 2\n  age_cat n_rows\n  &lt;fct&gt;    &lt;int&gt;\n1 0-4       1095\n2 5-9       1095\n3 10-14      941\n4 15-19      743\n5 20-29     1073\n6 30-49      754\n7 50-69       95\n8 70+          6\n9 &lt;NA&gt;        86\n\n\nThe above command can be shortened by using the count() function instead. count() does the following:\n\nGroups the data by the columns provided to it\n\nSummarises them with n() (creating column n)\n\nUn-groups the data\n\n\nlinelist %&gt;% \n  count(age_cat)\n\n  age_cat    n\n1     0-4 1095\n2     5-9 1095\n3   10-14  941\n4   15-19  743\n5   20-29 1073\n6   30-49  754\n7   50-69   95\n8     70+    6\n9    &lt;NA&gt;   86\n\n\nYou can change the name of the counts column from the default n to something else by specifying it to name =.\nTabulating counts of two or more grouping columns are still returned in “long” format, with the counts in the n column. See the page on Pivoting data to learn about “long” and “wide” data formats.\n\nlinelist %&gt;% \n  count(age_cat, outcome)\n\n   age_cat outcome   n\n1      0-4   Death 471\n2      0-4 Recover 364\n3      0-4    &lt;NA&gt; 260\n4      5-9   Death 476\n5      5-9 Recover 391\n6      5-9    &lt;NA&gt; 228\n7    10-14   Death 438\n8    10-14 Recover 303\n9    10-14    &lt;NA&gt; 200\n10   15-19   Death 323\n11   15-19 Recover 251\n12   15-19    &lt;NA&gt; 169\n13   20-29   Death 477\n14   20-29 Recover 367\n15   20-29    &lt;NA&gt; 229\n16   30-49   Death 329\n17   30-49 Recover 238\n18   30-49    &lt;NA&gt; 187\n19   50-69   Death  33\n20   50-69 Recover  38\n21   50-69    &lt;NA&gt;  24\n22     70+   Death   3\n23     70+ Recover   3\n24    &lt;NA&gt;   Death  32\n25    &lt;NA&gt; Recover  28\n26    &lt;NA&gt;    &lt;NA&gt;  26\n\n\n\n\nShow all levels\nIf you are tabling a column of class factor you can ensure that all levels are shown (not just the levels with values in the data) by adding .drop = FALSE into the summarise() or count() command.\nThis technique is useful to standardise your tables/plots. For example if you are creating figures for multiple sub-groups, or repeatedly creating the figure for routine reports. In each of these circumstances, the presence of values in the data may fluctuate, but you can define levels that remain constant.\nSee the page on Factors for more information.\n\n\nProportions\nProportions can be added by piping the table to mutate() to create a new column. Define the new column as the counts column (n by default) divided by the sum() of the counts column (this will return a proportion).\nNote that in this case, sum() in the mutate() command will return the sum of the whole column n for use as the proportion denominator. As explained in the Grouping data page, if sum() is used in grouped data (e.g. if the mutate() immediately followed a group_by() command), it will return sums by group. As stated just above, count() finishes its actions by ungrouping. Thus, in this scenario we get full column proportions.\nTo easily display percents, you can wrap the proportion in the function percent() from the package scales (note this convert to class character).\n\nage_summary &lt;- linelist %&gt;% \n  count(age_cat) %&gt;%                     # group and count by gender (produces \"n\" column)\n  mutate(                                # create percent of column - note the denominator\n    percent = scales::percent(n / sum(n))) \n\n# print\nage_summary\n\n  age_cat    n percent\n1     0-4 1095  18.60%\n2     5-9 1095  18.60%\n3   10-14  941  15.98%\n4   15-19  743  12.62%\n5   20-29 1073  18.22%\n6   30-49  754  12.81%\n7   50-69   95   1.61%\n8     70+    6   0.10%\n9    &lt;NA&gt;   86   1.46%\n\n\nBelow is a method to calculate proportions within groups. It relies on different levels of data grouping being selectively applied and removed. First, the data are grouped on outcome via group_by(). Then, count() is applied. This function further groups the data by age_cat and returns counts for each outcome-age-cat combination. Importantly - as it finishes its process, count() also ungroups the age_cat grouping, so the only remaining data grouping is the original grouping by outcome. Thus, the final step of calculating proportions (denominator sum(n)) is still grouped by outcome.\n\nage_by_outcome &lt;- linelist %&gt;%                  # begin with linelist\n  group_by(outcome) %&gt;%                         # group by outcome \n  count(age_cat) %&gt;%                            # group and count by age_cat, and then remove age_cat grouping\n  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group\n\n\n\n\n\n\n\n\n\nPlotting\nTo display a “long” table output like the above with ggplot() is relatively straight-forward. The data are naturally in “long” format, which is naturally accepted by ggplot(). See further examples in the pages ggplot basics and ggplot tips.\n\nlinelist %&gt;%                      # begin with linelist\n  count(age_cat, outcome) %&gt;%     # group and tabulate counts by two columns\n  ggplot()+                       # pass new data frame to ggplot\n    geom_col(                     # create bar plot\n      mapping = aes(   \n        x = outcome,              # map outcome to x-axis\n        fill = age_cat,           # map age_cat to the fill\n        y = n))                   # map the counts column `n` to the height\n\n\n\n\n\n\n\n\n\n\nSummary statistics\nOne major advantage of dplyr and summarise() is the ability to return more advanced statistical summaries like median(), mean(), max(), min(), sd() (standard deviation), and percentiles. You can also use sum() to return the number of rows that meet certain logical criteria. As above, these outputs can be produced for the whole data frame set, or by group.\nThe syntax is the same - within the summarise() parentheses you provide the names of each new summary column followed by an equals sign and a statistical function to apply. Within the statistical function, give the column(s) to be operated on and any relevant arguments (e.g. na.rm = TRUE for most mathematical functions).\nYou can also use sum() to return the number of rows that meet a logical criteria. The expression within is counted if it evaluates to TRUE. For example:\n\nsum(age_years &lt; 18, na.rm=T)\n\nsum(gender == \"male\", na.rm=T)\n\nsum(response %in% c(\"Likely\", \"Very Likely\"))\n\nBelow, linelist data are summarised to describe the days delay from symptom onset to hospital admission (column days_onset_hosp), by hospital.\n\nsummary_table &lt;- linelist %&gt;%                                        # begin with linelist, save out as new object\n  group_by(hospital) %&gt;%                                             # group all calculations by hospital\n  summarise(                                                         # only the below summary columns will be returned\n    cases       = n(),                                                # number of rows per group\n    delay_max   = max(days_onset_hosp, na.rm = T),                    # max delay\n    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # mean delay, rounded\n    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # standard deviation of delays, rounded\n    delay_3     = sum(days_onset_hosp &gt;= 3, na.rm = T),               # number of rows with delay of 3 or more days\n    pct_delay_3 = scales::percent(delay_3 / cases)                    # convert previously-defined delay column to percent \n  )\n\nsummary_table  # print\n\n# A tibble: 6 × 7\n  hospital               cases delay_max delay_mean delay_sd delay_3 pct_delay_3\n  &lt;chr&gt;                  &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;      \n1 Central Hospital         454        12        1.9      1.9     108 24%        \n2 Military Hospital        896        15        2.1      2.4     253 28%        \n3 Missing                 1469        22        2.1      2.3     399 27%        \n4 Other                    885        18        2        2.2     234 26%        \n5 Port Hospital           1762        16        2.1      2.2     470 27%        \n6 St. Mark's Maternity …   422        18        2.1      2.3     116 27%        \n\n\nSome tips:\n\nUse sum() with a logic statement to “count” rows that meet certain criteria (==)\n\nNote the use of na.rm = TRUE within mathematical functions like sum(), otherwise NA will be returned if there are any missing values\n\nUse the function percent() from the scales package to easily convert to percents\n\nSet accuracy = to 0.1 or 0.01 to ensure 1 or 2 decimal places respectively\n\n\nUse round() from base R to specify decimals\n\nTo calculate these statistics on the entire dataset, use summarise() without group_by()\n\nYou may create columns for the purposes of later calculations (e.g. denominators) that you eventually drop from your data frame with select().\n\n\n\nConditional statistics\nYou may want to return conditional statistics - e.g. the maximum of rows that meet certain criteria. This can be done by subsetting the column with brackets [ ]. The example below returns the maximum temperature for patients classified having or not having fever. Be aware however - it may be more appropriate to add another column to the group_by() command and pivot_wider() (as demonstrated below).\n\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    max_temp_fvr = max(temp[fever == \"yes\"], na.rm = T),\n    max_temp_no = max(temp[fever == \"no\"], na.rm = T)\n  )\n\n# A tibble: 6 × 3\n  hospital                             max_temp_fvr max_temp_no\n  &lt;chr&gt;                                       &lt;dbl&gt;       &lt;dbl&gt;\n1 Central Hospital                             40.4        38  \n2 Military Hospital                            40.5        38  \n3 Missing                                      40.6        38  \n4 Other                                        40.8        37.9\n5 Port Hospital                                40.6        38  \n6 St. Mark's Maternity Hospital (SMMH)         40.6        37.9\n\n\n\n\nGlueing together\nThe function str_glue() from stringr is useful to combine values from several columns into one new column. In this context this is typically used after the summarise() command.\nIn the Characters and strings page, various options for combining columns are discussed, including unite(), and paste0(). In this use case, we advocate for str_glue() because it is more flexible than unite() and has more simple syntax than paste0().\nBelow, the summary_table data frame (created above) is mutated such that columns delay_mean and delay_sd are combined, parentheses formating is added to the new column, and their respective old columns are removed.\nThen, to make the table more presentable, a total row is added with adorn_totals() from janitor (which ignores non-numeric columns). Lastly, we use select() from dplyr to both re-order and rename to nicer column names.\nNow you could pass to flextable and print the table to Word, .png, .jpeg, .html, Powerpoint, RMarkdown, etc.! (see the Tables for presentation page).\n\nsummary_table %&gt;% \n  mutate(delay = str_glue(\"{delay_mean} ({delay_sd})\")) %&gt;%  # combine and format other values\n  select(-c(delay_mean, delay_sd)) %&gt;%                       # remove two old columns   \n  adorn_totals(where = \"row\") %&gt;%                            # add total row\n  select(                                                    # order and rename cols\n    \"Hospital Name\"   = hospital,\n    \"Cases\"           = cases,\n    \"Max delay\"       = delay_max,\n    \"Mean (sd)\"       = delay,\n    \"Delay 3+ days\"   = delay_3,\n    \"% delay 3+ days\" = pct_delay_3\n    )\n\n                        Hospital Name Cases Max delay Mean (sd) Delay 3+ days\n                     Central Hospital   454        12 1.9 (1.9)           108\n                    Military Hospital   896        15 2.1 (2.4)           253\n                              Missing  1469        22 2.1 (2.3)           399\n                                Other   885        18   2 (2.2)           234\n                        Port Hospital  1762        16 2.1 (2.2)           470\n St. Mark's Maternity Hospital (SMMH)   422        18 2.1 (2.3)           116\n                                Total  5888       101         -          1580\n % delay 3+ days\n             24%\n             28%\n             27%\n             26%\n             27%\n             27%\n               -\n\n\n\nPercentiles\nPercentiles and quantiles in dplyr deserve a special mention. To return quantiles, use quantile() with the defaults or specify the value(s) you would like with probs =.\n\n# get default percentile values of age (0%, 25%, 50%, 75%, 100%)\nlinelist %&gt;% \n  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  age_percentiles\n1               0\n2               6\n3              13\n4              23\n5              84\n\n# get manually-specified percentile values of age (5%, 50%, 75%, 98%)\nlinelist %&gt;% \n  summarise(\n    age_percentiles = quantile(\n      age_years,\n      probs = c(.05, 0.5, 0.75, 0.98), \n      na.rm=TRUE)\n    )\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  age_percentiles\n1               1\n2              13\n3              23\n4              48\n\n\nIf you want to return quantiles by group, you may encounter long and less useful outputs if you simply add another column to group_by(). So, try this approach instead - create a column for each quantile level desired.\n\n# get manually-specified percentile values of age (5%, 50%, 75%, 98%)\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    p05 = quantile(age_years, probs = 0.05, na.rm=T),\n    p50 = quantile(age_years, probs = 0.5, na.rm=T),\n    p75 = quantile(age_years, probs = 0.75, na.rm=T),\n    p98 = quantile(age_years, probs = 0.98, na.rm=T)\n    )\n\n# A tibble: 6 × 5\n  hospital                               p05   p50   p75   p98\n  &lt;chr&gt;                                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Central Hospital                         1    12    21  48  \n2 Military Hospital                        1    13    24  45  \n3 Missing                                  1    13    23  48.2\n4 Other                                    1    13    23  50  \n5 Port Hospital                            1    14    24  49  \n6 St. Mark's Maternity Hospital (SMMH)     2    12    22  50.2\n\n\nWhile dplyr summarise() certainly offers more fine control, you may find that all the summary statistics you need can be produced with get_summary_stat() from the rstatix package. If operating on grouped data, if will return 0%, 25%, 50%, 75%, and 100%. If applied to ungrouped data, you can specify the percentiles with probs = c(.05, .5, .75, .98).\n\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  rstatix::get_summary_stats(age, type = \"quantile\")\n\n# A tibble: 6 × 8\n  hospital                         variable     n  `0%` `25%` `50%` `75%` `100%`\n  &lt;chr&gt;                            &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Central Hospital                 age        445     0     6    12    21     58\n2 Military Hospital                age        884     0     6    14    24     72\n3 Missing                          age       1441     0     6    13    23     76\n4 Other                            age        873     0     6    13    23     69\n5 Port Hospital                    age       1739     0     6    14    24     68\n6 St. Mark's Maternity Hospital (… age        420     0     7    12    22     84\n\n\n\nlinelist %&gt;% \n  rstatix::get_summary_stats(age, type = \"quantile\")\n\n# A tibble: 1 × 7\n  variable     n  `0%` `25%` `50%` `75%` `100%`\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 age       5802     0     6    13    23     84\n\n\n\n\n\nSummarise aggregated data\nIf you begin with aggregated data, using n() return the number of rows, not the sum of the aggregated counts. To get sums, use sum() on the data’s counts column.\nFor example, let’s say you are beginning with the data frame of counts below, called linelist_agg - it shows in “long” format the case counts by outcome and gender.\nBelow we create this example data frame of linelist case counts by outcome and gender (missing values removed for clarity).\n\nlinelist_agg &lt;- linelist %&gt;% \n  drop_na(gender, outcome) %&gt;% \n  count(outcome, gender)\n\nlinelist_agg\n\n  outcome gender    n\n1   Death      f 1227\n2   Death      m 1228\n3 Recover      f  953\n4 Recover      m  950\n\n\nTo sum the counts (in column n) by group you can use summarise() but set the new column equal to sum(n, na.rm=T). To add a conditional element to the sum operation, you can use the subset bracket [ ] syntax on the counts column.\n\nlinelist_agg %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(\n    total_cases  = sum(n, na.rm=T),\n    male_cases   = sum(n[gender == \"m\"], na.rm=T),\n    female_cases = sum(n[gender == \"f\"], na.rm=T))\n\n# A tibble: 2 × 4\n  outcome total_cases male_cases female_cases\n  &lt;chr&gt;         &lt;int&gt;      &lt;int&gt;        &lt;int&gt;\n1 Death          2455       1228         1227\n2 Recover        1903        950          953\n\n\n\n\nacross() multiple columns\nYou can use summarise() across multiple columns using across(). This makes life easier when you want to calculate the same statistics for many columns. Place across() within summarise() and specify the following:\n\n.cols = as either a vector of column names c() or “tidyselect” helper functions (explained below)\n\n.fns = the function to perform (no parentheses) - you can provide multiple within a list()\n\nBelow, mean() is applied to several numeric columns. A vector of columns are named explicitly to .cols = and a single function mean is specified (no parentheses) to .fns =. Any additional arguments for the function (e.g. na.rm=TRUE) are provided after .fns =, separated by a comma.\nIt can be difficult to get the order of parentheses and commas correct when using across(). Remember that within across() you must include the columns, the functions, and any extra arguments needed for the functions.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columns\n                   .fns = mean,                               # function\n                   na.rm=T))                                  # extra arguments\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(...)`.\nℹ In group 1: `outcome = \"Death\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 3 × 5\n  outcome age_years  temp wt_kg ht_cm\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Death        15.9  38.6  52.6  125.\n2 Recover      16.1  38.6  52.5  125.\n3 &lt;NA&gt;         16.2  38.6  53.0  125.\n\n\nMultiple functions can be run at once. Below the functions mean and sd are provided to .fns = within a list(). You have the opportunity to provide character names (e.g. “mean” and “sd”) which are appended in the new column names.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columns\n                   .fns = list(\"mean\" = mean, \"sd\" = sd),    # multiple functions \n                   na.rm=T))                                 # extra arguments\n\n# A tibble: 3 × 9\n  outcome age_years_mean age_years_sd temp_mean temp_sd wt_kg_mean wt_kg_sd\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 Death             15.9         12.3      38.6   0.962       52.6     18.4\n2 Recover           16.1         13.0      38.6   0.997       52.5     18.6\n3 &lt;NA&gt;              16.2         12.8      38.6   0.976       53.0     18.9\n# ℹ 2 more variables: ht_cm_mean &lt;dbl&gt;, ht_cm_sd &lt;dbl&gt;\n\n\nHere are those “tidyselect” helper functions you can provide to .cols = to select columns:\n\neverything() - all other columns not mentioned\n\nlast_col() - the last column\n\nwhere() - applies a function to all columns and selects those which are TRUE\n\nstarts_with() - matches to a specified prefix. Example: starts_with(\"date\")\nends_with() - matches to a specified suffix. Example: ends_with(\"_end\")\n\ncontains() - columns containing a character string. Example: contains(\"time\")\nmatches() - to apply a regular expression (regex). Example: contains(\"[pt]al\")\n\nnum_range() -\nany_of() - matches if column is named. Useful if the name might not exist. Example: any_of(date_onset, date_death, cardiac_arrest)\n\nFor example, to return the mean of every numeric column use where() and provide the function as.numeric() (without parentheses). All this remains within the across() command.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(\n    .cols = where(is.numeric),  # all numeric columns in the data frame\n    .fns = mean,\n    na.rm=T))\n\n# A tibble: 3 × 12\n  outcome generation   age age_years   lon   lat wt_kg ht_cm ct_blood  temp\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Death         16.7  15.9      15.9 -13.2  8.47  52.6  125.     21.3  38.6\n2 Recover       16.4  16.2      16.1 -13.2  8.47  52.5  125.     21.1  38.6\n3 &lt;NA&gt;          16.5  16.3      16.2 -13.2  8.47  53.0  125.     21.2  38.6\n# ℹ 2 more variables: bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\n\n\nPivot wider\nIf you prefer your table in “wide” format you can transform it using the tidyr pivot_wider() function. You will likely need to re-name the columns with rename(). For more information see the page on Pivoting data.\nThe example below begins with the “long” table age_by_outcome from the proportions section. We create it again and print, for clarity:\n\nage_by_outcome &lt;- linelist %&gt;%                  # begin with linelist\n  group_by(outcome) %&gt;%                         # group by outcome \n  count(age_cat) %&gt;%                            # group and count by age_cat, and then remove age_cat grouping\n  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group\n\n\n\n\n\n\n\nTo pivot wider, we create the new columns from the values in the existing column age_cat (by setting names_from = age_cat). We also specify that the new table values will come from the existing column n, with values_from = n. The columns not mentioned in our pivoting command (outcome) will remain unchanged on the far left side.\n\nage_by_outcome %&gt;% \n  select(-percent) %&gt;%   # keep only counts for simplicity\n  pivot_wider(names_from = age_cat, values_from = n)  \n\n# A tibble: 3 × 10\n# Groups:   outcome [3]\n  outcome `0-4` `5-9` `10-14` `15-19` `20-29` `30-49` `50-69` `70+`  `NA`\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 Death     471   476     438     323     477     329      33     3    32\n2 Recover   364   391     303     251     367     238      38     3    28\n3 &lt;NA&gt;      260   228     200     169     229     187      24    NA    26\n\n\n\n\nTotal rows\nWhen summarise() operates on grouped data it does not automatically produce “total” statistics. Below, two approaches to adding a total row are presented:\n\njanitor’s adorn_totals()\nIf your table consists only of counts or proportions/percents that can be summed into a total, then you can add sum totals using janitor’s adorn_totals() as described in the section above. Note that this function can only sum the numeric columns - if you want to calculate other total summary statistics see the next approach with dplyr.\nBelow, linelist is grouped by gender and summarised into a table that described the number of cases with known outcome, deaths, and recovered. Piping the table to adorn_totals() adds a total row at the bottom reflecting the sum of each column. The further adorn_*() functions adjust the display as noted in the code.\n\nlinelist %&gt;% \n  group_by(gender) %&gt;%\n  summarise(\n    known_outcome = sum(!is.na(outcome)),           # Number of rows in group where outcome is not missing\n    n_death  = sum(outcome == \"Death\", na.rm=T),    # Number of rows in group where outcome is Death\n    n_recover = sum(outcome == \"Recover\", na.rm=T), # Number of rows in group where outcome is Recovered\n  ) %&gt;% \n  adorn_totals() %&gt;%                                # Adorn total row (sums of each numeric column)\n  adorn_percentages(\"col\") %&gt;%                      # Get column proportions\n  adorn_pct_formatting() %&gt;%                        # Convert proportions to percents\n  adorn_ns(position = \"front\")                      # display % and counts (with counts in front)\n\n gender  known_outcome        n_death      n_recover\n      f 2,180  (47.8%) 1,227  (47.5%)   953  (48.1%)\n      m 2,178  (47.7%) 1,228  (47.6%)   950  (47.9%)\n   &lt;NA&gt;   207   (4.5%)   127   (4.9%)    80   (4.0%)\n  Total 4,565 (100.0%) 2,582 (100.0%) 1,983 (100.0%)\n\n\n\n\nsummarise() on “total” data and then bind_rows()\nIf your table consists of summary statistics such as median(), mean(), etc, the adorn_totals() approach shown above will not be sufficient. Instead, to get summary statistics for the entire dataset you must calculate them with a separate summarise() command and then bind the results to the original grouped summary table. To do the binding you can use bind_rows() from dplyr s described in the Joining data page. Below is an example:\nYou can make a summary table of outcome by hospital with group_by() and summarise() like this:\n\nby_hospital &lt;- linelist %&gt;% \n  filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%  # Remove cases with missing outcome or hospital\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T))               # median CT value per group\n  \nby_hospital # print table\n\n# A tibble: 10 × 4\n# Groups:   hospital [5]\n   hospital                             outcome     N ct_value\n   &lt;chr&gt;                                &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n 1 Central Hospital                     Death     193       22\n 2 Central Hospital                     Recover   165       22\n 3 Military Hospital                    Death     399       21\n 4 Military Hospital                    Recover   309       22\n 5 Other                                Death     395       22\n 6 Other                                Recover   290       21\n 7 Port Hospital                        Death     785       22\n 8 Port Hospital                        Recover   579       21\n 9 St. Mark's Maternity Hospital (SMMH) Death     199       22\n10 St. Mark's Maternity Hospital (SMMH) Recover   126       22\n\n\nTo get the totals, run the same summarise() command but only group the data by outcome (not by hospital), like this:\n\ntotals &lt;- linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # These statistics are now by outcome only     \n        ct_value = median(ct_blood, na.rm=T))\n\ntotals # print table\n\n# A tibble: 2 × 3\n  outcome     N ct_value\n  &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 Death    1971       22\n2 Recover  1469       22\n\n\nWe can bind these two data frames together. Note that by_hospital has 4 columns whereas totals has 3 columns. By using bind_rows(), the columns are combined by name, and any extra space is filled in with NA (e.g the column hospital values for the two new totals rows). After binding the rows, we convert these empty spaces to “Total” using replace_na() (see Cleaning data and core functions page).\n\ntable_long &lt;- bind_rows(by_hospital, totals) %&gt;% \n  mutate(hospital = replace_na(hospital, \"Total\"))\n\nHere is the new table with “Total” rows at the bottom.\n\n\n\n\n\n\nThis table is in a “long” format, which may be what you want. Optionally, you can pivot this table wider to make it more readable. See the section on pivoting wider above, and the Pivoting data page. You can also add more columns, and arrange it nicely. This code is below.\n\ntable_long %&gt;% \n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known)                                  # Arrange rows from lowest to highest (Total row at bottom)\n\n# A tibble: 6 × 8\n# Groups:   hospital [6]\n  hospital      N_Known N_Recover Pct_Recover ct_value_Recover N_Death Pct_Death\n  &lt;chr&gt;           &lt;int&gt;     &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;    \n1 St. Mark's M…     325       126 38.8%                     22     199 61.2%    \n2 Central Hosp…     358       165 46.1%                     22     193 53.9%    \n3 Other             685       290 42.3%                     21     395 57.7%    \n4 Military Hos…     708       309 43.6%                     22     399 56.4%    \n5 Port Hospital    1364       579 42.4%                     21     785 57.6%    \n6 Total            3440      1469 42.7%                     22    1971 57.3%    \n# ℹ 1 more variable: ct_value_Death &lt;dbl&gt;\n\n\nAnd then you can print this nicely as an image - below is the output printed with flextable. You can read more in depth about this example and how to achieve this “pretty” table in the Tables for presentation page.\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html#tbl_gt",
    "href": "new_pages/tables_descriptive.html#tbl_gt",
    "title": "17  Descriptive tables",
    "section": "17.5 gtsummary package",
    "text": "17.5 gtsummary package\nIf you want to print your summary statistics in a pretty, publication-ready graphic, you can use the gtsummary package and its function tbl_summary(). The code can seem complex at first, but the outputs look very nice and print to your RStudio Viewer panel as an HTML image. Read a vignette here.\nYou can also add the results of statistical tests to gtsummary tables. This process is described in the gtsummary section of the Simple statistical tests page.\nTo introduce tbl_summary() we will show the most basic behavior first, which actually produces a large and beautiful table. Then, we will examine in detail how to make adjustments and more tailored tables.\n\nSummary table\nThe default behavior of tbl_summary() is quite incredible - it takes the columns you provide and creates a summary table in one command. The function prints statistics appropriate to the column class: median and inter-quartile range (IQR) for numeric columns, and counts (%) for categorical columns. Missing values are converted to “Unknown”. Footnotes are added to the bottom to explain the statistics, while the total N is shown at the top.\n\nlinelist %&gt;% \n  select(age_years, gender, outcome, fever, temp, hospital) %&gt;%  # keep only the columns of interest\n  tbl_summary()                                                  # default\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 5,8881\n\n\n\n\nage_years\n13 (6, 23)\n\n\n    Unknown\n86\n\n\ngender\n\n\n\n\n    f\n2,807 (50%)\n\n\n    m\n2,803 (50%)\n\n\n    Unknown\n278\n\n\noutcome\n\n\n\n\n    Death\n2,582 (57%)\n\n\n    Recover\n1,983 (43%)\n\n\n    Unknown\n1,323\n\n\nfever\n4,549 (81%)\n\n\n    Unknown\n249\n\n\ntemp\n38.80 (38.20, 39.20)\n\n\n    Unknown\n149\n\n\nhospital\n\n\n\n\n    Central Hospital\n454 (7.7%)\n\n\n    Military Hospital\n896 (15%)\n\n\n    Missing\n1,469 (25%)\n\n\n    Other\n885 (15%)\n\n\n    Port Hospital\n1,762 (30%)\n\n\n    St. Mark's Maternity Hospital (SMMH)\n422 (7.2%)\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n\n\n\n\nAdjustments\nNow we will explain how the function works and how to make adjustments. The key arguments are detailed below:\nby =\nYou can stratify your table by a column (e.g. by outcome), creating a 2-way table.\nstatistic =\nUse an equations to specify which statistics to show and how to display them. There are two sides to the equation, separated by a tilde ~. On the right side, in quotes, is the statistical display desired, and on the left are the columns to which that display will apply.\n\nThe right side of the equation uses the syntax of str_glue() from stringr (see [Characters and Strings])(characters_strings.qmd), with the desired display string in quotes and the statistics themselves within curly brackets. You can include statistics like “n” (for counts), “N” (for denominator), “mean”, “median”, “sd”, “max”, “min”, percentiles as “p##” like “p25”, or percent of total as “p”. See ?tbl_summary for details.\n\nFor the left side of the equation, you can specify columns by name (e.g. age or c(age, gender)) or using helpers such as all_continuous(), all_categorical(), contains(), starts_with(), etc.\n\nA simple example of a statistic = equation might look like below, to only print the mean of column age_years:\n\nlinelist %&gt;% \n  select(age_years) %&gt;%         # keep only columns of interest \n  tbl_summary(                  # create summary table\n    statistic = age_years ~ \"{mean}\") # print mean of age\n\n\n\n\n\n\n\n\nCharacteristic\nN = 5,8881\n\n\n\n\nage_years\n16\n\n\n    Unknown\n86\n\n\n\n1 Mean\n\n\n\n\n\n\n\n\n\nA slightly more complex equation might look like \"({min}, {max})\", incorporating the max and min values within parentheses and separated by a comma:\n\nlinelist %&gt;% \n  select(age_years) %&gt;%                       # keep only columns of interest \n  tbl_summary(                                # create summary table\n    statistic = age_years ~ \"({min}, {max})\") # print min and max of age\n\n\n\n\n\n\n\n\nCharacteristic\nN = 5,8881\n\n\n\n\nage_years\n(0, 84)\n\n\n    Unknown\n86\n\n\n\n1 (Range)\n\n\n\n\n\n\n\n\n\nYou can also differentiate syntax for separate columns or types of columns. In the more complex example below, the value provided to statistc = is a list indicating that for all continuous columns the table should print mean with standard deviation in parentheses, while for all categorical columns it should print the n, denominator, and percent.\ndigits =\nAdjust the digits and rounding. Optionally, this can be specified to be for continuous columns only (as below).\nlabel =\nAdjust how the column name should be displayed. Provide the column name and its desired label separated by a tilde. The default is the column name.\nmissing_text =\nAdjust how missing values are displayed. The default is “Unknown”.\ntype =\nThis is used to adjust how many levels of the statistics are shown. The syntax is similar to statistic = in that you provide an equation with columns on the left and a value on the right. Two common scenarios include:\n\ntype = all_categorical() ~ \"categorical\" Forces dichotomous columns (e.g. fever yes/no) to show all levels instead of only the “yes” row\n\ntype = all_continuous() ~ \"continuous2\" Allows multi-line statistics per variable, as shown in a later section\n\nIn the example below, each of these arguments is used to modify the original summary table:\n\nlinelist %&gt;% \n  select(age_years, gender, outcome, fever, temp, hospital) %&gt;% # keep only columns of interest\n  tbl_summary(     \n    by = outcome,                                               # stratify entire table by outcome\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\",        # stats and format for continuous columns\n                     all_categorical() ~ \"{n} / {N} ({p}%)\"),   # stats and format for categorical columns\n    digits = all_continuous() ~ 1,                              # rounding for continuous columns\n    type   = all_categorical() ~ \"categorical\",                 # force all categorical levels to display\n    label  = list(                                              # display labels for column names\n      outcome   ~ \"Outcome\",                           \n      age_years ~ \"Age (years)\",\n      gender    ~ \"Gender\",\n      temp      ~ \"Temperature\",\n      hospital  ~ \"Hospital\"),\n    missing_text = \"Missing\"                                    # how missing values should display\n  )\n\n1323 observations missing `outcome` have been removed. To include these observations, use `forcats::fct_na_value_to_level()` on `outcome` column before passing to `tbl_summary()`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nDeath, N = 2,5821\nRecover, N = 1,9831\n\n\n\n\nAge (years)\n15.9 (12.3)\n16.1 (13.0)\n\n\n    Missing\n32\n28\n\n\nGender\n\n\n\n\n\n\n    f\n1,227 / 2,455 (50%)\n953 / 1,903 (50%)\n\n\n    m\n1,228 / 2,455 (50%)\n950 / 1,903 (50%)\n\n\n    Missing\n127\n80\n\n\nfever\n\n\n\n\n\n\n    no\n458 / 2,460 (19%)\n361 / 1,904 (19%)\n\n\n    yes\n2,002 / 2,460 (81%)\n1,543 / 1,904 (81%)\n\n\n    Missing\n122\n79\n\n\nTemperature\n38.6 (1.0)\n38.6 (1.0)\n\n\n    Missing\n60\n55\n\n\nHospital\n\n\n\n\n\n\n    Central Hospital\n193 / 2,582 (7.5%)\n165 / 1,983 (8.3%)\n\n\n    Military Hospital\n399 / 2,582 (15%)\n309 / 1,983 (16%)\n\n\n    Missing\n611 / 2,582 (24%)\n514 / 1,983 (26%)\n\n\n    Other\n395 / 2,582 (15%)\n290 / 1,983 (15%)\n\n\n    Port Hospital\n785 / 2,582 (30%)\n579 / 1,983 (29%)\n\n\n    St. Mark's Maternity Hospital (SMMH)\n199 / 2,582 (7.7%)\n126 / 1,983 (6.4%)\n\n\n\n1 Mean (SD); n / N (%)\n\n\n\n\n\n\n\n\n\n\n\nMulti-line stats for continuous variables\nIf you want to print multiple lines of statistics for continuous variables, you can indicate this by setting the type = to “continuous2”. You can combine all of the previously shown elements in one table by choosing which statistics you want to show. To do this you need to tell the function that you want to get a table back by entering the type as “continuous2”. The number of missing values is shown as “Unknown”.\n\nlinelist %&gt;% \n  select(age_years, temp) %&gt;%                      # keep only columns of interest\n  tbl_summary(                                     # create summary table\n    type = all_continuous() ~ \"continuous2\",       # indicate that you want to print multiple statistics \n    statistic = all_continuous() ~ c(\n      \"{mean} ({sd})\",                             # line 1: mean and SD\n      \"{median} ({p25}, {p75})\",                   # line 2: median and IQR\n      \"{min}, {max}\")                              # line 3: min and max\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 5,888\n\n\n\n\nage_years\n\n\n\n\n    Mean (SD)\n16 (13)\n\n\n    Median (IQR)\n13 (6, 23)\n\n\n    Range\n0, 84\n\n\n    Unknown\n86\n\n\ntemp\n\n\n\n\n    Mean (SD)\n38.56 (0.98)\n\n\n    Median (IQR)\n38.80 (38.20, 39.20)\n\n\n    Range\n35.20, 40.80\n\n\n    Unknown\n149\n\n\n\n\n\n\n\n\nThere are many other ways to modify these tables, including adding p-values, adjusting color and headings, etc. Many of these are described in the documentation (enter ?tbl_summary in Console), and some are given in the section on statistical tests.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html#base-r",
    "href": "new_pages/tables_descriptive.html#base-r",
    "title": "17  Descriptive tables",
    "section": "17.6 base R",
    "text": "17.6 base R\nYou can use the function table() to tabulate and cross-tabulate columns. Unlike the options above, you must specify the dataframe each time you reference a column name, as shown below.\nCAUTION: NA (missing) values will not be tabulated unless you include the argument useNA = \"always\" (which could also be set to “no” or “ifany”).\nTIP: You can use the %$% from magrittr to remove the need for repeating data frame calls within base functions. For example the below could be written linelist %$% table(outcome, useNA = \"always\") \n\ntable(linelist$outcome, useNA = \"always\")\n\n\n  Death Recover    &lt;NA&gt; \n   2582    1983    1323 \n\n\nMultiple columns can be cross-tabulated by listing them one after the other, separated by commas. Optionally, you can assign each column a “name” like Outcome = linelist$outcome.\n\nage_by_outcome &lt;- table(linelist$age_cat, linelist$outcome, useNA = \"always\") # save table as object\nage_by_outcome   # print table\n\n       \n        Death Recover &lt;NA&gt;\n  0-4     471     364  260\n  5-9     476     391  228\n  10-14   438     303  200\n  15-19   323     251  169\n  20-29   477     367  229\n  30-49   329     238  187\n  50-69    33      38   24\n  70+       3       3    0\n  &lt;NA&gt;     32      28   26\n\n\n\nProportions\nTo return proportions, passing the above table to the function prop.table(). Use the margins = argument to specify whether you want the proportions to be of rows (1), of columns (2), or of the whole table (3). For clarity, we pipe the table to the round() function from base R, specifying 2 digits.\n\n# get proportions of table defined above, by rows, rounded\nprop.table(age_by_outcome, 1) %&gt;% round(2)\n\n       \n        Death Recover &lt;NA&gt;\n  0-4    0.43    0.33 0.24\n  5-9    0.43    0.36 0.21\n  10-14  0.47    0.32 0.21\n  15-19  0.43    0.34 0.23\n  20-29  0.44    0.34 0.21\n  30-49  0.44    0.32 0.25\n  50-69  0.35    0.40 0.25\n  70+    0.50    0.50 0.00\n  &lt;NA&gt;   0.37    0.33 0.30\n\n\n\n\nTotals\nTo add row and column totals, pass the table to addmargins(). This works for both counts and proportions.\n\naddmargins(age_by_outcome)\n\n       \n        Death Recover &lt;NA&gt;  Sum\n  0-4     471     364  260 1095\n  5-9     476     391  228 1095\n  10-14   438     303  200  941\n  15-19   323     251  169  743\n  20-29   477     367  229 1073\n  30-49   329     238  187  754\n  50-69    33      38   24   95\n  70+       3       3    0    6\n  &lt;NA&gt;     32      28   26   86\n  Sum    2582    1983 1323 5888\n\n\n\n\nConvert to data frame\nConverting a table() object directly to a data frame is not straight-forward. One approach is demonstrated below:\n\nCreate the table, without using useNA = \"always\". Instead convert NA values to “(Missing)” with fct_explicit_na() from forcats.\n\nAdd totals (optional) by piping to addmargins()\n\nPipe to the base R function as.data.frame.matrix()\n\nPipe the table to the tibble function rownames_to_column(), specifying the name for the first column\n\nPrint, View, or export as desired. In this example we use flextable() from package flextable as described in the Tables for presentation page. This will print to the RStudio viewer pane as a pretty HTML image.\n\n\ntable(fct_explicit_na(linelist$age_cat), fct_explicit_na(linelist$outcome)) %&gt;% \n  addmargins() %&gt;% \n  as.data.frame.matrix() %&gt;% \n  tibble::rownames_to_column(var = \"Age Category\") %&gt;% \n  flextable::flextable()\n\nAge CategoryDeathRecover(Missing)Sum0-44713642601,0955-94763912281,09510-1443830320094115-1932325116974320-294773672291,07330-4932923818775450-693338249570+3306(Missing)32282686Sum2,5821,9831,3235,888",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_descriptive.html#resources",
    "href": "new_pages/tables_descriptive.html#resources",
    "title": "17  Descriptive tables",
    "section": "17.7 Resources",
    "text": "17.7 Resources\nMuch of the information in this page is adapted from these resources and vignettes online:\ngtsummary\ndplyr",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Descriptive tables</span>"
    ]
  },
  {
    "objectID": "new_pages/stat_tests.html",
    "href": "new_pages/stat_tests.html",
    "title": "18  Simple statistical tests",
    "section": "",
    "text": "18.1 Preparation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple statistical tests</span>"
    ]
  },
  {
    "objectID": "new_pages/stat_tests.html#preparation",
    "href": "new_pages/stat_tests.html#preparation",
    "title": "18  Simple statistical tests",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  skimr,        # get overview of data\n  tidyverse,    # data management + ggplot2 graphics, \n  gtsummary,    # summary statistics and tests\n  rstatix,      # statistics\n  corrr,        # correlation analayis for numeric variables\n  janitor,      # adding totals and percents to tables\n  flextable     # converting tables to HTML\n  )\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple statistical tests</span>"
    ]
  },
  {
    "objectID": "new_pages/stat_tests.html#base-r",
    "href": "new_pages/stat_tests.html#base-r",
    "title": "18  Simple statistical tests",
    "section": "18.2 base R",
    "text": "18.2 base R\nYou can use base R functions to conduct statistical tests. The commands are relatively simple and results will print to the R Console for simple viewing. However, the outputs are usually lists and so are harder to manipulate if you want to use the results in subsequent operations.\n\nT-tests\nA t-test, also called “Student’s t-Test”, is typically used to determine if there is a significant difference between the means of some numeric variable between two groups. Here we’ll show the syntax to do this test depending on whether the columns are in the same data frame.\nSyntax 1: This is the syntax when your numeric and categorical columns are in the same data frame. Provide the numeric column on the left side of the equation and the categorical column on the right side. Specify the dataset to data =. Optionally, set paired = TRUE, and conf.level = (0.95 default), and alternative = (either “two.sided”, “less”, or “greater”). Enter ?t.test for more details.\n\n## compare mean age by outcome group with a t-test\nt.test(age_years ~ gender, data = linelist)\n\n\n    Welch Two Sample t-test\n\ndata:  age_years by gender\nt = -21.344, df = 4902.3, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n -7.571920 -6.297975\nsample estimates:\nmean in group f mean in group m \n       12.60207        19.53701 \n\n\nSyntax 2: You can compare two separate numeric vectors using this alternative syntax. For example, if the two columns are in different data sets.\n\nt.test(df1$age_years, df2$age_years)\n\nYou can also use a t-test to determine whether a sample mean is significantly different from some specific value. Here we conduct a one-sample t-test with the known/hypothesized population mean as mu =:\n\nt.test(linelist$age_years, mu = 45)\n\n\n\nShapiro-Wilk test\nThe Shapiro-Wilk test can be used to determine whether a sample came from a normally-distributed population (an assumption of many other tests and analysis, such as the t-test). However, this can only be used on a sample between 3 and 5000 observations. For larger samples a quantile-quantile plot may be helpful.\n\nshapiro.test(linelist$age_years)\n\n\n\nWilcoxon rank sum test\nThe Wilcoxon rank sum test, also called the Mann–Whitney U test, is often used to help determine if two numeric samples are from the same distribution when their populations are not normally distributed or have unequal variance.\n\n## compare age distribution by outcome group with a wilcox test\nwilcox.test(age_years ~ outcome, data = linelist)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  age_years by outcome\nW = 2501868, p-value = 0.8308\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\nKruskal-Wallis test\nThe Kruskal-Wallis test is an extension of the Wilcoxon rank sum test that can be used to test for differences in the distribution of more than two samples. When only two samples are used it gives identical results to the Wilcoxon rank sum test.\n\n## compare age distribution by outcome group with a kruskal-wallis test\nkruskal.test(age_years ~ outcome, linelist)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  age_years by outcome\nKruskal-Wallis chi-squared = 0.045675, df = 1, p-value = 0.8308\n\n\n\n\nChi-squared test\nPearson’s Chi-squared test is used in testing for significant differences between categorical croups.\n\n## compare the proportions in each group with a chi-squared test\nchisq.test(linelist$gender, linelist$outcome)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  linelist$gender and linelist$outcome\nX-squared = 0.0011841, df = 1, p-value = 0.9725",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple statistical tests</span>"
    ]
  },
  {
    "objectID": "new_pages/stat_tests.html#rstatix-package",
    "href": "new_pages/stat_tests.html#rstatix-package",
    "title": "18  Simple statistical tests",
    "section": "18.3 rstatix package",
    "text": "18.3 rstatix package\nThe rstatix package offers the ability to run statistical tests and retrieve results in a “pipe-friendly” framework. The results are automatically in a data frame so that you can perform subsequent operations on the results. It is also easy to group the data being passed into the functions, so that the statistics are run for each group.\n\nSummary statistics\nThe function get_summary_stats() is a quick way to return summary statistics. Simply pipe your dataset to this function and provide the columns to analyse. If no columns are specified, the statistics are calculated for all columns.\nBy default, a full range of summary statistics are returned: n, max, min, median, 25%ile, 75%ile, IQR, median absolute deviation (mad), mean, standard deviation, standard error, and a confidence interval of the mean.\n\nlinelist %&gt;%\n  rstatix::get_summary_stats(age, temp)\n\n# A tibble: 2 × 13\n  variable     n   min   max median    q1    q3   iqr    mad  mean     sd    se\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 age       5802   0    84     13     6    23      17 11.9    16.1 12.6   0.166\n2 temp      5739  35.2  40.8   38.8  38.2  39.2     1  0.741  38.6  0.977 0.013\n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\nYou can specify a subset of summary statistics to return by providing one of the following values to type =: “full”, “common”, “robust”, “five_number”, “mean_sd”, “mean_se”, “mean_ci”, “median_iqr”, “median_mad”, “quantile”, “mean”, “median”, “min”, “max”.\nIt can be used with grouped data as well, such that a row is returned for each grouping-variable:\n\nlinelist %&gt;%\n  group_by(hospital) %&gt;%\n  rstatix::get_summary_stats(age, temp, type = \"common\")\n\n# A tibble: 12 × 11\n   hospital     variable     n   min   max median   iqr  mean     sd    se    ci\n   &lt;chr&gt;        &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Central Hos… age        445   0    58     12    15    15.7 12.5   0.591 1.16 \n 2 Central Hos… temp       450  35.2  40.4   38.8   1    38.5  0.964 0.045 0.089\n 3 Military Ho… age        884   0    72     14    18    16.1 12.4   0.417 0.818\n 4 Military Ho… temp       873  35.3  40.5   38.8   1    38.6  0.952 0.032 0.063\n 5 Missing      age       1441   0    76     13    17    16.0 12.9   0.339 0.665\n 6 Missing      temp      1431  35.8  40.6   38.9   1    38.6  0.97  0.026 0.05 \n 7 Other        age        873   0    69     13    17    16.0 12.5   0.422 0.828\n 8 Other        temp       862  35.7  40.8   38.8   1.1  38.5  1.01  0.034 0.067\n 9 Port Hospit… age       1739   0    68     14    18    16.3 12.7   0.305 0.598\n10 Port Hospit… temp      1713  35.5  40.6   38.8   1.1  38.6  0.981 0.024 0.046\n11 St. Mark's … age        420   0    84     12    15    15.7 12.4   0.606 1.19 \n12 St. Mark's … temp       410  35.9  40.6   38.8   1.1  38.5  0.983 0.049 0.095\n\n\nYou can also use rstatix to conduct statistical tests:\n\n\nT-test\nUse a formula syntax to specify the numeric and categorical columns:\n\nlinelist %&gt;% \n  t_test(age_years ~ gender)\n\n# A tibble: 1 × 10\n  .y.   group1 group2    n1    n2 statistic    df        p    p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 age_… f      m       2807  2803     -21.3 4902. 9.89e-97 9.89e-97 ****        \n\n\nOr use ~ 1 and specify mu = for a one-sample T-test. This can also be done by group.\n\nlinelist %&gt;% \n  t_test(age_years ~ 1, mu = 30)\n\n# A tibble: 1 × 7\n  .y.       group1 group2         n statistic    df     p\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 age_years 1      null model  5802     -84.2  5801     0\n\n\nIf applicable, the statistical tests can be done by group, as shown below:\n\nlinelist %&gt;% \n  group_by(gender) %&gt;% \n  t_test(age_years ~ 1, mu = 18)\n\n# A tibble: 3 × 8\n  gender .y.       group1 group2         n statistic    df         p\n* &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 f      age_years 1      null model  2807    -29.8   2806 7.52e-170\n2 m      age_years 1      null model  2803      5.70  2802 1.34e-  8\n3 &lt;NA&gt;   age_years 1      null model   192     -3.80   191 1.96e-  4\n\n\n\n\nShapiro-Wilk test\nAs stated above, sample size must be between 3 and 5000.\n\nlinelist %&gt;% \n  head(500) %&gt;%            # first 500 rows of case linelist, for example only\n  shapiro_test(age_years)\n\n# A tibble: 1 × 3\n  variable  statistic        p\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 age_years     0.917 6.67e-16\n\n\n\n\nWilcoxon rank sum test\n\nlinelist %&gt;% \n  wilcox_test(age_years ~ gender)\n\n# A tibble: 1 × 9\n  .y.       group1 group2    n1    n2 statistic        p    p.adj p.adj.signif\n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 age_years f      m       2807  2803   2829274 3.47e-74 3.47e-74 ****        \n\n\n\n\nKruskal-Wallis test\nAlso known as the Mann-Whitney U test.\n\nlinelist %&gt;% \n  kruskal_test(age_years ~ outcome)\n\n# A tibble: 1 × 6\n  .y.           n statistic    df     p method        \n* &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 age_years  5888    0.0457     1 0.831 Kruskal-Wallis\n\n\n\n\nChi-squared test\nThe chi-square test function accepts a table, so first we create a cross-tabulation. There are many ways to create a cross-tabulation (see Descriptive tables) but here we use tabyl() from janitor and remove the left-most column of value labels before passing to chisq_test().\n\nlinelist %&gt;% \n  tabyl(gender, outcome) %&gt;% \n  select(-1) %&gt;% \n  chisq_test()\n\n# A tibble: 1 × 6\n      n statistic     p    df method          p.signif\n* &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;   \n1  5888      3.53 0.473     4 Chi-square test ns      \n\n\nMany many more functions and statistical tests can be run with rstatix functions. See the documentation for rstatix online here or by entering ?rstatix.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple statistical tests</span>"
    ]
  },
  {
    "objectID": "new_pages/stat_tests.html#stats_gt",
    "href": "new_pages/stat_tests.html#stats_gt",
    "title": "18  Simple statistical tests",
    "section": "18.4 gtsummary package",
    "text": "18.4 gtsummary package\nUse gtsummary if you are looking to add the results of a statistical test to a pretty table that was created with this package (as described in the gtsummary section of the Descriptive tables page).\nPerforming statistical tests of comparison with tbl_summary is done by adding the add_p function to a table and specifying which test to use. It is possible to get p-values corrected for multiple testing by using the add_q function. Run ?tbl_summary for details.\n\nChi-squared test\nCompare the proportions of a categorical variable in two groups. The default statistical test for add_p() when applied to a categorical variable is to perform a chi-squared test of independence with continuity correction, but if any expected call count is below 5 then a Fisher’s exact test is used.\n\nlinelist %&gt;% \n  select(gender, outcome) %&gt;%    # keep variables of interest\n  tbl_summary(by = outcome) %&gt;%  # produce summary table and specify grouping variable\n  add_p()                        # specify what test to perform\n\n1323 observations missing `outcome` have been removed. To include these observations, use `forcats::fct_na_value_to_level()` on `outcome` column before passing to `tbl_summary()`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nDeath, N = 2,5821\nRecover, N = 1,9831\np-value2\n\n\n\n\ngender\n\n\n\n\n&gt;0.9\n\n\n    f\n1,227 (50%)\n953 (50%)\n\n\n\n\n    m\n1,228 (50%)\n950 (50%)\n\n\n\n\n    Unknown\n127\n80\n\n\n\n\n\n1 n (%)\n\n\n2 Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\nT-tests\nCompare the difference in means for a continuous variable in two groups. For example, compare the mean age by patient outcome.\n\nlinelist %&gt;% \n  select(age_years, outcome) %&gt;%             # keep variables of interest\n  tbl_summary(                               # produce summary table\n    statistic = age_years ~ \"{mean} ({sd})\", # specify what statistics to show\n    by = outcome) %&gt;%                        # specify the grouping variable\n  add_p(age_years ~ \"t.test\")                # specify what tests to perform\n\n1323 observations missing `outcome` have been removed. To include these observations, use `forcats::fct_na_value_to_level()` on `outcome` column before passing to `tbl_summary()`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nDeath, N = 2,5821\nRecover, N = 1,9831\np-value2\n\n\n\n\nage_years\n16 (12)\n16 (13)\n0.6\n\n\n    Unknown\n32\n28\n\n\n\n\n\n1 Mean (SD)\n\n\n2 Welch Two Sample t-test\n\n\n\n\n\n\n\n\n\n\n\nWilcoxon rank sum test\nCompare the distribution of a continuous variable in two groups. The default is to use the Wilcoxon rank sum test and the median (IQR) when comparing two groups. However for non-normally distributed data or comparing multiple groups, the Kruskal-wallis test is more appropriate.\n\nlinelist %&gt;% \n  select(age_years, outcome) %&gt;%                       # keep variables of interest\n  tbl_summary(                                         # produce summary table\n    statistic = age_years ~ \"{median} ({p25}, {p75})\", # specify what statistic to show (this is default so could remove)\n    by = outcome) %&gt;%                                  # specify the grouping variable\n  add_p(age_years ~ \"wilcox.test\")                     # specify what test to perform (default so could leave brackets empty)\n\n1323 observations missing `outcome` have been removed. To include these observations, use `forcats::fct_na_value_to_level()` on `outcome` column before passing to `tbl_summary()`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nDeath, N = 2,5821\nRecover, N = 1,9831\np-value2\n\n\n\n\nage_years\n13 (6, 23)\n13 (6, 23)\n0.8\n\n\n    Unknown\n32\n28\n\n\n\n\n\n1 Median (IQR)\n\n\n2 Wilcoxon rank sum test\n\n\n\n\n\n\n\n\n\n\n\nKruskal-wallis test\nCompare the distribution of a continuous variable in two or more groups, regardless of whether the data is normally distributed.\n\nlinelist %&gt;% \n  select(age_years, outcome) %&gt;%                       # keep variables of interest\n  tbl_summary(                                         # produce summary table\n    statistic = age_years ~ \"{median} ({p25}, {p75})\", # specify what statistic to show (default, so could remove)\n    by = outcome) %&gt;%                                  # specify the grouping variable\n  add_p(age_years ~ \"kruskal.test\")                    # specify what test to perform\n\n1323 observations missing `outcome` have been removed. To include these observations, use `forcats::fct_na_value_to_level()` on `outcome` column before passing to `tbl_summary()`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nDeath, N = 2,5821\nRecover, N = 1,9831\np-value2\n\n\n\n\nage_years\n13 (6, 23)\n13 (6, 23)\n0.8\n\n\n    Unknown\n32\n28\n\n\n\n\n\n1 Median (IQR)\n\n\n2 Kruskal-Wallis rank sum test",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple statistical tests</span>"
    ]
  },
  {
    "objectID": "new_pages/stat_tests.html#correlations",
    "href": "new_pages/stat_tests.html#correlations",
    "title": "18  Simple statistical tests",
    "section": "18.5 Correlations",
    "text": "18.5 Correlations\nCorrelation between numeric variables can be investigated using the tidyverse\ncorrr package. It allows you to compute correlations using Pearson, Kendall tau or Spearman rho. The package creates a table and also has a function to automatically plot the values.\n\ncorrelation_tab &lt;- linelist %&gt;% \n  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %&gt;%   # keep numeric variables of interest\n  correlate()      # create correlation table (using default pearson)\n\ncorrelation_tab    # print\n\n# A tibble: 6 × 7\n  term            generation      age ct_blood days_onset_hosp    wt_kg    ht_cm\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 generation        NA       -2.22e-2  0.179         -0.288    -0.0302  -0.00942\n2 age               -0.0222  NA        0.00849       -0.000635  0.833    0.877  \n3 ct_blood           0.179    8.49e-3 NA             -0.600    -0.00636  0.0181 \n4 days_onset_hosp   -0.288   -6.35e-4 -0.600         NA         0.0153  -0.00953\n5 wt_kg             -0.0302   8.33e-1 -0.00636        0.0153   NA        0.884  \n6 ht_cm             -0.00942  8.77e-1  0.0181        -0.00953   0.884   NA      \n\n## remove duplicate entries (the table above is mirrored) \ncorrelation_tab &lt;- correlation_tab %&gt;% \n  shave()\n\n## view correlation table \ncorrelation_tab\n\n# A tibble: 6 × 7\n  term            generation       age ct_blood days_onset_hosp  wt_kg ht_cm\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 generation        NA       NA        NA              NA       NA        NA\n2 age               -0.0222  NA        NA              NA       NA        NA\n3 ct_blood           0.179    0.00849  NA              NA       NA        NA\n4 days_onset_hosp   -0.288   -0.000635 -0.600          NA       NA        NA\n5 wt_kg             -0.0302   0.833    -0.00636         0.0153  NA        NA\n6 ht_cm             -0.00942  0.877     0.0181         -0.00953  0.884    NA\n\n## plot correlations \nrplot(correlation_tab)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple statistical tests</span>"
    ]
  },
  {
    "objectID": "new_pages/stat_tests.html#resources",
    "href": "new_pages/stat_tests.html#resources",
    "title": "18  Simple statistical tests",
    "section": "18.6 Resources",
    "text": "18.6 Resources\nMuch of the information in this page is adapted from these resources and vignettes online:\ngtsummary dplyr corrr sthda correlation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Simple statistical tests</span>"
    ]
  },
  {
    "objectID": "new_pages/regression.html",
    "href": "new_pages/regression.html",
    "title": "19  Univariate and multivariable regression",
    "section": "",
    "text": "19.1 Preparation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Univariate and multivariable regression</span>"
    ]
  },
  {
    "objectID": "new_pages/regression.html#preparation",
    "href": "new_pages/regression.html#preparation",
    "title": "19  Univariate and multivariable regression",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  tidyverse,    # data management + ggplot2 graphics, \n  stringr,      # manipulate text strings \n  purrr,        # loop over objects in a tidy way\n  gtsummary,    # summary statistics and tests \n  broom,        # tidy up results from regressions\n  lmtest,       # likelihood-ratio tests\n  parameters,   # alternative to tidy up results from regressions\n  see          # alternative to visualise forest plots\n  )\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.\n\n\n\n\n\n\n\n\nClean data\n\nStore explanatory variables\nWe store the names of the explanatory columns as a character vector. This will be referenced later.\n\n## define variables of interest \nexplanatory_vars &lt;- c(\"gender\", \"fever\", \"chills\", \"cough\", \"aches\", \"vomit\")\n\n\n\nConvert to 1’s and 0’s\nBelow we convert the explanatory columns from “yes”/“no”, “m”/“f”, and “dead”/“alive” to 1 / 0, to cooperate with the expectations of logistic regression models. To do this efficiently, used across() from dplyr to transform multiple columns at one time. The function we apply to each column is case_when() (also dplyr) which applies logic to convert specified values to 1’s and 0’s. See sections on across() and case_when() in the Cleaning data and core functions page).\nNote: the “.” below represents the column that is being processed by across() at that moment.\n\n## convert dichotomous variables to 0/1 \nlinelist &lt;- linelist %&gt;%  \n  mutate(across(                                      \n    .cols = all_of(c(explanatory_vars, \"outcome\")),  ## for each column listed and \"outcome\"\n    .fns = ~case_when(                              \n      . %in% c(\"m\", \"yes\", \"Death\")   ~ 1,           ## recode male, yes and death to 1\n      . %in% c(\"f\", \"no\",  \"Recover\") ~ 0,           ## female, no and recover to 0\n      TRUE                            ~ NA_real_)    ## otherwise set to missing\n    )\n  )\n\n\n\nDrop rows with missing values\nTo drop rows with missing values, can use the tidyr function drop_na(). However, we only want to do this for rows that are missing values in the columns of interest.\nThe first thing we must to is make sure our explanatory_vars vector includes the column age (age would have produced an error in the previous case_when() operation, which was only for dichotomous variables). Then we pipe the linelist to drop_na() to remove any rows with missing values in the outcome column or any of the explanatory_vars columns.\nBefore running the code, the number of rows in the linelist is nrow(linelist).\n\n## add in age_category to the explanatory vars \nexplanatory_vars &lt;- c(explanatory_vars, \"age_cat\")\n\n## drop rows with missing information for variables of interest \nlinelist &lt;- linelist %&gt;% \n  drop_na(any_of(c(\"outcome\", explanatory_vars)))\n\nThe number of rows remaining in linelist is nrow(linelist).",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Univariate and multivariable regression</span>"
    ]
  },
  {
    "objectID": "new_pages/regression.html#univariate",
    "href": "new_pages/regression.html#univariate",
    "title": "19  Univariate and multivariable regression",
    "section": "19.2 Univariate",
    "text": "19.2 Univariate\nJust like in the page on Descriptive tables, your use case will determine which R package you use. We present two options for doing univariate analysis:\n\nUse functions available in base R to quickly print results to the console. Use the broom package to tidy up the outputs.\n\nUse the gtsummary package to model and get publication-ready outputs\n\n\n\nbase R\n\nLinear regression\nThe base R function lm() perform linear regression, assessing the relationship between numeric response and explanatory variables that are assumed to have a linear relationship.\nProvide the equation as a formula, with the response and explanatory column names separated by a tilde ~. Also, specify the dataset to data =. Define the model results as an R object, to use later.\n\nlm_results &lt;- lm(ht_cm ~ age, data = linelist)\n\nYou can then run summary() on the model results to see the coefficients (Estimates), P-value, residuals, and other measures.\n\nsummary(lm_results)\n\n\nCall:\nlm(formula = ht_cm ~ age, data = linelist)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-128.579  -15.854    1.177   15.887  175.483 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  69.9051     0.5979   116.9   &lt;2e-16 ***\nage           3.4354     0.0293   117.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.75 on 4165 degrees of freedom\nMultiple R-squared:  0.7675,    Adjusted R-squared:  0.7674 \nF-statistic: 1.375e+04 on 1 and 4165 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively you can use the tidy() function from the broom package to pull the results in to a table. What the results tell us is that for each year increase in age the height increases by 3.5 cm and this is statistically significant.\n\ntidy(lm_results)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    69.9     0.598       117.       0\n2 age             3.44    0.0293      117.       0\n\n\nYou can then also use this regression to add it to a ggplot, to do this we first pull the points for the observed data and the fitted line in to one data frame using the augment() function from broom.\n\n## pull the regression points and observed data in to one dataset\npoints &lt;- augment(lm_results)\n\n## plot the data using age as the x-axis \nggplot(points, aes(x = age)) + \n  ## add points for height \n  geom_point(aes(y = ht_cm)) + \n  ## add your regression line \n  geom_line(aes(y = .fitted), colour = \"red\")\n\n\n\n\n\n\n\n\nIt is also possible to add a simple linear regression straight straight in ggplot using the geom_smooth() function.\n\n## add your data to a plot \n ggplot(linelist, aes(x = age, y = ht_cm)) + \n  ## show points\n  geom_point() + \n  ## add a linear regression \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSee the Resource section at the end of this chapter for more detailed tutorials.\n\n\nLogistic regression\nThe function glm() from the stats package (part of base R) is used to fit Generalized Linear Models (GLM).\nglm() can be used for univariate and multivariable logistic regression (e.g. to get Odds Ratios). Here are the core parts:\n\n# arguments for glm()\nglm(formula, family, data, weights, subset, ...)\n\n\nformula = The model is provided to glm() as an equation, with the outcome on the left and explanatory variables on the right of a tilde ~.\n\nfamily = This determines the type of model to run. For logistic regression, use family = \"binomial\", for poisson use family = \"poisson\". Other examples are in the table below.\n\ndata = Specify your data frame\n\nIf necessary, you can also specify the link function via the syntax family = familytype(link = \"linkfunction\")). You can read more in the documentation about other families and optional arguments such as weights = and subset = (?glm).\n\n\n\nFamily\nDefault link function\n\n\n\n\n\"binomial\"\n(link = \"logit\")\n\n\n\"gaussian\"\n(link = \"identity\")\n\n\n\"Gamma\"\n(link = \"inverse\")\n\n\n\"inverse.gaussian\"\n(link = \"1/mu^2\")\n\n\n\"poisson\"\n(link = \"log\")\n\n\n\"quasi\"\n(link = \"identity\", variance = \"constant\")\n\n\n\"quasibinomial\"\n(link = \"logit\")\n\n\n\"quasipoisson\"\n(link = \"log\")\n\n\n\nWhen running glm() it is most common to save the results as a named R object. Then you can print the results to your console using summary() as shown below, or perform other operations on the results (e.g. exponentiate).\nIf you need to run a negative binomial regression you can use the MASS package; the glm.nb() uses the same syntax as glm(). For a walk-through of different regressions, see the UCLA stats page.\n\n\nUnivariate glm()\nIn this example we are assessing the association between different age categories and the outcome of death (coded as 1 in the Preparation section). Below is a univariate model of outcome by age_cat. We save the model output as model and then print it with summary() to the console. Note the estimates provided are the log odds and that the baseline level is the first factor level of age_cat (“0-4”).\n\nmodel &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist)\nsummary(model)\n\n\nCall:\nglm(formula = outcome ~ age_cat, family = \"binomial\", data = linelist)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   0.233738   0.072805   3.210  0.00133 **\nage_cat5-9   -0.062898   0.101733  -0.618  0.53640   \nage_cat10-14  0.138204   0.107186   1.289  0.19726   \nage_cat15-19 -0.005565   0.113343  -0.049  0.96084   \nage_cat20-29  0.027511   0.102133   0.269  0.78765   \nage_cat30-49  0.063764   0.113771   0.560  0.57517   \nage_cat50-69 -0.387889   0.259240  -1.496  0.13459   \nage_cat70+   -0.639203   0.915770  -0.698  0.48518   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5705.1  on 4159  degrees of freedom\nAIC: 5721.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nTo alter the baseline level of a given variable, ensure the column is class Factor and move the desired level to the first position with fct_relevel() (see page on Factors). For example, below we take column age_cat and set “20-29” as the baseline before piping the modified data frame into glm().\n\nlinelist %&gt;% \n  mutate(age_cat = fct_relevel(age_cat, \"20-29\", after = 0)) %&gt;% \n  glm(formula = outcome ~ age_cat, family = \"binomial\") %&gt;% \n  summary()\n\n\nCall:\nglm(formula = outcome ~ age_cat, family = \"binomial\", data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.26125    0.07163   3.647 0.000265 ***\nage_cat0-4   -0.02751    0.10213  -0.269 0.787652    \nage_cat5-9   -0.09041    0.10090  -0.896 0.370220    \nage_cat10-14  0.11069    0.10639   1.040 0.298133    \nage_cat15-19 -0.03308    0.11259  -0.294 0.768934    \nage_cat30-49  0.03625    0.11302   0.321 0.748390    \nage_cat50-69 -0.41540    0.25891  -1.604 0.108625    \nage_cat70+   -0.66671    0.91568  -0.728 0.466546    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5705.1  on 4159  degrees of freedom\nAIC: 5721.1\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nPrinting results\nFor most uses, several modifications must be made to the above outputs. The function tidy() from the package broom is convenient for making the model results presentable.\nHere we demonstrate how to combine model outputs with a table of counts.\n\nGet the exponentiated log odds ratio estimates and confidence intervals by passing the model to tidy() and setting exponentiate = TRUE and conf.int = TRUE.\n\n\nmodel &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist) %&gt;% \n  tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;%        # exponentiate and produce CIs\n  mutate(across(where(is.numeric), round, digits = 2))  # round all numeric columns\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nBelow is the outputted tibble model:\n\n\n\n\n\n\n\nCombine these model results with a table of counts. Below, we create the a counts cross-table with the tabyl() function from janitor, as covered in the Descriptive tables page.\n\n\ncounts_table &lt;- linelist %&gt;% \n  janitor::tabyl(age_cat, outcome)\n\n\n\n\n\n\n\n\n\n\n\nHere is what this counts_table data frame looks like:\n\n\n\n\n\n\nNow we can bind the counts_table and the model results together horizontally with bind_cols() (dplyr). Remember that with bind_cols() the rows in the two data frames must be aligned perfectly. In this code, because we are binding within a pipe chain, we use . to represent the piped object counts_table as we bind it to model. To finish the process, we use select() to pick the desired columns and their order, and finally apply the base R round() function across all numeric columns to specify 2 decimal places.\n\ncombined &lt;- counts_table %&gt;%           # begin with table of counts\n  bind_cols(., model) %&gt;%              # combine with the outputs of the regression \n  select(term, 2:3, estimate,          # select and re-order cols\n         conf.low, conf.high, p.value) %&gt;% \n  mutate(across(where(is.numeric), round, digits = 2)) ## round to 2 decimal places\n\nHere is what the combined data frame looks like, printed nicely as an image with a function from flextable. The Tables for presentation explains how to customize such tables with flextable, or or you can use numerous other packages such as knitr or GT.\n\ncombined &lt;- combined %&gt;% \n  flextable::qflextable()\n\n\n\nLooping multiple univariate models\nBelow we present a method using glm() and tidy() for a more simple approach, see the section on gtsummary.\nTo run the models on several exposure variables to produce univariate odds ratios (i.e. not controlling for each other), you can use the approach below. It uses str_c() from stringr to create univariate formulas (see Characters and strings), runs the glm() regression on each formula, passes each glm() output to tidy() and finally collapses all the model outputs together with bind_rows() from tidyr. This approach uses map() from the package purrr to iterate - see the page on Iteration, loops, and lists for more information on this tool.\n\nCreate a vector of column names of the explanatory variables. We already have this as explanatory_vars from the Preparation section of this page.\nUse str_c() to create multiple string formulas, with outcome on the left, and a column name from explanatory_vars on the right. The period . substitutes for the column name in explanatory_vars.\n\n\nexplanatory_vars %&gt;% str_c(\"outcome ~ \", .)\n\n[1] \"outcome ~ gender\"  \"outcome ~ fever\"   \"outcome ~ chills\" \n[4] \"outcome ~ cough\"   \"outcome ~ aches\"   \"outcome ~ vomit\"  \n[7] \"outcome ~ age_cat\"\n\n\n\nPass these string formulas to map() and set ~glm() as the function to apply to each input. Within glm(), set the regression formula as as.formula(.x) where .x will be replaced by the string formula defined in the step above. map() will loop over each of the string formulas, running regressions for each one.\nThe outputs of this first map() are passed to a second map() command, which applies tidy() to the regression outputs.\nFinally the output of the second map() (a list of tidied data frames) is condensed with bind_rows(), resulting in one data frame with all the univariate results.\n\n\nmodels &lt;- explanatory_vars %&gt;%       # begin with variables of interest\n  str_c(\"outcome ~ \", .) %&gt;%         # combine each variable into formula (\"outcome ~ variable of interest\")\n  \n  # iterate through each univariate formula\n  map(                               \n    .f = ~glm(                       # pass the formulas one-by-one to glm()\n      formula = as.formula(.x),      # within glm(), the string formula is .x\n      family = \"binomial\",           # specify type of glm (logistic)\n      data = linelist)) %&gt;%          # dataset\n  \n  # tidy up each of the glm regression outputs from above\n  map(\n    .f = ~tidy(\n      .x, \n      exponentiate = TRUE,           # exponentiate \n      conf.int = TRUE)) %&gt;%          # return confidence intervals\n  \n  # collapse the list of regression outputs in to one data frame\n  bind_rows() %&gt;% \n  \n  # round all numeric columns\n  mutate(across(where(is.numeric), round, digits = 2))\n\nThis time, the end object models is longer because it now represents the combined results of several univariate regressions. Click through to see all the rows of model.\n\n\n\n\n\n\nAs before, we can create a counts table from the linelist for each explanatory variable, bind it to models, and make a nice table. We begin with the variables, and iterate through them with map(). We iterate through a user-defined function which involves creating a counts table with dplyr functions. Then the results are combined and bound with the models model results.\n\n## for each explanatory variable\nuniv_tab_base &lt;- explanatory_vars %&gt;% \n  map(.f = \n    ~{linelist %&gt;%                ## begin with linelist\n        group_by(outcome) %&gt;%     ## group data set by outcome\n        count(.data[[.x]]) %&gt;%    ## produce counts for variable of interest\n        pivot_wider(              ## spread to wide format (as in cross-tabulation)\n          names_from = outcome,\n          values_from = n) %&gt;% \n        drop_na(.data[[.x]]) %&gt;%         ## drop rows with missings\n        rename(\"variable\" = .x) %&gt;%      ## change variable of interest column to \"variable\"\n        mutate(variable = as.character(variable))} ## convert to character, else non-dichotomous (categorical) variables come out as factor and cant be merged\n      ) %&gt;% \n  \n  ## collapse the list of count outputs in to one data frame\n  bind_rows() %&gt;% \n  \n  ## merge with the outputs of the regression \n  bind_cols(., models) %&gt;% \n  \n  ## only keep columns interested in \n  select(term, 2:3, estimate, conf.low, conf.high, p.value) %&gt;% \n  \n  ## round decimal places\n  mutate(across(where(is.numeric), round, digits = 2))\n\nBelow is what the data frame looks like. See the page on Tables for presentation for ideas on how to further convert this table to pretty HTML output (e.g. with flextable).\n\n\n\n\n\n\n\n\n\n\ngtsummary package\nBelow we present the use of tbl_uvregression() from the gtsummary package. Just like in the page on Descriptive tables, gtsummary functions do a good job of running statistics and producing professional-looking outputs. This function produces a table of univariate regression results.\nWe select only the necessary columns from the linelist (explanatory variables and the outcome variable) and pipe them into tbl_uvregression(). We are going to run univariate regression on each of the columns we defined as explanatory_vars in the data Preparation section (gender, fever, chills, cough, aches, vomit, and age_cat).\nWithin the function itself, we provide the method = as glm (no quotes), the y = outcome column (outcome), specify to method.args = that we want to run logistic regression via family = binomial, and we tell it to exponentiate the results.\nThe output is HTML and contains the counts\n\nuniv_tab &lt;- linelist %&gt;% \n  dplyr::select(explanatory_vars, outcome) %&gt;% ## select variables of interest\n\n  tbl_uvregression(                         ## produce univariate table\n    method = glm,                           ## define regression want to run (generalised linear model)\n    y = outcome,                            ## define outcome variable\n    method.args = list(family = binomial),  ## define what type of glm want to run (logistic)\n    exponentiate = TRUE                     ## exponentiate to produce odds ratios (rather than log odds)\n  )\n\n## view univariate results table \nuniv_tab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nOR1\n95% CI1\np-value\n\n\n\n\ngender\n4,167\n1.00\n0.88, 1.13\n&gt;0.9\n\n\nfever\n4,167\n1.00\n0.85, 1.17\n&gt;0.9\n\n\nchills\n4,167\n1.03\n0.89, 1.21\n0.7\n\n\ncough\n4,167\n1.15\n0.97, 1.37\n0.11\n\n\naches\n4,167\n0.93\n0.76, 1.14\n0.5\n\n\nvomit\n4,167\n1.09\n0.96, 1.23\n0.2\n\n\nage_cat\n4,167\n\n\n\n\n\n\n\n\n    0-4\n\n\n—\n—\n\n\n\n\n    5-9\n\n\n0.94\n0.77, 1.15\n0.5\n\n\n    10-14\n\n\n1.15\n0.93, 1.42\n0.2\n\n\n    15-19\n\n\n0.99\n0.80, 1.24\n&gt;0.9\n\n\n    20-29\n\n\n1.03\n0.84, 1.26\n0.8\n\n\n    30-49\n\n\n1.07\n0.85, 1.33\n0.6\n\n\n    50-69\n\n\n0.68\n0.41, 1.13\n0.13\n\n\n    70+\n\n\n0.53\n0.07, 3.20\n0.5\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nThere are many modifications you can make to this table output, such as adjusting the text labels, bolding rows by their p-value, etc. See tutorials here and elsewhere online.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Univariate and multivariable regression</span>"
    ]
  },
  {
    "objectID": "new_pages/regression.html#stratified",
    "href": "new_pages/regression.html#stratified",
    "title": "19  Univariate and multivariable regression",
    "section": "19.3 Stratified",
    "text": "19.3 Stratified\nStratified analysis is currently still being worked on for gtsummary, this page will be updated in due course.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Univariate and multivariable regression</span>"
    ]
  },
  {
    "objectID": "new_pages/regression.html#multivariable",
    "href": "new_pages/regression.html#multivariable",
    "title": "19  Univariate and multivariable regression",
    "section": "19.4 Multivariable",
    "text": "19.4 Multivariable\nFor multivariable analysis, we again present two approaches:\n\nglm() and tidy()\n\ngtsummary package\n\nThe workflow is similar for each and only the last step of pulling together a final table is different.\n\nConduct multivariable\nHere we use glm() but add more variables to the right side of the equation, separated by plus symbols (+).\nTo run the model with all of our explanatory variables we would run:\n\nmv_reg &lt;- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = \"binomial\", data = linelist)\n\nsummary(mv_reg)\n\n\nCall:\nglm(formula = outcome ~ gender + fever + chills + cough + aches + \n    vomit + age_cat, family = \"binomial\", data = linelist)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   0.069054   0.131726   0.524    0.600\ngender        0.002448   0.065133   0.038    0.970\nfever         0.004309   0.080522   0.054    0.957\nchills        0.034112   0.078924   0.432    0.666\ncough         0.138584   0.089909   1.541    0.123\naches        -0.070705   0.104078  -0.679    0.497\nvomit         0.086098   0.062618   1.375    0.169\nage_cat5-9   -0.063562   0.101851  -0.624    0.533\nage_cat10-14  0.136372   0.107275   1.271    0.204\nage_cat15-19 -0.011074   0.113640  -0.097    0.922\nage_cat20-29  0.026552   0.102780   0.258    0.796\nage_cat30-49  0.059569   0.116402   0.512    0.609\nage_cat50-69 -0.388964   0.262384  -1.482    0.138\nage_cat70+   -0.647443   0.917375  -0.706    0.480\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5700.2  on 4153  degrees of freedom\nAIC: 5728.2\n\nNumber of Fisher Scoring iterations: 4\n\n\nIf you want to include two variables and an interaction between them you can separate them with an asterisk * instead of a +. Separate them with a colon : if you are only specifying the interaction. For example:\n\nglm(outcome ~ gender + age_cat * fever, family = \"binomial\", data = linelist)\n\nOptionally, you can use this code to leverage the pre-defined vector of column names and re-create the above command using str_c(). This might be useful if your explanatory variable names are changing, or you don’t want to type them all out again.\n\n## run a regression with all variables of interest \nmv_reg &lt;- explanatory_vars %&gt;%  ## begin with vector of explanatory column names\n  str_c(collapse = \"+\") %&gt;%     ## combine all names of the variables of interest separated by a plus\n  str_c(\"outcome ~ \", .) %&gt;%    ## combine the names of variables of interest with outcome in formula style\n  glm(family = \"binomial\",      ## define type of glm as logistic,\n      data = linelist)          ## define your dataset\n\n\nBuilding the model\nYou can build your model step-by-step, saving various models that include certain explanatory variables. You can compare these models with likelihood-ratio tests using lrtest() from the package lmtest, as below:\nNOTE: Using base anova(model1, model2, test = \"Chisq) produces the same results \n\nmodel1 &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist)\nmodel2 &lt;- glm(outcome ~ age_cat + gender, family = \"binomial\", data = linelist)\n\nlmtest::lrtest(model1, model2)\n\nLikelihood ratio test\n\nModel 1: outcome ~ age_cat\nModel 2: outcome ~ age_cat + gender\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)\n1   8 -2852.6                    \n2   9 -2852.6  1 2e-04     0.9883\n\n\nAnother option is to take the model object and apply the step() function from the stats package. Specify which variable selection direction you want use when building the model.\n\n## choose a model using forward selection based on AIC\n## you can also do \"backward\" or \"both\" by adjusting the direction\nfinal_mv_reg &lt;- mv_reg %&gt;%\n  step(direction = \"forward\", trace = FALSE)\n\nYou can also turn off scientific notation in your R session, for clarity:\n\noptions(scipen=999)\n\nAs described in the section on univariate analysis, pass the model output to tidy() to exponentiate the log odds and CIs. Finally we round all numeric columns to two decimal places. Scroll through to see all the rows.\n\nmv_tab_base &lt;- final_mv_reg %&gt;% \n  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;%  ## get a tidy dataframe of estimates \n  mutate(across(where(is.numeric), round, digits = 2))          ## round \n\nHere is what the resulting data frame looks like:\n\n\n\n\n\n\n\n\n\n\nCombine univariate and multivariable\n\nCombine with gtsummary\nThe gtsummary package provides the tbl_regression() function, which will take the outputs from a regression (glm() in this case) and produce an nice summary table.\n\n## show results table of final regression \nmv_tab &lt;- tbl_regression(final_mv_reg, exponentiate = TRUE)\n\nLet’s see the table:\n\nmv_tab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\ngender\n1.00\n0.88, 1.14\n&gt;0.9\n\n\nfever\n1.00\n0.86, 1.18\n&gt;0.9\n\n\nchills\n1.03\n0.89, 1.21\n0.7\n\n\ncough\n1.15\n0.96, 1.37\n0.12\n\n\naches\n0.93\n0.76, 1.14\n0.5\n\n\nvomit\n1.09\n0.96, 1.23\n0.2\n\n\nage_cat\n\n\n\n\n\n\n\n\n    0-4\n—\n—\n\n\n\n\n    5-9\n0.94\n0.77, 1.15\n0.5\n\n\n    10-14\n1.15\n0.93, 1.41\n0.2\n\n\n    15-19\n0.99\n0.79, 1.24\n&gt;0.9\n\n\n    20-29\n1.03\n0.84, 1.26\n0.8\n\n\n    30-49\n1.06\n0.85, 1.33\n0.6\n\n\n    50-69\n0.68\n0.40, 1.13\n0.14\n\n\n    70+\n0.52\n0.07, 3.19\n0.5\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nYou can also combine several different output tables produced by gtsummary with the tbl_merge() function. We now combine the multivariable results with the gtsummary univariate results that we created above:\n\n## combine with univariate results \ntbl_merge(\n  tbls = list(univ_tab, mv_tab),                          # combine\n  tab_spanner = c(\"**Univariate**\", \"**Multivariable**\")) # set header names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nUnivariate\nMultivariable\n\n\nN\nOR1\n95% CI1\np-value\nOR1\n95% CI1\np-value\n\n\n\n\ngender\n4,167\n1.00\n0.88, 1.13\n&gt;0.9\n1.00\n0.88, 1.14\n&gt;0.9\n\n\nfever\n4,167\n1.00\n0.85, 1.17\n&gt;0.9\n1.00\n0.86, 1.18\n&gt;0.9\n\n\nchills\n4,167\n1.03\n0.89, 1.21\n0.7\n1.03\n0.89, 1.21\n0.7\n\n\ncough\n4,167\n1.15\n0.97, 1.37\n0.11\n1.15\n0.96, 1.37\n0.12\n\n\naches\n4,167\n0.93\n0.76, 1.14\n0.5\n0.93\n0.76, 1.14\n0.5\n\n\nvomit\n4,167\n1.09\n0.96, 1.23\n0.2\n1.09\n0.96, 1.23\n0.2\n\n\nage_cat\n4,167\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    0-4\n\n\n—\n—\n\n\n—\n—\n\n\n\n\n    5-9\n\n\n0.94\n0.77, 1.15\n0.5\n0.94\n0.77, 1.15\n0.5\n\n\n    10-14\n\n\n1.15\n0.93, 1.42\n0.2\n1.15\n0.93, 1.41\n0.2\n\n\n    15-19\n\n\n0.99\n0.80, 1.24\n&gt;0.9\n0.99\n0.79, 1.24\n&gt;0.9\n\n\n    20-29\n\n\n1.03\n0.84, 1.26\n0.8\n1.03\n0.84, 1.26\n0.8\n\n\n    30-49\n\n\n1.07\n0.85, 1.33\n0.6\n1.06\n0.85, 1.33\n0.6\n\n\n    50-69\n\n\n0.68\n0.41, 1.13\n0.13\n0.68\n0.40, 1.13\n0.14\n\n\n    70+\n\n\n0.53\n0.07, 3.20\n0.5\n0.52\n0.07, 3.19\n0.5\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\nCombine with dplyr\nAn alternative way of combining the glm()/tidy() univariate and multivariable outputs is with the dplyr join functions.\n\nJoin the univariate results from earlier (univ_tab_base, which contains counts) with the tidied multivariable results mv_tab_base\n\nUse select() to keep only the columns we want, specify their order, and re-name them\n\nUse round() with two decimal places on all the column that are class Double\n\n\n## combine univariate and multivariable tables \nleft_join(univ_tab_base, mv_tab_base, by = \"term\") %&gt;% \n  ## choose columns and rename them\n  select( # new name =  old name\n    \"characteristic\" = term, \n    \"recovered\"      = \"0\", \n    \"dead\"           = \"1\", \n    \"univ_or\"        = estimate.x, \n    \"univ_ci_low\"    = conf.low.x, \n    \"univ_ci_high\"   = conf.high.x,\n    \"univ_pval\"      = p.value.x, \n    \"mv_or\"          = estimate.y, \n    \"mvv_ci_low\"     = conf.low.y, \n    \"mv_ci_high\"     = conf.high.y,\n    \"mv_pval\"        = p.value.y \n  ) %&gt;% \n  mutate(across(where(is.double), round, 2))   \n\n# A tibble: 20 × 11\n   characteristic recovered  dead univ_or univ_ci_low univ_ci_high univ_pval\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)          909  1168    1.28        1.18         1.4       0   \n 2 gender               916  1174    1           0.88         1.13      0.97\n 3 (Intercept)          340   436    1.28        1.11         1.48      0   \n 4 fever               1485  1906    1           0.85         1.17      0.99\n 5 (Intercept)         1472  1877    1.28        1.19         1.37      0   \n 6 chills               353   465    1.03        0.89         1.21      0.68\n 7 (Intercept)          272   309    1.14        0.97         1.34      0.13\n 8 cough               1553  2033    1.15        0.97         1.37      0.11\n 9 (Intercept)         1636  2114    1.29        1.21         1.38      0   \n10 aches                189   228    0.93        0.76         1.14      0.51\n11 (Intercept)          931  1144    1.23        1.13         1.34      0   \n12 vomit                894  1198    1.09        0.96         1.23      0.17\n13 (Intercept)          338   427    1.26        1.1          1.46      0   \n14 age_cat5-9           365   433    0.94        0.77         1.15      0.54\n15 age_cat10-14         273   396    1.15        0.93         1.42      0.2 \n16 age_cat15-19         238   299    0.99        0.8          1.24      0.96\n17 age_cat20-29         345   448    1.03        0.84         1.26      0.79\n18 age_cat30-49         228   307    1.07        0.85         1.33      0.58\n19 age_cat50-69          35    30    0.68        0.41         1.13      0.13\n20 age_cat70+             3     2    0.53        0.07         3.2       0.49\n# ℹ 4 more variables: mv_or &lt;dbl&gt;, mvv_ci_low &lt;dbl&gt;, mv_ci_high &lt;dbl&gt;,\n#   mv_pval &lt;dbl&gt;",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Univariate and multivariable regression</span>"
    ]
  },
  {
    "objectID": "new_pages/regression.html#forest-plot",
    "href": "new_pages/regression.html#forest-plot",
    "title": "19  Univariate and multivariable regression",
    "section": "19.5 Forest plot",
    "text": "19.5 Forest plot\nThis section shows how to produce a plot with the outputs of your regression. There are two options, you can build a plot yourself using ggplot2 or use a meta-package called easystats (a package that includes many packages).\nSee the page on ggplot basics if you are unfamiliar with the ggplot2 plotting package.\n\n\nggplot2 package\nYou can build a forest plot with ggplot() by plotting elements of the multivariable regression results. Add the layers of the plots using these “geoms”:\n\nestimates with geom_point()\n\nconfidence intervals with geom_errorbar()\n\na vertical line at OR = 1 with geom_vline()\n\nBefore plotting, you may want to use fct_relevel() from the forcats package to set the order of the variables/levels on the y-axis. ggplot() may display them in alpha-numeric order which would not work well for these age category values (“30” would appear before “5”). See the page on Factors for more details.\n\n## remove the intercept term from your multivariable results\nmv_tab_base %&gt;% \n  \n  #set order of levels to appear along y-axis\n  mutate(term = fct_relevel(\n    term,\n    \"vomit\", \"gender\", \"fever\", \"cough\", \"chills\", \"aches\",\n    \"age_cat5-9\", \"age_cat10-14\", \"age_cat15-19\", \"age_cat20-29\",\n    \"age_cat30-49\", \"age_cat50-69\", \"age_cat70+\")) %&gt;%\n  \n  # remove \"intercept\" row from plot\n  filter(term != \"(Intercept)\") %&gt;% \n  \n  ## plot with variable on the y axis and estimate (OR) on the x axis\n  ggplot(aes(x = estimate, y = term)) +\n  \n  ## show the estimate as a point\n  geom_point() + \n  \n  ## add in an error bar for the confidence intervals\n  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + \n  \n  ## show where OR = 1 is for reference as a dashed line\n  geom_vline(xintercept = 1, linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\neasystats packages\nAn alternative, if you do not want to the fine level of control that ggplot2 provides, is to use a combination of easystats packages.\nThe function model_parameters() from the parameters package does the equivalent of the broom package function tidy(). The see package then accepts those outputs and creates a default forest plot as a ggplot() object.\n\npacman::p_load(easystats)\n\n## remove the intercept term from your multivariable results\nfinal_mv_reg %&gt;% \n  model_parameters(exponentiate = TRUE) %&gt;% \n  plot()",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Univariate and multivariable regression</span>"
    ]
  },
  {
    "objectID": "new_pages/regression.html#resources",
    "href": "new_pages/regression.html#resources",
    "title": "19  Univariate and multivariable regression",
    "section": "19.6 Resources",
    "text": "19.6 Resources\nThe content of this page was informed by these resources and vignettes online:\nLinear regression in R\ngtsummary\nUCLA stats page\nsthda stepwise regression",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Univariate and multivariable regression</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html",
    "href": "new_pages/missing_data.html",
    "title": "20  Missing data",
    "section": "",
    "text": "20.1 Preparation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html#preparation",
    "href": "new_pages/missing_data.html#preparation",
    "title": "20  Missing data",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,           # import/export\n  tidyverse,     # data mgmt and viz\n  naniar,        # assess and visualize missingness\n  mice           # missing data imputation\n)\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.\n\n\n\n\n\n\n\n\nConvert missing on import\nWhen importing your data, be aware of values that should be classified as missing. For example, 99, 999, “Missing”, blank cells (““), or cells with an empty space (” “). You can convert these to NA (R’s version of missing data) during the data import command.\nSee the page on importing page section on Missing data for details, as the exact syntax varies by file type.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html#missing-values-in-r",
    "href": "new_pages/missing_data.html#missing-values-in-r",
    "title": "20  Missing data",
    "section": "20.2 Missing values in R",
    "text": "20.2 Missing values in R\nBelow we explore ways that missingness is presented and assessed in R, along with some adjacent values and functions.\n\nNA\nIn R, missing values are represented by a reserved (special) value - NA. Note that this is typed without quotes. “NA” is different and is just a normal character value (also a Beatles lyric from the song Hey Jude).\nYour data may have other ways of representing missingness, such as “99”, or “Missing”, or “Unknown” - you may even have empty character value “” which looks “blank”, or a single space ” “. Be aware of these and consider whether to convert them to NA during import or during data cleaning with na_if().\nIn your data cleaning, you may also want to convert the other way - changing all NA to “Missing” or similar with replace_na() or with fct_explicit_na() for factors.\n\n\nVersions of NA\nMost of the time, NA represents a missing value and everything works fine. However, in some circumstances you may encounter the need for variations of NA specific to an object class (character, numeric, etc). This will be rare, but you should be aware.\nThe typical scenario for this is when creating a new column with the dplyr function case_when(). As described in the Cleaning data and core functions page, this function evaluates every row in the data frame, assess whether the rows meets specified logical criteria (right side of the code), and assigns the correct new value (left side of the code). Importantly: all values on the right side must be the same class.\n\nlinelist &lt;- linelist %&gt;% \n  \n  # Create new \"age_years\" column from \"age\" column\n  mutate(age_years = case_when(\n    age_unit == \"years\"  ~ age,       # if age is given in years, assign original value\n    age_unit == \"months\" ~ age/12,    # if age is given in months, divide by 12\n    is.na(age_unit)      ~ age,       # if age UNIT is missing, assume years\n    TRUE                 ~ NA_real_)) # any other circumstance, assign missing\n\nIf you want NA on the right side, you may need to specify one of the special NA options listed below. If the other right side values are character, consider using “Missing” instead or otherwise use NA_character_. If they are all numeric, use NA_real_. If they are all dates or logical, you can use NA.\n\nNA - use for dates or logical TRUE/FALSE\nNA_character_ - use for characters\n\nNA_real_ - use for numeric\n\nAgain, it is not likely you will encounter these variations unless you are using case_when() to create a new column. See the R documentation on NA for more information.\n\n\nNULL\nNULL is another reserved value in R. It is the logical representation of a statement that is neither true nor false. It is returned by expressions or functions whose values are undefined. Generally do not assign NULL as a value, unless writing functions or perhaps writing a shiny app to return NULL in specific scenarios.\nNull-ness can be assessed using is.null() and conversion can made with as.null().\nSee this blog post on the difference between NULL and NA.\n\n\nNaN\nImpossible values are represented by the special value NaN. An example of this is when you force R to divide 0 by 0. You can assess this with is.nan(). You may also encounter complementary functions including is.infinite() and is.finite().\n\n\nInf\nInf represents an infinite value, such as when you divide a number by 0.\nAs an example of how this might impact your work: let’s say you have a vector/column z that contains these values: z &lt;- c(1, 22, NA, Inf, NaN, 5)\nIf you want to use max() on the column to find the highest value, you can use the na.rm = TRUE to remove the NA from the calculation, but the Inf and NaN remain and Inf will be returned. To resolve this, you can use brackets [ ] and is.finite() to subset such that only finite values are used for the calculation: max(z[is.finite(z)]).\n\nz &lt;- c(1, 22, NA, Inf, NaN, 5)\nmax(z)                           # returns NA\nmax(z, na.rm=T)                  # returns Inf\nmax(z[is.finite(z)])             # returns 22\n\n\n\nExamples\n\n\n\n\n\n\n\nR command\nOutcome\n\n\n\n\n5 / 0\nInf\n\n\n0 / 0\nNaN\n\n\n5 / NA\nNA\n\n\n5 / Inf |0NA - 5|NAInf / 5|Infclass(NA)| \"logical\"class(NaN)| \"numeric\"class(Inf)| \"numeric\"class(NULL)`\n“NULL”\n\n\n\n“NAs introduced by coercion” is a common warning message. This can happen if you attempt to make an illegal conversion like inserting a character value into a vector that is otherwise numeric.\n\nas.numeric(c(\"10\", \"20\", \"thirty\", \"40\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 10 20 NA 40\n\n\nNULL is ignored in a vector.\n\nmy_vector &lt;- c(25, NA, 10, NULL)  # define\nmy_vector                         # print\n\n[1] 25 NA 10\n\n\nVariance of one number results in NA.\n\nvar(22)\n\n[1] NA",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html#useful-functions",
    "href": "new_pages/missing_data.html#useful-functions",
    "title": "20  Missing data",
    "section": "20.3 Useful functions",
    "text": "20.3 Useful functions\nThe following are useful base R functions when assessing or handling missing values:\n\nis.na() and !is.na()\nUse is.na()to identify missing values, or use its opposite (with ! in front) to identify non-missing values. These both return a logical value (TRUE or FALSE). Remember that you can sum() the resulting vector to count the number TRUE, e.g. sum(is.na(linelist$date_outcome)).\n\nmy_vector &lt;- c(1, 4, 56, NA, 5, NA, 22)\nis.na(my_vector)\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n!is.na(my_vector)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n\nsum(is.na(my_vector))\n\n[1] 2\n\n\n\n\nna.omit()\nThis function, if applied to a data frame, will remove rows with any missing values. It is also from base R.\nIf applied to a vector, it will remove NA values from the vector it is applied to. For example:\n\nna.omit(my_vector)\n\n[1]  1  4 56  5 22\nattr(,\"na.action\")\n[1] 4 6\nattr(,\"class\")\n[1] \"omit\"\n\n\n\n\ndrop_na()\nThis is a tidyr function that is useful in a data cleaning pipeline. If run with the parentheses empty, it removes rows with any missing values. If column names are specified in the parentheses, rows with missing values in those columns will be dropped. You can also use “tidyselect” syntax to specify the columns.\n\nlinelist %&gt;% \n  drop_na(case_id, date_onset, age) # drops rows missing values for any of these columns\n\n\n\nna.rm = TRUE\nWhen you run a mathematical function such as max(), min(), sum() or mean(), if there are any NA values present the returned value will be NA. This default behavior is intentional, so that you are alerted if any of your data are missing.\nYou can avoid this by removing missing values from the calculation. To do this, include the argument na.rm = TRUE (“na.rm” stands for “remove NA”).\n\nmy_vector &lt;- c(1, 4, 56, NA, 5, NA, 22)\n\nmean(my_vector)     \n\n[1] NA\n\nmean(my_vector, na.rm = TRUE)\n\n[1] 17.6",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html#assess-missingness-in-a-data-frame",
    "href": "new_pages/missing_data.html#assess-missingness-in-a-data-frame",
    "title": "20  Missing data",
    "section": "20.4 Assess missingness in a data frame",
    "text": "20.4 Assess missingness in a data frame\nYou can use the package naniar to assess and visualize missingness in the data frame linelist.\n\n# install and/or load package\npacman::p_load(naniar)\n\n\nQuantifying missingness\nTo find the percent of all values that are missing use pct_miss(). Use n_miss() to get the number of missing values.\n\n# percent of ALL data frame values that are missing\npct_miss(linelist)\n\n[1] 6.688745\n\n\nThe two functions below return the percent of rows with any missing value, or that are entirely complete, respectively. Remember that NA means missing, and that `\"\" or \" \" will not be counted as missing.\n\n# Percent of rows with any value missing\npct_miss_case(linelist)   # use n_complete() for counts\n\n[1] 69.12364\n\n\n\n# Percent of rows that are complete (no values missing)  \npct_complete_case(linelist) # use n_complete() for counts\n\n[1] 30.87636\n\n\n\n\nVisualizing missingness\nThe gg_miss_var() function will show you the number (or %) of missing values in each column. A few nuances:\n\nYou can add a column name (not in quote) to the argument facet = to see the plot by groups\n\nBy default, counts are shown instead of percents, change this with show_pct = TRUE\n\nYou can add axis and title labels as for a normal ggplot() with + labs(...)\n\n\ngg_miss_var(linelist, show_pct = TRUE)\n\n\n\n\n\n\n\n\nHere the data are piped %&gt;% into the function. The facet = argument is also used to split the data.\n\nlinelist %&gt;% \n  gg_miss_var(show_pct = TRUE, facet = outcome)\n\n\n\n\n\n\n\n\nYou can use vis_miss() to visualize the data frame as a heatmap, showing whether each value is missing or not. You can also select() certain columns from the data frame and provide only those columns to the function.\n\n# Heatplot of missingness across the entire data frame  \nvis_miss(linelist)\n\n\n\n\n\n\n\n\n\n\nExplore and visualize missingness relationships\nHow do you visualize something that is not there??? By default, ggplot() removes points with missing values from plots.\nnaniar offers a solution via geom_miss_point(). When creating a scatterplot of two columns, records with one of the values missing and the other value present are shown by setting the missing values to 10% lower than the lowest value in the column, and coloring them distinctly.\nIn the scatterplot below, the red dots are records where the value for one column is present but the value for the other column is missing. This allows you to see the distribution of missing values in relation to the non-missing values.\n\nggplot(\n  data = linelist,\n  mapping = aes(x = age_years, y = temp)) +     \n  geom_miss_point()\n\n\n\n\n\n\n\n\nTo assess missingness in the data frame stratified by another column, consider gg_miss_fct(), which returns a heatmap of percent missingness in the data frame by a factor/categorical (or date) column:\n\ngg_miss_fct(linelist, age_cat5)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `age_cat5 = (function (x) ...`.\nCaused by warning:\n! `fct_explicit_na()` was deprecated in forcats 1.0.0.\nℹ Please use `fct_na_value_to_level()` instead.\nℹ The deprecated feature was likely used in the naniar package.\n  Please report the issue at &lt;https://github.com/njtierney/naniar/issues&gt;.\n\n\n\n\n\n\n\n\n\nThis function can also be used with a date column to see how missingness has changed over time:\n\ngg_miss_fct(linelist, date_onset)\n\nWarning: Removed 29 rows containing missing values or values outside the scale range\n(`geom_tile()`).\n\n\n\n\n\n\n\n\n\n\n\n“Shadow” columns\nAnother way to visualize missingness in one column by values in a second column is using the “shadow” that naniar can create. bind_shadow() creates a binary NA/not NA column for every existing column, and binds all these new columns to the original dataset with the appendix “_NA”. This doubles the number of columns - see below:\n\nshadowed_linelist &lt;- linelist %&gt;% \n  bind_shadow()\n\nnames(shadowed_linelist)\n\n [1] \"case_id\"                 \"generation\"             \n [3] \"date_infection\"          \"date_onset\"             \n [5] \"date_hospitalisation\"    \"date_outcome\"           \n [7] \"outcome\"                 \"gender\"                 \n [9] \"age\"                     \"age_unit\"               \n[11] \"age_years\"               \"age_cat\"                \n[13] \"age_cat5\"                \"hospital\"               \n[15] \"lon\"                     \"lat\"                    \n[17] \"infector\"                \"source\"                 \n[19] \"wt_kg\"                   \"ht_cm\"                  \n[21] \"ct_blood\"                \"fever\"                  \n[23] \"chills\"                  \"cough\"                  \n[25] \"aches\"                   \"vomit\"                  \n[27] \"temp\"                    \"time_admission\"         \n[29] \"bmi\"                     \"days_onset_hosp\"        \n[31] \"case_id_NA\"              \"generation_NA\"          \n[33] \"date_infection_NA\"       \"date_onset_NA\"          \n[35] \"date_hospitalisation_NA\" \"date_outcome_NA\"        \n[37] \"outcome_NA\"              \"gender_NA\"              \n[39] \"age_NA\"                  \"age_unit_NA\"            \n[41] \"age_years_NA\"            \"age_cat_NA\"             \n[43] \"age_cat5_NA\"             \"hospital_NA\"            \n[45] \"lon_NA\"                  \"lat_NA\"                 \n[47] \"infector_NA\"             \"source_NA\"              \n[49] \"wt_kg_NA\"                \"ht_cm_NA\"               \n[51] \"ct_blood_NA\"             \"fever_NA\"               \n[53] \"chills_NA\"               \"cough_NA\"               \n[55] \"aches_NA\"                \"vomit_NA\"               \n[57] \"temp_NA\"                 \"time_admission_NA\"      \n[59] \"bmi_NA\"                  \"days_onset_hosp_NA\"     \n\n\nThese “shadow” columns can be used to plot the proportion of values that are missing, by any another column.\nFor example, the plot below shows the proportion of records missing days_onset_hosp (number of days from symptom onset to hospitalisation), by that record’s value in date_hospitalisation. Essentially, you are plotting the density of the x-axis column, but stratifying the results (color =) by a shadow column of interest. This analysis works best if the x-axis is a numeric or date column.\n\nggplot(data = shadowed_linelist,          # data frame with shadow columns\n  mapping = aes(x = date_hospitalisation, # numeric or date column\n                colour = age_years_NA)) + # shadow column of interest\n  geom_density()                          # plots the density curves\n\n\n\n\n\n\n\n\nYou can also use these “shadow” columns to stratify a statistical summary, as shown below:\n\nlinelist %&gt;%\n  bind_shadow() %&gt;%                # create the shows cols\n  group_by(date_outcome_NA) %&gt;%    # shadow col for stratifying\n  summarise(across(\n    .cols = age_years,             # variable of interest for calculations\n    .fns = list(\"mean\" = mean,     # stats to calculate\n                \"sd\" = sd,\n                \"var\" = var,\n                \"min\" = min,\n                \"max\" = max),  \n    na.rm = TRUE))                 # other arguments for the stat calculations\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(...)`.\nℹ In group 1: `date_outcome_NA = !NA`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 2 × 6\n  date_outcome_NA age_years_mean age_years_sd age_years_var age_years_min\n  &lt;fct&gt;                    &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 !NA                       16.0         12.6          158.             0\n2 NA                        16.2         12.9          167.             0\n# ℹ 1 more variable: age_years_max &lt;dbl&gt;\n\n\nAn alternative way to plot the proportion of a column’s values that are missing over time is shown below. It does not involve naniar. This example shows percent of weekly observations that are missing).\n\nAggregate the data into a useful time unit (days, weeks, etc.), summarizing the proportion of observations with NA (and any other values of interest)\n\nPlot the proportion missing as a line using ggplot()\n\nBelow, we take the linelist, add a new column for week, group the data by week, and then calculate the percent of that week’s records where the value is missing. (note: if you want % of 7 days the calculation would be slightly different).\n\noutcome_missing &lt;- linelist %&gt;%\n  mutate(week = lubridate::floor_date(date_onset, \"week\")) %&gt;%   # create new week column\n  group_by(week) %&gt;%                                             # group the rows by week\n  summarise(                                                     # summarize each week\n    n_obs = n(),                                                  # number of records\n    \n    outcome_missing = sum(is.na(outcome) | outcome == \"\"),        # number of records missing the value\n    outcome_p_miss  = outcome_missing / n_obs,                    # proportion of records missing the value\n  \n    outcome_dead    = sum(outcome == \"Death\", na.rm=T),           # number of records as dead\n    outcome_p_dead  = outcome_dead / n_obs) %&gt;%                   # proportion of records as dead\n  \n  tidyr::pivot_longer(-week, names_to = \"statistic\") %&gt;%         # pivot all columns except week, to long format for ggplot\n  filter(stringr::str_detect(statistic, \"_p_\"))                  # keep only the proportion values\n\nThen we plot the proportion missing as a line, by week. The ggplot basics page if you are unfamiliar with the ggplot2 plotting package.\n\nggplot(data = outcome_missing)+\n    geom_line(\n      mapping = aes(x = week, y = value, group = statistic, color = statistic),\n      size = 2,\n      stat = \"identity\")+\n    labs(title = \"Weekly outcomes\",\n         x = \"Week\",\n         y = \"Proportion of weekly records\") + \n     scale_color_discrete(\n       name = \"\",\n       labels = c(\"Died\", \"Missing outcome\"))+\n    scale_y_continuous(breaks = c(seq(0,1,0.1)))+\n  theme_minimal()+\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html#using-data-with-missing-values",
    "href": "new_pages/missing_data.html#using-data-with-missing-values",
    "title": "20  Missing data",
    "section": "20.5 Using data with missing values",
    "text": "20.5 Using data with missing values\n\nFilter out rows with missing values\nTo quickly remove rows with missing values, use the dplyr function drop_na().\nThe original linelist has nrow(linelist) rows. The adjusted number of rows is shown below:\n\nlinelist %&gt;% \n  drop_na() %&gt;%     # remove rows with ANY missing values\n  nrow()\n\n[1] 1818\n\n\nYou can specify to drop rows with missingness in certain columns:\n\nlinelist %&gt;% \n  drop_na(date_onset) %&gt;% # remove rows missing date_onset \n  nrow()\n\n[1] 5632\n\n\nYou can list columns one after the other, or use “tidyselect” helper functions:\n\nlinelist %&gt;% \n  drop_na(contains(\"date\")) %&gt;% # remove rows missing values in any \"date\" column \n  nrow()\n\n[1] 3029\n\n\n\n\n\nHandling NA in ggplot()\nIt is often wise to report the number of values excluded from a plot in a caption. Below is an example:\nIn ggplot(), you can add labs() and within it a caption =. In the caption, you can use str_glue() from stringr package to paste values together into a sentence dynamically so they will adjust to the data. An example is below:\n\nNote the use of \\n for a new line.\n\nNote that if multiple column would contribute to values not being plotted (e.g. age or sex if those are reflected in the plot), then you must filter on those columns as well to correctly calculate the number not shown.\n\n\nlabs(\n  title = \"\",\n  y = \"\",\n  x = \"\",\n  caption  = stringr::str_glue(\n  \"n = {nrow(central_data)} from Central Hospital;\n  {nrow(central_data %&gt;% filter(is.na(date_onset)))} cases missing date of onset and not shown.\"))  \n\nSometimes, it can be easier to save the string as an object in commands prior to the ggplot() command, and simply reference the named string object within the str_glue().\n\n\n\nNA in factors\nIf your column of interest is a factor, use fct_explicit_na() from the forcats package to convert NA values to a character value. See more detail in the Factors page. By default, the new value is “(Missing)” but this can be adjusted via the na_level = argument.\n\npacman::p_load(forcats)   # load package\n\nlinelist &lt;- linelist %&gt;% \n  mutate(gender = fct_explicit_na(gender, na_level = \"Missing\"))\n\nlevels(linelist$gender)\n\n[1] \"f\"       \"m\"       \"Missing\"",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html#imputation",
    "href": "new_pages/missing_data.html#imputation",
    "title": "20  Missing data",
    "section": "20.6 Imputation",
    "text": "20.6 Imputation\nSometimes, when analyzing your data, it will be important to “fill in the gaps” and impute missing data While you can always simply analyze a dataset after removing all missing values, this can cause problems in many ways. Here are two examples:\n\nBy removing all observations with missing values or variables with a large amount of missing data, you might reduce your power or ability to do some types of analysis. For example, as we discovered earlier, only a small fraction of the observations in our linelist dataset have no missing data across all of our variables. If we removed the majority of our dataset we’d be losing a lot of information! And, most of our variables have some amount of missing data–for most analysis it’s probably not reasonable to drop every variable that has a lot of missing data either.\nDepending on why your data is missing, analysis of only non-missing data might lead to biased or misleading results. For example, as we learned earlier we are missing data for some patients about whether they’ve had some important symptoms like fever or cough. But, as one possibility, maybe that information wasn’t recorded for people that just obviously weren’t very sick. In that case, if we just removed these observations we’d be excluding some of the healthiest people in our dataset and that might really bias any results.\n\nIt’s important to think about why your data might be missing in addition to seeing how much is missing. Doing this can help you decide how important it might be to impute missing data, and also which method of imputing missing data might be best in your situation.\n\nTypes of missing data\nHere are three general types of missing data:\n\nMissing Completely at Random (MCAR). This means that there is no relationship between the probability of data being missing and any of the other variables in your data. The probability of being missing is the same for all cases This is a rare situation. But, if you have strong reason to believe your data is MCAR analyzing only non-missing data without imputing won’t bias your results (although you may lose some power). [TODO: consider discussing statistical tests for MCAR]\nMissing at Random (MAR). This name is actually a bit misleading as MAR means that your data is missing in a systematic, predictable way based on the other information you have. For example, maybe every observation in our dataset with a missing value for fever was actually not recorded because every patient with chills and and aches was just assumed to have a fever so their temperature was never taken. If true, we could easily predict that every missing observation with chills and aches has a fever as well and use this information to impute our missing data. In practice, this is more of a spectrum. Maybe if a patient had both chills and aches they were more likely to have a fever as well if they didn’t have their temperature taken, but not always. This is still predictable even if it isn’t perfectly predictable. This is a common type of missing data\nMissing not at Random (MNAR). Sometimes, this is also called Not Missing at Random (NMAR). This assumes that the probability of a value being missing is NOT systematic or predictable using the other information we have but also isn’t missing randomly. In this situation data is missing for unknown reasons or for reasons you don’t have any information about. For example, in our dataset maybe information on age is missing because some very elderly patients either don’t know or refuse to say how old they are. In this situation, missing data on age is related to the value itself (and thus isn’t random) and isn’t predictable based on the other information we have. MNAR is complex and often the best way of dealing with this is to try to collect more data or information about why the data is missing rather than attempt to impute it.\n\nIn general, imputing MCAR data is often fairly simple, while MNAR is very challenging if not impossible. Many of the common data imputation methods assume MAR.\n\n\nUseful packages\nSome useful packages for imputing missing data are Mmisc, missForest (which uses random forests to impute missing data), and mice (Multivariate Imputation by Chained Equations). For this section we’ll just use the mice package, which implements a variety of techniques. The maintainer of the mice package has published an online book about imputing missing data that goes into more detail here (https://stefvanbuuren.name/fimd/).\nHere is the code to load the mice package:\n\npacman::p_load(mice)\n\n\n\nMean Imputation\nSometimes if you are doing a simple analysis or you have strong reason to think you can assume MCAR, you can simply set missing numerical values to the mean of that variable. Perhaps we can assume that missing temperature measurements in our dataset were either MCAR or were just normal values. Here is the code to create a new variable that replaces missing temperature values with the mean temperature value in our dataset. However, in many situations replacing data with the mean can lead to bias, so be careful.\n\nlinelist &lt;- linelist %&gt;%\n  mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))\n\nYou could also do a similar process for replacing categorical data with a specific value. For our dataset, imagine you knew that all observations with a missing value for their outcome (which can be “Death” or “Recover”) were actually people that died (note: this is not actually true for this dataset):\n\nlinelist &lt;- linelist %&gt;%\n  mutate(outcome_replace_na_with_death = replace_na(outcome, \"Death\"))\n\n\n\nRegression imputation\nA somewhat more advanced method is to use some sort of statistical model to predict what a missing value is likely to be and replace it with the predicted value. Here is an example of creating predicted values for all the observations where temperature is missing, but age and fever are not, using simple linear regression using fever status and age in years as predictors. In practice you’d want to use a better model than this sort of simple approach.\n\nsimple_temperature_model_fit &lt;- lm(temp ~ fever + age_years, data = linelist)\n\n#using our simple temperature model to predict values just for the observations where temp is missing\npredictions_for_missing_temps &lt;- predict(simple_temperature_model_fit,\n                                        newdata = linelist %&gt;% filter(is.na(temp))) \n\nOr, using the same modeling approach through the mice package to create imputed values for the missing temperature observations:\n\nmodel_dataset &lt;- linelist %&gt;%\n  select(temp, fever, age_years)  \n\ntemp_imputed &lt;- mice(model_dataset,\n                            method = \"norm.predict\",\n                            seed = 1,\n                            m = 1,\n                            print = F)\n\nWarning: Number of logged events: 1\n\ntemp_imputed_values &lt;- temp_imputed$imp$temp\n\nThis is the same type of approach by some more advanced methods like using the missForest package to replace missing data with predicted values. In that case, the prediction model is a random forest instead of a linear regression. You can use other types of models to do this as well. However, while this approach works well under MCAR you should be a bit careful if you believe MAR or MNAR more accurately describes your situation. The quality of your imputation will depend on how good your prediction model is and even with a very good model the variability of your imputed data may be underestimated.\n\n\nLOCF and BOCF\nLast observation carried forward (LOCF) and baseline observation carried forward (BOCF) are imputation methods for time series/longitudinal data. The idea is to take the previous observed value as a replacement for the missing data. When multiple values are missing in succession, the method searches for the last observed value.\nThe fill() function from the tidyr package can be used for both LOCF and BOCF imputation (however, other packages such as HMISC, zoo, and data.table also include methods for doing this). To show the fill() syntax we’ll make up a simple time series dataset containing the number of cases of a disease for each quarter of the years 2000 and 2001. However, the year value for subsequent quarters after Q1 are missing so we’ll need to impute them. The fill() junction is also demonstrated in the Pivoting data page.\n\n#creating our simple dataset\ndisease &lt;- tibble::tribble(\n  ~quarter, ~year, ~cases,\n  \"Q1\",    2000,    66013,\n  \"Q2\",      NA,    69182,\n  \"Q3\",      NA,    53175,\n  \"Q4\",      NA,    21001,\n  \"Q1\",    2001,    46036,\n  \"Q2\",      NA,    58842,\n  \"Q3\",      NA,    44568,\n  \"Q4\",      NA,    50197)\n\n#imputing the missing year values:\ndisease %&gt;% fill(year)\n\n# A tibble: 8 × 3\n  quarter  year cases\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Q1       2000 66013\n2 Q2       2000 69182\n3 Q3       2000 53175\n4 Q4       2000 21001\n5 Q1       2001 46036\n6 Q2       2001 58842\n7 Q3       2001 44568\n8 Q4       2001 50197\n\n\nNote: make sure your data are sorted correctly before using the fill() function. fill() defaults to filling “down” but you can also impute values in different directions by changing the .direction parameter. We can make a similar dataset where the year value is recorded only at the end of the year and missing for earlier quarters:\n\n#creating our slightly different dataset\ndisease &lt;- tibble::tribble(\n  ~quarter, ~year, ~cases,\n  \"Q1\",      NA,    66013,\n  \"Q2\",      NA,    69182,\n  \"Q3\",      NA,    53175,\n  \"Q4\",    2000,    21001,\n  \"Q1\",      NA,    46036,\n  \"Q2\",      NA,    58842,\n  \"Q3\",      NA,    44568,\n  \"Q4\",    2001,    50197)\n\n#imputing the missing year values in the \"up\" direction:\ndisease %&gt;% fill(year, .direction = \"up\")\n\n# A tibble: 8 × 3\n  quarter  year cases\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Q1       2000 66013\n2 Q2       2000 69182\n3 Q3       2000 53175\n4 Q4       2000 21001\n5 Q1       2001 46036\n6 Q2       2001 58842\n7 Q3       2001 44568\n8 Q4       2001 50197\n\n\nIn this example, LOCF and BOCF are clearly the right things to do, but in more complicated situations it may be harder to decide if these methods are appropriate. For example, you may have missing laboratory values for a hospital patient after the first day. Sometimes, this can mean the lab values didn’t change…but it could also mean the patient recovered and their values would be very different after the first day! Use these methods with caution.\n\n\nMultiple Imputation\nThe online book we mentioned earlier by the author of the mice package (https://stefvanbuuren.name/fimd/) contains a detailed explanation of multiple imputation and why you’d want to use it. But, here is a basic explanation of the method:\nWhen you do multiple imputation, you create multiple datasets with the missing values imputed to plausible data values (depending on your research data you might want to create more or less of these imputed datasets, but the mice package sets the default number to 5). The difference is that rather than a single, specific value each imputed value is drawn from an estimated distribution (so it includes some randomness). As a result, each of these datasets will have slightly different different imputed values (however, the non-missing data will be the same in each of these imputed datasets). You still use some sort of predictive model to do the imputation in each of these new datasets (mice has many options for prediction methods including Predictive Mean Matching, logistic regression, and random forest) but the mice package can take care of many of the modeling details.\nThen, once you have created these new imputed datasets, you can apply then apply whatever statistical model or analysis you were planning to do for each of these new imputed datasets and pool the results of these models together. This works very well to reduce bias in both MCAR and many MAR settings and often results in more accurate standard error estimates.\nHere is an example of applying the Multiple Imputation process to predict temperature in our linelist dataset using a age and fever status (our simplified model_dataset from above):\n\n# imputing missing values for all variables in our model_dataset, and creating 10 new imputed datasets\nmultiple_imputation = mice(\n  model_dataset,\n  seed = 1,\n  m = 10,\n  print = FALSE) \n\nWarning: Number of logged events: 1\n\nmodel_fit &lt;- with(multiple_imputation, lm(temp ~ age_years + fever))\n\nbase::summary(mice::pool(model_fit))\n\n         term     estimate    std.error    statistic        df       p.value\n1 (Intercept) 3.703143e+01 0.0270863456 1.367162e+03  26.83673  1.583113e-66\n2   age_years 3.867829e-05 0.0006090202 6.350905e-02 171.44363  9.494351e-01\n3    feveryes 1.978044e+00 0.0193587115 1.021785e+02 176.51325 5.666771e-159\n\n\nHere we used the mice default method of imputation, which is Predictive Mean Matching. We then used these imputed datasets to separately estimate and then pool results from simple linear regressions on each of these datasets. There are many details we’ve glossed over and many settings you can adjust during the Multiple Imputation process while using the mice package. For example, you won’t always have numerical data and might need to use other imputation methods (you can still use the mice package for many other types of data and methods). But, for a more robust analysis when missing data is a significant concern, Multiple Imputation is good solution that isn’t always much more work than doing a complete case analysis.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/missing_data.html#resources",
    "href": "new_pages/missing_data.html#resources",
    "title": "20  Missing data",
    "section": "20.7 Resources",
    "text": "20.7 Resources\nVignette on the naniar package\nGallery of missing value visualizations\nOnline book about multiple imputation in R by the maintainer of the mice package",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Missing data</span>"
    ]
  },
  {
    "objectID": "new_pages/standardization.html",
    "href": "new_pages/standardization.html",
    "title": "21  Standardised rates",
    "section": "",
    "text": "21.1 Overview\nThere are two main ways to standardize: direct and indirect standardization. Let’s say we would like to the standardize mortality rate by age and sex for country A and country B, and compare the standardized rates between these countries.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Standardised rates</span>"
    ]
  },
  {
    "objectID": "new_pages/standardization.html#overview",
    "href": "new_pages/standardization.html#overview",
    "title": "21  Standardised rates",
    "section": "",
    "text": "For direct standardization, you will have to know the number of the at-risk population and the number of deaths for each stratum of age and sex, for country A and country B. One stratum in our example could be females between ages 15-44.\n\nFor indirect standardization, you only need to know the total number of deaths and the age- and sex structure of each country. This option is therefore feasible if age- and sex-specific mortality rates or population numbers are not available. Indirect standardization is furthermore preferable in case of small numbers per stratum, as estimates in direct standardization would be influenced by substantial sampling variation.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Standardised rates</span>"
    ]
  },
  {
    "objectID": "new_pages/standardization.html#preparation",
    "href": "new_pages/standardization.html#preparation",
    "title": "21  Standardised rates",
    "section": "21.2 Preparation",
    "text": "21.2 Preparation\nTo show how standardization is done, we will use fictitious population counts and death counts from country A and country B, by age (in 5 year categories) and sex (female, male). To make the datasets ready for use, we will perform the following preparation steps:\n\nLoad packages\n\nLoad datasets\n\nJoin the population and death data from the two countries\nPivot longer so there is one row per age-sex stratum\nClean the reference population (world standard population) and join it to the country data\n\nIn your scenario, your data may come in a different format. Perhaps your data are by province, city, or other catchment area. You may have one row for each death and information on age and sex for each (or a significant proportion) of these deaths. In this case, see the pages on Grouping data, Pivoting data, and Descriptive tables to create a dataset with event and population counts per age-sex stratum.\nWe also need a reference population, the standard population. For the purposes of this exercise we will use the world_standard_population_by_sex. The World standard population is based on the populations of 46 countries and was developed in 1960. There are many “standard” populations - as one example, the website of NHS Scotland is quite informative on the European Standard Population, World Standard Population and Scotland Standard Population.\n\n\nLoad packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n     rio,                 # import/export data\n     here,                # locate files\n     stringr,             # cleaning characters and strings\n     frailtypack,         # needed for dsr, for frailty models\n     dsr,                 # standardise rates\n     PHEindicatormethods, # alternative for rate standardisation\n     tidyverse)           # data management and visualization\n\nCAUTION: If you have a newer version of R, the dsr package cannot be directly downloaded from CRAN. However, it is still available from the CRAN archive. You can install and use this one. \nFor non-Mac users:\n\npackageurl &lt;- \"https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz\"\ninstall.packages(packageurl, repos=NULL, type=\"source\")\n\n\n# Other solution that may work\nrequire(devtools)\ndevtools::install_version(\"dsr\", version=\"0.2.2\", repos=\"http:/cran.us.r.project.org\")\n\nFor Mac users:\n\nrequire(devtools)\ndevtools::install_version(\"dsr\", version=\"0.2.2\", repos=\"https://mac.R-project.org\")\n\n\n\nLoad population data\nSee the Download handbook and data page for instructions on how to download all the example data in the handbook. You can import the Standardisation page data directly into R from our Github repository by running the following import() commands:\n\n# import demographics for country A directly from Github\nA_demo &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics.csv\")\n\n# import deaths for country A directly from Github\nA_deaths &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryA.csv\")\n\n# import demographics for country B directly from Github\nB_demo &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics_2.csv\")\n\n# import deaths for country B directly from Github\nB_deaths &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryB.csv\")\n\n# import demographics for country B directly from Github\nstandard_pop_data &lt;- import(\"https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/world_standard_population_by_sex.csv\")\n\nFirst we load the demographic data (counts of males and females by 5-year age category) for the two countries that we will be comparing, “Country A” and “Country B”.\n\n# Country A\nA_demo &lt;- import(\"country_demographics.csv\")\n\n\n\n\n\n\n\n\n# Country B\nB_demo &lt;- import(\"country_demographics_2.csv\")\n\n\n\n\n\n\n\n\n\nLoad death counts\nConveniently, we also have the counts of deaths during the time period of interest, by age and sex. Each country’s counts are in a separate file, shown below.\nDeaths in Country A\n\n\n\n\n\n\nDeaths in Country B\n\n\n\n\n\n\n\n\nClean populations and deaths\nWe need to join and transform these data in the following ways:\n\nCombine country populations into one dataset and pivot “long” so that each age-sex stratum is one row\n\nCombine country death counts into one dataset and pivot “long” so each age-sex stratum is one row\n\nJoin the deaths to the populations\n\nFirst, we combine the country populations datasets, pivot longer, and do minor cleaning. See the page on Pivoting data for more detail.\n\npop_countries &lt;- A_demo %&gt;%  # begin with country A dataset\n     bind_rows(B_demo) %&gt;%        # bind rows, because cols are identically named\n     pivot_longer(                       # pivot longer\n          cols = c(m, f),                   # columns to combine into one\n          names_to = \"Sex\",                 # name for new column containing the category (\"m\" or \"f\") \n          values_to = \"Population\") %&gt;%     # name for new column containing the numeric values pivoted\n     mutate(Sex = recode(Sex,            # re-code values for clarity\n          \"m\" = \"Male\",\n          \"f\" = \"Female\"))\n\nThe combined population data now look like this (click through to see countries A and B):\n\n\n\n\n\n\nAnd now we perform similar operations on the two deaths datasets.\n\ndeaths_countries &lt;- A_deaths %&gt;%    # begin with country A deaths dataset\n     bind_rows(B_deaths) %&gt;%        # bind rows with B dataset, because cols are identically named\n     pivot_longer(                  # pivot longer\n          cols = c(Male, Female),        # column to transform into one\n          names_to = \"Sex\",              # name for new column containing the category (\"m\" or \"f\") \n          values_to = \"Deaths\") %&gt;%      # name for new column containing the numeric values pivoted\n     rename(age_cat5 = AgeCat)      # rename for clarity\n\nThe deaths data now look like this, and contain data from both countries:\n\n\n\n\n\n\nWe now join the deaths and population data based on common columns Country, age_cat5, and Sex. This adds the column Deaths.\n\ncountry_data &lt;- pop_countries %&gt;% \n     left_join(deaths_countries, by = c(\"Country\", \"age_cat5\", \"Sex\"))\n\nWe can now classify Sex, age_cat5, and Country as factors and set the level order using fct_relevel() function from the forcats package, as described in the page on Factors. Note, classifying the factor levels doesn’t visibly change the data, but the arrange() command does sort it by Country, age category, and sex.\n\ncountry_data &lt;- country_data %&gt;% \n  mutate(\n    Country = fct_relevel(Country, \"A\", \"B\"),\n      \n    Sex = fct_relevel(Sex, \"Male\", \"Female\"),\n        \n    age_cat5 = fct_relevel(\n      age_cat5,\n      \"0-4\", \"5-9\", \"10-14\", \"15-19\",\n      \"20-24\", \"25-29\",  \"30-34\", \"35-39\",\n      \"40-44\", \"45-49\", \"50-54\", \"55-59\",\n      \"60-64\", \"65-69\", \"70-74\",\n      \"75-79\", \"80-84\", \"85\")) %&gt;% \n          \n  arrange(Country, age_cat5, Sex)\n\n\n\n\n\n\n\nCAUTION: If you have few deaths per stratum, consider using 10-, or 15-year categories, instead of 5-year categories for age.\n\n\nLoad reference population\nLastly, for the direct standardisation, we import the reference population (world “standard population” by sex)\n\n# Reference population\nstandard_pop_data &lt;- import(\"world_standard_population_by_sex.csv\")\n\n\n\n\n\n\n\n\n\n\nClean reference population\nThe age category values in the country_data and standard_pop_data data frames will need to be aligned.\nCurrently, the values of the column age_cat5 from the standard_pop_data data frame contain the word “years” and “plus”, while those of the country_data data frame do not. We will have to make the age category values match. We use str_replace_all() from the stringr package, as described in the page on Characters and strings, to replace these patterns with no space \"\".\nFurthermore, the package dsr expects that in the standard population, the column containing counts will be called \"pop\". So we rename that column accordingly.\n\n# Remove specific string from column values\nstandard_pop_clean &lt;- standard_pop_data %&gt;%\n     mutate(\n          age_cat5 = str_replace_all(age_cat5, \"years\", \"\"),   # remove \"year\"\n          age_cat5 = str_replace_all(age_cat5, \"plus\", \"\"),    # remove \"plus\"\n          age_cat5 = str_replace_all(age_cat5, \" \", \"\")) %&gt;%   # remove \" \" space\n     \n     rename(pop = WorldStandardPopulation)   # change col name to \"pop\", as this is expected by dsr package\n\nCAUTION: If you try to use str_replace_all() to remove a plus symbol, it won’t work because it is a special symbol. “Escape” the specialnes by putting two back slashes in front, as in str_replace_call(column, \"\\\\+\", \"\"). \n\n\nCreate dataset with standard population\nFinally, the package PHEindicatormethods, detailed below, expects the standard populations joined to the country event and population counts. So, we will create a dataset all_data for that purpose.\n\nall_data &lt;- left_join(country_data, standard_pop_clean, by=c(\"age_cat5\", \"Sex\"))\n\nThis complete dataset looks like this:",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Standardised rates</span>"
    ]
  },
  {
    "objectID": "new_pages/standardization.html#dsr-package",
    "href": "new_pages/standardization.html#dsr-package",
    "title": "21  Standardised rates",
    "section": "21.3 dsr package",
    "text": "21.3 dsr package\nBelow we demonstrate calculating and comparing directly standardized rates using the dsr package. The dsr package allows you to calculate and compare directly standardized rates (no indirectly standardized rates!).\nIn the data Preparation section, we made separate datasets for country counts and standard population:\n\nthe country_data object, which is a population table with the number of population and number of deaths per stratum per country\n\nthe standard_pop_clean object, containing the number of population per stratum for our reference population, the World Standard Population\n\nWe will use these separate datasets for the dsr approach.\n\n\nStandardized rates\nBelow, we calculate rates per country directly standardized for age and sex. We use the dsr() function.\nOf note - dsr() expects one data frame for the country populations and event counts (deaths), and a separate data frame with the reference population. It also expects that in this reference population dataset the unit-time column name is “pop” (we assured this in the data Preparation section).\nThere are many arguments, as annotated in the code below. Notably, event = is set to the column Deaths, and the fu = (“follow-up”) is set to the Population column. We set the subgroups of comparison as the column Country and we standardize based on age_cat5 and Sex. These last two columns are not assigned a particular named argument. See ?dsr for details.\n\n# Calculate rates per country directly standardized for age and sex\nmortality_rate &lt;- dsr::dsr(\n     data = country_data,  # specify object containing number of deaths per stratum\n     event = Deaths,       # column containing number of deaths per stratum \n     fu = Population,      # column containing number of population per stratum\n     subgroup = Country,   # units we would like to compare\n     age_cat5,             # other columns - rates will be standardized by these\n     Sex,\n     refdata = standard_pop_clean, # reference population data frame, with column called pop\n     method = \"gamma\",      # method to calculate 95% CI\n     sig = 0.95,            # significance level\n     mp = 100000,           # we want rates per 100.000 population\n     decimals = 2)          # number of decimals)\n\n\n# Print output as nice-looking HTML table\nknitr::kable(mortality_rate) # show mortality rate before and after direct standardization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgroup\nNumerator\nDenominator\nCrude Rate (per 1e+05)\n95% LCL (Crude)\n95% UCL (Crude)\nStd Rate (per 1e+05)\n95% LCL (Std)\n95% UCL (Std)\n\n\n\n\nA\n11344\n86790567\n13.07\n12.83\n13.31\n23.57\n23.08\n24.06\n\n\nB\n9955\n52898281\n18.82\n18.45\n19.19\n19.33\n18.46\n20.22\n\n\n\n\n\nAbove, we see that while country A had a lower crude mortality rate than country B, it has a higher standardized rate after direct age and sex standardization.\n\n\n\nStandardized rate ratios\n\n# Calculate RR\nmortality_rr &lt;- dsr::dsrr(\n     data = country_data, # specify object containing number of deaths per stratum\n     event = Deaths,      # column containing number of deaths per stratum \n     fu = Population,     # column containing number of population per stratum\n     subgroup = Country,  # units we would like to compare\n     age_cat5,\n     Sex,                 # characteristics to which we would like to standardize \n     refdata = standard_pop_clean, # reference population, with numbers in column called pop\n     refgroup = \"B\",      # reference for comparison\n     estimate = \"ratio\",  # type of estimate\n     sig = 0.95,          # significance level\n     mp = 100000,         # we want rates per 100.000 population\n     decimals = 2)        # number of decimals\n\n# Print table\nknitr::kable(mortality_rr) \n\n\n\n\n\n\n\n\n\n\n\n\nComparator\nReference\nStd Rate (per 1e+05)\nRate Ratio (RR)\n95% LCL (RR)\n95% UCL (RR)\n\n\n\n\nA\nB\n23.57\n1.22\n1.17\n1.27\n\n\nB\nB\n19.33\n1.00\n0.94\n1.06\n\n\n\n\n\nThe standardized mortality rate is 1.22 times higher in country A compared to country B (95% CI 1.17-1.27).\n\n\n\nStandardized rate difference\n\n# Calculate RD\nmortality_rd &lt;- dsr::dsrr(\n     data = country_data,       # specify object containing number of deaths per stratum\n     event = Deaths,            # column containing number of deaths per stratum \n     fu = Population,           # column containing number of population per stratum\n     subgroup = Country,        # units we would like to compare\n     age_cat5,                  # characteristics to which we would like to standardize\n     Sex,                        \n     refdata = standard_pop_clean, # reference population, with numbers in column called pop\n     refgroup = \"B\",            # reference for comparison\n     estimate = \"difference\",   # type of estimate\n     sig = 0.95,                # significance level\n     mp = 100000,               # we want rates per 100.000 population\n     decimals = 2)              # number of decimals\n\n# Print table\nknitr::kable(mortality_rd) \n\n\n\n\n\n\n\n\n\n\n\n\nComparator\nReference\nStd Rate (per 1e+05)\nRate Difference (RD)\n95% LCL (RD)\n95% UCL (RD)\n\n\n\n\nA\nB\n23.57\n4.24\n3.24\n5.24\n\n\nB\nB\n19.33\n0.00\n-1.24\n1.24\n\n\n\n\n\nCountry A has 4.24 additional deaths per 100.000 population (95% CI 3.24-5.24) compared to country A.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Standardised rates</span>"
    ]
  },
  {
    "objectID": "new_pages/standardization.html#standard_phe",
    "href": "new_pages/standardization.html#standard_phe",
    "title": "21  Standardised rates",
    "section": "21.4 PHEindicatormethods package",
    "text": "21.4 PHEindicatormethods package\nAnother way of calculating standardized rates is with the PHEindicatormethods package. This package allows you to calculate directly as well as indirectly standardized rates. We will show both.\nThis section will use the all_data data frame created at the end of the Preparation section. This data frame includes the country populations, death events, and the world standard reference population. You can view it here.\n\n\nDirectly standardized rates\nBelow, we first group the data by Country and then pass it to the function phe_dsr() to get directly standardized rates per country.\nOf note - the reference (standard) population can be provided as a column within the country-specific data frame or as a separate vector. If provided within the country-specific data frame, you have to set stdpoptype = \"field\". If provided as a vector, set stdpoptype = \"vector\". In the latter case, you have to make sure the ordering of rows by strata is similar in both the country-specific data frame and the reference population, as records will be matched by position. In our example below, we provided the reference population as a column within the country-specific data frame.\nSee the help with ?phr_dsr or the links in the References section for more information.\n\n# Calculate rates per country directly standardized for age and sex\nmortality_ds_rate_phe &lt;- all_data %&gt;%\n     group_by(Country) %&gt;%\n     PHEindicatormethods::phe_dsr(\n          x = Deaths,                 # column with observed number of events\n          n = Population,             # column with non-standard pops for each stratum\n          stdpop = pop,               # standard populations for each stratum\n          stdpoptype = \"field\")       # either \"vector\" for a standalone vector or \"field\" meaning std populations are in the data  \n\n# Print table\nknitr::kable(mortality_ds_rate_phe)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\ntotal_count\ntotal_pop\nvalue\nlowercl\nuppercl\nconfidence\nstatistic\nmethod\n\n\n\n\nA\n11344\n86790567\n23.56686\n23.08107\n24.05944\n95%\ndsr per 100000\nDobson\n\n\nB\n9955\n52898281\n19.32549\n18.45516\n20.20882\n95%\ndsr per 100000\nDobson\n\n\n\n\n\n\n\n\nIndirectly standardized rates\nFor indirect standardization, you need a reference population with the number of deaths and number of population per stratum. In this example, we will be calculating rates for country A using country B as the reference population, as the standard_pop_clean reference population does not include number of deaths per stratum.\nBelow, we first create the reference population from country B. Then, we pass mortality and population data for country A, combine it with the reference population, and pass it to the function calculate_ISRate(), to get indirectly standardized rates. Of course, you can do it also vice versa.\nOf note - in our example below, the reference population is provided as a separate data frame. In this case, we make sure that x =, n =, x_ref = and n_ref = vectors are all ordered by the same standardization category (stratum) values as that in our country-specific data frame, as records will be matched by position.\nSee the help with ?phr_isr or the links in the References section for more information.\n\n# Create reference population\nrefpopCountryB &lt;- country_data %&gt;% \n  filter(Country == \"B\") \n\n# Calculate rates for country A indirectly standardized by age and sex\nmortality_is_rate_phe_A &lt;- country_data %&gt;%\n     filter(Country == \"A\") %&gt;%\n     PHEindicatormethods::calculate_ISRate(\n          x = Deaths,                 # column with observed number of events\n          n = Population,             # column with non-standard pops for each stratum\n          x_ref = refpopCountryB$Deaths,  # reference number of deaths for each stratum\n          n_ref = refpopCountryB$Population)  # reference population for each stratum\n\n# Print table\nknitr::kable(mortality_is_rate_phe_A)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nobserved\nexpected\nref_rate\nvalue\nlowercl\nuppercl\nconfidence\nstatistic\nmethod\n\n\n\n\n11344\n15847.42\n18.81914\n13.47123\n13.22446\n13.72145\n95%\nindirectly standardised rate per 100000\nByars",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Standardised rates</span>"
    ]
  },
  {
    "objectID": "new_pages/standardization.html#resources",
    "href": "new_pages/standardization.html#resources",
    "title": "21  Standardised rates",
    "section": "21.5 Resources",
    "text": "21.5 Resources\nIf you would like to see another reproducible example using dsr please see this vignette\nFor another example using PHEindicatormethods, please go to this website\nSee the PHEindicatormethods reference pdf file",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Standardised rates</span>"
    ]
  },
  {
    "objectID": "new_pages/moving_average.html",
    "href": "new_pages/moving_average.html",
    "title": "22  Moving averages",
    "section": "",
    "text": "22.1 Preparation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Moving averages</span>"
    ]
  },
  {
    "objectID": "new_pages/moving_average.html#preparation",
    "href": "new_pages/moving_average.html#preparation",
    "title": "22  Moving averages",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  tidyverse,      # for data management and viz\n  slider,         # for calculating moving averages\n  tidyquant       # for calculating moving averages within ggplot\n)\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.xlsx\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Moving averages</span>"
    ]
  },
  {
    "objectID": "new_pages/moving_average.html#calculate-with-slider",
    "href": "new_pages/moving_average.html#calculate-with-slider",
    "title": "22  Moving averages",
    "section": "22.2 Calculate with slider",
    "text": "22.2 Calculate with slider\nUse this approach to calculate a moving average in a data frame prior to plotting.\nThe slider package provides several “sliding window” functions to compute rolling averages, cumulative sums, rolling regressions, etc. It treats a data frame as a vector of rows, allowing iteration row-wise over a data frame.\nHere are some of the common functions:\n\nslide_dbl() - iterates through a numeric (hence “_dbl”) column performing an operation using a sliding window\n\nslide_sum() - rolling sum shortcut function for slide_dbl()\n\nslide_mean() - rolling average shortcut function for slide_dbl()\n\nslide_index_dbl() - applies the rolling window on a numeric column using a separate column to index the window progression (useful if rolling by date with some dates absent)\n\nslide_index_sum() - rolling sum shortcut function with indexing\n\nslide_index_mean() - rolling mean shortcut function with indexing\n\n\nThe slider package has many other functions that are covered in the Resources section of this page. We briefly touch upon the most common.\nCore arguments\n\n.x, the first argument by default, is the vector to iterate over and to apply the function to\n\n.i = for the “index” versions of the slider functions - provide a column to “index” the roll on (see section below)\n\n.f =, the second argument by default, either:\n\nA function, written without parentheses, like mean, or\n\nA formula, which will be converted into a function. For example ~ .x - mean(.x) will return the result of the current value minus the mean of the window’s value\n\nFor more details see this reference material\n\nWindow size\nSpecify the size of the window by using either .before, .after, or both arguments:\n\n.before = - Provide an integer\n\n.after = - Provide an integer\n\n.complete = - Set this to TRUE if you only want calculation performed on complete windows\n\nFor example, to achieve a 7-day window including the current value and the six previous, use .before = 6. To achieve a “centered” window provide the same number to both .before = and .after =.\nBy default, .complete = will be FALSE so if the full window of rows does not exist, the functions will use available rows to perform the calculation. Setting to TRUE restricts so calculations are only performed on complete windows.\nExpanding window\nTo achieve cumulative operations, set the .before = argument to Inf. This will conduct the operation on the current value and all coming before.\n\nRolling by date\nThe most likely use-case of a rolling calculation in applied epidemiology is to examine a metric over time. For example, a rolling measurement of case incidence, based on daily case counts.\nIf you have clean time series data with values for every date, you may be OK to use slide_dbl(), as demonstrated here in the Time series and outbreak detection page.\nHowever, in many applied epidemiology circumstances you may have dates absent from your data, where there are no events recorded. In these cases, it is best to use the “index” versions of the slider functions.\n\n\nIndexed data\nBelow, we show an example using slide_index_dbl() on the case linelist. Let us say that our objective is to calculate a rolling 7-day incidence - the sum of cases using a rolling 7-day window. If you are looking for an example of rolling average, see the section below on grouped rolling.\nTo begin, the dataset daily_counts is created to reflect the daily case counts from the linelist, as calculated with count() from dplyr.\n\n# make dataset of daily counts\ndaily_counts &lt;- linelist %&gt;% \n  count(date_hospitalisation, name = \"new_cases\")\n\nHere is the daily_counts data frame - there are nrow(daily_counts) rows, each day is represented by one row, but especially early in the epidemic some days are not present (there were no cases admitted on those days).\n\n\n\n\n\n\nIt is crucial to recognize that a standard rolling function (like slide_dbl() would use a window of 7 rows, not 7 days. So, if there are any absent dates, some windows will actually extend more than 7 calendar days!\nA “smart” rolling window can be achieved with slide_index_dbl(). The “index” means that the function uses a separate column as an “index” for the rolling window. The window is not simply based on the rows of the data frame.\nIf the index column is a date, you have the added ability to specify the window extent to .before = and/or .after = in units of lubridate days() or months(). If you do these things, the function will include absent days in the windows as if they were there (as NA values).\nLet’s show a comparison. Below, we calculate rolling 7-day case incidence with regular and indexed windows.\n\nrolling &lt;- daily_counts %&gt;% \n  mutate(                                # create new columns\n    # Using slide_dbl()\n    ###################\n    reg_7day = slide_dbl(\n      new_cases,                         # calculate on new_cases\n      .f = ~sum(.x, na.rm = T),          # function is sum() with missing values removed\n      .before = 6),                      # window is the ROW and 6 prior ROWS\n    \n    # Using slide_index_dbl()\n    #########################\n    indexed_7day = slide_index_dbl(\n        new_cases,                       # calculate on new_cases\n        .i = date_hospitalisation,       # indexed with date_onset \n        .f = ~sum(.x, na.rm = TRUE),     # function is sum() with missing values removed\n        .before = days(6))               # window is the DAY and 6 prior DAYS\n    )\n\nObserve how in the regular column for the first 7 rows the count steadily increases despite the rows not being within 7 days of each other! The adjacent “indexed” column accounts for these absent calendar days, so its 7-day sums are much lower, at least in this period of the epidemic when the cases a farther between.\n\n\n\n\n\n\nNow you can plot these data using ggplot():\n\nggplot(data = rolling)+\n  geom_line(mapping = aes(x = date_hospitalisation, y = indexed_7day), size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRolling by group\nIf you group your data prior to using a slider function, the sliding windows will be applied by group. Be careful to arrange your rows in the desired order by group.\nEach time a new group begins, the sliding window will re-start. Therefore, one nuance to be aware of is that if your data are grouped and you have set .complete = TRUE, you will have empty values at each transition between groups. As the function moved downward through the rows, every transition in the grouping column will re-start the accrual of the minimum window size to allow a calculation.\nSee handbook page on Grouping data for details on grouping data.\nBelow, we count linelist cases by date and by hospital. Then we arrange the rows in ascending order, first ordering by hospital and then within that by date. Next we set group_by(). Then we can create our new rolling average.\n\ngrouped_roll &lt;- linelist %&gt;%\n\n  count(hospital, date_hospitalisation, name = \"new_cases\") %&gt;% \n\n  arrange(hospital, date_hospitalisation) %&gt;%   # arrange rows by hospital and then by date\n  \n  group_by(hospital) %&gt;%              # group by hospital \n    \n  mutate(                             # rolling average  \n    mean_7day_hosp = slide_index_dbl(\n      .x = new_cases,                 # the count of cases per hospital-day\n      .i = date_hospitalisation,      # index on date of admission\n      .f = mean,                      # use mean()                   \n      .before = days(6)               # use the day and the 6 days prior\n      )\n  )\n\nHere is the new dataset:\n\n\n\n\n\n\nWe can now plot the moving averages, displaying the data by group by specifying ~ hospital to facet_wrap() in ggplot(). For fun, we plot two geometries - a geom_col() showing the daily case counts and a geom_line() showing the 7-day moving average.\n\nggplot(data = grouped_roll)+\n  geom_col(                       # plot daly case counts as grey bars\n    mapping = aes(\n      x = date_hospitalisation,\n      y = new_cases),\n    fill = \"grey\",\n    width = 1)+\n  geom_line(                      # plot rolling average as line colored by hospital\n    mapping = aes(\n      x = date_hospitalisation,\n      y = mean_7day_hosp,\n      color = hospital),\n    size = 1)+\n  facet_wrap(~hospital, ncol = 2)+ # create mini-plots per hospital\n  theme_classic()+                 # simplify background  \n  theme(legend.position = \"none\")+ # remove legend\n  labs(                            # add plot labels\n    title = \"7-day rolling average of daily case incidence\",\n    x = \"Date of admission\",\n    y = \"Case incidence\")\n\n\n\n\n\n\n\n\nDANGER: If you get an error saying “slide() was deprecated in tsibble 0.9.0 and is now defunct. Please use slider::slide() instead.”, it means that the slide() function from the tsibble package is masking the slide() function from slider package. Fix this by specifying the package in the command, such as slider::slide_dbl().",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Moving averages</span>"
    ]
  },
  {
    "objectID": "new_pages/moving_average.html#calculate-with-tidyquant-within-ggplot",
    "href": "new_pages/moving_average.html#calculate-with-tidyquant-within-ggplot",
    "title": "22  Moving averages",
    "section": "22.3 Calculate with tidyquant within ggplot()",
    "text": "22.3 Calculate with tidyquant within ggplot()\nThe package tidyquant offers another approach to calculating moving averages - this time from within a ggplot() command itself.\nBelow the linelist data are counted by date of onset, and this is plotted as a faded line (alpha &lt; 1). Overlaid on top is a line created with geom_ma() from the package tidyquant, with a set window of 7 days (n = 7) with specified color and thickness.\nBy default geom_ma() uses a simple moving average (ma_fun = \"SMA\"), but other types can be specified, such as:\n\n“EMA” - exponential moving average (more weight to recent observations)\n\n“WMA” - weighted moving average (wts are used to weight observations in the moving average)\n\nOthers can be found in the function documentation\n\n\nlinelist %&gt;% \n  count(date_onset) %&gt;%                 # count cases per day\n  drop_na(date_onset) %&gt;%               # remove cases missing onset date\n  ggplot(aes(x = date_onset, y = n))+   # start ggplot\n    geom_line(                          # plot raw values\n      size = 1,\n      alpha = 0.2                       # semi-transparent line\n      )+             \n    tidyquant::geom_ma(                 # plot moving average\n      n = 7,           \n      size = 1,\n      color = \"blue\")+ \n  theme_minimal()                       # simple background\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\n\n\nSee this vignette for more details on the options available within tidyquant.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Moving averages</span>"
    ]
  },
  {
    "objectID": "new_pages/moving_average.html#resources",
    "href": "new_pages/moving_average.html#resources",
    "title": "22  Moving averages",
    "section": "22.4 Resources",
    "text": "22.4 Resources\nSee the helpful online vignette for the slider package\nThe slider github page\nA slider vignette\ntidyquant vignette\nIf your use case requires that you “skip over” weekends and even holidays, you might like almanac package.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Moving averages</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html",
    "href": "new_pages/time_series.html",
    "title": "23  Time series and outbreak detection",
    "section": "",
    "text": "23.1 Overview\nThis tab demonstrates the use of several packages for time series analysis. It primarily relies on packages from the tidyverts family, but will also use the RECON trending package to fit models that are more appropriate for infectious disease epidemiology.\nNote in the below example we use a dataset from the surveillance package on Campylobacter in Germany (see the data chapter, of the handbook for details). However, if you wanted to run the same code on a dataset with multiple countries or other strata, then there is an example code template for this in the r4epis github repo.\nTopics covered include:",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#overview",
    "href": "new_pages/time_series.html#overview",
    "title": "23  Time series and outbreak detection",
    "section": "",
    "text": "Time series data\nDescriptive analysis\nFitting regressions\nRelation of two time series\nOutbreak detection\nInterrupted time series",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#preparation",
    "href": "new_pages/time_series.html#preparation",
    "title": "23  Time series and outbreak detection",
    "section": "23.2 Preparation",
    "text": "23.2 Preparation\n\nPackages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(rio,          # File import\n               here,         # File locator\n               tsibble,      # handle time series datasets\n               slider,       # for calculating moving averages\n               imputeTS,     # for filling in missing values\n               feasts,       # for time series decomposition and autocorrelation\n               forecast,     # fit sin and cosin terms to data (note: must load after feasts)\n               trending,     # fit and assess models \n               tmaptools,    # for getting geocoordinates (lon/lat) based on place names\n               ecmwfr,       # for interacting with copernicus sateliate CDS API\n               stars,        # for reading in .nc (climate data) files\n               units,        # for defining units of measurement (climate data)\n               yardstick,    # for looking at model accuracy\n               surveillance,  # for aberration detection\n               tidyverse     # data management + ggplot2 graphics\n               )\n\n\n\nLoad data\nYou can download all the data used in this handbook via the instructions in the Download handbook and data page.\nThe example dataset used in this section is weekly counts of campylobacter cases reported in Germany between 2001 and 2011.  You can click here to download this data file (.xlsx).\nThis dataset is a reduced version of the dataset available in the surveillance package. (for details load the surveillance package and see ?campyDE)\nImport these data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import the counts into R\ncounts &lt;- rio::import(\"campylobacter_germany.xlsx\")\n\nThe first 10 rows of the counts are displayed below.\n\n\n\n\n\n\n\n\nClean data\nThe code below makes sure that the date column is in the appropriate format. For this tab we will be using the tsibble package and so the yearweek function will be used to create a calendar week variable. There are several other ways of doing this (see the Working with dates page for details), however for time series its best to keep within one framework (tsibble).\n\n## ensure the date column is in the appropriate format\ncounts$date &lt;- as.Date(counts$date)\n\n## create a calendar week variable \n## fitting ISO definitons of weeks starting on a monday\ncounts &lt;- counts %&gt;% \n     mutate(epiweek = yearweek(date, week_start = 1))\n\n\n\nDownload climate data\nIn the relation of two time series section of this page, we will be comparing campylobacter case counts to climate data.\nClimate data for anywhere in the world can be downloaded from the EU’s Copernicus Satellite. These are not exact measurements, but based on a model (similar to interpolation), however the benefit is global hourly coverage as well as forecasts.\nYou can download each of these climate data files from the Download handbook and data page.\nFor purposes of demonstration here, we will show R code to use the ecmwfr package to pull these data from the Copernicus climate data store. You will need to create a free account in order for this to work. The package website has a useful walkthrough of how to do this. Below is example code of how to go about doing this, once you have the appropriate API keys. You have to replace the X’s below with your account IDs. You will need to download one year of data at a time otherwise the server times-out.\nIf you are not sure of the coordinates for a location you want to download data for, you can use the tmaptools package to pull the coordinates off open street maps. An alternative option is the photon package, however this has not been released on to CRAN yet; the nice thing about photon is that it provides more contextual data for when there are several matches for your search.\n\n## retrieve location coordinates\ncoords &lt;- geocode_OSM(\"Germany\", geometry = \"point\")\n\n## pull together long/lats in format for ERA-5 querying (bounding box) \n## (as just want a single point can repeat coords)\nrequest_coords &lt;- str_glue_data(coords$coords, \"{y}/{x}/{y}/{x}\")\n\n\n## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)\n## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app\n## https://github.com/bluegreen-labs/ecmwfr\n\n## set up key for weather data \nwf_set_key(user = \"XXXXX\",\n           key = \"XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX\",\n           service = \"cds\") \n\n## run for each year of interest (otherwise server times out)\nfor (i in 2002:2011) {\n  \n  ## pull together a query \n  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax\n  ## change request to a list using addin button above (python to list)\n  ## Target is the name of the output file!!\n  request &lt;- request &lt;- list(\n    product_type = \"reanalysis\",\n    format = \"netcdf\",\n    variable = c(\"2m_temperature\", \"total_precipitation\"),\n    year = c(i),\n    month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n    day = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\",\n            \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\",\n            \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\"),\n    time = c(\"00:00\", \"01:00\", \"02:00\", \"03:00\", \"04:00\", \"05:00\", \"06:00\", \"07:00\",\n             \"08:00\", \"09:00\", \"10:00\", \"11:00\", \"12:00\", \"13:00\", \"14:00\", \"15:00\",\n             \"16:00\", \"17:00\", \"18:00\", \"19:00\", \"20:00\", \"21:00\", \"22:00\", \"23:00\"),\n    area = request_coords,\n    dataset_short_name = \"reanalysis-era5-single-levels\",\n    target = paste0(\"germany_weather\", i, \".nc\")\n  )\n  \n  ## download the file and store it in the current working directory\n  file &lt;- wf_request(user     = \"XXXXX\",  # user ID (for authentication)\n                     request  = request,  # the request\n                     transfer = TRUE,     # download the file\n                     path     = here::here(\"data\", \"Weather\")) ## path to save the data\n  }\n\n\n\nLoad climate data\nWhether you downloaded the climate data via our handbook, or used the code above, you now should have 10 years of “.nc” climate data files stored in the same folder on your computer.\nUse the code below to import these files into R with the stars package.\n\n## define path to weather folder \nfile_paths &lt;- list.files(\n  here::here(\"data\", \"time_series\", \"weather\"), # replace with your own file path \n  full.names = TRUE)\n\n## only keep those with the current name of interest \nfile_paths &lt;- file_paths[str_detect(file_paths, \"germany\")]\n\n## read in all the files as a stars object \ndata &lt;- stars::read_stars(file_paths)\n\nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \nt2m, tp, \n\n\nOnce these files have been imported as the object data, we will convert them to a data frame.\n\n## change to a data frame \ntemp_data &lt;- as_tibble(data) %&gt;% \n  ## add in variables and correct units\n  mutate(\n    ## create an calendar week variable \n    epiweek = tsibble::yearweek(time), \n    ## create a date variable (start of calendar week)\n    date = as.Date(epiweek),\n    ## change temperature from kelvin to celsius\n    t2m = set_units(t2m, celsius), \n    ## change precipitation from metres to millimetres \n    tp  = set_units(tp, mm)) %&gt;% \n  ## group by week (keep the date too though)\n  group_by(epiweek, date) %&gt;% \n  ## get the average per week\n  summarise(t2m = as.numeric(mean(t2m)), \n            tp = as.numeric(mean(tp)))\n\n`summarise()` has grouped output by 'epiweek'. You can override using the\n`.groups` argument.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#time-series-data",
    "href": "new_pages/time_series.html#time-series-data",
    "title": "23  Time series and outbreak detection",
    "section": "23.3 Time series data",
    "text": "23.3 Time series data\nThere are a number of different packages for structuring and handling time series data. As said, we will focus on the tidyverts family of packages and so will use the tsibble package to define our time series object. Having a data set defined as a time series object means it is much easier to structure our analysis.\nTo do this we use the tsibble() function and specify the “index”, i.e. the variable specifying the time unit of interest. In our case this is the epiweek variable.\nIf we had a data set with weekly counts by province, for example, we would also be able to specify the grouping variable using the key = argument. This would allow us to do analysis for each group.\n\n## define time series object \ncounts &lt;- tsibble(counts, index = epiweek)\n\nLooking at class(counts) tells you that on top of being a tidy data frame (“tbl_df”, “tbl”, “data.frame”), it has the additional properties of a time series data frame (“tbl_ts”).\nYou can take a quick look at your data by using ggplot2. We see from the plot that there is a clear seasonal pattern, and that there are no missings. However, there seems to be an issue with reporting at the beginning of each year; cases drop in the last week of the year and then increase for the first week of the next year.\n\n## plot a line graph of cases by week\nggplot(counts, aes(x = epiweek, y = case)) + \n     geom_line()\n\n\n\n\n\n\n\n\nDANGER: Most datasets aren’t as clean as this example. You will need to check for duplicates and missings as below. \n\n\nDuplicates\ntsibble does not allow duplicate observations. So each row will need to be unique, or unique within the group (key variable). The package has a few functions that help to identify duplicates. These include are_duplicated() which gives you a TRUE/FALSE vector of whether the row is a duplicate, and duplicates() which gives you a data frame of the duplicated rows.\nSee the page on De-duplication for more details on how to select rows you want.\n\n## get a vector of TRUE/FALSE whether rows are duplicates\nare_duplicated(counts, index = epiweek) \n\n## get a data frame of any duplicated rows \nduplicates(counts, index = epiweek) \n\n\n\n\nMissings\nWe saw from our brief inspection above that there are no missings, but we also saw there seems to be a problem with reporting delay around new year. One way to address this problem could be to set these values to missing and then to impute values. The simplest form of time series imputation is to draw a straight line between the last non-missing and the next non-missing value. To do this we will use the imputeTS package function na_interpolation().\nSee the Missing data page for other options for imputation.\nAnother alternative would be to calculate a moving average, to try and smooth over these apparent reporting issues (see next section, and the page on Moving averages).\n\n## create a variable with missings instead of weeks with reporting issues\ncounts &lt;- counts %&gt;% \n     mutate(case_miss = if_else(\n          ## if epiweek contains 52, 53, 1 or 2\n          str_detect(epiweek, \"W51|W52|W53|W01|W02\"), \n          ## then set to missing \n          NA_real_, \n          ## otherwise keep the value in case\n          case\n     ))\n\n## alternatively interpolate missings by linear trend \n## between two nearest adjacent points\ncounts &lt;- counts %&gt;% \n  mutate(case_int = imputeTS::na_interpolation(case_miss)\n         )\n\n## to check what values have been imputed compared to the original\nggplot_na_imputations(counts$case_miss, counts$case_int) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#descriptive-analysis",
    "href": "new_pages/time_series.html#descriptive-analysis",
    "title": "23  Time series and outbreak detection",
    "section": "23.4 Descriptive analysis",
    "text": "23.4 Descriptive analysis\n\n\nMoving averages\nIf data is very noisy (counts jumping up and down) then it can be helpful to calculate a moving average. In the example below, for each week we calculate the average number of cases from the four previous weeks. This smooths the data, to make it more interpretable. In our case this does not really add much, so we willstick to the interpolated data for further analysis. See the Moving averages page for more detail.\n\n## create a moving average variable (deals with missings)\ncounts &lt;- counts %&gt;% \n     ## create the ma_4w variable \n     ## slide over each row of the case variable\n     mutate(ma_4wk = slider::slide_dbl(case, \n                               ## for each row calculate the name\n                               ~ mean(.x, na.rm = TRUE),\n                               ## use the four previous weeks\n                               .before = 4))\n\n## make a quick visualisation of the difference \nggplot(counts, aes(x = epiweek)) + \n     geom_line(aes(y = case)) + \n     geom_line(aes(y = ma_4wk), colour = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nPeriodicity\nBelow we define a custom function to create a periodogram. See the Writing functions page for information about how to write functions in R.\nFirst, the function is defined. Its arguments include a dataset with a column counts, start_week = which is the first week of the dataset, a number to indicate how many periods per year (e.g. 52, 12), and lastly the output style (see details in the code below).\n\n## Function arguments\n#####################\n## x is a dataset\n## counts is variable with count data or rates within x \n## start_week is the first week in your dataset\n## period is how many units in a year \n## output is whether you want return spectral periodogram or the peak weeks\n  ## \"periodogram\" or \"weeks\"\n\n# Define function\nperiodogram &lt;- function(x, \n                        counts, \n                        start_week = c(2002, 1), \n                        period = 52, \n                        output = \"weeks\") {\n  \n\n    ## make sure is not a tsibble, filter to project and only keep columns of interest\n    prepare_data &lt;- dplyr::as_tibble(x)\n    \n    # prepare_data &lt;- prepare_data[prepare_data[[strata]] == j, ]\n    prepare_data &lt;- dplyr::select(prepare_data, {{counts}})\n    \n    ## create an intermediate \"zoo\" time series to be able to use with spec.pgram\n    zoo_cases &lt;- zoo::zooreg(prepare_data, \n                             start = start_week, frequency = period)\n    \n    ## get a spectral periodogram not using fast fourier transform \n    periodo &lt;- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)\n    \n    ## return the peak weeks \n    periodo_weeks &lt;- 1 / periodo$freq[order(-periodo$spec)] * period\n    \n    if (output == \"weeks\") {\n      periodo_weeks\n    } else {\n      periodo\n    }\n    \n}\n\n## get spectral periodogram for extracting weeks with the highest frequencies \n## (checking of seasonality) \nperiodo &lt;- periodogram(counts, \n                       case_int, \n                       start_week = c(2002, 1),\n                       output = \"periodogram\")\n\n## pull spectrum and frequence in to a dataframe for plotting\nperiodo &lt;- data.frame(periodo$freq, periodo$spec)\n\n## plot a periodogram showing the most frequently occuring periodicity \nggplot(data = periodo, \n                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + \n  geom_line() + \n  labs(x = \"Period (Weeks)\", y = \"Log(density)\")\n\n\n\n\n\n\n\n## get a vector weeks in ascending order \npeak_weeks &lt;- periodogram(counts, \n                          case_int, \n                          start_week = c(2002, 1), \n                          output = \"weeks\")\n\nNOTE: It is possible to use the above weeks to add them to sin and cosine terms, however we will use a function to generate these terms (see regression section below) \n\n\n\nDecomposition\nClassical decomposition is used to break a time series down several parts, which when taken together make up for the pattern you see. These different parts are:\n\nThe trend-cycle (the long-term direction of the data)\n\nThe seasonality (repeating patterns)\n\nThe random (what is left after removing trend and season)\n\n\n## decompose the counts dataset \ncounts %&gt;% \n  # using an additive classical decomposition model\n  model(classical_decomposition(case_int, type = \"additive\")) %&gt;% \n  ## extract the important information from the model\n  components() %&gt;% \n  ## generate a plot \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation\nAutocorrelation tells you about the relation between the counts of each week and the weeks before it (called lags).\nUsing the ACF() function, we can produce a plot which shows us a number of lines for the relation at different lags. Where the lag is 0 (x = 0), this line would always be 1 as it shows the relation between an observation and itself (not shown here). The first line shown here (x = 1) shows the relation between each observation and the observation before it (lag of 1), the second shows the relation between each observation and the observation before last (lag of 2) and so on until lag of 52 which shows the relation between each observation and the observation from 1 year (52 weeks before).\nUsing the PACF() function (for partial autocorrelation) shows the same type of relation but adjusted for all other weeks between. This is less informative for determining periodicity.\n\n## using the counts dataset\ncounts %&gt;% \n  ## calculate autocorrelation using a full years worth of lags\n  ACF(case_int, lag_max = 52) %&gt;% \n  ## show a plot\n  autoplot()\n\n\n\n\n\n\n\n## using the counts data set \ncounts %&gt;% \n  ## calculate the partial autocorrelation using a full years worth of lags\n  PACF(case_int, lag_max = 52) %&gt;% \n  ## show a plot\n  autoplot()\n\n\n\n\n\n\n\n\nYou can formally test the null hypothesis of independence in a time series (i.e.  that it is not autocorrelated) using the Ljung-Box test (in the stats package). A significant p-value suggests that there is autocorrelation in the data.\n\n## test for independance \nBox.test(counts$case_int, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  counts$case_int\nX-squared = 462.65, df = 1, p-value &lt; 2.2e-16",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#fitting-regressions",
    "href": "new_pages/time_series.html#fitting-regressions",
    "title": "23  Time series and outbreak detection",
    "section": "23.5 Fitting regressions",
    "text": "23.5 Fitting regressions\nIt is possible to fit a large number of different regressions to a time series, however, here we will demonstrate how to fit a negative binomial regression - as this is often the most appropriate for counts data in infectious diseases.\n\n\nFourier terms\nFourier terms are the equivalent of sin and cosin curves. The difference is that these are fit based on finding the most appropriate combination of curves to explain your data.\nIf only fitting one fourier term, this would be the equivalent of fitting a sin and a cosin for your most frequently occurring lag seen in your periodogram (in our case 52 weeks). We use the fourier() function from the forecast package.\nIn the below code we assign using the $, as fourier() returns two columns (one for sin one for cosin) and so these are added to the dataset as a list, called “fourier” - but this list can then be used as a normal variable in regression.\n\n## add in fourier terms using the epiweek and case_int variabless\ncounts$fourier &lt;- select(counts, epiweek, case_int) %&gt;% \n  fourier(K = 1)\n\n\n\n\nNegative binomial\nIt is possible to fit regressions using base stats or MASS functions (e.g. lm(), glm() and glm.nb()). However we will be using those from the trending package, as this allows for calculating appropriate confidence and prediction intervals (which are otherwise not available). The syntax is the same, and you specify an outcome variable then a tilde (~) and then add your various exposure variables of interest separated by a plus (+).\nThe other difference is that we first define the model and then fit() it to the data. This is useful because it allows for comparing multiple different models with the same syntax.\nTIP: If you wanted to use rates, rather than counts you could include the population variable as a logarithmic offset term, by adding offset(log(population). You would then need to set population to be 1, before using predict() in order to produce a rate. \nTIP: For fitting more complex models such as ARIMA or prophet, see the fable package.\n\n## define the model you want to fit (negative binomial) \nmodel &lt;- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the fourier terms to account for seasonality\n    fourier)\n\n## fit your model using the counts dataset\nfitted_model &lt;- trending::fit(model, data.frame(counts))\n\n## calculate confidence intervals and prediction intervals \nobserved &lt;- predict(fitted_model, simulate_pi = FALSE)\n\nestimate_res &lt;- data.frame(observed$result)\n\n## plot your regression \nggplot(data = estimate_res, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"Red\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nResiduals\nTo see how well our model fits the observed data we need to look at the residuals. The residuals are the difference between the observed counts and the counts estimated from the model. We could calculate this simply by using case_int - estimate, but the residuals() function extracts this directly from the regression for us.\nWhat we see from the below, is that we are not explaining all of the variation that we could with the model. It might be that we should fit more fourier terms, and address the amplitude. However for this example we will leave it as is. The plots show that our model does worse in the peaks and troughs (when counts are at their highest and lowest) and that it might be more likely to underestimate the observed counts.\n\n## calculate the residuals \nestimate_res &lt;- estimate_res %&gt;% \n  mutate(resid = fitted_model$result[[1]]$residuals)\n\n## are the residuals fairly constant over time (if not: outbreaks? change in practice?)\nestimate_res %&gt;%\n  ggplot(aes(x = epiweek, y = resid)) +\n  geom_line() +\n  geom_point() + \n  labs(x = \"epiweek\", y = \"Residuals\")\n\n\n\n\n\n\n\n## is there autocorelation in the residuals (is there a pattern to the error?)  \nestimate_res %&gt;% \n  as_tsibble(index = epiweek) %&gt;% \n  ACF(resid, lag_max = 52) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n## are residuals normally distributed (are under or over estimating?)  \nestimate_res %&gt;%\n  ggplot(aes(x = resid)) +\n  geom_histogram(binwidth = 100) +\n  geom_rug() +\n  labs(y = \"count\") \n\n\n\n\n\n\n\n## compare observed counts to their residuals \n  ## should also be no pattern \nestimate_res %&gt;%\n  ggplot(aes(x = estimate, y = resid)) +\n  geom_point() +\n  labs(x = \"Fitted\", y = \"Residuals\")\n\n\n\n\n\n\n\n## formally test autocorrelation of the residuals\n## H0 is that residuals are from a white-noise series (i.e. random)\n## test for independence \n## if p value significant then non-random\nBox.test(estimate_res$resid, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  estimate_res$resid\nX-squared = 336.25, df = 1, p-value &lt; 2.2e-16",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#relation-of-two-time-series",
    "href": "new_pages/time_series.html#relation-of-two-time-series",
    "title": "23  Time series and outbreak detection",
    "section": "23.6 Relation of two time series",
    "text": "23.6 Relation of two time series\nHere we look at using weather data (specifically the temperature) to explain campylobacter case counts.\n\n\nMerging datasets\nWe can join our datasets using the week variable. For more on merging see the handbook section on joining.\n\n## left join so that we only have the rows already existing in counts\n## drop the date variable from temp_data (otherwise is duplicated)\ncounts &lt;- left_join(counts, \n                    select(temp_data, -date),\n                    by = \"epiweek\")\n\n\n\n\nDescriptive analysis\nFirst plot your data to see if there is any obvious relation. The plot below shows that there is a clear relation in the seasonality of the two variables, and that temperature might peak a few weeks before the case number. For more on pivoting data, see the handbook section on pivoting data.\n\ncounts %&gt;% \n  ## keep the variables we are interested \n  select(epiweek, case_int, t2m) %&gt;% \n  ## change your data in to long format\n  pivot_longer(\n    ## use epiweek as your key\n    !epiweek,\n    ## move column names to the new \"measure\" column\n    names_to = \"measure\", \n    ## move cell values to the new \"values\" column\n    values_to = \"value\") %&gt;% \n  ## create a plot with the dataset above\n  ## plot epiweek on the x axis and values (counts/celsius) on the y \n  ggplot(aes(x = epiweek, y = value)) + \n    ## create a separate plot for temperate and case counts \n    ## let them set their own y-axes\n    facet_grid(measure ~ ., scales = \"free_y\") +\n    ## plot both as a line\n    geom_line()\n\n\n\n\n\n\n\n\n\n\n\nLags and cross-correlation\nTo formally test which weeks are most highly related between cases and temperature. We can use the cross-correlation function (CCF()) from the feasts package. You could also visualise (rather than using arrange) using the autoplot() function.\n\ncounts %&gt;% \n  ## calculate cross-correlation between interpolated counts and temperature\n  CCF(case_int, t2m,\n      ## set the maximum lag to be 52 weeks\n      lag_max = 52, \n      ## return the correlation coefficient \n      type = \"correlation\") %&gt;% \n  ## arange in decending order of the correlation coefficient \n  ## show the most associated lags\n  arrange(-ccf) %&gt;% \n  ## only show the top ten \n  slice_head(n = 10)\n\n# A tsibble: 10 x 2 [1W]\n        lag   ccf\n   &lt;cf_lag&gt; &lt;dbl&gt;\n 1      -4W 0.749\n 2      -5W 0.745\n 3      -3W 0.735\n 4      -6W 0.729\n 5      -2W 0.727\n 6      -7W 0.704\n 7      -1W 0.695\n 8      -8W 0.671\n 9       0W 0.649\n10      47W 0.638\n\n\nWe see from this that a lag of 4 weeks is most highly correlated, so we make a lagged temperature variable to include in our regression.\nDANGER: Note that the first four weeks of our data in the lagged temperature variable are missing (NA) - as there are not four weeks prior to get data from. In order to use this dataset with the trending predict() function, we need to use the the simulate_pi = FALSE argument within predict() further down. If we did want to use the simulate option, then we have to drop these missings and store as a new data set by adding drop_na(t2m_lag4) to the code chunk below.\n\ncounts &lt;- counts %&gt;% \n  ## create a new variable for temperature lagged by four weeks\n  mutate(t2m_lag4 = lag(t2m, n = 4))\n\n\n\n\nNegative binomial with two variables\nWe fit a negative binomial regression as done previously. This time we add the temperature variable lagged by four weeks.\nCAUTION: Note the use of simulate_pi = FALSE within the predict() argument. This is because the default behaviour of trending is to use the ciTools package to estimate a prediction interval. This does not work if there are NA counts, and also produces more granular intervals. See ?trending::predict.trending_model_fit for details. \n\n## define the model you want to fit (negative binomial) \nmodel &lt;- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the fourier terms to account for seasonality\n    fourier + \n    ## use the temperature lagged by four weeks \n    t2m_lag4\n    )\n\n## fit your model using the counts dataset\nfitted_model &lt;- trending::fit(model, data.frame(counts))\n\n## calculate confidence intervals and prediction intervals \nobserved &lt;- predict(fitted_model, simulate_pi = FALSE)\n\nTo investigate the individual terms, we can pull the original negative binomial regression out of the trending format using get_model() and pass this to the broom package tidy() function to retrieve exponentiated estimates and associated confidence intervals.\nWhat this shows us is that lagged temperature, after controlling for trend and seasonality, is similar to the case counts (estimate ~ 1) and significantly associated. This suggests that it might be a good variable for use in predicting future case numbers (as climate forecasts are readily available).\n\nfitted_model %&gt;% \n  ## extract original negative binomial regression\n  get_fitted_model() #%&gt;% \n\n[[1]]\n\nCall:  glm.nb(formula = case_int ~ epiweek + fourier + t2m_lag4, data = data.frame(counts), \n    init.theta = 32.80689607, link = log)\n\nCoefficients:\n (Intercept)       epiweek  fourierS1-52  fourierC1-52      t2m_lag4  \n   5.825e+00     8.464e-05    -2.850e-01    -1.954e-01     6.672e-03  \n\nDegrees of Freedom: 504 Total (i.e. Null);  500 Residual\n  (4 observations deleted due to missingness)\nNull Deviance:      2015 \nResidual Deviance: 508.2    AIC: 6784\n\n  ## get a tidy dataframe of results\n  #tidy(exponentiate = TRUE, \n  #     conf.int = TRUE)\n\nA quick visual inspection of the model shows that it might do a better job of estimating the observed case counts.\n\nestimate_res &lt;- data.frame(observed$result)\n\n## plot your regression \nggplot(data = estimate_res, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"Red\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n\n\n\n\n\n\n\n\n\nResiduals\nWe investigate the residuals again to see how well our model fits the observed data. The results and interpretation here are similar to those of the previous regression, so it may be more feasible to stick with the simpler model without temperature.\n\n## calculate the residuals \nestimate_res &lt;- estimate_res %&gt;% \n  mutate(resid = case_int - estimate)\n\n## are the residuals fairly constant over time (if not: outbreaks? change in practice?)\nestimate_res %&gt;%\n  ggplot(aes(x = epiweek, y = resid)) +\n  geom_line() +\n  geom_point() + \n  labs(x = \"epiweek\", y = \"Residuals\")\n\n\n\n\n\n\n\n## is there autocorelation in the residuals (is there a pattern to the error?)  \nestimate_res %&gt;% \n  as_tsibble(index = epiweek) %&gt;% \n  ACF(resid, lag_max = 52) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n## are residuals normally distributed (are under or over estimating?)  \nestimate_res %&gt;%\n  ggplot(aes(x = resid)) +\n  geom_histogram(binwidth = 100) +\n  geom_rug() +\n  labs(y = \"count\") \n\n\n\n\n\n\n\n## compare observed counts to their residuals \n  ## should also be no pattern \nestimate_res %&gt;%\n  ggplot(aes(x = estimate, y = resid)) +\n  geom_point() +\n  labs(x = \"Fitted\", y = \"Residuals\")\n\n\n\n\n\n\n\n## formally test autocorrelation of the residuals\n## H0 is that residuals are from a white-noise series (i.e. random)\n## test for independence \n## if p value significant then non-random\nBox.test(estimate_res$resid, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  estimate_res$resid\nX-squared = 339.52, df = 1, p-value &lt; 2.2e-16",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#outbreak-detection",
    "href": "new_pages/time_series.html#outbreak-detection",
    "title": "23  Time series and outbreak detection",
    "section": "23.7 Outbreak detection",
    "text": "23.7 Outbreak detection\nWe will demonstrate two (similar) methods of detecting outbreaks here. The first builds on the sections above. We use the trending package to fit regressions to previous years, and then predict what we expect to see in the following year. If observed counts are above what we expect, then it could suggest there is an outbreak. The second method is based on similar principles but uses the surveillance package, which has a number of different algorithms for aberration detection.\nCAUTION: Normally, you are interested in the current year (where you only know counts up to the present week). So in this example we are pretending to be in week 39 of 2011.\n\n\ntrending package\nFor this method we define a baseline (which should usually be about 5 years of data). We fit a regression to the baseline data, and then use that to predict the estimates for the next year.\n\n\nCut-off date\nIt is easier to define your dates in one place and then use these throughout the rest of your code.\nHere we define a start date (when our observations started) and a cut-off date (the end of our baseline period - and when the period we want to predict for starts). ~We also define how many weeks are in our year of interest (the one we are going to be predicting)~. We also define how many weeks are between our baseline cut-off and the end date that we are interested in predicting for.\nNOTE: In this example we pretend to currently be at the end of September 2011 (“2011 W39”).\n\n## define start date (when observations began)\nstart_date &lt;- min(counts$epiweek)\n\n## define a cut-off week (end of baseline, start of prediction period)\ncut_off &lt;- yearweek(\"2010-12-31\")\n\n## define the last date interested in (i.e. end of prediction)\nend_date &lt;- yearweek(\"2011-12-31\")\n\n## find how many weeks in period (year) of interest\nnum_weeks &lt;- as.numeric(end_date - cut_off)\n\n\n\n\nAdd rows\nTo be able to forecast in a tidyverse format, we need to have the right number of rows in our dataset, i.e. one row for each week up to the end_datedefined above. The code below allows you to add these rows for by a grouping variable - for example if we had multiple countries in one dataset, we could group by country and then add rows appropriately for each. The group_by_key() function from tsibble allows us to do this grouping and then pass the grouped data to dplyr functions, group_modify() and add_row(). Then we specify the sequence of weeks between one after the maximum week currently available in the data and the end week.\n\n## add in missing weeks till end of year \ncounts &lt;- counts %&gt;%\n  ## group by the region\n  group_by_key() %&gt;%\n  ## for each group add rows from the highest epiweek to the end of year\n  group_modify(~add_row(.,\n                        epiweek = seq(max(.$epiweek) + 1, \n                                      end_date,\n                                      by = 1)))\n\n\n\n\nFourier terms\nWe need to redefine our fourier terms - as we want to fit them to the baseline date only and then predict (extrapolate) those terms for the next year. To do this we need to combine two output lists from the fourier() function together; the first one is for the baseline data, and the second one predicts for the year of interest (by defining the h argument).\nN.b. to bind rows we have to use rbind() (rather than tidyverse bind_rows) as the fourier columns are a list (so not named individually).\n\n## define fourier terms (sincos) \ncounts &lt;- counts %&gt;% \n  mutate(\n    ## combine fourier terms for weeks prior to  and after 2010 cut-off date\n    ## (nb. 2011 fourier terms are predicted)\n    fourier = rbind(\n      ## get fourier terms for previous years\n      fourier(\n        ## only keep the rows before 2011\n        filter(counts, \n               epiweek &lt;= cut_off), \n        ## include one set of sin cos terms \n        K = 1\n        ), \n      ## predict the fourier terms for 2011 (using baseline data)\n      fourier(\n        ## only keep the rows before 2011\n        filter(counts, \n               epiweek &lt;= cut_off),\n        ## include one set of sin cos terms \n        K = 1, \n        ## predict 52 weeks ahead\n        h = num_weeks\n        )\n      )\n    )\n\n\n\n\nSplit data and fit regression\nWe now have to split our dataset in to the baseline period and the prediction period. This is done using the dplyr group_split() function after group_by(), and will create a list with two data frames, one for before your cut-off and one for after.\nWe then use the purrr package pluck() function to pull the datasets out of the list (equivalent of using square brackets, e.g. dat[[1]]), and can then fit our model to the baseline data, and then use the predict() function for our data of interest after the cut-off.\nSee the page on Iteration, loops, and lists to learn more about purrr.\nCAUTION: Note the use of simulate_pi = FALSE within the predict() argument. This is because the default behaviour of trending is to use the ciTools package to estimate a prediction interval. This does not work if there are NA counts, and also produces more granular intervals. See ?trending::predict.trending_model_fit for details. \n\n# split data for fitting and prediction\ndat &lt;- counts %&gt;% \n  group_by(epiweek &lt;= cut_off) %&gt;%\n  group_split()\n\n## define the model you want to fit (negative binomial) \nmodel &lt;- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier\n)\n\n# define which data to use for fitting and which for predicting\nfitting_data &lt;- pluck(dat, 2)\npred_data &lt;- pluck(dat, 1) %&gt;% \n  select(case_int, epiweek, fourier)\n\n# fit model \nfitted_model &lt;- trending::fit(model, data.frame(fitting_data))\n\n# get confint and estimates for fitted data\nobserved &lt;- fitted_model %&gt;% \n  predict(simulate_pi = FALSE)\n\n# forecast with data want to predict with \nforecasts &lt;- fitted_model %&gt;% \n  predict(data.frame(pred_data), simulate_pi = FALSE)\n\n## combine baseline and predicted datasets\nobserved &lt;- bind_rows(observed$result, forecasts$result)\n\nAs previously, we can visualise our model with ggplot. We highlight alerts with red dots for observed counts above the 95% prediction interval. This time we also add a vertical line to label when the forecast starts.\n\n## plot your regression \nggplot(data = observed, aes(x = epiweek)) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate),\n            col = \"grey\") + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add in a line for your observed case counts\n  geom_line(aes(y = case_int), \n            col = \"black\") + \n  ## plot in points for the observed counts above expected\n  geom_point(\n    data = filter(observed, case_int &gt; upper_pi), \n    aes(y = case_int), \n    colour = \"red\", \n    size = 2) + \n  ## add vertical line and label to show where forecasting started\n  geom_vline(\n           xintercept = as.Date(cut_off), \n           linetype = \"dashed\") + \n  annotate(geom = \"text\", \n           label = \"Forecast\", \n           x = cut_off, \n           y = max(observed$upper_pi) - 250, \n           angle = 90, \n           vjust = 1\n           ) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n\nWarning: Removed 13 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction validation\nBeyond inspecting residuals, it is important to investigate how good your model is at predicting cases in the future. This gives you an idea of how reliable your threshold alerts are.\nThe traditional way of validating is to see how well you can predict the latest year before the present one (because you don’t yet know the counts for the “current year”). For example in our data set we would use the data from 2002 to 2009 to predict 2010, and then see how accurate those predictions are. Then refit the model to include 2010 data and use that to predict 2011 counts.\nAs can be seen in the figure below by Hyndman et al in “Forecasting principles and practice”.\n figure reproduced with permission from the authors\nThe downside of this is that you are not using all the data available to you, and it is not the final model that you are using for prediction.\nAn alternative is to use a method called cross-validation. In this scenario you roll over all of the data available to fit multiple models to predict one year ahead. You use more and more data in each model, as seen in the figure below from the same [Hyndman et al text]((https://otexts.com/fpp3/). For example, the first model uses 2002 to predict 2003, the second uses 2002 and 2003 to predict 2004, and so on.  figure reproduced with permission from the authors\nIn the below we use purrr package map() function to loop over each dataset. We then put estimates in one data set and merge with the original case counts, to use the yardstick package to compute measures of accuracy. We compute four measures including: Root mean squared error (RMSE), Mean absolute error (MAE), Mean absolute scaled error (MASE), Mean absolute percent error (MAPE).\nCAUTION: Note the use of simulate_pi = FALSE within the predict() argument. This is because the default behaviour of trending is to use the ciTools package to estimate a prediction interval. This does not work if there are NA counts, and also produces more granular intervals. See ?trending::predict.trending_model_fit for details. \n\n## Cross validation: predicting week(s) ahead based on sliding window\n\n## expand your data by rolling over in 52 week windows (before + after) \n## to predict 52 week ahead\n## (creates longer and longer chains of observations - keeps older data)\n\n## define window want to roll over\nroll_window &lt;- 52\n\n## define weeks ahead want to predict \nweeks_ahead &lt;- 52\n\n## create a data set of repeating, increasingly long data\n## label each data set with a unique id\n## only use cases before year of interest (i.e. 2011)\ncase_roll &lt;- counts %&gt;% \n  filter(epiweek &lt; cut_off) %&gt;% \n  ## only keep the week and case counts variables\n  select(epiweek, case_int) %&gt;% \n    ## drop the last x observations \n    ## depending on how many weeks ahead forecasting \n    ## (otherwise will be an actual forecast to \"unknown\")\n    slice(1:(n() - weeks_ahead)) %&gt;%\n    as_tsibble(index = epiweek) %&gt;% \n    ## roll over each week in x after windows to create grouping ID \n    ## depending on what rolling window specify\n    stretch_tsibble(.init = roll_window, .step = 1) %&gt;% \n  ## drop the first couple - as have no \"before\" cases\n  filter(.id &gt; roll_window)\n\n\n## for each of the unique data sets run the code below\nforecasts &lt;- purrr::map(unique(case_roll$.id), \n                        function(i) {\n  \n  ## only keep the current fold being fit \n  mini_data &lt;- filter(case_roll, .id == i) %&gt;% \n    as_tibble()\n  \n  ## create an empty data set for forecasting on \n  forecast_data &lt;- tibble(\n    epiweek = seq(max(mini_data$epiweek) + 1,\n                  max(mini_data$epiweek) + weeks_ahead,\n                  by = 1),\n    case_int = rep.int(NA, weeks_ahead),\n    .id = rep.int(i, weeks_ahead)\n  )\n  \n  ## add the forecast data to the original \n  mini_data &lt;- bind_rows(mini_data, forecast_data)\n  \n  ## define the cut off based on latest non missing count data \n  cv_cut_off &lt;- mini_data %&gt;% \n    ## only keep non-missing rows\n    drop_na(case_int) %&gt;% \n    ## get the latest week\n    summarise(max(epiweek)) %&gt;% \n    ## extract so is not in a dataframe\n    pull()\n  \n  ## make mini_data back in to a tsibble\n  mini_data &lt;- tsibble(mini_data, index = epiweek)\n  \n  ## define fourier terms (sincos) \n  mini_data &lt;- mini_data %&gt;% \n    mutate(\n    ## combine fourier terms for weeks prior to  and after cut-off date\n    fourier = rbind(\n      ## get fourier terms for previous years\n      forecast::fourier(\n        ## only keep the rows before cut-off\n        filter(mini_data, \n               epiweek &lt;= cv_cut_off), \n        ## include one set of sin cos terms \n        K = 1\n        ), \n      ## predict the fourier terms for following year (using baseline data)\n      fourier(\n        ## only keep the rows before cut-off\n        filter(mini_data, \n               epiweek &lt;= cv_cut_off),\n        ## include one set of sin cos terms \n        K = 1, \n        ## predict 52 weeks ahead\n        h = weeks_ahead\n        )\n      )\n    )\n  \n  \n  # split data for fitting and prediction\n  dat &lt;- mini_data %&gt;% \n    group_by(epiweek &lt;= cv_cut_off) %&gt;%\n    group_split()\n\n  ## define the model you want to fit (negative binomial) \n  model &lt;- glm_nb_model(\n    ## set number of cases as outcome of interest\n    case_int ~\n      ## use epiweek to account for the trend\n      epiweek +\n      ## use the furier terms to account for seasonality\n      fourier\n  )\n\n  # define which data to use for fitting and which for predicting\n  fitting_data &lt;- pluck(dat, 2)\n  pred_data &lt;- pluck(dat, 1)\n  \n  # fit model \n  fitted_model &lt;- trending::fit(model, fitting_data)\n  \n  # forecast with data want to predict with \n  forecasts &lt;- fitted_model %&gt;% \n    predict(data.frame(pred_data), simulate_pi = FALSE)\n  forecasts &lt;- data.frame(forecasts$result[[1]]) %&gt;% \n       ## only keep the week and the forecast estimate\n    select(epiweek, estimate)\n    \n  }\n  )\n\n## make the list in to a data frame with all the forecasts\nforecasts &lt;- bind_rows(forecasts)\n\n## join the forecasts with the observed\nforecasts &lt;- left_join(forecasts, \n                       select(counts, epiweek, case_int),\n                       by = \"epiweek\")\n\n## using {yardstick} compute metrics\n  ## RMSE: Root mean squared error\n  ## MAE:  Mean absolute error  \n  ## MASE: Mean absolute scaled error\n  ## MAPE: Mean absolute percent error\nmodel_metrics &lt;- bind_rows(\n  ## in your forcasted dataset compare the observed to the predicted\n  rmse(forecasts, case_int, estimate), \n  mae( forecasts, case_int, estimate),\n  mase(forecasts, case_int, estimate),\n  mape(forecasts, case_int, estimate),\n  ) %&gt;% \n  ## only keep the metric type and its output\n  select(Metric  = .metric, \n         Measure = .estimate) %&gt;% \n  ## make in to wide format so can bind rows after\n  pivot_wider(names_from = Metric, values_from = Measure)\n\n## return model metrics \nmodel_metrics\n\n# A tibble: 1 × 4\n   rmse   mae  mase  mape\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  252.  199.  1.96  17.3\n\n\n\n\n\n\nsurveillance package\nIn this section we use the surveillance package to create alert thresholds based on outbreak detection algorithms. There are several different methods available in the package, however we will focus on two options here. For details, see these papers on the application and theory of the alogirthms used.\nThe first option uses the improved Farrington method. This fits a negative binomial glm (including trend) and down-weights past outbreaks (outliers) to create a threshold level.\nThe second option use the glrnb method. This also fits a negative binomial glm but includes trend and fourier terms (so is favoured here). The regression is used to calculate the “control mean” (~fitted values) - it then uses a computed generalized likelihood ratio statistic to assess if there is shift in the mean for each week. Note that the threshold for each week takes in to account previous weeks so if there is a sustained shift an alarm will be triggered. (Also note that after each alarm the algorithm is reset)\nIn order to work with the surveillance package, we first need to define a “surveillance time series” object (using the sts() function) to fit within the framework.\n\n## define surveillance time series object\n## nb. you can include a denominator with the population object (see ?sts)\ncounts_sts &lt;- sts(observed = counts$case_int[!is.na(counts$case_int)],\n                  start = c(\n                    ## subset to only keep the year from start_date \n                    as.numeric(str_sub(start_date, 1, 4)), \n                    ## subset to only keep the week from start_date\n                    as.numeric(str_sub(start_date, 7, 8))), \n                  ## define the type of data (in this case weekly)\n                  freq = 52)\n\n## define the week range that you want to include (ie. prediction period)\n## nb. the sts object only counts observations without assigning a week or \n## year identifier to them - so we use our data to define the appropriate observations\nweekrange &lt;- cut_off - start_date\n\n\n\nFarrington method\nWe then define each of our parameters for the Farrington method in a list. Then we run the algorithm using farringtonFlexible() and then we can extract the threshold for an alert using farringtonmethod@upperboundto include this in our dataset. It is also possible to extract a TRUE/FALSE for each week if it triggered an alert (was above the threshold) using farringtonmethod@alarm.\n\n## define control\nctrl &lt;- list(\n  ## define what time period that want threshold for (i.e. 2011)\n  range = which(counts_sts@epoch &gt; weekrange),\n  b = 9, ## how many years backwards for baseline\n  w = 2, ## rolling window size in weeks\n  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)\n  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)\n  trend = TRUE,\n  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)\n  thresholdMethod = \"nbPlugin\",\n  populationOffset = TRUE\n  )\n\n## apply farrington flexible method\nfarringtonmethod &lt;- farringtonFlexible(counts_sts, ctrl)\n\n## create a new variable in the original dataset called threshold\n## containing the upper bound from farrington \n## nb. this is only for the weeks in 2011 (so need to subset rows)\ncounts[which(counts$epiweek &gt;= cut_off & \n               !is.na(counts$case_int)),\n              \"threshold\"] &lt;- farringtonmethod@upperbound\n\nWe can then visualise the results in ggplot as done previously.\n\nggplot(counts, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in upper bound of aberration algorithm\n  geom_line(aes(y = threshold, colour = \"Alert threshold\"), \n            linetype = \"dashed\", \n            size = 1.5) +\n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Alert threshold\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic() + \n  ## remove title of legend \n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nGLRNB method\nSimilarly for the GLRNB method we define each of our parameters for the in a list, then fit the algorithm and extract the upper bounds.\nCAUTION: This method uses “brute force” (similar to bootstrapping) for calculating thresholds, so can take a long time!\nSee the GLRNB vignette for details.\n\n## define control options\nctrl &lt;- list(\n  ## define what time period that want threshold for (i.e. 2011)\n  range = which(counts_sts@epoch &gt; weekrange),\n  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include\n  trend = TRUE,   ## whether to include trend or not\n  refit = FALSE), ## whether to refit model after each alarm\n  ## cARL = threshold for GLR statistic (arbitrary)\n     ## 3 ~ middle ground for minimising false positives\n     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)\n   c.ARL = 2,\n   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak\n   ret = \"cases\"     ## return threshold upperbound as case counts\n  )\n\n## apply the glrnb method\nglrnbmethod &lt;- glrnb(counts_sts, control = ctrl, verbose = FALSE)\n\n## create a new variable in the original dataset called threshold\n## containing the upper bound from glrnb \n## nb. this is only for the weeks in 2011 (so need to subset rows)\ncounts[which(counts$epiweek &gt;= cut_off & \n               !is.na(counts$case_int)),\n              \"threshold_glrnb\"] &lt;- glrnbmethod@upperbound\n\nVisualise the outputs as previously.\n\nggplot(counts, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in upper bound of aberration algorithm\n  geom_line(aes(y = threshold_glrnb, colour = \"Alert threshold\"), \n            linetype = \"dashed\", \n            size = 1.5) +\n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Alert threshold\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic() + \n  ## remove title of legend \n  theme(legend.title = element_blank())",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#interrupted-timeseries",
    "href": "new_pages/time_series.html#interrupted-timeseries",
    "title": "23  Time series and outbreak detection",
    "section": "23.8 Interrupted timeseries",
    "text": "23.8 Interrupted timeseries\nInterrupted timeseries (also called segmented regression or intervention analysis), is often used in assessing the impact of vaccines on the incidence of disease. But it can be used for assessing impact of a wide range of interventions or introductions. For example changes in hospital procedures or the introduction of a new disease strain to a population. In this example we will pretend that a new strain of Campylobacter was introduced to Germany at the end of 2008, and see if that affects the number of cases. We will use negative binomial regression again. The regression this time will be split in to two parts, one before the intervention (or introduction of new strain here) and one after (the pre and post-periods). This allows us to calculate an incidence rate ratio comparing the two time periods. Explaining the equation might make this clearer (if not then just ignore!).\nThe negative binomial regression can be defined as follows:\n\\[\\log(Y_t)= β_0 + β_1 \\times t+ β_2 \\times δ(t-t_0) + β_3\\times(t-t_0 )^+ + log(pop_t) + e_t\\]\nWhere: \\(Y_t\\)is the number of cases observed at time \\(t\\)\n\\(pop_t\\) is the population size in 100,000s at time \\(t\\) (not used here)\n\\(t_0\\) is the last year of the of the pre-period (including transition time if any)\n\\(δ(x\\) is the indicator function (it is 0 if x≤0 and 1 if x&gt;0)\n\\((x)^+\\) is the cut off operator (it is x if x&gt;0 and 0 otherwise)\n\\(e_t\\) denotes the residual Additional terms trend and season can be added as needed.\n\\(β_2 \\times δ(t-t_0) + β_3\\times(t-t_0 )^+\\) is the generalised linear part of the post-period and is zero in the pre-period. This means that the \\(β_2\\) and \\(β_3\\) estimates are the effects of the intervention.\nWe need to re-calculate the fourier terms without forecasting here, as we will use all the data available to us (i.e. retrospectively). Additionally we need to calculate the extra terms needed for the regression.\n\n## add in fourier terms using the epiweek and case_int variabless\ncounts$fourier &lt;- select(counts, epiweek, case_int) %&gt;% \n  as_tsibble(index = epiweek) %&gt;% \n  fourier(K = 1)\n\n## define intervention week \nintervention_week &lt;- yearweek(\"2008-12-31\")\n\n## define variables for regression \ncounts &lt;- counts %&gt;% \n  mutate(\n    ## corresponds to t in the formula\n      ## count of weeks (could probably also just use straight epiweeks var)\n    # linear = row_number(epiweek), \n    ## corresponds to delta(t-t0) in the formula\n      ## pre or post intervention period\n    intervention = as.numeric(epiweek &gt;= intervention_week), \n    ## corresponds to (t-t0)^+ in the formula\n      ## count of weeks post intervention\n      ## (choose the larger number between 0 and whatever comes from calculation)\n    time_post = pmax(0, epiweek - intervention_week + 1))\n\nWe then use these terms to fit a negative binomial regression, and produce a table with percentage change. What this example shows is that there was no significant change.\nCAUTION: Note the use of simulate_pi = FALSE within the predict() argument. This is because the default behaviour of trending is to use the ciTools package to estimate a prediction interval. This does not work if there are NA counts, and also produces more granular intervals. See ?trending::predict.trending_model_fit for details. \n\n## define the model you want to fit (negative binomial) \nmodel &lt;- glm_nb_model(\n  ## set number of cases as outcome of interest\n  case_int ~\n    ## use epiweek to account for the trend\n    epiweek +\n    ## use the furier terms to account for seasonality\n    fourier + \n    ## add in whether in the pre- or post-period \n    intervention + \n    ## add in the time post intervention \n    time_post\n    )\n\n## fit your model using the counts dataset\nfitted_model &lt;- trending::fit(model, counts)\n\n## calculate confidence intervals and prediction intervals \nobserved &lt;- predict(fitted_model, simulate_pi = FALSE)\n\n\n## show estimates and percentage change in a table\nfitted_model %&gt;% \n  ## extract original negative binomial regression\n  get_model() %&gt;% \n  ## get a tidy dataframe of results\n  tidy(exponentiate = TRUE, \n       conf.int = TRUE) %&gt;% \n  ## only keep the intervention value \n  filter(term == \"intervention\") %&gt;% \n  ## change the IRR to percentage change for estimate and CIs \n  mutate(\n    ## for each of the columns of interest - create a new column\n    across(\n      all_of(c(\"estimate\", \"conf.low\", \"conf.high\")), \n      ## apply the formula to calculate percentage change\n            .f = function(i) 100 * (i - 1), \n      ## add a suffix to new column names with \"_perc\"\n      .names = \"{.col}_perc\")\n    ) %&gt;% \n  ## only keep (and rename) certain columns \n  select(\"IRR\" = estimate, \n         \"95%CI low\" = conf.low, \n         \"95%CI high\" = conf.high,\n         \"Percentage change\" = estimate_perc, \n         \"95%CI low (perc)\" = conf.low_perc, \n         \"95%CI high (perc)\" = conf.high_perc,\n         \"p-value\" = p.value)\n\nAs previously we can visualise the outputs of the regression.\n\nestimate_res &lt;- data.frame(observed$result)\n\nggplot(estimate_res, aes(x = epiweek)) + \n  ## add in observed case counts as a line\n  geom_line(aes(y = case_int, colour = \"Observed\")) + \n  ## add in a line for the model estimate\n  geom_line(aes(y = estimate, col = \"Estimate\")) + \n  ## add in a band for the prediction intervals \n  geom_ribbon(aes(ymin = lower_pi, \n                  ymax = upper_pi), \n              alpha = 0.25) + \n  ## add vertical line and label to show where forecasting started\n  geom_vline(\n           xintercept = as.Date(intervention_week), \n           linetype = \"dashed\") + \n  annotate(geom = \"text\", \n           label = \"Intervention\", \n           x = intervention_week, \n           y = max(observed$upper_pi), \n           angle = 90, \n           vjust = 1\n           ) + \n  ## define colours\n  scale_colour_manual(values = c(\"Observed\" = \"black\", \n                                 \"Estimate\" = \"red\")) + \n  ## make a traditional plot (with black axes and white background)\n  theme_classic()\n\nWarning: Unknown or uninitialised column: `upper_pi`.\n\n\nWarning in max(observed$upper_pi): no non-missing arguments to max; returning\n-Inf",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/time_series.html#resources",
    "href": "new_pages/time_series.html#resources",
    "title": "23  Time series and outbreak detection",
    "section": "23.9 Resources",
    "text": "23.9 Resources\nforecasting: principles and practice textbook\nEPIET timeseries analysis case studies\nPenn State course Surveillance package manuscript",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time series and outbreak detection</span>"
    ]
  },
  {
    "objectID": "new_pages/epidemic_models.html",
    "href": "new_pages/epidemic_models.html",
    "title": "24  Epidemic modeling",
    "section": "",
    "text": "24.1 Overview\nThere exists a growing body of tools for epidemic modelling that lets us conduct fairly complex analyses with minimal effort. This section will provide an overview on how to use these tools to:\nIt is not intended as an overview of the methodologies and statistical methods underlying these tools, so please refer to the Resources tab for links to some papers covering this. Make sure you have an understanding of the methods before using these tools; this will ensure you can accurately interpret their results.\nBelow is an example of one of the outputs we’ll be producing in this section.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Epidemic modeling</span>"
    ]
  },
  {
    "objectID": "new_pages/epidemic_models.html#overview",
    "href": "new_pages/epidemic_models.html#overview",
    "title": "24  Epidemic modeling",
    "section": "",
    "text": "estimate the effective reproduction number Rt and related statistics such as the doubling time\nproduce short-term projections of future incidence",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Epidemic modeling</span>"
    ]
  },
  {
    "objectID": "new_pages/epidemic_models.html#preparation",
    "href": "new_pages/epidemic_models.html#preparation",
    "title": "24  Epidemic modeling",
    "section": "24.2 Preparation",
    "text": "24.2 Preparation\nWe will use two different methods and packages for Rt estimation, namely EpiNow and EpiEstim, as well as the projections package for forecasting case incidence.\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n   rio,          # File import\n   here,         # File locator\n   tidyverse,    # Data management + ggplot2 graphics\n   epicontacts,  # Analysing transmission networks\n   EpiNow2,      # Rt estimation\n   EpiEstim,     # Rt estimation\n   projections,  # Incidence projections\n   incidence2,   # Handling incidence data\n   epitrix,      # Useful epi functions\n   distcrete     # Discrete delay distributions\n)\n\nWe will use the cleaned case linelist for all analyses in this section. If you want to follow along, click to download the “clean” linelist (as .rds file). See the Download handbook and data page to download all example data used in this handbook.\n\n# import the cleaned linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Epidemic modeling</span>"
    ]
  },
  {
    "objectID": "new_pages/epidemic_models.html#estimating-rt",
    "href": "new_pages/epidemic_models.html#estimating-rt",
    "title": "24  Epidemic modeling",
    "section": "24.3 Estimating Rt",
    "text": "24.3 Estimating Rt\n\nEpiNow2 vs. EpiEstim\nThe reproduction number R is a measure of the transmissibility of a disease and is defined as the expected number of secondary cases per infected case. In a fully susceptible population, this value represents the basic reproduction number R0. However, as the number of susceptible individuals in a population changes over the course of an outbreak or pandemic, and as various response measures are implemented, the most commonly used measure of transmissibility is the effective reproduction number Rt; this is defined as the expected number of secondary cases per infected case at a given time t.\nThe EpiNow2 package provides the most sophisticated framework for estimating Rt. It has two key advantages over the other commonly used package, EpiEstim:\n\nIt accounts for delays in reporting and can therefore estimate Rt even when recent data is incomplete.\nIt estimates Rt on dates of infection rather than the dates of onset of reporting, which means that the effect of an intervention will be immediately reflected in a change in Rt, rather than with a delay.\n\nHowever, it also has two key disadvantages:\n\nIt requires knowledge of the generation time distribution (i.e. distribution of delays between infection of a primary and secondary cases), incubation period distribution (i.e. distribution of delays between infection and symptom onset) and any further delay distribution relevant to your data (e.g. if you have dates of reporting, you require the distribution of delays from symptom onset to reporting). While this will allow more accurate estimation of Rt, EpiEstim only requires the serial interval distribution (i.e. the distribution of delays between symptom onset of a primary and a secondary case), which may be the only distribution available to you.\nEpiNow2 is significantly slower than EpiEstim, anecdotally by a factor of about 100-1000! For example, estimating Rt for the sample outbreak considered in this section takes about four hours (this was run for a large number of iterations to ensure high accuracy and could probably be reduced if necessary, however the points stands that the algorithm is slow in general). This may be unfeasible if you are regularly updating your Rt estimates.\n\nWhich package you choose to use will therefore depend on the data, time and computational resources available to you.\n\n\nEpiNow2\n\nEstimating delay distributions\nThe delay distributions required to run EpiNow2 depend on the data you have. Essentially, you need to be able to describe the delay from the date of infection to the date of the event you want to use to estimate Rt. If you are using dates of onset, this would simply be the incubation period distribution. If you are using dates of reporting, you require the delay from infection to reporting. As this distribution is unlikely to be known directly, EpiNow2 lets you chain multiple delay distributions together; in this case, the delay from infection to symptom onset (e.g. the incubation period, which is likely known) and from symptom onset to reporting (which you can often estimate from the data).\nAs we have the dates of onset for all our cases in the example linelist, we will only require the incubation period distribution to link our data (e.g. dates of symptom onset) to the date of infection. We can either estimate this distribution from the data or use values from the literature.\nA literature estimate of the incubation period of Ebola (taken from this paper) with a mean of 9.1, standard deviation of 7.3 and maximum value of 30 would be specified as follows:\n\nincubation_period_lit &lt;- list(\n  mean = log(9.1),\n  mean_sd = log(0.1),\n  sd = log(7.3),\n  sd_sd = log(0.1),\n  max = 30\n)\n\nNote that EpiNow2 requires these delay distributions to be provided on a log scale, hence the log call around each value (except the max parameter which, confusingly, has to be provided on a natural scale). The mean_sd and sd_sd define the standard deviation of the mean and standard deviation estimates. As these are not known in this case, we choose the fairly arbitrary value of 0.1.\nIn this analysis, we instead estimate the incubation period distribution from the linelist itself using the function bootstrapped_dist_fit, which will fit a lognormal distribution to the observed delays between infection and onset in the linelist.\n\n## estimate incubation period\nincubation_period &lt;- bootstrapped_dist_fit(\n  linelist$date_onset - linelist$date_infection,\n  dist = \"lognormal\",\n  max_value = 100,\n  bootstraps = 1\n)\n\nThe other distribution we require is the generation time. As we have data on infection times and transmission links, we can estimate this distribution from the linelist by calculating the delay between infection times of infector-infectee pairs. To do this, we use the handy get_pairwise function from the package epicontacts, which allows us to calculate pairwise differences of linelist properties between transmission pairs. We first create an epicontacts object (see Transmission chains page for further details):\n\n## generate contacts\ncontacts &lt;- linelist %&gt;%\n  transmute(\n    from = infector,\n    to = case_id\n  ) %&gt;%\n  drop_na()\n\n## generate epicontacts object\nepic &lt;- make_epicontacts(\n  linelist = linelist,\n  contacts = contacts, \n  directed = TRUE\n)\n\nWe then fit the difference in infection times between transmission pairs, calculated using get_pairwise, to a gamma distribution:\n\n## estimate gamma generation time\ngeneration_time &lt;- bootstrapped_dist_fit(\n  get_pairwise(epic, \"date_infection\"),\n  dist = \"gamma\",\n  max_value = 20,\n  bootstraps = 1\n)\n\n\n\nRunning EpiNow2\nNow we just need to calculate daily incidence from the linelist, which we can do easily with the dplyr functions group_by() and n(). Note that EpiNow2 requires the column names to be date and confirm.\n\n## get incidence from onset dates\ncases &lt;- linelist %&gt;%\n  group_by(date = date_onset) %&gt;%\n  summarise(confirm = n())\n\nWe can then estimate Rt using the epinow function. Some notes on the inputs:\n\nWe can provide any number of ‘chained’ delay distributions to the delays argument; we would simply insert them alongside the incubation_period object within the delay_opts function.\nreturn_output ensures the output is returned within R and not just saved to a file.\nverbose specifies that we want a readout of the progress.\nhorizon indicates how many days we want to project future incidence for.\nWe pass additional options to the stan argument to specify how long we want to run the inference for. Increasing samples and chains will give you a more accurate estimate that better characterises uncertainty, however will take longer to run.\n\n\n## run epinow\nepinow_res &lt;- epinow(\n  reported_cases = cases,\n  generation_time = generation_time,\n  delays = delay_opts(incubation_period),\n  return_output = TRUE,\n  verbose = TRUE,\n  horizon = 21,\n  stan = stan_opts(samples = 750, chains = 4)\n)\n\n\n\nAnalysing outputs\nOnce the code has finished running, we can plot a summary very easily as follows. Scroll the image to see the full extent.\n\n## plot summary figure\nplot(epinow_res)\n\n\n\n\n\n\n\n\nWe can also look at various summary statistics:\n\n## summary table\nepinow_res$summary\n\n                                 measure                  estimate\n                                  &lt;char&gt;                    &lt;char&gt;\n1: New confirmed cases by infection date                4 (2 -- 6)\n2:        Expected change in daily cases                    Unsure\n3:            Effective reproduction no.        0.88 (0.73 -- 1.1)\n4:                        Rate of growth -0.012 (-0.028 -- 0.0052)\n5:          Doubling/halving time (days)          -60 (130 -- -25)\n    numeric_estimate\n              &lt;list&gt;\n1: &lt;data.table[1x9]&gt;\n2:              0.56\n3: &lt;data.table[1x9]&gt;\n4: &lt;data.table[1x9]&gt;\n5: &lt;data.table[1x9]&gt;\n\n\nFor further analyses and custom plotting, you can access the summarised daily estimates via $estimates$summarised. We will convert this from the default data.table to a tibble for ease of use with dplyr.\n\n## extract summary and convert to tibble\nestimates &lt;- as_tibble(epinow_res$estimates$summarised)\nestimates\n\n\n\n\n\n\n\nAs an example, let’s make a plot of the doubling time and Rt. We will only look at the first few months of the outbreak when Rt is well above one, to avoid plotting extremely high doublings times.\nWe use the formula log(2)/growth_rate to calculate the doubling time from the estimated growth rate.\n\n## make wide df for median plotting\ndf_wide &lt;- estimates %&gt;%\n  filter(\n    variable %in% c(\"growth_rate\", \"R\"),\n    date &lt; as.Date(\"2014-09-01\")\n  ) %&gt;%\n  ## convert growth rates to doubling times\n  mutate(\n    across(\n      c(median, lower_90:upper_90),\n      ~ case_when(\n        variable == \"growth_rate\" ~ log(2)/.x,\n        TRUE ~ .x\n      )\n    ),\n    ## rename variable to reflect transformation\n    variable = replace(variable, variable == \"growth_rate\", \"doubling_time\")\n  )\n\n## make long df for quantile plotting\ndf_long &lt;- df_wide %&gt;%\n  ## here we match matching quantiles (e.g. lower_90 to upper_90)\n  pivot_longer(\n    lower_90:upper_90,\n    names_to = c(\".value\", \"quantile\"),\n    names_pattern = \"(.+)_(.+)\"\n  )\n\n## make plot\nggplot() +\n  geom_ribbon(\n    data = df_long,\n    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),\n    color = NA\n  ) +\n  geom_line(\n    data = df_wide,\n    aes(x = date, y = median)\n  ) +\n  ## use label_parsed to allow subscript label\n  facet_wrap(\n    ~ variable,\n    ncol = 1,\n    scales = \"free_y\",\n    labeller = as_labeller(c(R = \"R[t]\", doubling_time = \"Doubling~time\"), label_parsed),\n    strip.position = 'left'\n  ) +\n  ## manually define quantile transparency\n  scale_alpha_manual(\n    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    alpha = \"Credibel\\ninterval\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b %d\\n%Y\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    strip.background = element_blank(),\n    strip.placement = 'outside'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nEpiEstim\nTo run EpiEstim, we need to provide data on daily incidence and specify the serial interval (i.e. the distribution of delays between symptom onset of primary and secondary cases).\nIncidence data can be provided to EpiEstim as a vector, a data frame, or an incidence object from the original incidence package. You can even distinguish between imports and locally acquired infections; see the documentation at ?estimate_R for further details.\nWe will create the input using incidence2. See the page on Epidemic curves for more examples with the incidence2 package. Since there have been updates to the incidence2 package that don’t completely align with estimateR()’s expected input, there are some minor additional steps needed. The incidence object consists of a tibble with dates and their respective case counts. We use complete() from tidyr to ensure all dates are included (even those with no cases), and then rename() the columns to align with what is expected by estimate_R() in a later step.\n\n## get incidence from onset date\ncases &lt;- incidence2::incidence(linelist, date_index = \"date_onset\") %&gt;% # get case counts by day\n  tidyr::complete(date_index = seq.Date(                              # ensure all dates are represented\n    from = min(date_index, na.rm = T),\n    to = max(date_index, na.rm=T),\n    by = \"day\"),\n    fill = list(count = 0)) %&gt;%                                       # convert NA counts to 0\n  rename(I = count,                                                   # rename to names expected by estimateR\n         dates = date_index)\n\nThe package provides several options for specifying the serial interval, the details of which are provided in the documentation at ?estimate_R. We will cover two of them here.\n\nUsing serial interval estimates from the literature\nUsing the option method = \"parametric_si\", we can manually specify the mean and standard deviation of the serial interval in a config object created using the function make_config. We use a mean and standard deviation of 12.0 and 5.2, respectively, defined in this paper:\n\n## make config\nconfig_lit &lt;- make_config(\n  mean_si = 12.0,\n  std_si = 5.2\n)\n\nWe can then estimate Rt with the estimate_R function:\n\ncases &lt;- cases %&gt;% \n     filter(!is.na(date))\n\n\n#create a dataframe for the function estimate_R()\ncases_incidence &lt;- data.frame(dates = seq.Date(from = min(cases$dates),\n                               to = max(cases$dates), \n                               by = 1))\n\ncases_incidence &lt;- left_join(cases_incidence, cases) %&gt;% \n     select(dates, I) %&gt;% \n     mutate(I = ifelse(is.na(I), 0, I))\n\nJoining with `by = join_by(dates)`\n\nepiestim_res_lit &lt;- estimate_R(\n  incid = cases_incidence,\n  method = \"parametric_si\",\n  config = config_lit\n)\n\nDefault config will estimate R on weekly sliding windows.\n    To change this change the t_start and t_end arguments. \n\n\nand plot a summary of the outputs:\n\nplot(epiestim_res_lit)\n\n\n\n\n\n\n\n\n\n\nUsing serial interval estimates from the data\nAs we have data on dates of symptom onset and transmission links, we can also estimate the serial interval from the linelist by calculating the delay between onset dates of infector-infectee pairs. As we did in the EpiNow2 section, we will use the get_pairwise function from the epicontacts package, which allows us to calculate pairwise differences of linelist properties between transmission pairs. We first create an epicontacts object (see Transmission chains page for further details):\n\n## generate contacts\ncontacts &lt;- linelist %&gt;%\n  transmute(\n    from = infector,\n    to = case_id\n  ) %&gt;%\n  drop_na()\n\n## generate epicontacts object\nepic &lt;- make_epicontacts(\n  linelist = linelist,\n  contacts = contacts, \n  directed = TRUE\n)\n\nWe then fit the difference in onset dates between transmission pairs, calculated using get_pairwise, to a gamma distribution. We use the handy fit_disc_gamma from the epitrix package for this fitting procedure, as we require a discretised distribution.\n\n## estimate gamma serial interval\nserial_interval &lt;- fit_disc_gamma(get_pairwise(epic, \"date_onset\"))\n\nWe then pass this information to the config object, run EpiEstim again and plot the results:\n\n## make config\nconfig_emp &lt;- make_config(\n  mean_si = serial_interval$mu,\n  std_si = serial_interval$sd\n)\n\n## run epiestim\nepiestim_res_emp &lt;- estimate_R(\n  incid = cases_incidence,\n  method = \"parametric_si\",\n  config = config_emp\n)\n\nDefault config will estimate R on weekly sliding windows.\n    To change this change the t_start and t_end arguments. \n\n## plot outputs\nplot(epiestim_res_emp)\n\n\n\n\n\n\n\n\n\n\nSpecifying estimation time windows\nThese default options will provide a weekly sliding estimate and might act as a warning that you are estimating Rt too early in the outbreak for a precise estimate. You can change this by setting a later start date for the estimation as shown below. Unfortunately, EpiEstim only provides a very clunky way of specifying these estimations times, in that you have to provide a vector of integers referring to the start and end dates for each time window.\n\n## define a vector of dates starting on June 1st\nstart_dates &lt;- seq.Date(\n  as.Date(\"2014-06-01\"),\n  max(cases$dates) - 7,\n  by = 1\n) %&gt;%\n  ## subtract the starting date to convert to numeric\n  `-`(min(cases$dates)) %&gt;%\n  ## convert to integer\n  as.integer()\n\n## add six days for a one week sliding window\nend_dates &lt;- start_dates + 6\n  \n## make config\nconfig_partial &lt;- make_config(\n  mean_si = 12.0,\n  std_si = 5.2,\n  t_start = start_dates,\n  t_end = end_dates\n)\n\nNow we re-run EpiEstim and can see that the estimates only start from June:\n\n## run epiestim\nepiestim_res_partial &lt;- estimate_R(\n  incid = cases_incidence,\n  method = \"parametric_si\",\n  config = config_partial\n)\n\n## plot outputs\nplot(epiestim_res_partial)\n\n\n\n\n\n\n\n\n\n\nAnalysing outputs\nThe main outputs can be accessed via $R. As an example, we will create a plot of Rt and a measure of “transmission potential” given by the product of Rt and the number of cases reported on that day; this represents the expected number of cases in the next generation of infection.\n\n## make wide dataframe for median\ndf_wide &lt;- epiestim_res_lit$R %&gt;%\n  rename_all(clean_labels) %&gt;%\n  rename(\n    lower_95_r = quantile_0_025_r,\n    lower_90_r = quantile_0_05_r,\n    lower_50_r = quantile_0_25_r,\n    upper_50_r = quantile_0_75_r,\n    upper_90_r = quantile_0_95_r,\n    upper_95_r = quantile_0_975_r,\n    ) %&gt;%\n  mutate(\n    ## extract the median date from t_start and t_end\n    dates = epiestim_res_emp$dates[round(map2_dbl(t_start, t_end, median))],\n    var = \"R[t]\"\n  ) %&gt;%\n  ## merge in daily incidence data\n  left_join(cases, \"dates\") %&gt;%\n  ## calculate risk across all r estimates\n  mutate(\n    across(\n      lower_95_r:upper_95_r,\n      ~ .x*I,\n      .names = \"{str_replace(.col, '_r', '_risk')}\"\n    )\n  ) %&gt;%\n  ## seperate r estimates and risk estimates\n  pivot_longer(\n    contains(\"median\"),\n    names_to = c(\".value\", \"variable\"),\n    names_pattern = \"(.+)_(.+)\"\n  ) %&gt;%\n  ## assign factor levels\n  mutate(variable = factor(variable, c(\"risk\", \"r\")))\n\n## make long dataframe from quantiles\ndf_long &lt;- df_wide %&gt;%\n  select(-variable, -median) %&gt;%\n  ## seperate r/risk estimates and quantile levels\n  pivot_longer(\n    contains(c(\"lower\", \"upper\")),\n    names_to = c(\".value\", \"quantile\", \"variable\"),\n    names_pattern = \"(.+)_(.+)_(.+)\"\n  ) %&gt;%\n  mutate(variable = factor(variable, c(\"risk\", \"r\")))\n\n## make plot\nggplot() +\n  geom_ribbon(\n    data = df_long,\n    aes(x = dates, ymin = lower, ymax = upper, alpha = quantile),\n    color = NA\n  ) +\n  geom_line(\n    data = df_wide,\n    aes(x = dates, y = median),\n    alpha = 0.2\n  ) +\n  ## use label_parsed to allow subscript label\n  facet_wrap(\n    ~ variable,\n    ncol = 1,\n    scales = \"free_y\",\n    labeller = as_labeller(c(r = \"R[t]\", risk = \"Transmission~potential\"), label_parsed),\n    strip.position = 'left'\n  ) +\n  ## manually define quantile transparency\n  scale_alpha_manual(\n    values = c(`50` = 0.7, `90` = 0.4, `95` = 0.2),\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    alpha = \"Credible\\ninterval\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b %d\\n%Y\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    strip.background = element_blank(),\n    strip.placement = 'outside'\n  )",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Epidemic modeling</span>"
    ]
  },
  {
    "objectID": "new_pages/epidemic_models.html#projecting-incidence",
    "href": "new_pages/epidemic_models.html#projecting-incidence",
    "title": "24  Epidemic modeling",
    "section": "24.4 Projecting incidence",
    "text": "24.4 Projecting incidence\n\nEpiNow2\nBesides estimating Rt, EpiNow2 also supports forecasting of Rt and projections of case numbers by integration with the EpiSoon package under the hood. All you need to do is specify the horizon argument in your epinow function call, indicating how many days you want to project into the future; see the EpiNow2 section under the “Estimating Rt” for details on how to get EpiNow2 up and running. In this section, we will just plot the outputs from that analysis, stored in the epinow_res object.\n\n## define minimum date for plot\nmin_date &lt;- as.Date(\"2015-03-01\")\n\n## extract summarised estimates\nestimates &lt;-  as_tibble(epinow_res$estimates$summarised)\n\n## extract raw data on case incidence\nobservations &lt;- as_tibble(epinow_res$estimates$observations) %&gt;%\n  filter(date &gt; min_date)\n\n## extract forecasted estimates of case numbers\ndf_wide &lt;- estimates %&gt;%\n  filter(\n    variable == \"reported_cases\",\n    type == \"forecast\",\n    date &gt; min_date\n  )\n\n## convert to even longer format for quantile plotting\ndf_long &lt;- df_wide %&gt;%\n  ## here we match matching quantiles (e.g. lower_90 to upper_90)\n  pivot_longer(\n    lower_90:upper_90,\n    names_to = c(\".value\", \"quantile\"),\n    names_pattern = \"(.+)_(.+)\"\n  )\n\n## make plot\nggplot() +\n  geom_histogram(\n    data = observations,\n    aes(x = date, y = confirm),\n    stat = 'identity',\n    binwidth = 1\n  ) +\n  geom_ribbon(\n    data = df_long,\n    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),\n    color = NA\n  ) +\n  geom_line(\n    data = df_wide,\n    aes(x = date, y = median)\n  ) +\n  geom_vline(xintercept = min(df_long$date), linetype = 2) +\n  ## manually define quantile transparency\n  scale_alpha_manual(\n    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    x = NULL,\n    y = \"Daily reported cases\",\n    alpha = \"Credible\\ninterval\"\n  ) +\n  scale_x_date(\n    date_breaks = \"1 month\",\n    date_labels = \"%b %d\\n%Y\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\n\nprojections\nThe projections package developed by RECON makes it very easy to make short term incidence forecasts, requiring only knowledge of the effective reproduction number Rt and the serial interval. Here we will cover how to use serial interval estimates from the literature and how to use our own estimates from the linelist.\n\nUsing serial interval estimates from the literature\nprojections requires a discretised serial interval distribution of the class distcrete from the package distcrete. We will use a gamma distribution with a mean of 12.0 and and standard deviation of 5.2 defined in this paper. To convert these values into the shape and scale parameters required for a gamma distribution, we will use the function gamma_mucv2shapescale from the epitrix package.\n\n## get shape and scale parameters from the mean mu and the coefficient of\n## variation (e.g. the ratio of the standard deviation to the mean)\nshapescale &lt;- epitrix::gamma_mucv2shapescale(mu = 12.0, cv = 5.2/12)\n\n## make distcrete object\nserial_interval_lit &lt;- distcrete::distcrete(\n  name = \"gamma\",\n  interval = 1,\n  shape = shapescale$shape,\n  scale = shapescale$scale\n)\n\nHere is a quick check to make sure the serial interval looks correct. We access the density of the gamma distribution we have just defined by $d, which is equivalent to calling dgamma:\n\n## check to make sure the serial interval looks correct\nqplot(\n  x = 0:50, y = serial_interval_lit$d(0:50), geom = \"area\",\n  xlab = \"Serial interval\", ylab = \"Density\"\n)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\n\n\nUsing serial interval estimates from the data\nAs we have data on dates of symptom onset and transmission links, we can also estimate the serial interval from the linelist by calculating the delay between onset dates of infector-infectee pairs. As we did in the EpiNow2 section, we will use the get_pairwise function from the epicontacts package, which allows us to calculate pairwise differences of linelist properties between transmission pairs. We first create an epicontacts object (see Transmission chains page for further details):\n\n## generate contacts\ncontacts &lt;- linelist %&gt;%\n  transmute(\n    from = infector,\n    to = case_id\n  ) %&gt;%\n  drop_na()\n\n## generate epicontacts object\nepic &lt;- make_epicontacts(\n  linelist = linelist,\n  contacts = contacts, \n  directed = TRUE\n)\n\nWe then fit the difference in onset dates between transmission pairs, calculated using get_pairwise, to a gamma distribution. We use the handy fit_disc_gamma from the epitrix package for this fitting procedure, as we require a discretised distribution.\n\n## estimate gamma serial interval\nserial_interval &lt;- fit_disc_gamma(get_pairwise(epic, \"date_onset\"))\n\n## inspect estimate\nserial_interval[c(\"mu\", \"sd\")]\n\n$mu\n[1] 11.51047\n\n$sd\n[1] 7.696056\n\n\n\n\nProjecting incidence\nTo project future incidence, we still need to provide historical incidence in the form of an incidence object, as well as a sample of plausible Rt values. We will generate these values using the Rt estimates generated by EpiEstim in the previous section (under “Estimating Rt”) and stored in the epiestim_res_emp object. In the code below, we extract the mean and standard deviation estimates of Rt for the last time window of the outbreak (using the tail function to access the last element in a vector), and simulate 1000 values from a gamma distribution using rgamma. You can also provide your own vector of Rt values that you want to use for forward projections.\n\n## create incidence object from dates of onset\ninc &lt;- incidence::incidence(linelist$date_onset)\n\n256 missing observations were removed.\n\n## extract plausible r values from most recent estimate\nmean_r &lt;- tail(epiestim_res_emp$R$`Mean(R)`, 1)\nsd_r &lt;- tail(epiestim_res_emp$R$`Std(R)`, 1)\nshapescale &lt;- gamma_mucv2shapescale(mu = mean_r, cv = sd_r/mean_r)\nplausible_r &lt;- rgamma(1000, shape = shapescale$shape, scale = shapescale$scale)\n\n## check distribution\nqplot(x = plausible_r, geom = \"histogram\", xlab = expression(R[t]), ylab = \"Counts\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe then use the project() function to make the actual forecast. We specify how many days we want to project for via the n_days arguments, and specify the number of simulations using the n_sim argument.\n\n## make projection\nproj &lt;- project(\n  x = inc,\n  R = plausible_r,\n  si = serial_interval$distribution,\n  n_days = 21,\n  n_sim = 1000\n)\n\nWe can then handily plot the incidence and projections using the plot() and add_projections() functions. We can easily subset the incidence object to only show the most recent cases by using the square bracket operator.\n\n## plot incidence and projections\nplot(inc[inc$dates &gt; as.Date(\"2015-03-01\")]) %&gt;%\n  add_projections(proj)\n\n\n\n\n\n\n\n\nYou can also easily extract the raw estimates of daily case numbers by converting the output to a dataframe.\n\n## convert to data frame for raw data\nproj_df &lt;- as.data.frame(proj)\nproj_df",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Epidemic modeling</span>"
    ]
  },
  {
    "objectID": "new_pages/epidemic_models.html#resources",
    "href": "new_pages/epidemic_models.html#resources",
    "title": "24  Epidemic modeling",
    "section": "24.5 Resources",
    "text": "24.5 Resources\n\nHere is the paper describing the methodology implemented in EpiEstim.\nHere is the paper describing the methodology implemented in EpiNow2.\nHere is a paper describing various methodological and practical considerations for estimating Rt.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Epidemic modeling</span>"
    ]
  },
  {
    "objectID": "new_pages/contact_tracing.html",
    "href": "new_pages/contact_tracing.html",
    "title": "25  Contact tracing",
    "section": "",
    "text": "25.1 Preparation",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Contact tracing</span>"
    ]
  },
  {
    "objectID": "new_pages/contact_tracing.html#preparation",
    "href": "new_pages/contact_tracing.html#preparation",
    "title": "25  Contact tracing",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,          # importing data  \n  here,         # relative file pathways  \n  janitor,      # data cleaning and tables\n  lubridate,    # working with dates\n  epikit,       # age_categories() function\n  apyramid,     # age pyramids\n  tidyverse,    # data manipulation and visualization\n  RColorBrewer, # color palettes\n  formattable,  # fancy tables\n  kableExtra    # table formatting\n)\n\n\n\nImport data\nWe will import sample datasets of contacts, and of their “follow-up”. These data have been retrieved and un-nested from the Go.Data API and stored as “.rds” files.\nYou can download all the example data for this handbook from the Download handbook and data page.\nIf you want to download the example contact tracing data specific to this page, use the three download links below:\n Click to download the case investigation data (.rds file) \n Click to download the contact registration data (.rds file) \n Click to download the contact follow-up data (.rds file) \n\n\n\nIn their original form in the downloadable files, the data reflect data as provided by the Go.Data API (learn about APIs here). For example purposes here, we will clean the data to make it easier to read on this page. If you are using a Go.Data instance, you can view complete instructions on how to retrieve your data here.\nBelow, the datasets are imported using the import() function from the rio package. See the page on Import and export for various ways to import data. We use here() to specify the file path - you should provide the file path specific to your computer. We then use select() to select only certain columns of the data, to simplify for purposes of demonstration.\n\nCase data\nThese data are a table of the cases, and information about them.\n\ncases &lt;- import(here(\"data\", \"godata\", \"cases_clean.rds\")) %&gt;% \n  select(case_id, firstName, lastName, gender, age, age_class,\n         occupation, classification, was_contact, hospitalization_typeid)\n\nHere are the nrow(cases) cases:\n\n\n\n\n\n\n\n\nContacts data\nThese data are a table of all the contacts and information about them. Again, provide your own file path. After importing we perform a few preliminary data cleaning steps including:\n\nSet age_class as a factor and reverse the level order so that younger ages are first\n\nSelect only certain column, while re-naming a one of them\n\nArtificially assign rows with missing admin level 2 to “Djembe”, to improve clarity of some example visualisations\n\n\ncontacts &lt;- import(here(\"data\", \"godata\", \"contacts_clean.rds\")) %&gt;% \n  mutate(age_class = forcats::fct_rev(age_class)) %&gt;% \n  select(contact_id, contact_status, firstName, lastName, gender, age,\n         age_class, occupation, date_of_reporting, date_of_data_entry,\n         date_of_last_exposure = date_of_last_contact,\n         date_of_followup_start, date_of_followup_end, risk_level, was_case, admin_2_name) %&gt;% \n  mutate(admin_2_name = replace_na(admin_2_name, \"Djembe\"))\n\nHere are the nrow(contacts) rows of the contacts dataset:\n\n\n\n\n\n\n\n\nFollow-up data\nThese data are records of the “follow-up” interactions with the contacts. Each contact is supposed to have an encounter each day for 14 days after their exposure.\nWe import and perform a few cleaning steps. We select certain columns, and also convert a character column to all lowercase values.\n\nfollowups &lt;- rio::import(here::here(\"data\", \"godata\", \"followups_clean.rds\")) %&gt;% \n  select(contact_id, followup_status, followup_number,\n         date_of_followup, admin_2_name, admin_1_name) %&gt;% \n  mutate(followup_status = str_to_lower(followup_status))\n\nHere are the first 50 rows of the nrow(followups)-row followups dataset (each row is a follow-up interaction, with outcome status in the followup_status column):\n\n\n\n\n\n\n\n\nRelationships data\nHere we import data showing the relationship between cases and contacts. We select certain column to show.\n\nrelationships &lt;- rio::import(here::here(\"data\", \"godata\", \"relationships_clean.rds\")) %&gt;% \n  select(source_visualid, source_gender, source_age, date_of_last_contact,\n         date_of_data_entry, target_visualid, target_gender,\n         target_age, exposure_type)\n\nBelow are the first 50 rows of the relationships dataset, which records all relationships between cases and contacts.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Contact tracing</span>"
    ]
  },
  {
    "objectID": "new_pages/contact_tracing.html#descriptive-analyses",
    "href": "new_pages/contact_tracing.html#descriptive-analyses",
    "title": "25  Contact tracing",
    "section": "25.2 Descriptive analyses",
    "text": "25.2 Descriptive analyses\nYou can use the techniques covered in other pages of this handbook to conduct descriptive analyses of your cases, contacts, and their relationships. Below are some examples.\n\nDemographics\nAs demonstrated in the page covering Demographic pyramids, you can visualise the age and gender distribution (here we use the apyramid package).\n\nAge and Gender of contacts\nThe pyramid below compares the age distribution of contacts, by gender. Note that contacts missing age are included in their own bar at the top. You can change this default behavior, but then consider listing the number missing in a caption.\n\napyramid::age_pyramid(\n  data = contacts,                                   # use contacts dataset\n  age_group = \"age_class\",                           # categorical age column\n  split_by = \"gender\") +                             # gender for halfs of pyramid\n  labs(\n    fill = \"Gender\",                                 # title of legend\n    title = \"Age/Sex Pyramid of COVID-19 contacts\")+ # title of the plot\n  theme_minimal()                                    # simple background\n\n\n\n\n\n\n\n\nWith the Go.Data data structure, the relationships dataset contains the ages of both cases and contacts, so you could use that dataset and create an age pyramid showing the differences between these two groups of people. The relationships data frame will be mutated to transform the numberic age columns into categories (see the Cleaning data and core functions page). We also pivot the dataframe longer to facilitate easy plotting with ggplot2 (see Pivoting data).\n\nrelation_age &lt;- relationships %&gt;% \n  select(source_age, target_age) %&gt;% \n  transmute(                              # transmute is like mutate() but removes all other columns not mentioned\n    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),\n    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5)),\n    ) %&gt;% \n  pivot_longer(cols = contains(\"class\"), names_to = \"category\", values_to = \"age_class\")  # pivot longer\n\n\nrelation_age\n\n# A tibble: 200 × 2\n   category         age_class\n   &lt;chr&gt;            &lt;fct&gt;    \n 1 source_age_class 80+      \n 2 target_age_class 15-19    \n 3 source_age_class &lt;NA&gt;     \n 4 target_age_class 50-54    \n 5 source_age_class &lt;NA&gt;     \n 6 target_age_class 20-24    \n 7 source_age_class 30-34    \n 8 target_age_class 45-49    \n 9 source_age_class 40-44    \n10 target_age_class 30-34    \n# ℹ 190 more rows\n\n\nNow we can plot this transformed dataset with age_pyramid() as before, but replacing gender with category (contact, or case).\n\napyramid::age_pyramid(\n  data = relation_age,                               # use modified relationship dataset\n  age_group = \"age_class\",                           # categorical age column\n  split_by = \"category\") +                           # by cases and contacts\n  scale_fill_manual(\n    values = c(\"orange\", \"purple\"),                  # to specify colors AND labels\n    labels = c(\"Case\", \"Contact\"))+\n  labs(\n    fill = \"Legend\",                                           # title of legend\n    title = \"Age/Sex Pyramid of COVID-19 contacts and cases\")+ # title of the plot\n  theme_minimal()                                              # simple background\n\n\n\n\n\n\n\n\nWe can also view other characteristics such as occupational breakdown (e.g. in form of a pie chart).\n\n# Clean dataset and get counts by occupation\nocc_plot_data &lt;- cases %&gt;% \n  mutate(occupation = forcats::fct_explicit_na(occupation),  # make NA missing values a category\n         occupation = forcats::fct_infreq(occupation)) %&gt;%   # order factor levels in order of frequency\n  count(occupation)                                          # get counts by occupation\n  \n# Make pie chart\nggplot(data = occ_plot_data, mapping = aes(x = \"\", y = n, fill = occupation))+\n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start = 0) +\n  labs(\n    fill = \"Occupation\",\n    title = \"Known occupations of COVID-19 cases\")+\n  theme_minimal() +                    \n  theme(axis.line = element_blank(),\n        axis.title = element_blank(),\n        axis.text = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nContacts per case\nThe number of contacts per case can be an important metric to assess quality of contact enumeration and the compliance of the population toward public health response.\nDepending on your data structure, this can be assessed with a dataset that contains all cases and contacts. In the Go.Data datasets, the links between cases (“sources”) and contacts (“targets”) is stored in the relationships dataset.\nIn this dataset, each row is a contact, and the source case is listed in the row. There are no contacts who have relationships with multiple cases, but if this exists you may need to account for those before plotting (and explore them too!).\nWe begin by counting the number of rows (contacts) per source case. This is saved as a data frame.\n\ncontacts_per_case &lt;- relationships %&gt;% \n  count(source_visualid)\n\ncontacts_per_case\n\n   source_visualid  n\n1   CASE-2020-0001 13\n2   CASE-2020-0002  5\n3   CASE-2020-0003  2\n4   CASE-2020-0004  4\n5   CASE-2020-0005  5\n6   CASE-2020-0006  3\n7   CASE-2020-0008  3\n8   CASE-2020-0009  3\n9   CASE-2020-0010  3\n10  CASE-2020-0012  3\n11  CASE-2020-0013  5\n12  CASE-2020-0014  3\n13  CASE-2020-0016  3\n14  CASE-2020-0018  4\n15  CASE-2020-0022  3\n16  CASE-2020-0023  4\n17  CASE-2020-0030  3\n18  CASE-2020-0031  3\n19  CASE-2020-0034  4\n20  CASE-2020-0036  1\n21  CASE-2020-0037  3\n22  CASE-2020-0045  3\n23            &lt;NA&gt; 17\n\n\nWe use geom_histogram() to plot these data as a histogram.\n\nggplot(data = contacts_per_case)+        # begin with count data frame created above\n  geom_histogram(mapping = aes(x = n))+  # print histogram of number of contacts per case\n  scale_y_continuous(expand = c(0,0))+   # remove excess space below 0 on y-axis\n  theme_light()+                         # simplify background\n  labs(\n    title = \"Number of contacts per case\",\n    y = \"Cases\",\n    x = \"Contacts per case\"\n  )",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Contact tracing</span>"
    ]
  },
  {
    "objectID": "new_pages/contact_tracing.html#contact-follow-up",
    "href": "new_pages/contact_tracing.html#contact-follow-up",
    "title": "25  Contact tracing",
    "section": "25.3 Contact Follow Up",
    "text": "25.3 Contact Follow Up\nContact tracing data often contain “follow-up” data, which record outcomes of daily symptom checks of persons in quarantine. Analysis of this data can inform response strategy, identify contacts at-risk of loss-to-follow-up or at-risk of developing disease.\n\nData cleaning\nThese data can exist in a variety of formats. They may exist as a “wide” format Excel sheet with one row per contact, and one column per follow-up “day”. See Pivoting data for descriptions of “long” and “wide” data and how to pivot data wider or longer.\nIn our Go.Data example, these data are stored in the followups data frame, which is in a “long” format with one row per follow-up interaction. The first 50 rows look like this:\n\n\n\n\n\n\nCAUTION: Beware of duplicates when dealing with followup data; as there could be several erroneous followups on the same day for a given contact. Perhaps it seems to be an error but reflects reality - e.g. a contact tracer could submit a follow-up form early in the day when they could not reach the contact, and submit a second form when they were later reached. It will depend on the operational context for how you want to handle duplicates - just make sure to document your approach clearly. \nLet’s see how many instances of “duplicate” rows we have:\n\nfollowups %&gt;% \n  count(contact_id, date_of_followup) %&gt;%   # get unique contact_days\n  filter(n &gt; 1)                             # view records where count is more than 1  \n\n  contact_id date_of_followup n\n1       &lt;NA&gt;       2020-09-03 2\n2       &lt;NA&gt;       2020-09-04 2\n3       &lt;NA&gt;       2020-09-05 2\n\n\nIn our example data, the only records that this applies to are ones missing an ID! We can remove those. But, for purposes of demonstration we will go show the steps for de-duplication so there is only one follow-up encoutner per person per day. See the page on De-duplication for more detail. We will assume that the most recent encounter record is the correct one. We also take the opportunity to clean the followup_number column (the “day” of follow-up which should range 1 - 14).\n\nfollowups_clean &lt;- followups %&gt;%\n  \n  # De-duplicate\n  group_by(contact_id, date_of_followup) %&gt;%        # group rows per contact-day\n  arrange(contact_id, desc(date_of_followup)) %&gt;%   # arrange rows, per contact-day, by date of follow-up (most recent at top)\n  slice_head() %&gt;%                                  # keep only the first row per unique contact id  \n  ungroup() %&gt;% \n  \n  # Other cleaning\n  mutate(followup_number = replace(followup_number, followup_number &gt; 14, NA)) %&gt;% # clean erroneous data\n  drop_na(contact_id)                               # remove rows with missing contact_id\n\nFor each follow-up encounter, we have a follow-up status (such as whether the encounter occurred and if so, did the contact have symptoms or not). To see all the values we can run a quick tabyl() (from janitor) or table() (from base R) (see Descriptive tables) by followup_status to see the frequency of each of the outcomes.\nIn this dataset, “seen_not_ok” means “seen with symptoms”, and “seen_ok” means “seen without symptoms”.\n\nfollowups_clean %&gt;% \n  tabyl(followup_status)\n\n followup_status   n    percent\n          missed  10 0.02325581\n   not_attempted   5 0.01162791\n   not_performed 319 0.74186047\n     seen_not_ok   6 0.01395349\n         seen_ok  90 0.20930233\n\n\n\n\nPlot over time\nAs the dates data are continuous, we will use a histogram to plot them with date_of_followup assigned to the x-axis. We can achieve a “stacked” histogram by specifying a fill = argument within aes(), which we assign to the column followup_status. Consequently, you can set the legend title using the fill = argument of labs().\nWe can see that the contacts were identified in waves (presumably corresponding with epidemic waves of cases), and that follow-up completion did not seemingly improve over the course of the epidemic.\n\nggplot(data = followups_clean)+\n  geom_histogram(mapping = aes(x = date_of_followup, fill = followup_status)) +\n  scale_fill_discrete(drop = FALSE)+   # show all factor levels (followup_status) in the legend, even those not used\n  theme_classic() +\n  labs(\n    x = \"\",\n    y = \"Number of contacts\",\n    title = \"Daily Contact Followup Status\",\n    fill = \"Followup Status\",\n    subtitle = str_glue(\"Data as of {max(followups$date_of_followup, na.rm=T)}\"))   # dynamic subtitle\n\n\n\n\n\n\n\n\nCAUTION: If you are preparing many plots (e.g. for multiple jurisdictions) you will want the legends to appear identically even with varying levels of data completion or data composition. There may be plots for which not all follow-up statuses are present in the data, but you still want those categories to appear the legends. In ggplots (like above), you can specify the drop = FALSE argument of the scale_fill_discrete(). In tables, use tabyl() which shows counts for all factor levels, or if using count() from dplyr add the argument .drop = FALSE to include counts for all factor levels.\n\n\nDaily individual tracking\nIf your outbreak is small enough, you may want to look at each contact individually and see their status over the course of their follow-up. Fortunately, this followups dataset already contains a column with the day “number” of follow-up (1-14). If this does not exist in your data, you could create it by calculating the difference between the encounter date and the date follow-up was intended to begin for the contact.\nA convenient visualisation mechanism (if the number of cases is not too large) can be a heat plot, made with geom_tile(). See more details in the heat plot page.\n\nggplot(data = followups_clean)+\n  geom_tile(mapping = aes(x = followup_number, y = contact_id, fill = followup_status),\n            color = \"grey\")+       # grey gridlines\n  scale_fill_manual( values = c(\"yellow\", \"grey\", \"orange\", \"darkred\", \"darkgreen\"))+\n  theme_minimal()+\n  scale_x_continuous(breaks = seq(from = 1, to = 14, by = 1))\n\n\n\n\n\n\n\n\n\n\nAnalyse by group\nPerhaps these follow-up data are being viewed on a daily or weekly basis for operational decision-making. You may want more meaningful disaggregations by geographic area or by contact-tracing team. We can do this by adjusting the columns provided to group_by().\n\nplot_by_region &lt;- followups_clean %&gt;%                                        # begin with follow-up dataset\n  count(admin_1_name, admin_2_name, followup_status) %&gt;%   # get counts by unique region-status (creates column 'n' with counts)\n  \n  # begin ggplot()\n  ggplot(                                         # begin ggplot\n    mapping = aes(x = reorder(admin_2_name, n),     # reorder admin factor levels by the numeric values in column 'n'\n                  y = n,                            # heights of bar from column 'n'\n                  fill = followup_status,           # color stacked bars by their status\n                  label = n))+                      # to pass to geom_label()              \n  geom_col()+                                     # stacked bars, mapping inherited from above \n  geom_text(                                      # add text, mapping inherited from above\n    size = 3,                                         \n    position = position_stack(vjust = 0.5), \n    color = \"white\",           \n    check_overlap = TRUE,\n    fontface = \"bold\")+\n  coord_flip()+\n  labs(\n    x = \"\",\n    y = \"Number of contacts\",\n    title = \"Contact Followup Status, by Region\",\n    fill = \"Followup Status\",\n    subtitle = str_glue(\"Data as of {max(followups_clean$date_of_followup, na.rm=T)}\")) +\n  theme_classic()+                                                                      # Simplify background\n  facet_wrap(~admin_1_name, strip.position = \"right\", scales = \"free_y\", ncol = 1)      # introduce facets \n\nplot_by_region",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Contact tracing</span>"
    ]
  },
  {
    "objectID": "new_pages/contact_tracing.html#kpi-tables",
    "href": "new_pages/contact_tracing.html#kpi-tables",
    "title": "25  Contact tracing",
    "section": "25.4 KPI Tables",
    "text": "25.4 KPI Tables\nThere are a number of different Key Performance Indicators (KPIs) that can be calculated and tracked at varying levels of disaggregations and across different time periods to monitor contact tracing performance. Once you have the calculations down and the basic table format; it is fairly easy to swap in and out different KPIs.\nThere are numerous sources of contact tracing KPIs, such as this one from ResolveToSaveLives.org. The majority of the work will be walking through your data structure and thinking through all of the inclusion/exclusion criteria. We show a few examples below; using Go.Data metadata structure:\n\n\n\n\n\n\n\n\n\nCategory\nIndicator\nGo.Data Numerator\nGo.Data Denominator\n\n\n\n\nProcess Indicator - Speed of Contact Tracing\n% cases interviewed and isolated within 24h of case report\nCOUNT OF case_id WHERE (date_of_reporting - date_of_data_entry) &lt; 1 day AND (isolation_startdate - date_of_data_entry) &lt; 1 day\nCOUNT OF case_id\n\n\nProcess Indicator - Speed of Contact Tracing\n% contacts notified and quarantined within 24h of elicitation\nCOUNT OF contact_id WHERE followup_status == “SEEN_NOT_OK” OR “SEEN_OK” AND date_of_followup - date_of_reporting &lt; 1 day\nCOUNT OF contact_id\n\n\nProcess Indicator - Completeness of Testing\n% new symptomatic cases tested and interviewed within 3 days of onset of symptoms\nCOUNT OF case_id WHERE (date_of_reporting - date_of_onset) &lt; =3 days\nCOUNT OF case_id\n\n\nOutcome Indicator - Overall\n% new cases among existing contact list\nCOUNT OF case_id WHERE was_contact == “TRUE”\nCOUNT OF case_id\n\n\n\nBelow we will walk through a sample exercise of creating a nice table visual to show contact follow-up across admin areas. At the end, we will make it fit for presentation with the formattable package (but you could use other packages like flextable - see Tables for presentation).\nHow you create a table like this will depend on the structure of your contact tracing data. Use the Descriptive tables page to learn how to summarise data using dplyr functions.\nWe will create a table that will be dynamic and change as the data change. To make the results interesting, we will set a report_date to allow us to simulate running the table on a certain day (we pick 10th June 2020). The data are filtered to that date.\n\n# Set \"Report date\" to simulate running the report with data \"as of\" this date\nreport_date &lt;- as.Date(\"2020-06-10\")\n\n# Create follow-up data to reflect the report date.\ntable_data &lt;- followups_clean %&gt;% \n  filter(date_of_followup &lt;= report_date)\n\nNow, based on our data structure, we will do the following:\n\nBegin with the followups data and summarise it to contain, for each unique contact:\n\n\n\nThe date of latest record (no matter the status of the encounter)\n\nThe date of latest encounter where the contact was “seen”\n\nThe encounter status at that final “seen” encounter (e.g. with symptoms, without symptoms)\n\n\n\nJoin these data to the contacts data, which contains other information such as the overall contact status, date of last exposure to a case, etc. Also we will calculate metrics of interest for each contact such as days since last exposure\n\nWe group the enhanced contact data by geographic region (admin_2_name) and calculate summary statistics per region\n\nFinally, we format the table nicely for presentation\n\nFirst we summarise the follow-up data to get the information of interest:\n\nfollowup_info &lt;- table_data %&gt;% \n  group_by(contact_id) %&gt;% \n  summarise(\n    date_last_record   = max(date_of_followup, na.rm=T),\n    date_last_seen     = max(date_of_followup[followup_status %in% c(\"seen_ok\", \"seen_not_ok\")], na.rm=T),\n    status_last_record = followup_status[which(date_of_followup == date_last_record)]) %&gt;% \n  ungroup()\n\nHere is how these data look:\n\n\n\n\n\n\nNow we will add this information to the contacts dataset, and calculate some additional columns.\n\ncontacts_info &lt;- followup_info %&gt;% \n  right_join(contacts, by = \"contact_id\") %&gt;% \n  mutate(\n    database_date       = max(date_last_record, na.rm=T),\n    days_since_seen     = database_date - date_last_seen,\n    days_since_exposure = database_date - date_of_last_exposure\n    )\n\nHere is how these data look. Note contacts column to the right, and new calculated column at the far right.\n\n\n\n\n\n\nNext we summarise the contacts data by region, to achieve a concise data frame of summary statistic columns.\n\ncontacts_table &lt;- contacts_info %&gt;% \n  \n  group_by(`Admin 2` = admin_2_name) %&gt;%\n  \n  summarise(\n    `Registered contacts` = n(),\n    `Active contacts`     = sum(contact_status == \"UNDER_FOLLOW_UP\", na.rm=T),\n    `In first week`       = sum(days_since_exposure &lt; 8, na.rm=T),\n    `In second week`      = sum(days_since_exposure &gt;= 8 & days_since_exposure &lt; 15, na.rm=T),\n    `Became case`         = sum(contact_status == \"BECAME_CASE\", na.rm=T),\n    `Lost to follow up`   = sum(days_since_seen &gt;= 3, na.rm=T),\n    `Never seen`          = sum(is.na(date_last_seen)),\n    `Followed up - signs` = sum(status_last_record == \"Seen_not_ok\" & date_last_record == database_date, na.rm=T),\n    `Followed up - no signs` = sum(status_last_record == \"Seen_ok\" & date_last_record == database_date, na.rm=T),\n    `Not Followed up`     = sum(\n      (status_last_record == \"NOT_ATTEMPTED\" | status_last_record == \"NOT_PERFORMED\") &\n        date_last_record == database_date, na.rm=T)) %&gt;% \n    \n  arrange(desc(`Registered contacts`))\n\n\n\n\n\n\n\nAnd now we apply styling from the formattable and knitr packages, including a footnote that shows the “as of” date.\n\ncontacts_table %&gt;%\n  mutate(\n    `Admin 2` = formatter(\"span\", style = ~ formattable::style(\n      color = ifelse(`Admin 2` == NA, \"red\", \"grey\"),\n      font.weight = \"bold\",font.style = \"italic\"))(`Admin 2`),\n    `Followed up - signs`= color_tile(\"white\", \"orange\")(`Followed up - signs`),\n    `Followed up - no signs`= color_tile(\"white\", \"#A0E2BD\")(`Followed up - no signs`),\n    `Became case`= color_tile(\"white\", \"grey\")(`Became case`),\n    `Lost to follow up`= color_tile(\"white\", \"grey\")(`Lost to follow up`), \n    `Never seen`= color_tile(\"white\", \"red\")(`Never seen`),\n    `Active contacts` = color_tile(\"white\", \"#81A4CE\")(`Active contacts`)\n  ) %&gt;%\n  kable(\"html\", escape = F, align =c(\"l\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\")) %&gt;%\n  kable_styling(\"hover\", full_width = FALSE) %&gt;%\n  add_header_above(c(\" \" = 3, \n                     \"Of contacts currently under follow up\" = 5,\n                     \"Status of last visit\" = 3)) %&gt;% \n  kableExtra::footnote(general = str_glue(\"Data are current to {format(report_date, '%b %d %Y')}\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf contacts currently under follow up\n\n\nStatus of last visit\n\n\n\nAdmin 2\nRegistered contacts\nActive contacts\nIn first week\nIn second week\nBecame case\nLost to follow up\nNever seen\nFollowed up - signs\nFollowed up - no signs\nNot Followed up\n\n\n\n\nDjembe \n59\n30\n44\n0\n2\n15\n22\n0\n0\n0\n\n\nTrumpet\n3\n1\n3\n0\n0\n0\n0\n0\n0\n0\n\n\nVenu \n2\n0\n0\n0\n2\n0\n2\n0\n0\n0\n\n\nCongas \n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n\n\nCornet \n1\n0\n1\n0\n1\n0\n1\n0\n0\n0\n\n\n\nNote: \n\n\n\n\n\n\n\n\n\n\n\n\n Data are current to Jun 10 2020",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Contact tracing</span>"
    ]
  },
  {
    "objectID": "new_pages/contact_tracing.html#transmission-matrices",
    "href": "new_pages/contact_tracing.html#transmission-matrices",
    "title": "25  Contact tracing",
    "section": "25.5 Transmission Matrices",
    "text": "25.5 Transmission Matrices\nAs discussed in the Heat plots page, you can create a matrix of “who infected whom” using geom_tile().\nWhen new contacts are created, Go.Data stores this relationship information in the relationships API endpoint; and we can see the first 50 rows of this dataset below. This means that we can create a heat plot with relatively few steps given each contact is already joined to it’s source case.\n\n\n\n\n\n\nAs done above for the age pyramid comparing cases and contacts, we can select the few variables we need and create columns with categorical age groupings for both sources (cases) and targets (contacts).\n\nheatmap_ages &lt;- relationships %&gt;% \n  select(source_age, target_age) %&gt;% \n  mutate(                              # transmute is like mutate() but removes all other columns\n    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),\n    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5))) \n\nAs described previously, we create cross-tabulation;\n\ncross_tab &lt;- table(\n  source_cases = heatmap_ages$source_age_class,\n  target_cases = heatmap_ages$target_age_class)\n\ncross_tab\n\n            target_cases\nsource_cases 0-4 5-9 10-14 15-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54\n       0-4     0   0     0     0     0     0     0     0     0     1     0\n       5-9     0   0     1     0     0     0     0     1     0     0     0\n       10-14   0   0     0     0     0     0     0     0     0     0     0\n       15-19   0   0     0     0     0     0     0     0     0     0     0\n       20-24   1   1     0     1     2     0     2     1     0     0     0\n       25-29   1   2     0     0     0     0     0     0     0     0     0\n       30-34   0   0     0     0     0     0     0     0     1     1     0\n       35-39   0   2     0     0     0     0     0     0     0     1     0\n       40-44   0   0     0     0     1     0     2     1     0     3     1\n       45-49   1   2     2     0     0     0     3     0     1     0     3\n       50-54   1   2     1     2     0     0     1     0     0     3     4\n       55-59   0   1     0     0     1     1     2     0     0     0     0\n       60-64   0   0     0     0     0     0     0     0     0     0     0\n       65-69   0   0     0     0     0     0     0     0     0     0     0\n       70-74   0   0     0     0     0     0     0     0     0     0     0\n       75-79   0   0     0     0     0     0     0     0     0     0     0\n       80+     1   0     0     2     1     0     0     0     1     0     0\n            target_cases\nsource_cases 55-59 60-64 65-69 70-74 75-79 80+\n       0-4       1     0     0     0     0   0\n       5-9       1     0     0     0     0   0\n       10-14     0     0     0     0     0   0\n       15-19     0     0     0     0     0   0\n       20-24     1     0     0     0     0   1\n       25-29     0     0     0     0     0   0\n       30-34     1     0     0     0     0   0\n       35-39     0     0     0     0     0   0\n       40-44     1     0     0     0     1   1\n       45-49     2     1     0     0     0   1\n       50-54     1     0     1     0     0   1\n       55-59     0     0     0     0     0   0\n       60-64     0     0     0     0     0   0\n       65-69     0     0     0     0     0   0\n       70-74     0     0     0     0     0   0\n       75-79     0     0     0     0     0   0\n       80+       0     0     0     0     0   0\n\n\nconvert into long format with proportions;\n\nlong_prop &lt;- data.frame(prop.table(cross_tab))\n\nand create a heat-map for age.\n\nggplot(data = long_prop)+       # use long data, with proportions as Freq\n  geom_tile(                    # visualize it in tiles\n    aes(\n      x = target_cases,         # x-axis is case age\n      y = source_cases,     # y-axis is infector age\n      fill = Freq))+            # color of the tile is the Freq column in the data\n  scale_fill_gradient(          # adjust the fill color of the tiles\n    low = \"blue\",\n    high = \"orange\")+\n  theme(axis.text.x = element_text(angle = 90))+\n  labs(                         # labels\n    x = \"Target case age\",\n    y = \"Source case age\",\n    title = \"Who infected whom\",\n    subtitle = \"Frequency matrix of transmission events\",\n    fill = \"Proportion of all\\ntranmsission events\"     # legend title\n  )",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Contact tracing</span>"
    ]
  },
  {
    "objectID": "new_pages/contact_tracing.html#resources",
    "href": "new_pages/contact_tracing.html#resources",
    "title": "25  Contact tracing",
    "section": "25.6 Resources",
    "text": "25.6 Resources\nhttps://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting\nhttps://worldhealthorganization.github.io/godata/\nhttps://community-godata.who.int/",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Contact tracing</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html",
    "href": "new_pages/survey_analysis.html",
    "title": "26  Survey analysis",
    "section": "",
    "text": "26.1 Overview\nThis page demonstrates the use of several packages for survey analysis.\nMost survey R packages rely on the survey package for doing weighted analysis. We will use survey as well as srvyr (a wrapper for survey allowing for tidyverse-style coding) and gtsummary (a wrapper for survey allowing for publication ready tables). While the original survey package does not allow for tidyverse-style coding, it does have the added benefit of allowing for survey-weighted generalised linear models (which will be added to this page at a later date). We will also demonstrate using a function from the sitrep package to create sampling weights (n.b this package is currently not yet on CRAN, but can be installed from github).\nMost of this page is based off work done for the “R4Epis” project; for detailed code and R-markdown templates see the “R4Epis” github page. Some of the survey package based code is based off early versions of EPIET case studies.\nAt current this page does not address sample size calculations or sampling. For a simple to use sample size calculator see OpenEpi. The GIS basics page of the handbook will eventually have a section on spatial random sampling, and this page will eventually have a section on sampling frames as well as sample size calculations.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#overview",
    "href": "new_pages/survey_analysis.html#overview",
    "title": "26  Survey analysis",
    "section": "",
    "text": "Survey data\nObservation time\nWeighting\nSurvey design objects\nDescriptive analysis\nWeighted proportions\nWeighted rates",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#preparation",
    "href": "new_pages/survey_analysis.html#preparation",
    "title": "26  Survey analysis",
    "section": "26.2 Preparation",
    "text": "26.2 Preparation\n\nPackages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load packages with library() from base R. See the page on R basics for more information on R packages.\nHere we also demonstrate using the p_load_gh() function from pacman to install a load a package from github which has not yet been published on CRAN.\n\n## load packages from CRAN\npacman::p_load(rio,          # File import\n               here,         # File locator\n               tidyverse,    # data management + ggplot2 graphics\n               tsibble,      # handle time series datasets\n               survey,       # for survey functions\n               srvyr,        # dplyr wrapper for survey package\n               gtsummary,    # wrapper for survey package to produce tables\n               apyramid,     # a package dedicated to creating age pyramids\n               patchwork,    # for combining ggplots\n               ggforce       # for alluvial/sankey plots\n               ) \n\n## load packages from github\npacman::p_load_gh(\n     \"R4EPI/sitrep\"          # for observation time / weighting functions\n)\n\n\n\nLoad data\nThe example dataset used in this section:\n\nfictional mortality survey data.\nfictional population counts for the survey area.\ndata dictionary for the fictional mortality survey data.\n\nThis is based off the MSF OCA ethical review board pre-approved survey. The fictional dataset was produced as part of the “R4Epis” project. This is all based off data collected using KoboToolbox, which is a data collection software based off Open Data Kit.\nKobo allows you to export both the collected data, as well as the data dictionary for that dataset. We strongly recommend doing this as it simplifies data cleaning and is useful for looking up variables/questions.\nTIP: The Kobo data dictionary has variable names in the “name” column of the survey sheet. Possible values for each variable are specified in choices sheet. In the choices tab, “name” has the shortened value and the “label::english” and “label::french” columns have the appropriate long versions. Using the epidict package msf_dict_survey() function to import a Kobo dictionary excel file will re-format this for you so it can be used easily to recode. \nCAUTION: The example dataset is not the same as an export (as in Kobo you export different questionnaire levels individually) - see the survey data section below to merge the different levels.\nThe dataset is imported using the import() function from the rio package. See the page on Import and export for various ways to import data.\n\n# import the survey data\nsurvey_data &lt;- rio::import(\"survey_data.xlsx\")\n\n# import the dictionary into R\nsurvey_dict &lt;- rio::import(\"survey_dict.xlsx\") \n\nThe first 10 rows of the survey are displayed below.\n\n\n\n\n\n\nWe also want to import the data on sampling population so that we can produce appropriate weights. This data can be in different formats, however we would suggest to have it as seen below (this can just be typed in to an excel).\n\n# import the population data\npopulation &lt;- rio::import(\"population.xlsx\")\n\nThe first 10 rows of the survey are displayed below.\n\n\n\n\n\n\nFor cluster surveys you may want to add survey weights at the cluster level. You could read this data in as above. Alternatively if there are only a few counts, these could be entered as below in to a tibble. In any case you will need to have one column with a cluster identifier which matches your survey data, and another column with the number of households in each cluster.\n\n## define the number of households in each cluster\ncluster_counts &lt;- tibble(cluster = c(\"village_1\", \"village_2\", \"village_3\", \"village_4\", \n                                     \"village_5\", \"village_6\", \"village_7\", \"village_8\",\n                                     \"village_9\", \"village_10\"), \n                         households = c(700, 400, 600, 500, 300, \n                                        800, 700, 400, 500, 500))\n\n\n\nClean data\nThe below makes sure that the date column is in the appropriate format. There are several other ways of doing this (see the Working with dates page for details), however using the dictionary to define dates is quick and easy.\nWe also create an age group variable using the age_categories() function from epikit - see cleaning data handbook section for details. In addition, we create a character variable defining which district the various clusters are in.\nFinally, we recode all of the yes/no variables to TRUE/FALSE variables - otherwise these cant be used by the survey proportion functions.\n\n## select the date variable names from the dictionary \nDATEVARS &lt;- survey_dict %&gt;% \n  filter(type == \"date\") %&gt;% \n  filter(name %in% names(survey_data)) %&gt;% \n  ## filter to match the column names of your data\n  pull(name) # select date vars\n  \n## change to dates \nsurvey_data &lt;- survey_data %&gt;%\n  mutate(across(all_of(DATEVARS), as.Date))\n\n\n## add those with only age in months to the year variable (divide by twelve)\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(age_years = if_else(is.na(age_years), \n                             age_months / 12, \n                             age_years))\n\n## define age group variable\nsurvey_data &lt;- survey_data %&gt;% \n     mutate(age_group = age_categories(age_years, \n                                    breakers = c(0, 3, 15, 30, 45)\n                                    ))\n\n\n## create a character variable based off groups of a different variable \nsurvey_data &lt;- survey_data %&gt;% \n  mutate(health_district = case_when(\n    cluster_number %in% c(1:5) ~ \"district_a\", \n    TRUE ~ \"district_b\"\n  ))\n\n\n## select the yes/no variable names from the dictionary \nYNVARS &lt;- survey_dict %&gt;% \n  filter(type == \"yn\") %&gt;% \n  filter(name %in% names(survey_data)) %&gt;% \n  ## filter to match the column names of your data\n  pull(name) # select yn vars\n  \n## change to dates \nsurvey_data &lt;- survey_data %&gt;%\n  mutate(across(all_of(YNVARS), \n                str_detect, \n                pattern = \"yes\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(all_of(YNVARS), str_detect, pattern = \"yes\")`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#survey-data",
    "href": "new_pages/survey_analysis.html#survey-data",
    "title": "26  Survey analysis",
    "section": "26.3 Survey data",
    "text": "26.3 Survey data\nThere numerous different sampling designs that can be used for surveys. Here we will demonstrate code for: - Stratified - Cluster - Stratified and cluster\nAs described above (depending on how you design your questionnaire) the data for each level would be exported as a separate dataset from Kobo. In our example there is one level for households and one level for individuals within those households.\nThese two levels are linked by a unique identifier. For a Kobo dataset this variable is “_index” at the household level, which matches the “_parent_index” at the individual level. This will create new rows for household with each matching individual, see the handbook section on joining for details.\n\n## join the individual and household data to form a complete data set\nsurvey_data &lt;- left_join(survey_data_hh, \n                         survey_data_indiv,\n                         by = c(\"_index\" = \"_parent_index\"))\n\n\n## create a unique identifier by combining indeces of the two levels \nsurvey_data &lt;- survey_data %&gt;% \n     mutate(uid = str_glue(\"{index}_{index_y}\"))",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#observation-time",
    "href": "new_pages/survey_analysis.html#observation-time",
    "title": "26  Survey analysis",
    "section": "26.4 Observation time",
    "text": "26.4 Observation time\nFor mortality surveys we want to now how long each individual was present for in the location to be able to calculate an appropriate mortality rate for our period of interest. This is not relevant to all surveys, but particularly for mortality surveys this is important as they are conducted frequently among mobile or displaced populations.\nTo do this we first define our time period of interest, also known as a recall period (i.e. the time that participants are asked to report on when answering questions). We can then use this period to set inappropriate dates to missing, i.e. if deaths are reported from outside the period of interest.\n\n## set the start/end of recall period\n## can be changed to date variables from dataset \n## (e.g. arrival date & date questionnaire)\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(recall_start = as.Date(\"2018-01-01\"), \n         recall_end   = as.Date(\"2018-05-01\")\n  )\n\n\n# set inappropriate dates to NA based on rules \n## e.g. arrivals before start, departures departures after end\nsurvey_data &lt;- survey_data %&gt;%\n      mutate(\n           arrived_date = if_else(arrived_date &lt; recall_start, \n                                 as.Date(NA),\n                                  arrived_date),\n           birthday_date = if_else(birthday_date &lt; recall_start,\n                                  as.Date(NA),\n                                  birthday_date),\n           left_date = if_else(left_date &gt; recall_end,\n                              as.Date(NA),\n                               left_date),\n           death_date = if_else(death_date &gt; recall_end,\n                               as.Date(NA),\n                               death_date)\n           )\n\nWe can then use our date variables to define start and end dates for each individual. We can use the find_start_date() function from sitrep to fine the causes for the dates and then use that to calculate the difference between days (person-time).\nstart date: Earliest appropriate arrival event within your recall period Either the beginning of your recall period (which you define in advance), or a date after the start of recall if applicable (e.g. arrivals or births)\nend date: Earliest appropriate departure event within your recall period Either the end of your recall period, or a date before the end of recall if applicable (e.g. departures, deaths)\n\n## create new variables for start and end dates/causes\nsurvey_data &lt;- survey_data %&gt;% \n     ## choose earliest date entered in survey\n     ## from births, household arrivals, and camp arrivals \n     find_start_date(\"birthday_date\",\n                  \"arrived_date\",\n                  period_start = \"recall_start\",\n                  period_end   = \"recall_end\",\n                  datecol      = \"startdate\",\n                  datereason   = \"startcause\" \n                 ) %&gt;%\n     ## choose earliest date entered in survey\n     ## from camp departures, death and end of the study\n     find_end_date(\"left_date\",\n                \"death_date\",\n                period_start = \"recall_start\",\n                period_end   = \"recall_end\",\n                datecol      = \"enddate\",\n                datereason   = \"endcause\" \n               )\n\n\n## label those that were present at the start/end (except births/deaths)\nsurvey_data &lt;- survey_data %&gt;% \n     mutate(\n       ## fill in start date to be the beginning of recall period (for those empty) \n       startdate = if_else(is.na(startdate), recall_start, startdate), \n       ## set the start cause to present at start if equal to recall period \n       ## unless it is equal to the birth date \n       startcause = if_else(startdate == recall_start & startcause != \"birthday_date\",\n                              \"Present at start\", startcause), \n       ## fill in end date to be end of recall period (for those empty) \n       enddate = if_else(is.na(enddate), recall_end, enddate), \n       ## set the end cause to present at end if equall to recall end \n       ## unless it is equal to the death date\n       endcause = if_else(enddate == recall_end & endcause != \"death_date\", \n                            \"Present at end\", endcause))\n\n\n## Define observation time in days\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(obstime = as.numeric(enddate - startdate))",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#weighting",
    "href": "new_pages/survey_analysis.html#weighting",
    "title": "26  Survey analysis",
    "section": "26.5 Weighting",
    "text": "26.5 Weighting\nIt is important that you drop erroneous observations before adding survey weights. For example if you have observations with negative observation time, you will need to check those (you can do this with the assert_positive_timespan() function from sitrep. Another thing is if you want to drop empty rows (e.g. with drop_na(uid)) or remove duplicates (see handbook section on De-duplication for details). Those without consent need to be dropped too.\nIn this example we filter for the cases we want to drop and store them in a separate data frame - this way we can describe those that were excluded from the survey. We then use the anti_join() function from dplyr to remove these dropped cases from our survey data.\nDANGER: You cant have missing values in your weight variable, or any of the variables relevant to your survey design (e.g. age, sex, strata or cluster variables).\n\n## store the cases that you drop so you can describe them (e.g. non-consenting \n## or wrong village/cluster)\ndropped &lt;- survey_data %&gt;% \n  filter(!consent | is.na(startdate) | is.na(enddate) | village_name == \"other\")\n\n## use the dropped cases to remove the unused rows from the survey data set  \nsurvey_data &lt;- anti_join(survey_data, dropped, by = names(dropped))\n\nAs mentioned above we demonstrate how to add weights for three different study designs (stratified, cluster and stratified cluster). These require information on the source population and/or the clusters surveyed. We will use the stratified cluster code for this example, but use whichever is most appropriate for your study design.\n\n# stratified ------------------------------------------------------------------\n# create a variable called \"surv_weight_strata\"\n# contains weights for each individual - by age group, sex and health district\nsurvey_data &lt;- add_weights_strata(x = survey_data,\n                                         p = population,\n                                         surv_weight = \"surv_weight_strata\",\n                                         surv_weight_ID = \"surv_weight_ID_strata\",\n                                         age_group, sex, health_district)\n\n## cluster ---------------------------------------------------------------------\n\n# get the number of people of individuals interviewed per household\n# adds a variable with counts of the household (parent) index variable\nsurvey_data &lt;- survey_data %&gt;%\n  add_count(index, name = \"interviewed\")\n\n\n## create cluster weights\nsurvey_data &lt;- add_weights_cluster(x = survey_data,\n                                          cl = cluster_counts,\n                                          eligible = member_number,\n                                          interviewed = interviewed,\n                                          cluster_x = village_name,\n                                          cluster_cl = cluster,\n                                          household_x = index,\n                                          household_cl = households,\n                                          surv_weight = \"surv_weight_cluster\",\n                                          surv_weight_ID = \"surv_weight_ID_cluster\",\n                                          ignore_cluster = FALSE,\n                                          ignore_household = FALSE)\n\n\n# stratified and cluster ------------------------------------------------------\n# create a survey weight for cluster and strata\nsurvey_data &lt;- survey_data %&gt;%\n  mutate(surv_weight_cluster_strata = surv_weight_strata * surv_weight_cluster)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#survey-design-objects",
    "href": "new_pages/survey_analysis.html#survey-design-objects",
    "title": "26  Survey analysis",
    "section": "26.6 Survey design objects",
    "text": "26.6 Survey design objects\nCreate survey object according to your study design. Used the same way as data frames to calculate weight proportions etc. Make sure that all necessary variables are created before this.\nThere are four options, comment out those you do not use: - Simple random - Stratified - Cluster - Stratified cluster\nFor this template - we will pretend that we cluster surveys in two separate strata (health districts A and B). So to get overall estimates we need have combined cluster and strata weights.\nAs mentioned previously, there are two packages available for doing this. The classic one is survey and then there is a wrapper package called srvyr that makes tidyverse-friendly objects and functions. We will demonstrate both, but note that most of the code in this chapter will use srvyr based objects. The one exception is that the gtsummary package only accepts survey objects.\n\n26.6.1 Survey package\nThe survey package effectively uses base R coding, and so it is not possible to use pipes (%&gt;%) or other dplyr syntax. With the survey package we use the svydesign() function to define a survey object with appropriate clusters, weights and strata.\nNOTE: we need to use the tilde (~) in front of variables, this is because the package uses the base R syntax of assigning variables based on formulae. \n\n# simple random ---------------------------------------------------------------\nbase_survey_design_simple &lt;- svydesign(ids = ~1, # 1 for no cluster ids\n                   weights = NULL,               # No weight added\n                   strata = NULL,                # sampling was simple (no strata)\n                   data = survey_data            # have to specify the dataset\n                  )\n\n## stratified ------------------------------------------------------------------\nbase_survey_design_strata &lt;- svydesign(ids = ~1,  # 1 for no cluster ids\n                   weights = ~surv_weight_strata, # weight variable created above\n                   strata = ~health_district,     # sampling was stratified by district\n                   data = survey_data             # have to specify the dataset\n                  )\n\n# cluster ---------------------------------------------------------------------\nbase_survey_design_cluster &lt;- svydesign(ids = ~village_name, # cluster ids\n                   weights = ~surv_weight_cluster, # weight variable created above\n                   strata = NULL,                 # sampling was simple (no strata)\n                   data = survey_data              # have to specify the dataset\n                  )\n\n# stratified cluster ----------------------------------------------------------\nbase_survey_design &lt;- svydesign(ids = ~village_name,      # cluster ids\n                   weights = ~surv_weight_cluster_strata, # weight variable created above\n                   strata = ~health_district,             # sampling was stratified by district\n                   data = survey_data                     # have to specify the dataset\n                  )\n\n\n\n26.6.2 Srvyr package\nWith the srvyr package we can use the as_survey_design() function, which has all the same arguments as above but allows pipes (%&gt;%), and so we do not need to use the tilde (~).\n\n## simple random ---------------------------------------------------------------\nsurvey_design_simple &lt;- survey_data %&gt;% \n  as_survey_design(ids = 1, # 1 for no cluster ids \n                   weights = NULL, # No weight added\n                   strata = NULL # sampling was simple (no strata)\n                  )\n## stratified ------------------------------------------------------------------\nsurvey_design_strata &lt;- survey_data %&gt;%\n  as_survey_design(ids = 1, # 1 for no cluster ids\n                   weights = surv_weight_strata, # weight variable created above\n                   strata = health_district # sampling was stratified by district\n                  )\n## cluster ---------------------------------------------------------------------\nsurvey_design_cluster &lt;- survey_data %&gt;%\n  as_survey_design(ids = village_name, # cluster ids\n                   weights = surv_weight_cluster, # weight variable created above\n                   strata = NULL # sampling was simple (no strata)\n                  )\n\n## stratified cluster ----------------------------------------------------------\nsurvey_design &lt;- survey_data %&gt;%\n  as_survey_design(ids = village_name, # cluster ids\n                   weights = surv_weight_cluster_strata, # weight variable created above\n                   strata = health_district # sampling was stratified by district\n                  )",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#descriptive-analysis",
    "href": "new_pages/survey_analysis.html#descriptive-analysis",
    "title": "26  Survey analysis",
    "section": "26.7 Descriptive analysis",
    "text": "26.7 Descriptive analysis\nBasic descriptive analysis and visualisation is covered extensively in other chapters of the handbook, so we will not dwell on it here. For details see the chapters on descriptive tables, statistical tests, tables for presentation, ggplot basics and R markdown reports.\nIn this section we will focus on how to investigate bias in your sample and visualise this. We will also look at visualising population flow in a survey setting using alluvial/sankey diagrams.\nIn general, you should consider including the following descriptive analyses:\n\nFinal number of clusters, households and individuals included\n\nNumber of excluded individuals and the reasons for exclusion\nMedian (range) number of households per cluster and individuals per household\n\n\n26.7.1 Sampling bias\nCompare the proportions in each age group between your sample and the source population. This is important to be able to highlight potential sampling bias. You could similarly repeat this looking at distributions by sex.\nNote that these p-values are just indicative, and a descriptive discussion (or visualisation with age-pyramids below) of the distributions in your study sample compared to the source population is more important than the binomial test itself. This is because increasing sample size will more often than not lead to differences that may be irrelevant after weighting your data.\n\n## counts and props of the study population\nag &lt;- survey_data %&gt;% \n  group_by(age_group) %&gt;% \n  drop_na(age_group) %&gt;% \n  tally() %&gt;% \n  mutate(proportion = n / sum(n), \n         n_total = sum(n))\n\n## counts and props of the source population\npropcount &lt;- population %&gt;% \n  group_by(age_group) %&gt;%\n    tally(population) %&gt;%\n    mutate(proportion = n / sum(n))\n\n## bind together the columns of two tables, group by age, and perform a \n## binomial test to see if n/total is significantly different from population\n## proportion.\n  ## suffix here adds to text to the end of columns in each of the two datasets\nleft_join(ag, propcount, by = \"age_group\", suffix = c(\"\", \"_pop\")) %&gt;%\n  group_by(age_group) %&gt;%\n  ## broom::tidy(binom.test()) makes a data frame out of the binomial test and\n  ## will add the variables p.value, parameter, conf.low, conf.high, method, and\n  ## alternative. We will only use p.value here. You can include other\n  ## columns if you want to report confidence intervals\n  mutate(binom = list(broom::tidy(binom.test(n, n_total, proportion_pop)))) %&gt;%\n  unnest(cols = c(binom)) %&gt;% # important for expanding the binom.test data frame\n  mutate(proportion_pop = proportion_pop * 100) %&gt;%\n  ## Adjusting the p-values to correct for false positives \n  ## (because testing multiple age groups). This will only make \n  ## a difference if you have many age categories\n  mutate(p.value = p.adjust(p.value, method = \"holm\")) %&gt;%\n                      \n  ## Only show p-values over 0.001 (those under report as &lt;0.001)\n  mutate(p.value = ifelse(p.value &lt; 0.001, \n                          \"&lt;0.001\", \n                          as.character(round(p.value, 3)))) %&gt;% \n  \n  ## rename the columns appropriately\n  select(\n    \"Age group\" = age_group,\n    \"Study population (n)\" = n,\n    \"Study population (%)\" = proportion,\n    \"Source population (n)\" = n_pop,\n    \"Source population (%)\" = proportion_pop,\n    \"P-value\" = p.value\n  )\n\n# A tibble: 5 × 6\n# Groups:   Age group [5]\n  `Age group` `Study population (n)` `Study population (%)`\n  &lt;chr&gt;                        &lt;int&gt;                  &lt;dbl&gt;\n1 0-2                             12                 0.0256\n2 3-14                            42                 0.0896\n3 15-29                           64                 0.136 \n4 30-44                           52                 0.111 \n5 45+                            299                 0.638 \n# ℹ 3 more variables: `Source population (n)` &lt;dbl&gt;,\n#   `Source population (%)` &lt;dbl&gt;, `P-value` &lt;chr&gt;\n\n\n\n\n26.7.2 Demographic pyramids\nDemographic (or age-sex) pyramids are an easy way of visualising the distribution in your survey population. It is also worth considering creating descriptive tables of age and sex by survey strata. We will demonstrate using the apyramid package as it allows for weighted proportions using our survey design object created above. Other options for creating demographic pyramids are covered extensively in that chapter of the handbook. We will also use a wrapper function from apyramid called age_pyramid() which saves a few lines of coding for producing a plot with proportions.\nAs with the formal binomial test of difference, seen above in the sampling bias section, we are interested here in visualising whether our sampled population is substantially different from the source population and whether weighting corrects this difference. To do this we will use the patchwork package to show our ggplot visualisations side-by-side; for details see the section on combining plots in ggplot tips chapter of the handbook. We will visualise our source population, our un-weighted survey population and our weighted survey population. You may also consider visualising by each strata of your survey - in our example here that would be by using the argument stack_by  = \"health_district\" (see ?plot_age_pyramid for details).\nNOTE: The x and y axes are flipped in pyramids \n\n## define x-axis limits and labels ---------------------------------------------\n## (update these numbers to be the values for your graph)\nmax_prop &lt;- 35      # choose the highest proportion you want to show \nstep &lt;- 5           # choose the space you want beween labels \n\n## this part defines vector using the above numbers with axis breaks\nbreaks &lt;- c(\n    seq(max_prop/100 * -1, 0 - step/100, step/100), \n    0, \n    seq(0 + step / 100, max_prop/100, step/100)\n    )\n\n## this part defines vector using the above numbers with axis limits\nlimits &lt;- c(max_prop/100 * -1, max_prop/100)\n\n## this part defines vector using the above numbers with axis labels\nlabels &lt;-  c(\n      seq(max_prop, step, -step), \n      0, \n      seq(step, max_prop, step)\n    )\n\n\n## create plots individually  --------------------------------------------------\n\n## plot the source population \n## nb: this needs to be collapsed for the overall population (i.e. removing health districts)\nsource_population &lt;- population %&gt;%\n  ## ensure that age and sex are factors\n  mutate(age_group = factor(age_group, \n                            levels = c(\"0-2\", \n                                       \"3-14\", \n                                       \"15-29\",\n                                       \"30-44\", \n                                       \"45+\")), \n         sex = factor(sex)) %&gt;% \n  group_by(age_group, sex) %&gt;% \n  ## add the counts for each health district together \n  summarise(population = sum(population)) %&gt;% \n  ## remove the grouping so can calculate overall proportion\n  ungroup() %&gt;% \n  mutate(proportion = population / sum(population)) %&gt;% \n  ## plot pyramid \n  age_pyramid(\n            age_group = age_group, \n            split_by = sex, \n            count = proportion, \n            proportional = TRUE) +\n  ## only show the y axis label (otherwise repeated in all three plots)\n  labs(title = \"Source population\", \n       y = \"\", \n       x = \"Age group (years)\") + \n  ## make the x axis the same for all plots \n  scale_y_continuous(breaks = breaks, \n    limits = limits, \n    labels = labels)\n  \n  \n## plot the unweighted sample population \nsample_population &lt;- age_pyramid(survey_data, \n                 age_group = \"age_group\", \n                 split_by = \"sex\",\n                 proportion = TRUE) + \n  ## only show the x axis label (otherwise repeated in all three plots)\n  labs(title = \"Unweighted sample population\", \n       y = \"Proportion (%)\", \n       x = \"\") + \n  ## make the x axis the same for all plots \n  scale_y_continuous(breaks = breaks, \n    limits = limits, \n    labels = labels)\n\n\n## plot the weighted sample population \nweighted_population &lt;- survey_design %&gt;% \n  ## make sure the variables are factors\n  mutate(age_group = factor(age_group), \n         sex = factor(sex)) %&gt;%\n  age_pyramid(\n    age_group = \"age_group\",\n    split_by = \"sex\", \n    proportion = TRUE) +\n  ## only show the x axis label (otherwise repeated in all three plots)\n  labs(title = \"Weighted sample population\", \n       y = \"\", \n       x = \"\")  + \n  ## make the x axis the same for all plots \n  scale_y_continuous(breaks = breaks, \n    limits = limits, \n    labels = labels)\n\n## combine all three plots  ----------------------------------------------------\n## combine three plots next to eachother using + \nsource_population + sample_population + weighted_population + \n  ## only show one legend and define theme \n  ## note the use of & for combining theme with plot_layout()\n  plot_layout(guides = \"collect\") & \n  theme(legend.position = \"bottom\",                    # move legend to bottom\n        legend.title = element_blank(),                # remove title\n        text = element_text(size = 18),                # change text size\n        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1) # turn x-axis text\n       )\n\n\n\n\n\n\n\n\n\n\n26.7.3 Alluvial/sankey diagram\nVisualising starting points and outcomes for individuals can be very helpful to get an overview. There is quite an obvious application for mobile populations, however there are numerous other applications such as cohorts or any other situation where there are transitions in states for individuals. These diagrams have several different names including alluvial, sankey and parallel sets - the details are in the handbook chapter on diagrams and charts.\n\n## summarize data\nflow_table &lt;- survey_data %&gt;%\n  count(startcause, endcause, sex) %&gt;%  # get counts \n  gather_set_data(x = c(\"startcause\", \"endcause\"))     # change format for plotting\n\n\n## plot your dataset \n  ## on the x axis is the start and end causes\n  ## gather_set_data generates an ID for each possible combination\n  ## splitting by y gives the possible start/end combos\n  ## value as n gives it as counts (could also be changed to proportion)\nggplot(flow_table, aes(x, id = id, split = y, value = n)) +\n  ## colour lines by sex \n  geom_parallel_sets(aes(fill = sex), alpha = 0.5, axis.width = 0.2) +\n  ## fill in the label boxes grey\n  geom_parallel_sets_axes(axis.width = 0.15, fill = \"grey80\", color = \"grey80\") +\n  ## change text colour and angle (needs to be adjusted)\n  geom_parallel_sets_labels(color = \"black\", angle = 0, size = 5) +\n  ## remove axis labels\n  theme_void()+\n  ## move legend to bottom\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#weighted-proportions",
    "href": "new_pages/survey_analysis.html#weighted-proportions",
    "title": "26  Survey analysis",
    "section": "26.8 Weighted proportions",
    "text": "26.8 Weighted proportions\nThis section will detail how to produce tables for weighted counts and proportions, with associated confidence intervals and design effect. There are four different options using functions from the following packages: survey, srvyr, sitrep and gtsummary. For minimal coding to produce a standard epidemiology style table, we would recommend the sitrep function - which is a wrapper for srvyr code; note however that this is not yet on CRAN and may change in the future. Otherwise, the survey code is likely to be the most stable long-term, whereas srvyr will fit most nicely within tidyverse work-flows. While gtsummary functions hold a lot of potential, they appear to be experimental and incomplete at the time of writing.\n\n26.8.1 Survey package\nWe can use the svyciprop() function from survey to get weighted proportions and accompanying 95% confidence intervals. An appropriate design effect can be extracted using the svymean() rather than svyprop() function. It is worth noting that svyprop() only appears to accept variables between 0 and 1 (or TRUE/FALSE), so categorical variables will not work.\nNOTE: Functions from survey also accept srvyr design objects, but here we have used the survey design object just for consistency \n\n## produce weighted counts \nsvytable(~died, base_survey_design)\n\ndied\n     FALSE       TRUE \n1406244.43   76213.01 \n\n## produce weighted proportions\nsvyciprop(~died, base_survey_design, na.rm = T)\n\n              2.5% 97.5%\ndied 0.0514 0.0208  0.12\n\n## get the design effect \nsvymean(~died, base_survey_design, na.rm = T, deff = T) %&gt;% \n  deff()\n\ndiedFALSE  diedTRUE \n 3.755508  3.755508 \n\n\nWe can combine the functions from survey shown above in to a function which we define ourselves below, called svy_prop; and we can then use that function together with map() from the purrr package to iterate over several variables and create a table. See the handbook iteration chapter for details on purrr.\n\n# Define function to calculate weighted counts, proportions, CI and design effect\n# x is the variable in quotation marks \n# design is your survey design object\n\nsvy_prop &lt;- function(design, x) {\n  \n  ## put the variable of interest in a formula \n  form &lt;- as.formula(paste0( \"~\" , x))\n  ## only keep the TRUE column of counts from svytable\n  weighted_counts &lt;- svytable(form, design)[[2]]\n  ## calculate proportions (multiply by 100 to get percentages)\n  weighted_props &lt;- svyciprop(form, design, na.rm = TRUE) * 100\n  ## extract the confidence intervals and multiply to get percentages\n  weighted_confint &lt;- confint(weighted_props) * 100\n  ## use svymean to calculate design effect and only keep the TRUE column\n  design_eff &lt;- deff(svymean(form, design, na.rm = TRUE, deff = TRUE))[[TRUE]]\n  \n  ## combine in to one data frame\n  full_table &lt;- cbind(\n    \"Variable\"        = x,\n    \"Count\"           = weighted_counts,\n    \"Proportion\"      = weighted_props,\n    weighted_confint, \n    \"Design effect\"   = design_eff\n    )\n  \n  ## return table as a dataframe\n  full_table &lt;- data.frame(full_table, \n             ## remove the variable names from rows (is a separate column now)\n             row.names = NULL)\n  \n  ## change numerics back to numeric\n  full_table[ , 2:6] &lt;- as.numeric(full_table[, 2:6])\n  \n  ## return dataframe\n  full_table\n}\n\n## iterate over several variables to create a table \npurrr::map(\n  ## define variables of interest\n  c(\"left\", \"died\", \"arrived\"), \n  ## state function using and arguments for that function (design)\n  svy_prop, design = base_survey_design) %&gt;% \n  ## collapse list in to a single data frame\n  bind_rows() %&gt;% \n  ## round \n  mutate(across(where(is.numeric), round, digits = 1))\n\n  Variable    Count Proportion X2.5. X97.5. Design.effect\n1     left 701199.1       47.3  39.2   55.5           2.4\n2     died  76213.0        5.1   2.1   12.1           3.8\n3  arrived 761799.0       51.4  40.9   61.7           3.9\n\n\n\n\n26.8.2 Srvyr package\nWith srvyr we can use dplyr syntax to create a table. Note that the survey_mean() function is used and the proportion argument is specified, and also that the same function is used to calculate design effect. This is because srvyr wraps around both of the survey package functions svyciprop() and svymean(), which are used in the above section.\nNOTE: It does not seem to be possible to get proportions from categorical variables using srvyr either, if you need this then check out the section below using sitrep \n\n## use the srvyr design object\nsurvey_design %&gt;% \n  summarise(\n    ## produce the weighted counts \n    counts = survey_total(died), \n    ## produce weighted proportions and confidence intervals \n    ## multiply by 100 to get a percentage \n    props = survey_mean(died, \n                        proportion = TRUE, \n                        vartype = \"ci\") * 100, \n    ## produce the design effect \n    deff = survey_mean(died, deff = TRUE)) %&gt;% \n  ## only keep the rows of interest\n  ## (drop standard errors and repeat proportion calculation)\n  select(counts, props, props_low, props_upp, deff_deff)\n\n# A tibble: 1 × 5\n  counts props props_low props_upp deff_deff\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 76213.  5.14      2.08      12.1      3.76\n\n\nHere too we could write a function to then iterate over multiple variables using the purrr package. See the handbook iteration chapter for details on purrr.\n\n# Define function to calculate weighted counts, proportions, CI and design effect\n# design is your survey design object\n# x is the variable in quotation marks \n\n\nsrvyr_prop &lt;- function(design, x) {\n  \n  summarise(\n    ## using the survey design object\n    design, \n    ## produce the weighted counts \n    counts = survey_total(.data[[x]]), \n    ## produce weighted proportions and confidence intervals \n    ## multiply by 100 to get a percentage \n    props = survey_mean(.data[[x]], \n                        proportion = TRUE, \n                        vartype = \"ci\") * 100, \n    ## produce the design effect \n    deff = survey_mean(.data[[x]], deff = TRUE)) %&gt;% \n  ## add in the variable name\n  mutate(variable = x) %&gt;% \n  ## only keep the rows of interest\n  ## (drop standard errors and repeat proportion calculation)\n  select(variable, counts, props, props_low, props_upp, deff_deff)\n  \n}\n  \n\n## iterate over several variables to create a table \npurrr::map(\n  ## define variables of interest\n  c(\"left\", \"died\", \"arrived\"), \n  ## state function using and arguments for that function (design)\n  ~srvyr_prop(.x, design = survey_design)) %&gt;% \n  ## collapse list in to a single data frame\n  bind_rows()\n\n# A tibble: 3 × 6\n  variable  counts props props_low props_upp deff_deff\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 left     701199. 47.3      39.2       55.5      2.38\n2 died      76213.  5.14      2.08      12.1      3.76\n3 arrived  761799. 51.4      40.9       61.7      3.93\n\n\n\n\n26.8.3 Sitrep package\nThe tab_survey() function from sitrep is a wrapper for srvyr, allowing you to create weighted tables with minimal coding. It also allows you to calculate weighted proportions for categorical variables.\n\n## using the survey design object\nsurvey_design %&gt;% \n  ## pass the names of variables of interest unquoted\n  tab_survey(arrived, left, died, education_level,\n             deff = TRUE,   # calculate the design effect\n             pretty = TRUE  # merge the proportion and 95%CI\n             )\n\nWarning: removing 257 missing value(s) from `education_level`\n\n\n# A tibble: 9 × 5\n  variable        value            n  deff ci               \n  &lt;chr&gt;           &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            \n1 arrived         TRUE       761799.  3.93 51.4% (40.9-61.7)\n2 arrived         FALSE      720658.  3.93 48.6% (38.3-59.1)\n3 left            TRUE       701199.  2.38 47.3% (39.2-55.5)\n4 left            FALSE      781258.  2.38 52.7% (44.5-60.8)\n5 died            TRUE        76213.  3.76 5.1% (2.1-12.1)  \n6 died            FALSE     1406244.  3.76 94.9% (87.9-97.9)\n7 education_level higher     171644.  4.70 42.4% (26.9-59.7)\n8 education_level primary    102609.  2.37 25.4% (16.2-37.3)\n9 education_level secondary  130201.  6.68 32.2% (16.5-53.3)\n\n\n\n\n26.8.4 Gtsummary package\nWith gtsummary there does not seem to be inbuilt functions yet to add confidence intervals or design effect. Here we show how to define a function for adding confidence intervals and then add confidence intervals to a gtsummary table created using the tbl_svysummary() function.\n\nconfidence_intervals &lt;- function(data, variable, by, ...) {\n  \n  ## extract the confidence intervals and multiply to get percentages\n  props &lt;- svyciprop(as.formula(paste0( \"~\" , variable)),\n              data, na.rm = TRUE)\n  \n  ## extract the confidence intervals \n  as.numeric(confint(props) * 100) %&gt;% ## make numeric and multiply for percentage\n    round(., digits = 1) %&gt;%           ## round to one digit\n    c(.) %&gt;%                           ## extract the numbers from matrix\n    paste0(., collapse = \"-\")          ## combine to single character\n}\n\n## using the survey package design object\ntbl_svysummary(base_survey_design, \n               include = c(arrived, left, died),   ## define variables want to include\n               statistic = list(everything() ~ c(\"{n} ({p}%)\"))) %&gt;% ## define stats of interest\n  add_n() %&gt;%  ## add the weighted total \n  add_stat(fns = everything() ~ confidence_intervals) %&gt;% ## add CIs\n  ## modify the column headers\n  modify_header(\n    list(\n      n ~ \"**Weighted total (N)**\",\n      stat_0 ~ \"**Weighted Count**\",\n      add_stat_1 ~ \"**95%CI**\"\n    )\n    )\n\n\n\n\n\n\n\n\nCharacteristic\nWeighted total (N)\nWeighted Count1\n95%CI\n\n\n\n\narrived\n1,482,457\n761,799 (51%)\n40.9-61.7\n\n\nleft\n1,482,457\n701,199 (47%)\n39.2-55.5\n\n\ndied\n1,482,457\n76,213 (5.1%)\n2.1-12.1\n\n\n\n1 n (%)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#weighted-ratios",
    "href": "new_pages/survey_analysis.html#weighted-ratios",
    "title": "26  Survey analysis",
    "section": "26.9 Weighted ratios",
    "text": "26.9 Weighted ratios\nSimilarly for weighted ratios (such as for mortality ratios) you can use the survey or the srvyr package. You could similarly write functions (similar to those above) to iterate over several variables. You could also create a function for gtsummary as above but currently it does not have inbuilt functionality.\n\n26.9.1 Survey package\n\nratio &lt;- svyratio(~died, \n         denominator = ~obstime, \n         design = base_survey_design)\n\nci &lt;- confint(ratio)\n\ncbind(\n  ratio$ratio * 10000, \n  ci * 10000\n)\n\n      obstime    2.5 %   97.5 %\ndied 5.981922 1.194294 10.76955\n\n\n\n\n26.9.2 Srvyr package\n\nsurvey_design %&gt;% \n  ## survey ratio used to account for observation time \n  summarise(\n    mortality = survey_ratio(\n      as.numeric(died) * 10000, \n      obstime, \n      vartype = \"ci\")\n    )\n\n# A tibble: 1 × 3\n  mortality mortality_low mortality_upp\n      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1      5.98         0.349          11.6",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survey_analysis.html#resources",
    "href": "new_pages/survey_analysis.html#resources",
    "title": "26  Survey analysis",
    "section": "26.10 Resources",
    "text": "26.10 Resources\nUCLA stats page\nAnalyze survey data free\nsrvyr packge\ngtsummary package\nEPIET survey case studies",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Survey analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survival_analysis.html",
    "href": "new_pages/survival_analysis.html",
    "title": "27  Survival analysis",
    "section": "",
    "text": "27.1 Overview\nSurvival analysis focuses on describing for a given individual or group of individuals, a defined point of event called the failure (occurrence of a disease, cure from a disease, death, relapse after response to treatment…) that occurs after a period of time called failure time (or follow-up time in cohort/population-based studies) during which individuals are observed. To determine the failure time, it is then necessary to define a time of origin (that can be the inclusion date, the date of diagnosis…).\nThe target of inference for survival analysis is then the time between an origin and an event. In current medical research, it is widely used in clinical studies to assess the effect of a treatment for instance, or in cancer epidemiology to assess a large variety of cancer survival measures.\nIt is usually expressed through the survival probability which is the probability that the event of interest has not occurred by a duration t.\nCensoring: Censoring occurs when at the end of follow-up, some of the individuals have not had the event of interest, and thus their true time to event is unknown. We will mostly focus on right censoring here but for more details on censoring and survival analysis in general, you can see references.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survival_analysis.html#preparation",
    "href": "new_pages/survival_analysis.html#preparation",
    "title": "27  Survival analysis",
    "section": "27.2 Preparation",
    "text": "27.2 Preparation\n\nLoad packages\nTo run survival analyses in R, one the most widely used package is the survival package. We first install it and then load it as well as the other packages that will be used in this section:\nIn this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\nThis page explores survival analyses using the linelist used in most of the previous pages and on which we apply some changes to have a proper survival data.\n\n\nImport dataset\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import linelist\nlinelist_case_data &lt;- rio::import(\"linelist_cleaned.rds\")\n\n\n\nData management and transformation\nIn short, survival data can be described as having the following three characteristics:\n\nthe dependent variable or response is the waiting time until the occurrence of a well-defined event,\nobservations are censored, in the sense that for some units the event of interest has not occurred at the time the data are analyzed, and\nthere are predictors or explanatory variables whose effect on the waiting time we wish to assess or control.\n\nThus, we will create different variables needed to respect that structure and run the survival analysis.\nWe define:\n\na new data frame linelist_surv for this analysis\n\nour event of interest as being “death” (hence our survival probability will be the probability of being alive after a certain time after the time of origin),\nthe follow-up time (futime) as the time between the time of onset and the time of outcome in days,\ncensored patients as those who recovered or for whom the final outcome is not known ie the event “death” was not observed (event=0).\n\nCAUTION: Since in a real cohort study, the information on the time of origin and the end of the follow-up is known given individuals are observed, we will remove observations where the date of onset or the date of outcome is unknown. Also the cases where the date of onset is later than the date of outcome will be removed since they are considered as wrong.\nTIP: Given that filtering to greater than (&gt;) or less than (&lt;) a date can remove rows with missing values, applying the filter on the wrong dates will also remove the rows with missing dates.\nWe then use case_when() to create a column age_cat_small in which there are only 3 age categories.\n\n#create a new data called linelist_surv from the linelist_case_data\n\nlinelist_surv &lt;-  linelist_case_data %&gt;% \n     \n  dplyr::filter(\n       # remove observations with wrong or missing dates of onset or date of outcome\n       date_outcome &gt; date_onset) %&gt;% \n  \n  dplyr::mutate(\n       # create the event var which is 1 if the patient died and 0 if he was right censored\n       event = ifelse(is.na(outcome) | outcome == \"Recover\", 0, 1), \n    \n       # create the var on the follow-up time in days\n       futime = as.double(date_outcome - date_onset), \n    \n       # create a new age category variable with only 3 strata levels\n       age_cat_small = dplyr::case_when( \n            age_years &lt; 5  ~ \"0-4\",\n            age_years &gt;= 5 & age_years &lt; 20 ~ \"5-19\",\n            age_years &gt;= 20   ~ \"20+\"),\n       \n       # previous step created age_cat_small var as character.\n       # now convert it to factor and specify the levels.\n       # Note that the NA values remain NA's and are not put in a level \"unknown\" for example,\n       # since in the next analyses they have to be removed.\n       age_cat_small = fct_relevel(age_cat_small, \"0-4\", \"5-19\", \"20+\")\n       )\n\nTIP: We can verify the new columns we have created by doing a summary on the futime and a cross-tabulation between event and outcome from which it was created. Besides this verification it is a good habit to communicate the median follow-up time when interpreting survival analysis results.\n\nsummary(linelist_surv$futime)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    6.00   10.00   11.98   16.00   64.00 \n\n# cross tabulate the new event var and the outcome var from which it was created\n# to make sure the code did what it was intended to\nlinelist_surv %&gt;% \n  tabyl(outcome, event)\n\n outcome    0    1\n   Death    0 1952\n Recover 1547    0\n    &lt;NA&gt; 1040    0\n\n\nNow we cross-tabulate the new age_cat_small var and the old age_cat col to ensure correct assingments\n\nlinelist_surv %&gt;% \n  tabyl(age_cat_small, age_cat)\n\n age_cat_small 0-4 5-9 10-14 15-19 20-29 30-49 50-69 70+ NA_\n           0-4 834   0     0     0     0     0     0   0   0\n          5-19   0 852   717   575     0     0     0   0   0\n           20+   0   0     0     0   862   554    69   5   0\n          &lt;NA&gt;   0   0     0     0     0     0     0   0  71\n\n\nNow we review the 10 first observations of the linelist_surv data looking at specific variables (including those newly created).\n\nlinelist_surv %&gt;% \n  select(case_id, age_cat_small, date_onset, date_outcome, outcome, event, futime) %&gt;% \n  head(10)\n\n   case_id age_cat_small date_onset date_outcome outcome event futime\n1   8689b7           0-4 2014-05-13   2014-05-18 Recover     0      5\n2   11f8ea           20+ 2014-05-16   2014-05-30 Recover     0     14\n3   893f25           0-4 2014-05-21   2014-05-29 Recover     0      8\n4   be99c8          5-19 2014-05-22   2014-05-24 Recover     0      2\n5   07e3e8          5-19 2014-05-27   2014-06-01 Recover     0      5\n6   369449           0-4 2014-06-02   2014-06-07   Death     1      5\n7   f393b4           20+ 2014-06-05   2014-06-18 Recover     0     13\n8   1389ca           20+ 2014-06-05   2014-06-09   Death     1      4\n9   2978ac          5-19 2014-06-06   2014-06-15   Death     1      9\n10  fc15ef          5-19 2014-06-16   2014-07-09 Recover     0     23\n\n\nWe can also cross-tabulate the columns age_cat_small and gender to have more details on the distribution of this new column by gender. We use tabyl() and the adorn functions from janitor as described in the Descriptive tables page.\n\n\nlinelist_surv %&gt;% \n  tabyl(gender, age_cat_small, show_na = F) %&gt;% \n  adorn_totals(where = \"both\") %&gt;% \n  adorn_percentages() %&gt;% \n  adorn_pct_formatting() %&gt;% \n  adorn_ns(position = \"front\")\n\n gender         0-4          5-19           20+          Total\n      f 482 (22.4%) 1,184 (54.9%)   490 (22.7%) 2,156 (100.0%)\n      m 325 (15.0%)   880 (40.6%)   960 (44.3%) 2,165 (100.0%)\n  Total 807 (18.7%) 2,064 (47.8%) 1,450 (33.6%) 4,321 (100.0%)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survival_analysis.html#basics-of-survival-analysis",
    "href": "new_pages/survival_analysis.html#basics-of-survival-analysis",
    "title": "27  Survival analysis",
    "section": "27.3 Basics of survival analysis",
    "text": "27.3 Basics of survival analysis\n\nBuilding a surv-type object\nWe will first use Surv() from survival to build a survival object from the follow-up time and event columns.\nThe result of such a step is to produce an object of type Surv that condenses the time information and whether the event of interest (death) was observed. This object will ultimately be used in the right-hand side of subsequent model formulae (see documentation).\n\n# Use Suv() syntax for right-censored data\nsurvobj &lt;- Surv(time = linelist_surv$futime,\n                event = linelist_surv$event)\n\n\n\n\n\n\nTo review, here are the first 10 rows of the linelist_surv data, viewing only some important columns.\n\nlinelist_surv %&gt;% \n  select(case_id, date_onset, date_outcome, futime, outcome, event) %&gt;% \n  head(10)\n\n   case_id date_onset date_outcome futime outcome event\n1   8689b7 2014-05-13   2014-05-18      5 Recover     0\n2   11f8ea 2014-05-16   2014-05-30     14 Recover     0\n3   893f25 2014-05-21   2014-05-29      8 Recover     0\n4   be99c8 2014-05-22   2014-05-24      2 Recover     0\n5   07e3e8 2014-05-27   2014-06-01      5 Recover     0\n6   369449 2014-06-02   2014-06-07      5   Death     1\n7   f393b4 2014-06-05   2014-06-18     13 Recover     0\n8   1389ca 2014-06-05   2014-06-09      4   Death     1\n9   2978ac 2014-06-06   2014-06-15      9   Death     1\n10  fc15ef 2014-06-16   2014-07-09     23 Recover     0\n\n\nAnd here are the first 10 elements of survobj. It prints as essentially a vector of follow-up time, with “+” to represent if an observation was right-censored. See how the numbers align above and below.\n\n#print the 50 first elements of the vector to see how it presents\nhead(survobj, 10)\n\n [1]  5+ 14+  8+  2+  5+  5  13+  4   9  23+\n\n\n\n\nRunning initial analyses\nWe then start our analysis using the survfit() function to produce a survfit object, which fits the default calculations for Kaplan Meier (KM) estimates of the overall (marginal) survival curve, which are in fact a step function with jumps at observed event times. The final survfit object contains one or more survival curves and is created using the Surv object as a response variable in the model formula.\nNOTE: The Kaplan-Meier estimate is a nonparametric maximum likelihood estimate (MLE) of the survival function. . (see resources for more information).\nThe summary of this survfit object will give what is called a life table. For each time step of the follow-up (time) where an event happened (in ascending order):\n\nthe number of people who were at risk of developing the event (people who did not have the event yet nor were censored: n.risk)\n\nthose who did develop the event (n.event)\n\nand from the above: the probability of not developing the event (probability of not dying, or of surviving past that specific time)\n\nfinally, the standard error and the confidence interval for that probability are derived and displayed\n\nWe fit the KM estimates using the formula where the previously Surv object “survobj” is the response variable. “~ 1” precises we run the model for the overall survival.\n\n# fit the KM estimates using a formula where the Surv object \"survobj\" is the response variable.\n# \"~ 1\" signifies that we run the model for the overall survival  \nlinelistsurv_fit &lt;-  survival::survfit(survobj ~ 1)\n\n#print its summary for more details\nsummary(linelistsurv_fit)\n\nCall: survfit(formula = survobj ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1   4539      30    0.993 0.00120        0.991        0.996\n    2   4500      69    0.978 0.00217        0.974        0.982\n    3   4394     149    0.945 0.00340        0.938        0.952\n    4   4176     194    0.901 0.00447        0.892        0.910\n    5   3899     214    0.852 0.00535        0.841        0.862\n    6   3592     210    0.802 0.00604        0.790        0.814\n    7   3223     179    0.757 0.00656        0.745        0.770\n    8   2899     167    0.714 0.00700        0.700        0.728\n    9   2593     145    0.674 0.00735        0.660        0.688\n   10   2311     109    0.642 0.00761        0.627        0.657\n   11   2081     119    0.605 0.00788        0.590        0.621\n   12   1843      89    0.576 0.00809        0.560        0.592\n   13   1608      55    0.556 0.00823        0.540        0.573\n   14   1448      43    0.540 0.00837        0.524        0.556\n   15   1296      31    0.527 0.00848        0.511        0.544\n   16   1152      48    0.505 0.00870        0.488        0.522\n   17   1002      29    0.490 0.00886        0.473        0.508\n   18    898      21    0.479 0.00900        0.462        0.497\n   19    798       7    0.475 0.00906        0.457        0.493\n   20    705       4    0.472 0.00911        0.454        0.490\n   21    626      13    0.462 0.00932        0.444        0.481\n   22    546       8    0.455 0.00948        0.437        0.474\n   23    481       5    0.451 0.00962        0.432        0.470\n   24    436       4    0.447 0.00975        0.428        0.466\n   25    378       4    0.442 0.00993        0.423        0.462\n   26    336       3    0.438 0.01010        0.419        0.458\n   27    297       1    0.436 0.01017        0.417        0.457\n   29    235       1    0.435 0.01030        0.415        0.455\n   38     73       1    0.429 0.01175        0.406        0.452\n\n\nWhile using summary() we can add the option times and specify certain times at which we want to see the survival information\n\n#print its summary at specific times\nsummary(linelistsurv_fit, times = c(5,10,20,30,60))\n\nCall: survfit(formula = survobj ~ 1)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5   3899     656    0.852 0.00535        0.841        0.862\n   10   2311     810    0.642 0.00761        0.627        0.657\n   20    705     446    0.472 0.00911        0.454        0.490\n   30    210      39    0.435 0.01030        0.415        0.455\n   60      2       1    0.429 0.01175        0.406        0.452\n\n\nWe can also use the print() function. The print.rmean = TRUE argument is used to obtain the mean survival time and its standard error (se).\nNOTE: The restricted mean survival time (RMST) is a specific survival measure more and more used in cancer survival analysis and which is often defined as the area under the survival curve, given we observe patients up to restricted time T (more details in Resources section).\n\n# print linelistsurv_fit object with mean survival time and its se. \nprint(linelistsurv_fit, print.rmean = TRUE)\n\nCall: survfit(formula = survobj ~ 1)\n\n        n events rmean* se(rmean) median 0.95LCL 0.95UCL\n[1,] 4539   1952   33.1     0.539     17      16      18\n    * restricted mean with upper limit =  64 \n\n\nTIP: We can create the surv object directly in the survfit() function and save a line of code. This will then look like: linelistsurv_quick &lt;-  survfit(Surv(futime, event) ~ 1, data=linelist_surv).\n\n\nCumulative hazard\nBesides the summary() function, we can also use the str() function that gives more details on the structure of the survfit() object. It is a list of 16 elements.\nAmong these elements is an important one: cumhaz, which is a numeric vector. This could be plotted to allow show the cumulative hazard, with the hazard being the instantaneous rate of event occurrence (see references).\n\nstr(linelistsurv_fit)\n\nList of 16\n $ n        : int 4539\n $ time     : num [1:59] 1 2 3 4 5 6 7 8 9 10 ...\n $ n.risk   : num [1:59] 4539 4500 4394 4176 3899 ...\n $ n.event  : num [1:59] 30 69 149 194 214 210 179 167 145 109 ...\n $ n.censor : num [1:59] 9 37 69 83 93 159 145 139 137 121 ...\n $ surv     : num [1:59] 0.993 0.978 0.945 0.901 0.852 ...\n $ std.err  : num [1:59] 0.00121 0.00222 0.00359 0.00496 0.00628 ...\n $ cumhaz   : num [1:59] 0.00661 0.02194 0.05585 0.10231 0.15719 ...\n $ std.chaz : num [1:59] 0.00121 0.00221 0.00355 0.00487 0.00615 ...\n $ type     : chr \"right\"\n $ logse    : logi TRUE\n $ conf.int : num 0.95\n $ conf.type: chr \"log\"\n $ lower    : num [1:59] 0.991 0.974 0.938 0.892 0.841 ...\n $ upper    : num [1:59] 0.996 0.982 0.952 0.91 0.862 ...\n $ call     : language survfit(formula = survobj ~ 1)\n - attr(*, \"class\")= chr \"survfit\"\n\n\n\n\n\nPlotting Kaplan-Meir curves\nOnce the KM estimates are fitted, we can visualize the probability of being alive through a given time using the basic plot() function that draws the “Kaplan-Meier curve”. In other words, the curve below is a conventional illustration of the survival experience in the whole patient group.\nWe can quickly verify the follow-up time min and max on the curve.\nAn easy way to interpret is to say that at time zero, all the participants are still alive and survival probability is then 100%. This probability decreases over time as patients die. The proportion of participants surviving past 60 days of follow-up is around 40%.\n\nplot(linelistsurv_fit, \n     xlab = \"Days of follow-up\",    # x-axis label\n     ylab=\"Survival Probability\",   # y-axis label\n     main= \"Overall survival curve\" # figure title\n     )\n\n\n\n\n\n\n\n\nThe confidence interval of the KM survival estimates are also plotted by default and can be dismissed by adding the option conf.int = FALSE to the plot() command.\nSince the event of interest is “death”, drawing a curve describing the complements of the survival proportions will lead to drawing the cumulative mortality proportions. This can be done with lines(), which adds information to an existing plot.\n\n# original plot\nplot(\n  linelistsurv_fit,\n  xlab = \"Days of follow-up\",       \n  ylab = \"Survival Probability\",       \n  mark.time = TRUE,              # mark events on the curve: a \"+\" is printed at every event\n  conf.int = FALSE,              # do not plot the confidence interval\n  main = \"Overall survival curve and cumulative mortality\"\n  )\n\n# draw an additional curve to the previous plot\nlines(\n  linelistsurv_fit,\n  lty = 3,             # use different line type for clarity\n  fun = \"event\",       # draw the cumulative events instead of the survival \n  mark.time = FALSE,\n  conf.int = FALSE\n  )\n\n# add a legend to the plot\nlegend(\n  \"topright\",                               # position of legend\n  legend = c(\"Survival\", \"Cum. Mortality\"), # legend text \n  lty = c(1, 3),                            # line types to use in the legend\n  cex = .85,                                # parametes that defines size of legend text\n  bty = \"n\"                                 # no box type to be drawn for the legend\n  )",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survival_analysis.html#comparison-of-survival-curves",
    "href": "new_pages/survival_analysis.html#comparison-of-survival-curves",
    "title": "27  Survival analysis",
    "section": "27.4 Comparison of survival curves",
    "text": "27.4 Comparison of survival curves\nTo compare the survival within different groups of our observed participants or patients, we might need to first look at their respective survival curves and then run tests to evaluate the difference between independent groups. This comparison can concern groups based on gender, age, treatment, comorbidity…\n\nLog rank test\nThe log rank test is a popular test that compares the entire survival experience between two or more independent groups and can be thought of as a test of whether the survival curves are identical (overlapping) or not (null hypothesis of no difference in survival between the groups). The survdiff() function of the survival package allows running the log-rank test when we specify rho = 0 (which is the default). The test results gives a chi-square statistic along with a p-value since the log rank statistic is approximately distributed as a chi-square test statistic.\nWe first try to compare the survival curves by gender group. For this, we first try to visualize it (check whether the two survival curves are overlapping). A new survfit object will be created with a slightly different formula. Then the survdiff object will be created.\nBy supplying ~ gender as the right side of the formula, we no longer plot the overall survival but instead by gender.\n\n# create the new survfit object based on gender\nlinelistsurv_fit_sex &lt;-  survfit(Surv(futime, event) ~ gender, data = linelist_surv)\n\nNow we can plot the survival curves by gender. Have a look at the order of the strata levels in the gender column before defining your colors and legend.\n\n# set colors\ncol_sex &lt;- c(\"lightgreen\", \"darkgreen\")\n\n# create plot\nplot(\n  linelistsurv_fit_sex,\n  col = col_sex,\n  xlab = \"Days of follow-up\",\n  ylab = \"Survival Probability\")\n\n# add legend\nlegend(\n  \"topright\",\n  legend = c(\"Female\",\"Male\"),\n  col = col_sex,\n  lty = 1,\n  cex = .9,\n  bty = \"n\")\n\n\n\n\n\n\n\n\nAnd now we can compute the test of the difference between the survival curves using survdiff()\n\n#compute the test of the difference between the survival curves\nsurvival::survdiff(\n  Surv(futime, event) ~ gender, \n  data = linelist_surv\n  )\n\nCall:\nsurvival::survdiff(formula = Surv(futime, event) ~ gender, data = linelist_surv)\n\nn=4321, 218 observations deleted due to missingness.\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\ngender=f 2156      924      909     0.255     0.524\ngender=m 2165      929      944     0.245     0.524\n\n Chisq= 0.5  on 1 degrees of freedom, p= 0.5 \n\n\nWe see that the survival curve for women and the one for men overlap and the log-rank test does not give evidence of a survival difference between women and men.\nSome other R packages allow illustrating survival curves for different groups and testing the difference all at once. Using the ggsurvplot() function from the survminer package, we can also include in our curve the printed risk tables for each group, as well the p-value from the log-rank test.\nCAUTION: survminer functions require that you specify the survival object and again specify the data used to fit the survival object. Remember to do this to avoid non-specific error messages. \n\nsurvminer::ggsurvplot(\n    linelistsurv_fit_sex, \n    data = linelist_surv,          # again specify the data used to fit linelistsurv_fit_sex \n    conf.int = FALSE,              # do not show confidence interval of KM estimates\n    surv.scale = \"percent\",        # present probabilities in the y axis in %\n    break.time.by = 10,            # present the time axis with an increment of 10 days\n    xlab = \"Follow-up days\",\n    ylab = \"Survival Probability\",\n    pval = T,                      # print p-value of Log-rank test \n    pval.coord = c(40,.91),        # print p-value at these plot coordinates\n    risk.table = T,                # print the risk table at bottom \n    legend.title = \"Gender\",       # legend characteristics\n    legend.labs = c(\"Female\",\"Male\"),\n    font.legend = 10, \n    palette = \"Dark2\",             # specify color palette \n    surv.median.line = \"hv\",       # draw horizontal and vertical lines to the median survivals\n    ggtheme = theme_light()        # simplify plot background\n)\n\n\n\n\n\n\n\n\nWe may also want to test for differences in survival by the source of infection (source of contamination).\nIn this case, the Log rank test gives enough evidence of a difference in the survival probabilities at alpha= 0.005. The survival probabilities for patients that were infected at funerals are higher than the survival probabilities for patients that got infected in other places, suggesting a survival benefit.\n\nlinelistsurv_fit_source &lt;-  survfit(\n  Surv(futime, event) ~ source,\n  data = linelist_surv\n  )\n\n# plot\nggsurvplot( \n  linelistsurv_fit_source,\n  data = linelist_surv,\n  size = 1, linetype = \"strata\",   # line types\n  conf.int = T,\n  surv.scale = \"percent\",  \n  break.time.by = 10, \n  xlab = \"Follow-up days\",\n  ylab= \"Survival Probability\",\n  pval = T,\n  pval.coord = c(40,.91),\n  risk.table = T,\n  legend.title = \"Source of \\ninfection\",\n  legend.labs = c(\"Funeral\", \"Other\"),\n  font.legend = 10,\n  palette = c(\"#E7B800\",\"#3E606F\"),\n  surv.median.line = \"hv\", \n  ggtheme = theme_light()\n)\n\nWarning in geom_segment(aes(x = 0, y = max(y2), xend = max(x1), yend = max(y2)), : All aesthetics have length 1, but the data has 2 rows.\nℹ Did you mean to use `annotate()`?\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Did you mean to use `annotate()`?\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Did you mean to use `annotate()`?\nAll aesthetics have length 1, but the data has 2 rows.\nℹ Did you mean to use `annotate()`?",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survival_analysis.html#cox-regression-analysis",
    "href": "new_pages/survival_analysis.html#cox-regression-analysis",
    "title": "27  Survival analysis",
    "section": "27.5 Cox regression analysis",
    "text": "27.5 Cox regression analysis\nCox proportional hazards regression is one of the most popular regression techniques for survival analysis. Other models can also be used since the Cox model requires important assumptions that need to be verified for an appropriate use such as the proportional hazards assumption: see references.\nIn a Cox proportional hazards regression model, the measure of effect is the hazard rate (HR), which is the risk of failure (or the risk of death in our example), given that the participant has survived up to a specific time. Usually, we are interested in comparing independent groups with respect to their hazards, and we use a hazard ratio, which is analogous to an odds ratio in the setting of multiple logistic regression analysis. The cox.ph() function from the survival package is used to fit the model. The function cox.zph() from survival package may be used to test the proportional hazards assumption for a Cox regression model fit.\nNOTE: A probability must lie in the range 0 to 1. However, the hazard represents the expected number of events per one unit of time.\n\nIf the hazard ratio for a predictor is close to 1 then that predictor does not affect survival,\nif the HR is less than 1, then the predictor is protective (i.e., associated with improved survival),\nand if the HR is greater than 1, then the predictor is associated with increased risk (or decreased survival).\n\n\nFitting a Cox model\nWe can first fit a model to assess the effect of age and gender on the survival. By just printing the model, we have the information on:\n\nthe estimated regression coefficients coef which quantifies the association between the predictors and the outcome,\ntheir exponential (for interpretability, exp(coef)) which produces the hazard ratio,\ntheir standard error se(coef),\nthe z-score: how many standard errors is the estimated coefficient away from 0,\nand the p-value: the probability that the estimated coefficient could be 0.\n\nThe summary() function applied to the cox model object gives more information, such as the confidence interval of the estimated HR and the different test scores.\nThe effect of the first covariate gender is presented in the first row. genderm (male) is printed, implying that the first strata level (“f”), i.e the female group, is the reference group for the gender. Thus the interpretation of the test parameter is that of men compared to women. The p-value indicates there was not enough evidence of an effect of the gender on the expected hazard or of an association between gender and all-cause mortality.\nThe same lack of evidence is noted regarding age-group.\n\n#fitting the cox model\nlinelistsurv_cox_sexage &lt;-  survival::coxph(\n              Surv(futime, event) ~ gender + age_cat_small, \n              data = linelist_surv\n              )\n\n\n#printing the model fitted\nlinelistsurv_cox_sexage\n\nCall:\nsurvival::coxph(formula = Surv(futime, event) ~ gender + age_cat_small, \n    data = linelist_surv)\n\n                      coef exp(coef) se(coef)      z     p\ngenderm           -0.03149   0.96900  0.04767 -0.661 0.509\nage_cat_small5-19  0.09400   1.09856  0.06454  1.456 0.145\nage_cat_small20+   0.05032   1.05161  0.06953  0.724 0.469\n\nLikelihood ratio test=2.8  on 3 df, p=0.4243\nn= 4321, number of events= 1853 \n   (218 observations deleted due to missingness)\n\n#summary of the model\nsummary(linelistsurv_cox_sexage)\n\nCall:\nsurvival::coxph(formula = Surv(futime, event) ~ gender + age_cat_small, \n    data = linelist_surv)\n\n  n= 4321, number of events= 1853 \n   (218 observations deleted due to missingness)\n\n                      coef exp(coef) se(coef)      z Pr(&gt;|z|)\ngenderm           -0.03149   0.96900  0.04767 -0.661    0.509\nage_cat_small5-19  0.09400   1.09856  0.06454  1.456    0.145\nage_cat_small20+   0.05032   1.05161  0.06953  0.724    0.469\n\n                  exp(coef) exp(-coef) lower .95 upper .95\ngenderm               0.969     1.0320    0.8826     1.064\nage_cat_small5-19     1.099     0.9103    0.9680     1.247\nage_cat_small20+      1.052     0.9509    0.9176     1.205\n\nConcordance= 0.514  (se = 0.007 )\nLikelihood ratio test= 2.8  on 3 df,   p=0.4\nWald test            = 2.78  on 3 df,   p=0.4\nScore (logrank) test = 2.78  on 3 df,   p=0.4\n\n\nIt was interesting to run the model and look at the results but a first look to verify whether the proportional hazards assumptions is respected could help saving time.\n\ntest_ph_sexage &lt;- survival::cox.zph(linelistsurv_cox_sexage)\ntest_ph_sexage\n\n              chisq df    p\ngender        0.454  1 0.50\nage_cat_small 0.838  2 0.66\nGLOBAL        1.399  3 0.71\n\n\nNOTE: A second argument called method can be specified when computing the cox model, that determines how ties are handled. The default is “efron”, and the other options are “breslow” and “exact”.\nIn another model we add more risk factors such as the source of infection and the number of days between date of onset and admission. This time, we first verify the proportional hazards assumption before going forward.\nIn this model, we have included a continuous predictor (days_onset_hosp). In this case we interpret the parameter estimates as the increase in the expected log of the relative hazard for each one unit increase in the predictor, holding other predictors constant. We first verify the proportional hazards assumption.\n\n#fit the model\nlinelistsurv_cox &lt;-  coxph(\n                        Surv(futime, event) ~ gender + age_years+ source + days_onset_hosp,\n                        data = linelist_surv\n                        )\n\n\n#test the proportional hazard model\nlinelistsurv_ph_test &lt;- cox.zph(linelistsurv_cox)\nlinelistsurv_ph_test\n\n                   chisq df       p\ngender           0.45062  1    0.50\nage_years        0.00199  1    0.96\nsource           1.79622  1    0.18\ndays_onset_hosp 31.66167  1 1.8e-08\nGLOBAL          34.08502  4 7.2e-07\n\n\nThe graphical verification of this assumption may be performed with the function ggcoxzph() from the survminer package.\n\nsurvminer::ggcoxzph(linelistsurv_ph_test)\n\n\n\n\n\n\n\n\nThe model results indicate there is a negative association between onset to admission duration and all-cause mortality. The expected hazard is 0.9 times lower in a person who who is one day later admitted than another, holding gender constant. Or in a more straightforward explanation, a one unit increase in the duration of onset to admission is associated with a 10.7% (coef *100) decrease in the risk of death.\nResults show also a positive association between the source of infection and the all-cause mortality. Which is to say there is an increased risk of death (1.21x) for patients that got a source of infection other than funerals.\n\n#print the summary of the model\nsummary(linelistsurv_cox)\n\nCall:\ncoxph(formula = Surv(futime, event) ~ gender + age_years + source + \n    days_onset_hosp, data = linelist_surv)\n\n  n= 2772, number of events= 1180 \n   (1767 observations deleted due to missingness)\n\n                     coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \ngenderm          0.004710  1.004721  0.060827  0.077   0.9383    \nage_years       -0.002249  0.997753  0.002421 -0.929   0.3528    \nsourceother      0.178393  1.195295  0.084291  2.116   0.0343 *  \ndays_onset_hosp -0.104063  0.901169  0.014245 -7.305 2.77e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                exp(coef) exp(-coef) lower .95 upper .95\ngenderm            1.0047     0.9953    0.8918    1.1319\nage_years          0.9978     1.0023    0.9930    1.0025\nsourceother        1.1953     0.8366    1.0133    1.4100\ndays_onset_hosp    0.9012     1.1097    0.8764    0.9267\n\nConcordance= 0.566  (se = 0.009 )\nLikelihood ratio test= 71.31  on 4 df,   p=1e-14\nWald test            = 59.22  on 4 df,   p=4e-12\nScore (logrank) test = 59.54  on 4 df,   p=4e-12\n\n\nWe can verify this relationship with a table:\n\nlinelist_case_data %&gt;% \n  tabyl(days_onset_hosp, outcome) %&gt;% \n  adorn_percentages() %&gt;%  \n  adorn_pct_formatting()\n\n days_onset_hosp Death Recover   NA_\n               0 44.3%   31.4% 24.3%\n               1 46.6%   32.2% 21.2%\n               2 43.0%   32.8% 24.2%\n               3 45.0%   32.3% 22.7%\n               4 41.5%   38.3% 20.2%\n               5 40.0%   36.2% 23.8%\n               6 32.2%   48.7% 19.1%\n               7 31.8%   38.6% 29.5%\n               8 29.8%   38.6% 31.6%\n               9 30.3%   51.5% 18.2%\n              10 16.7%   58.3% 25.0%\n              11 36.4%   45.5% 18.2%\n              12 18.8%   62.5% 18.8%\n              13 10.0%   60.0% 30.0%\n              14 10.0%   50.0% 40.0%\n              15 28.6%   42.9% 28.6%\n              16 20.0%   80.0%  0.0%\n              17  0.0%  100.0%  0.0%\n              18  0.0%  100.0%  0.0%\n              22  0.0%  100.0%  0.0%\n              NA 52.7%   31.2% 16.0%\n\n\nWe would need to consider and investigate why this association exists in the data. One possible explanation could be that patients who live long enough to be admitted later had less severe disease to begin with. Another perhaps more likely explanation is that since we used a simulated fake dataset, this pattern does not reflect reality!\n\n\n\nForest plots\nWe can then visualize the results of the cox model using the practical forest plots with the ggforest() function of the survminer package.\n\nggforest(linelistsurv_cox, data = linelist_surv)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survival_analysis.html#time-dependent-covariates-in-survival-models",
    "href": "new_pages/survival_analysis.html#time-dependent-covariates-in-survival-models",
    "title": "27  Survival analysis",
    "section": "27.6 Time-dependent covariates in survival models",
    "text": "27.6 Time-dependent covariates in survival models\nSome of the following sections have been adapted with permission from an excellent introduction to survival analysis in R by Dr. Emily Zabor\nIn the last section we covered using Cox regression to examine associations between covariates of interest and survival outcomes.But these analyses rely on the covariate being measured at baseline, that is, before follow-up time for the event begins.\nWhat happens if you are interested in a covariate that is measured after follow-up time begins? Or, what if you have a covariate that can change over time?\nFor example, maybe you are working with clinical data where you repeated measures of hospital laboratory values that can change over time. This is an example of a Time Dependent Covariate. In order to address this you need a special setup, but fortunately the cox model is very flexible and this type of data can also be modeled with tools from the survival package.\n\nTime-dependent covariate setup\nAnalysis of time-dependent covariates in R requires setup of a special dataset. If interested, see the more detailed paper on this by the author of the survival package Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model.\nFor this, we’ll use a new dataset from the SemiCompRisks package named BMT, which includes data on 137 bone marrow transplant patients. The variables we’ll focus on are:\n\nT1 - time (in days) to death or last follow-up\n\ndelta1 - death indicator; 1-Dead, 0-Alive\n\nTA - time (in days) to acute graft-versus-host disease\n\ndeltaA - acute graft-versus-host disease indicator;\n\n1 - Developed acute graft-versus-host disease\n\n0 - Never developed acute graft-versus-host disease\n\n\nWe’ll load this dataset from the survival package using the base R command data(), which can be used for loading data that is already included in a R package that is loaded. The data frame BMT will appear in your R environment.\n\ndata(BMT, package = \"SemiCompRisks\")\n\n\nAdd unique patient identifier\nThere is no unique ID column in the BMT data, which is needed to create the type of dataset we want. So we use the function rowid_to_column() from the tidyverse package tibble to create a new id column called my_id (adds column at start of data frame with sequential row ids, starting at 1). We name the data frame bmt.\n\nbmt &lt;- rowid_to_column(BMT, \"my_id\")\n\nThe dataset now looks like this:\n\n\n\n\n\n\n\n\nExpand patient rows\nNext, we’ll use the tmerge() function with the event() and tdc() helper functions to create the restructured dataset. Our goal is to restructure the dataset to create a separate row for each patient for each time interval where they have a different value for deltaA. In this case, each patient can have at most two rows depending on whether they developed acute graft-versus-host disease during the data collection period. We’ll call our new indicator for the development of acute graft-versus-host disease agvhd.\n\ntmerge() creates a long dataset with multiple time intervals for the different covariate values for each patient\nevent() creates the new event indicator to go with the newly-created time intervals\ntdc() creates the time-dependent covariate column, agvhd, to go with the newly created time intervals\n\n\ntd_dat &lt;- \n  tmerge(\n    data1 = bmt %&gt;% select(my_id, T1, delta1), \n    data2 = bmt %&gt;% select(my_id, T1, delta1, TA, deltaA), \n    id = my_id, \n    death = event(T1, delta1),\n    agvhd = tdc(TA)\n    )\n\nTo see what this does, let’s look at the data for the first 5 individual patients.\nThe variables of interest in the original data looked like this:\n\nbmt %&gt;% \n  select(my_id, T1, delta1, TA, deltaA) %&gt;% \n  filter(my_id %in% seq(1, 5))\n\n  my_id   T1 delta1   TA deltaA\n1     1 2081      0   67      1\n2     2 1602      0 1602      0\n3     3 1496      0 1496      0\n4     4 1462      0   70      1\n5     5 1433      0 1433      0\n\n\nThe new dataset for these same patients looks like this:\n\ntd_dat %&gt;% \n  filter(my_id %in% seq(1, 5))\n\n  my_id   T1 delta1 tstart tstop death agvhd\n1     1 2081      0      0    67     0     0\n2     1 2081      0     67  2081     0     1\n3     2 1602      0      0  1602     0     0\n4     3 1496      0      0  1496     0     0\n5     4 1462      0      0    70     0     0\n6     4 1462      0     70  1462     0     1\n7     5 1433      0      0  1433     0     0\n\n\nNow some of our patients have two rows in the dataset corresponding to intervals where they have a different value of our new variable, agvhd. For example, Patient 1 now has two rows with a agvhd value of zero from time 0 to time 67, and a value of 1 from time 67 to time 2081.\n\n\n\nCox regression with time-dependent covariates\nNow that we’ve reshaped our data and added the new time-dependent aghvd variable, let’s fit a simple single variable cox regression model. We can use the same coxph() function as before, we just need to change our Surv() function to specify both the start and stop time for each interval using the time1 = and time2 = arguments.\n\nbmt_td_model = coxph(\n  Surv(time = tstart, time2 = tstop, event = death) ~ agvhd, \n  data = td_dat\n  )\n\nsummary(bmt_td_model)\n\nCall:\ncoxph(formula = Surv(time = tstart, time2 = tstop, event = death) ~ \n    agvhd, data = td_dat)\n\n  n= 163, number of events= 80 \n\n        coef exp(coef) se(coef)    z Pr(&gt;|z|)\nagvhd 0.3351    1.3980   0.2815 1.19    0.234\n\n      exp(coef) exp(-coef) lower .95 upper .95\nagvhd     1.398     0.7153    0.8052     2.427\n\nConcordance= 0.535  (se = 0.024 )\nLikelihood ratio test= 1.33  on 1 df,   p=0.2\nWald test            = 1.42  on 1 df,   p=0.2\nScore (logrank) test = 1.43  on 1 df,   p=0.2\n\n\nAgain, we’ll visualize our cox model results using the ggforest() function from the survminer package.:\n\nggforest(bmt_td_model, data = td_dat)\n\n\n\n\n\n\n\n\nAs you can see from the forest plot, confidence interval, and p-value, there does not appear to be a strong association between death and acute graft-versus-host disease in the context of our simple model.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/survival_analysis.html#resources",
    "href": "new_pages/survival_analysis.html#resources",
    "title": "27  Survival analysis",
    "section": "27.7 Resources",
    "text": "27.7 Resources\nSurvival Analysis Part I: Basic concepts and first analyses\nSurvival Analysis in R\nSurvival analysis in infectious disease research: Describing events in time\nChapter on advanced survival models Princeton\nUsing Time Dependent Covariates and Time Dependent Coefficients in the Cox Model\nSurvival analysis cheatsheet R\nSurvminer cheatsheet\nPaper on different survival measures for cancer registry data with Rcode provided as supplementary materials",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html",
    "href": "new_pages/gis.html",
    "title": "28  GIS basics",
    "section": "",
    "text": "28.1 Overview\nSpatial aspects of your data can provide a lot of insights into the situation of the outbreak, and to answer questions such as:\nThe current focus of this GIS page to address the needs of applied epidemiologists in outbreak response. We will explore basic spatial data visualization methods using tmap and ggplot2 packages. We will also walk through some of the basic spatial data management and querying methods with the sf package. Lastly, we will briefly touch upon concepts of spatial statistics such as spatial relationships, spatial autocorrelation, and spatial regression using the spdep package.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#overview",
    "href": "new_pages/gis.html#overview",
    "title": "28  GIS basics",
    "section": "",
    "text": "Where are the current disease hotspots?\nHow have the hotspots have changed over time?\nHow is the access to health facilities? Are any improvements needed?",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#key-terms",
    "href": "new_pages/gis.html#key-terms",
    "title": "28  GIS basics",
    "section": "28.2 Key terms",
    "text": "28.2 Key terms\nBelow we introduce some key terminology. For a thorough introduction to GIS and spatial analysis, we suggest that you review one of the longer tutorials or courses listed in the References section.\nGeographic Information System (GIS) - A GIS is a framework or environment for gathering, managing, analyzing, and visualizing spatial data.\n\nGIS software\nSome popular GIS software allow point-and-click interaction for map development and spatial analysis. These tools comes with advantages such as not needing to learn code and the ease of manually selecting and placing icons and features on a map. Here are two popular ones:\nArcGIS - A commercial GIS software developed by the company ESRI, which is very popular but quite expensive\nQGIS - A free open-source GIS software that can do almost anything that ArcGIS can do. You can download QGIS here\nUsing R as a GIS can seem more intimidating at first because instead of “point-and-click”, it has a “command-line interface” (you must code to acquire the desired outcome). However, this is a major advantage if you need to repetitively produce maps or create an analysis that is reproducible.\n\n\nSpatial data\nThe two primary forms of spatial data used in GIS are vector and raster data:\nVector Data - The most common format of spatial data used in GIS, vector data are comprised of geometric features of vertices and paths. Vector spatial data can be further divided into three widely-used types:\n\nPoints - A point consists of a coordinate pair (x,y) representing a specific location in a coordinate system. Points are the most basic form of spatial data, and may be used to denote a case (i.e. patient home) or a location (i.e. hospital) on a map.\nLines - A line is composed of two connected points. Lines have a length, and may be used to denote things like roads or rivers.\nPolygons - A polygon is composed of at least three line segments connected by points. Polygon features have a length (i.e. the perimeter of the area) as well as an area measurement. Polygons may be used to note an area (i.e. a village) or a structure (i.e. the actual area of a hospital).\n\nRaster Data - An alternative format for spatial data, raster data is a matrix of cells (e.g. pixels) with each cell containing information such as height, temperature, slope, forest cover, etc. These are often aerial photographs, satellite imagery, etc. Rasters can also be used as “base maps” below vector data.\n\n\nVisualizing spatial data\nTo visually represent spatial data on a map, GIS software requires you to provide sufficient information about where different features should be, in relation to one another. If you are using vector data, which will be true for most use cases, this information will typically be stored in a shapefile:\nShapefiles - A shapefile is a common data format for storing “vector” spatial data consisting or lines, points, or polygons. A single shapefile is actually a collection of at least three files - .shp, .shx, and .dbf. All of these sub-component files must be present in a given directory (folder) for the shapefile to be readable. These associated files can be compressed into a ZIP folder to be sent via email or download from a website.\nThe shapefile will contain information about the features themselves, as well as where to locate them on the Earth’s surface. This is important because while the Earth is a globe, maps are typically two-dimensional; choices about how to “flatten” spatial data can have a big impact on the look and interpretation of the resulting map.\nCoordinate Reference Systems (CRS) - A CRS is a coordinate-based system used to locate geographical features on the Earth’s surface. It has a few key components:\n\nCoordinate System - There are many many different coordinate systems, so make sure you know which system your coordinates are from. Degrees of latitude/longitude are common, but you could also see UTM coordinates.\nUnits - Know what the units are for your coordinate system (e.g. decimal degrees, meters)\nDatum - A particular modeled version of the Earth. These have been revised over the years, so ensure that your map layers are using the same datum.\nProjection - A reference to the mathematical equation that was used to project the truly round earth onto a flat surface (map).\n\nRemember that you can summarise spatial data without using the mapping tools shown below. Sometimes a simple table by geography (e.g. district, country, etc.) is all that is needed!",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#getting-started-with-gis",
    "href": "new_pages/gis.html#getting-started-with-gis",
    "title": "28  GIS basics",
    "section": "28.3 Getting started with GIS",
    "text": "28.3 Getting started with GIS\nThere are a couple of key items you will need to have and to think about to make a map. These include:\n\nA dataset – this can be in a spatial data format (such as shapefiles, as noted above) or it may not be in a spatial format (for instance just as a csv).\nIf your dataset is not in a spatial format you will also need a reference dataset. Reference data consists of the spatial representation of the data and the related attributes, which would include material containing the location and address information of specific features.\n\nIf you are working with pre-defined geographic boundaries (for example, administrative regions), reference shapefiles are often freely available to download from a government agency or data sharing organization. When in doubt, a good place to start is to Google “[regions] shapefile”\nIf you have address information, but no latitude and longitude, you may need to use a geocoding engine to get the spatial reference data for your records.\n\nAn idea about how you want to present the information in your datasets to your target audience. There are many different types of maps, and it is important to think about which type of map best fits your needs.\n\n\nTypes of maps for visualizing your data\nChoropleth map - a type of thematic map where colors, shading, or patterns are used to represent geographic regions in relation to their value of an attribute. For instance a larger value could be indicated by a darker colour than a smaller value. This type of map is particularly useful when visualizing a variable and how it changes across defined regions or geopolitical areas.\n\n\n\n\n\n\n\n\n\nCase density heatmap - a type of thematic map where colours are used to represent intensity of a value, however, it does not use defined regions or geopolitical boundaries to group data. This type of map is typically used for showing ‘hot spots’ or areas with a high density or concentration of points.\n\n\n\n\n\n\n\n\n\nDot density map - a thematic map type that uses dots to represent attribute values in your data. This type of map is best used to visualize the scatter of your data and visually scan for clusters.\nProportional symbols map (graduated symbols map) - a thematic map similar to a choropleth map, but instead of using colour to indicate the value of an attribute it uses a symbol (usually a circle) in relation to the value. For instance a larger value could be indicated by a larger symbol than a smaller value. This type of map is best used when you want to visualize the size or quantity of your data across geographic regions.\nYou can also combine several different types of visualizations to show complex geographic patterns. For example, the cases (dots) in the map below are colored according to their closest health facility (see legend). The large red circles show health facility catchment areas of a certain radius, and the bright red case-dots those that were outside any catchment range:\n\n\n\n\n\n\n\n\n\nNote: The primary focus of this GIS page is based on the context of field outbreak response. Therefore the contents of the page will cover the basic spatial data manipulations, visualizations, and analyses.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#preparation",
    "href": "new_pages/gis.html#preparation",
    "title": "28  GIS basics",
    "section": "28.4 Preparation",
    "text": "28.4 Preparation\n\nLoad packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,           # to import data\n  here,          # to locate files\n  tidyverse,     # to clean, handle, and plot the data (includes ggplot2 package)\n  sf,            # to manage spatial data using a Simple Feature format\n  tmap,          # to produce simple maps, works for both interactive and static maps\n  janitor,       # to clean column names\n  OpenStreetMap, # to add OSM basemap in ggplot map\n  spdep          # spatial statistics\n  )\n\nYou can see an overview of all the R packages that deal with spatial data at the CRAN “Spatial Task View”.\n\n\nSample case data\nFor demonstration purposes, we will work with a random sample of 1000 cases from the simulated Ebola epidemic linelist dataframe (computationally, working with fewer cases is easier to display in this handbook). If you want to follow along, click to download the “clean” linelist (as .rds file).\nSince we are taking a random sample of the cases, your results may look slightly different from what is demonstrated here when you run the codes on your own.\nImport data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import clean case linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")  \n\nNext we select a random sample of 1000 rows using sample() from base R.\n\n# generate 1000 random row numbers, from the number of rows in linelist\nsample_rows &lt;- sample(nrow(linelist), 1000)\n\n# subset linelist to keep only the sample rows, and all columns\nlinelist &lt;- linelist[sample_rows,]\n\nNow we want to convert this linelist which is class dataframe, to an object of class “sf” (spatial features). Given that the linelist has two columns “lon” and “lat” representing the longitude and latitude of each case’s residence, this will be easy.\nWe use the package sf (spatial features) and its function st_as_sf() to create the new object we call linelist_sf. This new object looks essentially the same as the linelist, but the columns lon and lat have been designated as coordinate columns, and a coordinate reference system (CRS) has been assigned for when the points are displayed. 4326 identifies our coordinates as based on the World Geodetic System 1984 (WGS84) - which is standard for GPS coordinates.\n\n# Create sf object\nlinelist_sf &lt;- linelist %&gt;%\n     sf::st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\nThis is how the original linelist dataframe looks like. In this demonstration, we will only use the column date_onset and geometry (which was constructed from the longitude and latitude fields above and is the last column in the data frame).\n\nDT::datatable(head(linelist_sf, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )\n\n\n\n\n\n\n\nAdmin boundary shapefiles\nSierra Leone: Admin boundary shapefiles\nIn advance, we have downloaded all administrative boundaries for Sierra Leone from the Humanitarian Data Exchange (HDX) website here. Alternatively, you can download these and all other example data for this handbook via our R package, as explained in the Download handbook and data page.\nNow we are going to do the following to save the Admin Level 3 shapefile in R:\n\nImport the shapefile\n\nClean the column names\n\nFilter rows to keep only areas of interest\n\nTo import a shapefile we use the read_sf() function from sf. It is provided the filepath via here(). - in our case the file is within our R project in the “data”, “gis”, and “shp” subfolders, with filename “sle_adm3.shp” (see pages on Import and export and R projects for more information). You will need to provide your own file path.\nNext we use clean_names() from the janitor package to standardize the column names of the shapefile. We also use filter() to keep only the rows with admin2name of “Western Area Urban” or “Western Area Rural”.\n\n# ADM3 level clean\nsle_adm3 &lt;- sle_adm3_raw %&gt;%\n  janitor::clean_names() %&gt;% # standardize column names\n  filter(admin2name %in% c(\"Western Area Urban\", \"Western Area Rural\")) # filter to keep certain areas\n\nBelow you can see the how the shapefile looks after import and cleaning. Scroll to the right to see how there are columns with admin level 0 (country), admin level 1, admin level 2, and finally admin level 3. Each level has a character name and a unique identifier “pcode”. The pcode expands with each increasing admin level e.g. SL (Sierra Leone) -&gt; SL04 (Western) -&gt; SL0410 (Western Area Rural) -&gt; SL040101 (Koya Rural).\n\n\n\n\n\n\n\n\nPopulation data\nSierra Leone: Population by ADM3\nThese data can again be downloaded from HDX (link here) or via our epirhandbook R package as explained in this page. We use import() to load the .csv file. We also pass the imported file to clean_names() to standardize the column name syntax.\n\n# Population by ADM3\nsle_adm3_pop &lt;- import(here(\"data\", \"gis\", \"population\", \"sle_admpop_adm3_2020.csv\")) %&gt;%\n  janitor::clean_names()\n\nHere is what the population file looks like. Scroll to the right to see how each jurisdiction has columns with male population, female populaton, total population, and the population break-down in columns by age group.\n\n\n\n\n\n\n\n\nHealth Facilities\nSierra Leone: Health facility data from OpenStreetMap\nAgain we have downloaded the locations of health facilities from HDX here or via instructions in the Download handbook and data page.\nWe import the facility points shapefile with read_sf(), again clean the column names, and then filter to keep only the points tagged as either “hospital”, “clinic”, or “doctors”.\n\n# OSM health facility shapefile\nsle_hf &lt;- sf::read_sf(here(\"data\", \"gis\", \"shp\", \"sle_hf.shp\")) %&gt;% \n  janitor::clean_names() %&gt;%\n  filter(amenity %in% c(\"hospital\", \"clinic\", \"doctors\"))\n\nHere is the resulting dataframe - scroll right to see the facility name and geometry coordinates.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#plotting-coordinates",
    "href": "new_pages/gis.html#plotting-coordinates",
    "title": "28  GIS basics",
    "section": "28.5 Plotting coordinates",
    "text": "28.5 Plotting coordinates\nThe easiest way to plot X-Y coordinates (longitude/latitude, points), in this case of cases, is to draw them as points directly from the linelist_sf object which we created in the preparation section.\nThe package tmap offers simple mapping capabilities for both static (“plot” mode) and interactive (“view” mode) with just a few lines of code. The tmap syntax is similar to that of ggplot2, such that commands are added to each other with +. Read more detail in this vignette.\n\nSet the tmap mode. In this case we will use “plot” mode, which produces static outputs.\n\n\ntmap_mode(\"plot\") # choose either \"view\" or \"plot\"\n\nBelow, the points are plotted alone.tm_shape() is provided with the linelist_sf objects. We then add points via tm_dots(), specifying the size and color. Because linelist_sf is an sf object, we have already designated the two columns that contain the lat/long coordinates and the coordinate reference system (CRS):\n\n# Just the cases (points)\ntm_shape(linelist_sf) + tm_dots(size=0.08, col='blue')\n\n\n\n\n\n\n\n\nAlone, the points do not tell us much. So we should also map the administrative boundaries:\nAgain we use tm_shape() (see documentation) but instead of providing the case points shapefile, we provide the administrative boundary shapefile (polygons).\nWith the bbox = argument (bbox stands for “bounding box”) we can specify the coordinate boundaries. First we show the map display without bbox, and then with it.\n\n# Just the administrative boundaries (polygons)\ntm_shape(sle_adm3) +               # admin boundaries shapefile\n  tm_polygons(col = \"#F7F7F7\")+    # show polygons in light grey\n  tm_borders(col = \"#000000\",      # show borders with color and line weight\n             lwd = 2) +\n  tm_text(\"admin3name\")            # column text to display for each polygon\n\n\n# Same as above, but with zoom from bounding box\ntm_shape(sle_adm3,\n         bbox = c(-13.3, 8.43,    # corner\n                  -13.2, 8.5)) +  # corner\n  tm_polygons(col = \"#F7F7F7\") +\n  tm_borders(col = \"#000000\", lwd = 2) +\n  tm_text(\"admin3name\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now both points and polygons together:\n\n# All together\ntm_shape(sle_adm3, bbox = c(-13.3, 8.43, -13.2, 8.5)) +     #\n  tm_polygons(col = \"#F7F7F7\") +\n  tm_borders(col = \"#000000\", lwd = 2) +\n  tm_text(\"admin3name\")+\ntm_shape(linelist_sf) +\n  tm_dots(size=0.08, col='blue', alpha = 0.5) +\n  tm_layout(title = \"Distribution of Ebola cases\")   # give title to map\n\n\n\n\n\n\n\n\nTo read a good comparison of mapping options in R, see this blog post.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#spatial-joins",
    "href": "new_pages/gis.html#spatial-joins",
    "title": "28  GIS basics",
    "section": "28.6 Spatial joins",
    "text": "28.6 Spatial joins\nYou may be familiar with joining data from one dataset to another one. Several methods are discussed in the Joining data page of this handbook. A spatial join serves a similar purpose but leverages spatial relationships. Instead of relying on common values in columns to correctly match observations, you can utilize their spatial relationships, such as one feature being within another, or the nearest neighbor to another, or within a buffer of a certain radius from another, etc.\nThe sf package offers various methods for spatial joins. See more documentation about the st_join method and spatial join types in this reference.\n\nPoints in polygon\nSpatial assign administrative units to cases\nHere is an interesting conundrum: the case linelist does not contain any information about the administrative units of the cases. Although it is ideal to collect such information during the initial data collection phase, we can also assign administrative units to individual cases based on their spatial relationships (i.e. point intersects with a polygon).\nBelow, we will spatially intersect our case locations (points) with the ADM3 boundaries (polygons):\n\nBegin with the linelist (points)\n\nSpatial join to the boundaries, setting the type of join at “st_intersects”\n\nUse select() to keep only certain of the new administrative boundary columns\n\n\nlinelist_adm &lt;- linelist_sf %&gt;%\n  \n  # join the administrative boundary file to the linelist, based on spatial intersection\n  sf::st_join(sle_adm3, join = st_intersects)\n\nAll the columns from sle_adms have been added to the linelist! Each case now has columns detailing the administrative levels that it falls within. In this example, we only want to keep two of the new columns (admin level 3), so we select() the old column names and just the two additional of interest:\n\nlinelist_adm &lt;- linelist_sf %&gt;%\n  \n  # join the administrative boundary file to the linelist, based on spatial intersection\n  sf::st_join(sle_adm3, join = st_intersects) %&gt;% \n  \n  # Keep the old column names and two new admin ones of interest\n  select(names(linelist_sf), admin3name, admin3pcod)\n\nBelow, just for display purposes you can see the first ten cases and that their admin level 3 (ADM3) jurisdictions that have been attached, based on where the point spatially intersected with the polygon shapes.\n\n# Now you will see the ADM3 names attached to each case\nlinelist_adm %&gt;% select(case_id, admin3name, admin3pcod)\n\nSimple feature collection with 1000 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -13.27122 ymin: 8.448447 xmax: -13.20684 ymax: 8.490397\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     case_id     admin3name admin3pcod                   geometry\n376   cbce06        West II   SL040207 POINT (-13.23143 8.466892)\n4115  ba5769 Mountain Rural   SL040102 POINT (-13.21752 8.451579)\n5685  58982e        West II   SL040207   POINT (-13.25187 8.4793)\n5426  b4bc41        East II   SL040204 POINT (-13.22543 8.486554)\n4951  95c347 Mountain Rural   SL040102 POINT (-13.24198 8.454378)\n4252  4669b3       West III   SL040208 POINT (-13.26526 8.471718)\n3728  135bec       West III   SL040208 POINT (-13.26403 8.475445)\n1852  fd9c82       West III   SL040208 POINT (-13.26097 8.457779)\n4322  7525ac        East II   SL040204 POINT (-13.22541 8.485722)\n5227  5c033b        East II   SL040204 POINT (-13.21212 8.482889)\n\n\nNow we can describe our cases by administrative unit - something we were not able to do before the spatial join!\n\n# Make new dataframe containing counts of cases by administrative unit\ncase_adm3 &lt;- linelist_adm %&gt;%          # begin with linelist with new admin cols\n  as_tibble() %&gt;%                      # convert to tibble for better display\n  group_by(admin3pcod, admin3name) %&gt;% # group by admin unit, both by name and pcode \n  summarise(cases = n()) %&gt;%           # summarize and count rows\n  arrange(desc(cases))                     # arrange in descending order\n\ncase_adm3\n\n# A tibble: 10 × 3\n# Groups:   admin3pcod [10]\n   admin3pcod admin3name     cases\n   &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt;\n 1 SL040102   Mountain Rural   252\n 2 SL040208   West III         247\n 3 SL040207   West II          182\n 4 SL040204   East II          120\n 5 SL040201   Central I         66\n 6 SL040206   West I            44\n 7 SL040203   East I            43\n 8 SL040205   East III          23\n 9 SL040202   Central II        22\n10 &lt;NA&gt;       &lt;NA&gt;               1\n\n\nWe can also create a bar plot of case counts by administrative unit.\nIn this example, we begin the ggplot() with the linelist_adm, so that we can apply factor functions like fct_infreq() which orders the bars by frequency (see page on Factors for tips).\n\nggplot(\n    data = linelist_adm,                       # begin with linelist containing admin unit info\n    mapping = aes(\n      x = fct_rev(fct_infreq(admin3name))))+ # x-axis is admin units, ordered by frequency (reversed)\n  geom_bar()+                                # create bars, height is number of rows\n  coord_flip()+                              # flip X and Y axes for easier reading of adm units\n  theme_classic()+                           # simplify background\n  labs(                                      # titles and labels\n    x = \"Admin level 3\",\n    y = \"Number of cases\",\n    title = \"Number of cases, by adminstative unit\",\n    caption = \"As determined by a spatial join, from 1000 randomly sampled cases from linelist\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nNearest neighbor\nFinding the nearest health facility / catchment area\nIt might be useful to know where the health facilities are located in relation to the disease hot spots.\nWe can use the st_nearest_feature join method from the st_join() function (sf package) to visualize the closest health facility to individual cases.\n\nWe begin with the shapefile linelist linelist_sf\n\nWe spatially join with sle_hf, which is the locations of health facilities and clinics (points)\n\n\n# Closest health facility to each case\nlinelist_sf_hf &lt;- linelist_sf %&gt;%                  # begin with linelist shapefile  \n  st_join(sle_hf, join = st_nearest_feature) %&gt;%   # data from nearest clinic joined to case data \n  select(case_id, osm_id, name, amenity) %&gt;%       # keep columns of interest, including id, name, type, and geometry of healthcare facility\n  rename(\"nearest_clinic\" = \"name\")                # re-name for clarity\n\nWe can see below (first 50 rows) that the each case now has data on the nearest clinic/hospital\n\n\n\n\n\n\nWe can see that “Den Clinic” is the closest health facility for about ~30% of the cases.\n\n# Count cases by health facility\nhf_catchment &lt;- linelist_sf_hf %&gt;%   # begin with linelist including nearest clinic data\n  as.data.frame() %&gt;%                # convert from shapefile to dataframe\n  count(nearest_clinic,              # count rows by \"name\" (of clinic)\n        name = \"case_n\") %&gt;%         # assign new counts column as \"case_n\"\n  arrange(desc(case_n))              # arrange in descending order\n\nhf_catchment                         # print to console\n\n                         nearest_clinic case_n\n1                            Den Clinic    386\n2       Shriners Hospitals for Children    336\n3         GINER HALL COMMUNITY HOSPITAL    151\n4                             panasonic     51\n5 Princess Christian Maternity Hospital     32\n6                     ARAB EGYPT CLINIC     20\n7                  MABELL HEALTH CENTER     14\n8                                  &lt;NA&gt;     10\n\n\nTo visualize the results, we can use tmap - this time interactive mode for easier viewing\n\ntmap_mode(\"view\")   # set tmap mode to interactive  \n\n# plot the cases and clinic points \ntm_shape(linelist_sf_hf) +            # plot cases\n  tm_dots(size=0.08,                  # cases colored by nearest clinic\n          col='nearest_clinic') +    \ntm_shape(sle_hf) +                    # plot clinic facilities in large black dots\n  tm_dots(size=0.3, col='black', alpha = 0.4) +      \n  tm_text(\"name\") +                   # overlay with name of facility\ntm_view(set.view = c(-13.2284, 8.4699, 13), # adjust zoom (center coords, zoom)\n        set.zoom.limits = c(13,14))+\ntm_layout(title = \"Cases, colored by nearest clinic\")\n\n\n\n\n\n\n\nBuffers\nWe can also explore how many cases are located within 2.5km (~30 mins) walking distance from the closest health facility.\nNote: For more accurate distance calculations, it is better to re-project your sf object to the respective local map projection system such as UTM (Earth projected onto a planar surface). In this example, for simplicity we will stick to the World Geodetic System (WGS84) Geograhpic coordinate system (Earth represented in a spherical / round surface, therefore the units are in decimal degrees). We will use a general conversion of: 1 decimal degree = ~111km.\nSee more information about map projections and coordinate systems at this esri article. This blog talks about different types of map projection and how one can choose a suitable projection depending on the area of interest and the context of your map / analysis.\nFirst, create a circular buffer with a radius of ~2.5km around each health facility. This is done with the function st_buffer() from tmap. Because the unit of the map is in lat/long decimal degrees, that is how “0.02” is interpreted. If your map coordinate system is in meters, the number must be provided in meters.\n\nsle_hf_2k &lt;- sle_hf %&gt;%\n  st_buffer(dist=0.02)       # decimal degrees translating to approximately 2.5km \n\nBelow we plot the buffer zones themselves, with the :\n\ntmap_mode(\"plot\")\n# Create circular buffers\ntm_shape(sle_hf_2k) +\n  tm_borders(col = \"black\", lwd = 2)+\ntm_shape(sle_hf) +                    # plot clinic facilities in large red dots\n  tm_dots(size=0.3, col='black')      \n\n\n\n\n\n\n\n\n**Second, we intersect these buffers with the cases (points) using st_join() and the join type of st_intersects*. That is, the data from the buffers are joined to the points that they intersect with.\n\n# Intersect the cases with the buffers\nlinelist_sf_hf_2k &lt;- linelist_sf_hf %&gt;%\n  st_join(sle_hf_2k, join = st_intersects, left = TRUE) %&gt;%\n  filter(osm_id.x==osm_id.y | is.na(osm_id.y)) %&gt;%\n  select(case_id, osm_id.x, nearest_clinic, amenity.x, osm_id.y)\n\nNow we can count the results: nrow(linelist_sf_hf_2k[is.na(linelist_sf_hf_2k$osm_id.y),]) out of 1000 cases did not intersect with any buffer (that value is missing), and so live more than 30 mins walk from the nearest health facility.\n\n# Cases which did not get intersected with any of the health facility buffers\nlinelist_sf_hf_2k %&gt;% \n  filter(is.na(osm_id.y)) %&gt;%\n  nrow()\n\n[1] 1000\n\n\nWe can visualize the results such that cases that did not intersect with any buffer appear in red.\n\ntmap_mode(\"view\")\n\n# First display the cases in points\ntm_shape(linelist_sf_hf) +\n  tm_dots(size=0.08, col='nearest_clinic') +\n\n# plot clinic facilities in large black dots\ntm_shape(sle_hf) +                    \n  tm_dots(size=0.3, col='black')+   \n\n# Then overlay the health facility buffers in polylines\ntm_shape(sle_hf_2k) +\n  tm_borders(col = \"black\", lwd = 2) +\n\n# Highlight cases that are not part of any health facility buffers\n# in red dots  \ntm_shape(linelist_sf_hf_2k %&gt;%  filter(is.na(osm_id.y))) +\n  tm_dots(size=0.1, col='red') +\ntm_view(set.view = c(-13.2284,8.4699, 13), set.zoom.limits = c(13,14))+\n\n# add title  \ntm_layout(title = \"Cases by clinic catchment area\")\n\n\n\n\n\n\n\nOther spatial joins\nAlternative values for argument join include (from the documentation)\n\nst_contains_properly\n\nst_contains\n\nst_covered_by\n\nst_covers\n\nst_crosses\n\nst_disjoint\n\nst_equals_exact\n\nst_equals\n\nst_is_within_distance\n\nst_nearest_feature\n\nst_overlaps\n\nst_touches\n\nst_within",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#choropleth-maps",
    "href": "new_pages/gis.html#choropleth-maps",
    "title": "28  GIS basics",
    "section": "28.7 Choropleth maps",
    "text": "28.7 Choropleth maps\nChoropleth maps can be useful to visualize your data by pre-defined area, usually administrative unit or health area. In outbreak response this can help to target resource allocation for specific areas with high incidence rates, for example.\nNow that we have the administrative unit names assigned to all cases (see section on spatial joins, above), we can start mapping the case counts by area (choropleth maps).\nSince we also have population data by ADM3, we can add this information to the case_adm3 table created previously.\nWe begin with the dataframe created in the previous step case_adm3, which is a summary table of each administrative unit and its number of cases.\n\nThe population data sle_adm3_pop are joined using a left_join() from dplyr on the basis of common values across column admin3pcod in the case_adm3 dataframe, and column adm_pcode in the sle_adm3_pop dataframe. See page on Joining data).\n\nselect() is applied to the new dataframe, to keep only the useful columns - total is total population\n\nCases per 10,000 populaton is calculated as a new column with mutate()\n\n\n# Add population data and calculate cases per 10K population\ncase_adm3 &lt;- case_adm3 %&gt;% \n     left_join(sle_adm3_pop,                             # add columns from pop dataset\n               by = c(\"admin3pcod\" = \"adm3_pcode\")) %&gt;%  # join based on common values across these two columns\n     select(names(case_adm3), total) %&gt;%                 # keep only important columns, including total population\n     mutate(case_10kpop = round(cases/total * 10000, 3)) # make new column with case rate per 10000, rounded to 3 decimals\n\ncase_adm3                                                # print to console for viewing\n\n# A tibble: 10 × 5\n# Groups:   admin3pcod [10]\n   admin3pcod admin3name     cases  total case_10kpop\n   &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;       &lt;dbl&gt;\n 1 SL040102   Mountain Rural   252  33993       74.1 \n 2 SL040208   West III         247 210252       11.7 \n 3 SL040207   West II          182 145109       12.5 \n 4 SL040204   East II          120  99821       12.0 \n 5 SL040201   Central I         66  69683        9.47\n 6 SL040206   West I            44  60186        7.31\n 7 SL040203   East I            43  68284        6.30\n 8 SL040205   East III          23 500134        0.46\n 9 SL040202   Central II        22  23874        9.22\n10 &lt;NA&gt;       &lt;NA&gt;               1     NA       NA   \n\n\nJoin this table with the ADM3 polygons shapefile for mapping\n\ncase_adm3_sf &lt;- case_adm3 %&gt;%                 # begin with cases & rate by admin unit\n  left_join(sle_adm3, by=\"admin3pcod\") %&gt;%    # join to shapefile data by common column\n  select(objectid, admin3pcod,                # keep only certain columns of interest\n         admin3name = admin3name.x,           # clean name of one column\n         admin2name, admin1name,\n         cases, total, case_10kpop,\n         geometry) %&gt;%                        # keep geometry so polygons can be plotted\n  drop_na(objectid) %&gt;%                       # drop any empty rows\n  st_as_sf()                                  # convert to shapefile\n\nMapping the results\n\n# tmap mode\ntmap_mode(\"plot\")               # view static map\n\n# plot polygons\ntm_shape(case_adm3_sf) + \n        tm_polygons(\"cases\") +  # color by number of cases column\n        tm_text(\"admin3name\")   # name display\n\n\n\n\n\n\n\n\nWe can also map the incidence rates\n\n# Cases per 10K population\ntmap_mode(\"plot\")             # static viewing mode\n\n# plot\ntm_shape(case_adm3_sf) +                # plot polygons\n  tm_polygons(\"case_10kpop\",            # color by column containing case rate\n              breaks=c(0, 10, 50, 100), # define break points for colors\n              palette = \"Purples\"       # use a purple color palette\n              ) +\n  tm_text(\"admin3name\")                 # display text",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#mapping-with-ggplot2",
    "href": "new_pages/gis.html#mapping-with-ggplot2",
    "title": "28  GIS basics",
    "section": "28.8 Mapping with ggplot2",
    "text": "28.8 Mapping with ggplot2\nIf you are already familiar with using ggplot2, you can use that package instead to create static maps of your data. The geom_sf() function will draw different objects based on which features (points, lines, or polygons) are in your data. For example, you can use geom_sf() in a ggplot() using sf data with polygon geometry to create a choropleth map.\nTo illustrate how this works, we can start with the ADM3 polygons shapefile that we used earlier. Recall that these are Admin Level 3 regions in Sierra Leone:\n\nsle_adm3\n\nSimple feature collection with 12 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -13.29894 ymin: 8.094272 xmax: -12.91333 ymax: 8.499809\nGeodetic CRS:  WGS 84\n# A tibble: 12 × 20\n   objectid admin3name   admin3pcod admin3ref_n admin2name admin2pcod admin1name\n *    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     \n 1      155 Koya Rural   SL040101   Koya Rural  Western A… SL0401     Western   \n 2      156 Mountain Ru… SL040102   Mountain R… Western A… SL0401     Western   \n 3      157 Waterloo Ru… SL040103   Waterloo R… Western A… SL0401     Western   \n 4      158 York Rural   SL040104   York Rural  Western A… SL0401     Western   \n 5      159 Central I    SL040201   Central I   Western A… SL0402     Western   \n 6      160 East I       SL040203   East I      Western A… SL0402     Western   \n 7      161 East II      SL040204   East II     Western A… SL0402     Western   \n 8      162 Central II   SL040202   Central II  Western A… SL0402     Western   \n 9      163 West III     SL040208   West III    Western A… SL0402     Western   \n10      164 West I       SL040206   West I      Western A… SL0402     Western   \n11      165 West II      SL040207   West II     Western A… SL0402     Western   \n12      167 East III     SL040205   East III    Western A… SL0402     Western   \n# ℹ 13 more variables: admin1pcod &lt;chr&gt;, admin0name &lt;chr&gt;, admin0pcod &lt;chr&gt;,\n#   date &lt;date&gt;, valid_on &lt;date&gt;, valid_to &lt;date&gt;, shape_leng &lt;dbl&gt;,\n#   shape_area &lt;dbl&gt;, rowcacode0 &lt;chr&gt;, rowcacode1 &lt;chr&gt;, rowcacode2 &lt;chr&gt;,\n#   rowcacode3 &lt;chr&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\nWe can use the left_join() function from dplyr to add the data we would like to map to the shapefile object. In this case, we are going to use the case_adm3 data frame that we created earlier to summarize case counts by administrative region; however, we can use this same approach to map any data stored in a data frame.\n\nsle_adm3_dat &lt;- sle_adm3 %&gt;% \n  inner_join(case_adm3, by = \"admin3pcod\") # inner join = retain only if in both data objects\n\nselect(sle_adm3_dat, admin3name.x, cases) # print selected variables to console\n\nSimple feature collection with 9 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -13.29894 ymin: 8.384533 xmax: -13.12612 ymax: 8.499809\nGeodetic CRS:  WGS 84\n# A tibble: 9 × 3\n  admin3name.x   cases                                                  geometry\n  &lt;chr&gt;          &lt;int&gt;                                        &lt;MULTIPOLYGON [°]&gt;\n1 Mountain Rural   252 (((-13.21496 8.474341, -13.21479 8.474289, -13.21465 8.4…\n2 Central I         66 (((-13.22646 8.489716, -13.22648 8.48955, -13.22644 8.48…\n3 East I            43 (((-13.2129 8.494033, -13.21076 8.494026, -13.21013 8.49…\n4 East II          120 (((-13.22653 8.491883, -13.22647 8.491853, -13.22642 8.4…\n5 Central II        22 (((-13.23154 8.491768, -13.23141 8.491566, -13.23144 8.4…\n6 West III         247 (((-13.28529 8.497354, -13.28456 8.496497, -13.28403 8.4…\n7 West I            44 (((-13.24677 8.493453, -13.24669 8.493285, -13.2464 8.49…\n8 West II          182 (((-13.25698 8.485518, -13.25685 8.485501, -13.25668 8.4…\n9 East III          23 (((-13.20465 8.485758, -13.20461 8.485698, -13.20449 8.4…\n\n\nTo make a column chart of case counts by region, using ggplot2, we could then call geom_col() as follows:\n\nggplot(data=sle_adm3_dat) +\n  geom_col(aes(x=fct_reorder(admin3name.x, cases, .desc=T),   # reorder x axis by descending 'cases'\n               y=cases)) +                                  # y axis is number of cases by region\n  theme_bw() +\n  labs(                                                     # set figure text\n    title=\"Number of cases, by administrative unit\",\n    x=\"Admin level 3\",\n    y=\"Number of cases\"\n  ) + \n  guides(x=guide_axis(angle=45))                            # angle x-axis labels 45 degrees to fit better\n\n\n\n\n\n\n\n\nIf we want to use ggplot2 to instead make a choropleth map of case counts, we can use similar syntax to call the geom_sf() function:\n\nggplot(data=sle_adm3_dat) + \n  geom_sf(aes(fill=cases))    # set fill to vary by case count variable\n\n\n\n\n\n\n\n\nWe can then customize the appearance of our map using grammar that is consistent across ggplot2, for example:\n\nggplot(data=sle_adm3_dat) +                           \n  geom_sf(aes(fill=cases)) +                        \n  scale_fill_continuous(high=\"#54278f\", low=\"#f2f0f7\") +    # change color gradient\n  theme_bw() +\n  labs(title = \"Number of cases, by administrative unit\",   # set figure text\n       subtitle = \"Admin level 3\"\n  )\n\n\n\n\n\n\n\n\nFor R users who are comfortable working with ggplot2, geom_sf() offers a simple and direct implementation that is suitable for basic map visualizations. To learn more, read the geom_sf() vignette or the ggplot2 book.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#basemaps",
    "href": "new_pages/gis.html#basemaps",
    "title": "28  GIS basics",
    "section": "28.9 Basemaps",
    "text": "28.9 Basemaps\n\nOpenStreetMap\nBelow we describe how to achieve a basemap for a ggplot2 map using OpenStreetMap features. Alternative methods include using ggmap which requires free registration with Google (details).\nOpenStreetMap is a collaborative project to create a free editable map of the world. The underlying geolocation data (e.g. locations of cities, roads, natural features, airports, schools, hospitals, roads etc) are considered the primary output of the project.\nFirst we load the OpenStreetMap package, from which we will get our basemap.\nThen, we create the object map, which we define using the function openmap() from OpenStreetMap package (documentation). We provide the following:\n\nupperLeft and lowerRight Two coordinate pairs specifying the limits of the basemap tile\n\nIn this case we’ve put in the max and min from the linelist rows, so the map will respond dynamically to the data\n\n\nzoom = (if null it is determined automatically)\n\ntype = which type of basemap - we have listed several possibilities here and the code is currently using the first one ([1]) “osm”\n\nmergeTiles = we chose TRUE so the basetiles are all merged into one\n\n\n# load package\npacman::p_load(OpenStreetMap)\n\n# Fit basemap by range of lat/long coordinates. Choose tile type\nmap &lt;- OpenStreetMap::openmap(\n  upperLeft = c(max(linelist$lat, na.rm=T), max(linelist$lon, na.rm=T)),   # limits of basemap tile\n  lowerRight = c(min(linelist$lat, na.rm=T), min(linelist$lon, na.rm=T)),\n  zoom = NULL,\n  type = c(\"osm\", \"stamen-toner\", \"stamen-terrain\", \"stamen-watercolor\", \"esri\",\"esri-topo\")[1])\n\nIf we plot this basemap right now, using autoplot.OpenStreetMap() from OpenStreetMap package, you see that the units on the axes are not latitude/longitude coordinates. It is using a different coordinate system. To correctly display the case residences (which are stored in lat/long), this must be changed.\n\nautoplot.OpenStreetMap(map)\n\n\n\n\n\n\n\n\nThus, we want to convert the map to latitude/longitude with the openproj() function from OpenStreetMap package. We provide the basemap map and also provide the Coordinate Reference System (CRS) we want. We do this by providing the “proj.4” character string for the WGS 1984 projection, but you can provide the CRS in other ways as well. (see this page to better understand what a proj.4 string is)\n\n# Projection WGS84\nmap_latlon &lt;- openproj(map, projection = \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\")\n\nNow when we create the plot we see that along the axes are latitude and longitude coordinate. The coordinate system has been converted. Now our cases will plot correctly if overlaid!\n\n# Plot map. Must use \"autoplot\" in order to work with ggplot\nautoplot.OpenStreetMap(map_latlon)\n\n\n\n\n\n\n\n\nSee the tutorials here and here for more info.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#contoured-density-heatmaps",
    "href": "new_pages/gis.html#contoured-density-heatmaps",
    "title": "28  GIS basics",
    "section": "28.10 Contoured density heatmaps",
    "text": "28.10 Contoured density heatmaps\nBelow we describe how to achieve a contoured density heatmap of cases, over a basemap, beginning with a linelist (one row per case).\n\nCreate basemap tile from OpenStreetMap, as described above\n\nPlot the cases from linelist using the latitude and longitude columns\n\nConvert the points to a density heatmap with stat_density_2d() from ggplot2,\n\nWhen we have a basemap with lat/long coordinates, we can plot our cases on top using the lat/long coordinates of their residence.\nBuilding on the function autoplot.OpenStreetMap() to create the basemap, ggplot2 functions will easily add on top, as shown with geom_point() below:\n\n# Plot map. Must be autoplotted to work with ggplot\nautoplot.OpenStreetMap(map_latlon)+                 # begin with the basemap\n  geom_point(                                       # add xy points from linelist lon and lat columns \n    data = linelist,                                \n    aes(x = lon, y = lat),\n    size = 1, \n    alpha = 0.5,\n    show.legend = FALSE) +                          # drop legend entirely\n  labs(x = \"Longitude\",                             # titles & labels\n       y = \"Latitude\",\n       title = \"Cumulative cases\")\n\n\n\n\n\n\n\n\nThe map above might be difficult to interpret, especially with the points overlapping. So you can instead plot a 2d density map using the ggplot2 function stat_density_2d(). You are still using the linelist lat/lon coordinates, but a 2D kernel density estimation is performed and the results are displayed with contour lines - like a topographical map. Read the full documentation here.\n\n# begin with the basemap\nautoplot.OpenStreetMap(map_latlon)+\n  \n  # add the density plot\n  ggplot2::stat_density_2d(\n        data = linelist,\n        aes(\n          x = lon,\n          y = lat,\n          fill = ..level..,\n          alpha = ..level..),\n        bins = 10,\n        geom = \"polygon\",\n        contour_var = \"count\",\n        show.legend = F) +                          \n  \n  # specify color scale\n  scale_fill_gradient(low = \"black\", high = \"red\")+\n  \n  # labels \n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       title = \"Distribution of cumulative cases\")\n\n\n\n\n\n\n\n\n\n\nTime series heatmap\nThe density heatmap above shows cumulative cases. We can examine the outbreak over time and space by faceting the heatmap based on the month of symptom onset, as derived from the linelist.\nWe begin in the linelist, creating a new column with the Year and Month of onset. The format() function from base R changes how a date is displayed. In this case we want “YYYY-MM”.\n\n# Extract month of onset\nlinelist &lt;- linelist %&gt;% \n  mutate(date_onset_ym = format(date_onset, \"%Y-%m\"))\n\n# Examine the values \ntable(linelist$date_onset_ym, useNA = \"always\")\n\n\n2014-04 2014-05 2014-06 2014-07 2014-08 2014-09 2014-10 2014-11 2014-12 2015-01 \n      1       6      20      36      92     192     175     134     108      80 \n2015-02 2015-03 2015-04    &lt;NA&gt; \n     52      40      31      33 \n\n\nNow, we simply introduce facetting via ggplot2 to the density heatmap. facet_wrap() is applied, using the new column as rows. We set the number of facet columns to 3 for clarity.\n\n# packages\npacman::p_load(OpenStreetMap, tidyverse)\n\n# begin with the basemap\nautoplot.OpenStreetMap(map_latlon)+\n  \n  # add the density plot\n  ggplot2::stat_density_2d(\n        data = linelist,\n        aes(\n          x = lon,\n          y = lat,\n          fill = ..level..,\n          alpha = ..level..),\n        bins = 10,\n        geom = \"polygon\",\n        contour_var = \"count\",\n        show.legend = F) +                          \n  \n  # specify color scale\n  scale_fill_gradient(low = \"black\", high = \"red\")+\n  \n  # labels \n  labs(x = \"Longitude\",\n       y = \"Latitude\",\n       title = \"Distribution of cumulative cases over time\")+\n  \n  # facet the plot by month-year of onset\n  facet_wrap(~ date_onset_ym, ncol = 4)",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#spatial-statistics",
    "href": "new_pages/gis.html#spatial-statistics",
    "title": "28  GIS basics",
    "section": "28.11 Spatial statistics",
    "text": "28.11 Spatial statistics\nMost of our discussion so far has focused on visualization of spatial data. In some cases, you may also be interested in using spatial statistics to quantify the spatial relationships of attributes in your data. This section will provide a very brief overview of some key concepts in spatial statistics, and suggest some resources that will be helpful to explore if you wish to do more comprehensive spatial analyses.\n\nSpatial relationships\nBefore we can calculate any spatial statistics, we need to specify the relationships between features in our data. There are many ways to conceptualize spatial relationships, but a simple and commonly-applicable model to use is that of adjacency - specifically, that we expect a geographic relationship between areas that share a border or “neighbour” one another.\nWe can quantify adjacency relationships between administrative region polygons in the sle_adm3 data we have been using with the spdep package. We will specify queen contiguity, which means that regions will be neighbors if they share at least one point along their borders. The alternative would be rook contiguity, which requires that regions share an edge - in our case, with irregular polygons, the distinction is trivial, but in some cases the choice between queen and rook can be influential.\n\nsle_nb &lt;- spdep::poly2nb(sle_adm3_dat, queen=T) # create neighbors \nsle_adjmat &lt;- spdep::nb2mat(sle_nb)    # create matrix summarizing neighbor relationships\nsle_listw &lt;- spdep::nb2listw(sle_nb)   # create listw (list of weights) object -- we will need this later\n\nsle_nb\n\nNeighbour list object:\nNumber of regions: 9 \nNumber of nonzero links: 30 \nPercentage nonzero weights: 37.03704 \nAverage number of links: 3.333333 \n\nround(sle_adjmat, digits = 2)\n\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n1 0.00 0.20 0.00 0.20 0.00  0.2 0.00 0.20 0.20\n2 0.25 0.00 0.00 0.25 0.25  0.0 0.00 0.25 0.00\n3 0.00 0.00 0.00 0.50 0.00  0.0 0.00 0.00 0.50\n4 0.25 0.25 0.25 0.00 0.00  0.0 0.00 0.00 0.25\n5 0.00 0.33 0.00 0.00 0.00  0.0 0.33 0.33 0.00\n6 0.50 0.00 0.00 0.00 0.00  0.0 0.00 0.50 0.00\n7 0.00 0.00 0.00 0.00 0.50  0.0 0.00 0.50 0.00\n8 0.20 0.20 0.00 0.00 0.20  0.2 0.20 0.00 0.00\n9 0.33 0.00 0.33 0.33 0.00  0.0 0.00 0.00 0.00\nattr(,\"call\")\nspdep::nb2mat(neighbours = sle_nb)\n\n\nThe matrix printed above shows the relationships between the 9 regions in our sle_adm3 data. A score of 0 indicates two regions are not neighbors, while any value other than 0 indicates a neighbor relationship. The values in the matrix are scaled so that each region has a total row weight of 1.\nA better way to visualize these neighbor relationships is by plotting them:\n\nplot(sle_adm3_dat$geometry) +                                           # plot region boundaries\n  spdep::plot.nb(sle_nb,as(sle_adm3_dat, 'Spatial'), col='grey', add=T) # add neighbor relationships\n\n\n\n\n\n\n\n\nWe have used an adjacency approach to identify neighboring polygons; the neighbors we identified are also sometimes called contiguity-based neighbors. But this is just one way of choosing which regions are expected to have a geographic relationship. The most common alternative approaches for identifying geographic relationships generate distance-based neighbors; briefly, these are:\n\nK-nearest neighbors - Based on the distance between centroids (the geographically-weighted center of each polygon region), select the n closest regions as neighbors. A maximum-distance proximity threshold may also be specified. In spdep, you can use knearneigh() (see documentation).\nDistance threshold neighbors - Select all neighbors within a distance threshold. In spdep, these neighbor relationships can be identified using dnearneigh() (see documentation).\n\n\n\nSpatial autocorrelation\nTobler’s oft-cited first law of geography states that “everything is related to everything else, but near things are more related than distant things.” In epidemiology, this often means that risk of a particular health outcome in a given region is more similar to its neighboring regions than to those far away. This concept has been formalized as spatial autocorrelation - the statistical property that geographic features with similar values are clustered together in space. Statistical measures of spatial autocorrelation can be used to quantify the extent of spatial clustering in your data, locate where clustering occurs, and identify shared patterns of spatial autocorrelation between distinct variables in your data. This section gives an overview of some common measures of spatial autocorrelation and how to calculate them in R.\nMoran’s I - This is a global summary statistic of the correlation between the value of a variable in one region, and the values of the same variable in neighboring regions. The Moran’s I statistic typically ranges from -1 to 1. A value of 0 indicates no pattern of spatial correlation, while values closer to 1 or -1 indicate stronger spatial autocorrelation (similar values close together) or spatial dispersion (dissimilar values close together), respectively.\nFor an example, we will calculate a Moran’s I statistic to quantify the spatial autocorrelation in Ebola cases we mapped earlier (remember, this is a subset of cases from the simulated epidemic linelist dataframe). The spdep package has a function, moran.test, that can do this calculation for us:\n\nmoran_i &lt;-spdep::moran.test(sle_adm3_dat$cases,    # numeric vector with variable of interest\n                            listw=sle_listw)       # listw object summarizing neighbor relationships\n\nmoran_i                                            # print results of Moran's I test\n\n\n    Moran I test under randomisation\n\ndata:  sle_adm3_dat$cases  \nweights: sle_listw    \n\nMoran I statistic standard deviate = 1.7943, p-value = 0.03639\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n        0.2605303        -0.1250000         0.0461688 \n\n\nThe output from the moran.test() function shows us a Moran I statistic of round(moran_i$estimate[1],2). This indicates the presence of spatial autocorrelation in our data - specifically, that regions with similar numbers of Ebola cases are likely to be close together. The p-value provided by moran.test() is generated by comparison to the expectation under null hypothesis of no spatial autocorrelation, and can be used if you need to report the results of a formal hypothesis test.\nLocal Moran’s I - We can decompose the (global) Moran’s I statistic calculated above to identify localized spatial autocorrelation; that is, to identify specific clusters in our data. This statistic, which is sometimes called a Local Indicator of Spatial Association (LISA) statistic, summarizes the extent of spatial autocorrelation around each individual region. It can be useful for finding “hot” and “cold” spots on the map.\nTo show an example, we can calculate and map Local Moran’s I for the Ebola case counts used above, with the local_moran() function from spdep:\n\n# calculate local Moran's I\nlocal_moran &lt;- spdep::localmoran(                  \n  sle_adm3_dat$cases,                              # variable of interest\n  listw=sle_listw                                  # listw object with neighbor weights\n)\n\n# join results to sf data\nsle_adm3_dat&lt;- cbind(sle_adm3_dat, local_moran)    \n\n# plot map\nggplot(data=sle_adm3_dat) +\n  geom_sf(aes(fill=Ii)) +\n  theme_bw() +\n  scale_fill_gradient2(low=\"#2c7bb6\", mid=\"#ffffbf\", high=\"#d7191c\",\n                       name=\"Local Moran's I\") +\n  labs(title=\"Local Moran's I statistic for Ebola cases\",\n       subtitle=\"Admin level 3 regions, Sierra Leone\")\n\n\n\n\n\n\n\n\nGetis-Ord Gi* - This is another statistic that is commonly used for hotspot analysis; in large part, the popularity of this statistic relates to its use in the Hot Spot Analysis tool in ArcGIS. It is based on the assumption that typically, the difference in a variable’s value between neighboring regions should follow a normal distribution. It uses a z-score approach to identify regions that have significantly higher (hot spot) or significantly lower (cold spot) values of a specified variable, compared to their neighbors.\nWe can calculate and map the Gi* statistic using the localG() function from spdep:\n\n# Perform local G analysis\ngetis_ord &lt;- spdep::localG(\n  sle_adm3_dat$cases,\n  sle_listw\n)\n\n# join results to sf data\nsle_adm3_dat$getis_ord &lt;- as.numeric(getis_ord)\n\n# plot map\nggplot(data=sle_adm3_dat) +\n  geom_sf(aes(fill=getis_ord)) +\n  theme_bw() +\n  scale_fill_gradient2(low=\"#2c7bb6\", mid=\"#ffffbf\", high=\"#d7191c\",\n                       name=\"Gi*\") +\n  labs(title=\"Getis-Ord Gi* statistic for Ebola cases\",\n       subtitle=\"Admin level 3 regions, Sierra Leone\")\n\n\n\n\n\n\n\n\nAs you can see, the map of Getis-Ord Gi* looks slightly different from the map of Local Moran’s I produced earlier. This reflects that the method used to calculate these two statistics are slightly different; which one you should use depends on your specific use case and the research question of interest.\nLee’s L test - This is a statistical test for bivariate spatial correlation. It allows you to test whether the spatial pattern for a given variable x is similar to the spatial pattern of another variable, y, that is hypothesized to be related spatially to x.\nTo give an example, let’s test whether the spatial pattern of Ebola cases from the simulated epidemic is correlated with the spatial pattern of population. To start, we need to have a population variable in our sle_adm3 data. We can use the total variable from the sle_adm3_pop dataframe that we loaded earlier.\n\nsle_adm3_dat &lt;- sle_adm3_dat %&gt;% \n  rename(population = total)                          # rename 'total' to 'population'\n\nWe can quickly visualize the spatial patterns of the two variables side by side, to see whether they look similar:\n\ntmap_mode(\"plot\")\n\ncases_map &lt;- tm_shape(sle_adm3_dat) + tm_polygons(\"cases\") + tm_layout(main.title=\"Cases\")\npop_map &lt;- tm_shape(sle_adm3_dat) + tm_polygons(\"population\") + tm_layout(main.title=\"Population\")\n\ntmap_arrange(cases_map, pop_map, ncol=2)   # arrange into 2x1 facets\n\n\n\n\n\n\n\n\nVisually, the patterns seem dissimilar. We can use the lee.test() function in spdep to test statistically whether the pattern of spatial autocorrelation in the two variables is related. The L statistic will be close to 0 if there is no correlation between the patterns, close to 1 if there is a strong positive correlation (i.e. the patterns are similar), and close to -1 if there is a strong negative correlation (i.e. the patterns are inverse).\n\nlee_test &lt;- spdep::lee.test(\n  x=sle_adm3_dat$cases,          # variable 1 to compare\n  y=sle_adm3_dat$population,     # variable 2 to compare\n  listw=sle_listw                # listw object with neighbor weights\n)\n\nlee_test\n\n\n    Lee's L statistic randomisation\n\ndata:  sle_adm3_dat$cases ,  sle_adm3_dat$population \nweights: sle_listw  \n\nLee's L statistic standard deviate = -0.86794, p-value = 0.8073\nalternative hypothesis: greater\nsample estimates:\nLee's L statistic       Expectation          Variance \n      -0.12740001       -0.03092239        0.01235574 \n\n\nThe output above shows that the Lee’s L statistic for our two variables was round(lee_test$estimate[1],2), which indicates weak negative correlation. This confirms our visual assessment that the pattern of cases and population are not related to one another, and provides evidence that the spatial pattern of cases is not strictly a result of population density in high-risk areas.\nThe Lee L statistic can be useful for making these kinds of inferences about the relationship between spatially distributed variables; however, to describe the nature of the relationship between two variables in more detail, or adjust for confounding, spatial regression techniques will be needed. These are described briefly in the following section.\n\n\nSpatial regression\nYou may wish to make statistical inferences about the relationships between variables in your spatial data. In these cases, it is useful to consider spatial regression techniques - that is, approaches to regression that explicitly consider the spatial organization of units in your data. Some reasons that you may need to consider spatial regression models, rather than standard regression models such as GLMs, include:\n\nStandard regression models assume that residuals are independent from one another. In the presence of strong spatial autocorrelation, the residuals of a standard regression model are likely to be spatially autocorrelated as well, thus violating this assumption. This can lead to problems with interpreting the model results, in which case a spatial model would be preferred.\nRegression models also typically assume that the effect of a variable x is constant over all observations. In the case of spatial heterogeneity, the effects we wish to estimate may vary over space, and we may be interested in quantifying those differences. In this case, spatial regression models offer more flexibility for estimating and interpreting effects.\n\nThe details of spatial regression approaches are beyond the scope of this handbook. This section will instead provide an overview of the most common spatial regression models and their uses, and refer you to references that may of use if you wish to explore this area further.\nSpatial error models - These models assume that the error terms across spatial units are correlated, in which case the data would violate the assumptions of a standard OLS model. Spatial error models are also sometimes referred to as simultaneous autoregressive (SAR) models. They can be fit using the errorsarlm() function in the spatialreg package (spatial regression functions which used to be a part of spdep).\nSpatial lag models - These models assume that the dependent variable for a region i is influenced not only by value of independent variables in i, but also by the values of those variables in regions neighboring i. Like spatial error models, spatial lag models are also sometimes described as simultaneous autoregressive (SAR) models. They can be fit using the lagsarlm() function in the spatialreg package.\nThe spdep package contains several useful diagnostic tests for deciding between standard OLS, spatial lag, and spatial error models. These tests, called Lagrange Multiplier diagnostics, can be used to identify the type of spatial dependence in your data and choose which model is most appropriate. The function lm.LMtests() can be used to calculate all of the Lagrange Multiplier tests. Anselin (1988) also provides a useful flow chart tool to decide which spatial regression model to use based on the results of the Lagrange Multiplier tests:\n\n\n\n\n\n\n\n\n\nBayesian hierarchical models - Bayesian approaches are commonly used for some applications in spatial analysis, most commonly for disease mapping. They are preferred in cases where case data are sparsely distributed (for example, in the case of a rare outcome) or statistically “noisy”, as they can be used to generate “smoothed” estimates of disease risk by accounting for the underlying latent spatial process. This may improve the quality of estimates. They also allow investigator pre-specification (via choice of prior) of complex spatial correlation patterns that may exist in the data, which can account for spatially-dependent and -independent variation in both independent and dependent variables. In R, Bayesian hierarchical models can be fit using the CARbayes package (see vignette) or R-INLA (see website and textbook). R can also be used to call external software that does Bayesian estimation, such as JAGS or WinBUGS.",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/gis.html#resources",
    "href": "new_pages/gis.html#resources",
    "title": "28  GIS basics",
    "section": "28.12 Resources",
    "text": "28.12 Resources\n\nR Simple Features and sf package vignette\nR tmap package vignette\nggmap: Spatial Visualization with ggplot2\nIntro to making maps with R, overview of different packages\nSpatial Data in R (EarthLab course)\nApplied Spatial Data Analysis in R textbook\nSpatialEpiApp - a Shiny app that is downloadable as an R package, allowing you to provide your own data and conduct mapping, cluster analysis, and spatial statistics.\nAn Introduction to Spatial Econometrics in R workshop",
    "crumbs": [
      "Analysis",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>GIS basics</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_presentation.html",
    "href": "new_pages/tables_presentation.html",
    "title": "29  Tables for presentation",
    "section": "",
    "text": "29.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Tables for presentation</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_presentation.html#preparation",
    "href": "new_pages/tables_presentation.html#preparation",
    "title": "29  Tables for presentation",
    "section": "",
    "text": "Load packages\nInstall and load flextable. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,            # import/export\n  here,           # file pathways\n  flextable,      # make HTML tables \n  officer,        # helper functions for tables\n  tidyverse)      # data management, summary, and visualization\n\n\n\nImport data\nTo begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.\n\n\n\n\n\n\n\n\nPrepare table\nBefore beginning to use flextable you will need to create your table as a data frame. See the page on Descriptive tables and Pivoting data to learn how to create a data frame using packages such as janitor and dplyr. You must arrange the content in rows and columns as you want it displayed. Then, the data frame will be passed to flextable to display it with colors, headers, fonts, etc.\nBelow is an example from the Descriptive tables page of converting the case linelist into a data frame that summarises patient outcomes and CT values by hospital, with a Totals row at the bottom. The output is saved as table.\n\ntable &lt;- linelist %&gt;% \n  \n  # Get summary values per hospital-outcome group\n  ###############################################\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T)) %&gt;%           # median CT value per group\n  \n  # add totals\n  ############\n  bind_rows(                                           # Bind the previous table with this mini-table of totals\n    linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # Number of rows for whole dataset     \n        ct_value = median(ct_blood, na.rm=T))) %&gt;%     # Median CT for whole dataset\n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known)                                    # Arrange rows from lowest to highest (Total row at bottom)\n\ntable  # print\n\n# A tibble: 7 × 8\n# Groups:   hospital [7]\n  hospital      N_Known N_Recover Pct_Recover ct_value_Recover N_Death Pct_Death\n  &lt;chr&gt;           &lt;int&gt;     &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;    \n1 St. Mark's M…     325       126 38.8%                     22     199 61.2%    \n2 Central Hosp…     358       165 46.1%                     22     193 53.9%    \n3 Other             685       290 42.3%                     21     395 57.7%    \n4 Military Hos…     708       309 43.6%                     22     399 56.4%    \n5 Missing          1125       514 45.7%                     21     611 54.3%    \n6 Port Hospital    1364       579 42.4%                     21     785 57.6%    \n7 Total            3440      1469 42.7%                     22    1971 57.3%    \n# ℹ 1 more variable: ct_value_Death &lt;dbl&gt;",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Tables for presentation</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_presentation.html#basic-flextable",
    "href": "new_pages/tables_presentation.html#basic-flextable",
    "title": "29  Tables for presentation",
    "section": "29.2 Basic flextable",
    "text": "29.2 Basic flextable\n\nCreate a flextable\nTo create and manage flextable objects, we first pass the data frame through the flextable() function. We save the result as my_table.\n\nmy_table &lt;- flextable(table) \nmy_table\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nAfter doing this, we can progressively pipe the my_table object through more flextable formatting functions.\nIn this page for sake of clarity we will save the table at intermediate steps as my_table, adding flextable functions bit-by-bit. If you want to see all the code from beginning to end written in one chunk, visit the All code together section below.\nThe general syntax of each line of flextable code is as follows:\n\nfunction(table, i = X, j = X, part = \"X\"), where:\n\nThe ‘function’ can be one of many different functions, such as width() to determine column widths, bg() to set background colours, align() to set whether text is centre/right/left aligned, and so on.\ntable = is the name of the data frame, although does not need to be stated if the data frame is piped into the function.\npart = refers to which part of the table the function is being applied to. E.g. “header”, “body” or “all”.\ni = specifies the row to apply the function to, where ‘X’ is the row number. If multiple rows, e.g. the first to third rows, one can specify: i = c(1:3). Note if ‘body’ is selected, the first row starts from underneath the header section.\nj = specifies the column to apply the function to, where ‘x’ is the column number or name. If multiple columns, e.g. the fifth and sixth, one can specify: j = c(5,6).\n\n\nYou can find the complete list of flextable formatting function here or review the documentation by entering ?flextable.\n\n\nColumn width\nWe can use the autofit() function, which nicely stretches out the table so that each cell only has one row of text. The function qflextable() is a convenient shorthand for flextable() and autofit().\n\nmy_table %&gt;% autofit()\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nHowever, this might not always be appropriate, especially if there are very long values within cells, meaning the table might not fit on the page.\nInstead, we can specify widths with the width() function. It can take some playing around to know what width value to put. In the example below, we specify different widths for column 1, column 2, and columns 4 to 8.\n\nmy_table &lt;- my_table %&gt;% \n  width(j=1, width = 2.7) %&gt;% \n  width(j=2, width = 1.5) %&gt;% \n  width(j=c(4,5,7,8), width = 1)\n\nmy_table\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\nColumn headers\nWe want more clearer headers for easier interpretation of the table contents.\nFor this table, we will want to add a second header layer so that columns covering the same subgroups can be grouped together. We do this with the add_header_row() function with top = TRUE. We provide the new name of each column to values =, leaving empty values \"\" for columns we know we will merge together later.\nWe also rename the header names in the now-second header in a separate set_header_labels() command.\nFinally, to “combine” certain column headers in the top header we use merge_at() to merge the column headers in the top header row.\n\nmy_table &lt;- my_table %&gt;% \n  \n  add_header_row(\n    top = TRUE,                # New header goes on top of existing header row\n    values = c(\"Hospital\",     # Header values for each column below\n               \"Total cases with known outcome\", \n               \"Recovered\",    # This will be the top-level header for this and two next columns\n               \"\",\n               \"\",\n               \"Died\",         # This will be the top-level header for this and two next columns\n               \"\",             # Leave blank, as it will be merged with \"Died\"\n               \"\")) %&gt;% \n    \n  set_header_labels(         # Rename the columns in original header row\n      hospital = \"\", \n      N_Known = \"\",                  \n      N_Recover = \"Total\",\n      Pct_Recover = \"% of cases\",\n      ct_value_Recover = \"Median CT values\",\n      N_Death = \"Total\",\n      Pct_Death = \"% of cases\",\n      ct_value_Death = \"Median CT values\")  %&gt;% \n  \n  merge_at(i = 1, j = 3:5, part = \"header\") %&gt;% # Horizontally merge columns 3 to 5 in new header row\n  merge_at(i = 1, j = 6:8, part = \"header\")     # Horizontally merge columns 6 to 8 in new header row\n\nmy_table  # print\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\nBorders and background\nYou can adjust the borders, internal lines, etc. with various flextable functions. It is often easier to start by removing all existing borders with border_remove().\nThen, you can apply default border themes by passing the table to theme_box(), theme_booktabs(), or theme_alafoli().\nYou can add vertical and horizontal lines with a variety of functions. hline() and vline() add lines to a specified row or column, respectively. Within each, you must specify the part = as either “all”, “body”, or “header”. For vertical lines, specify the column to j =, and for horizontal lines the row to i =. Other functions like vline_right(), vline_left(), hline_top(), and hline_bottom() add lines to the outsides only.\nIn all of these functions, the actual line style itself must be specified to border = and must be the output of a separate command using the fp_border() function from the officer package. This function helps you define the width and color of the line. You can define this above the table commands, as shown below.\n\n# define style for border line\nborder_style = officer::fp_border(color=\"black\", width=1)\n\n# add border lines to table\nmy_table &lt;- my_table %&gt;% \n\n  # Remove all existing borders\n  border_remove() %&gt;%  \n  \n  # add horizontal lines via a pre-determined theme setting\n  theme_booktabs() %&gt;% \n  \n  # add vertical lines to separate Recovered and Died sections\n  vline(part = \"all\", j = 2, border = border_style) %&gt;%   # at column 2 \n  vline(part = \"all\", j = 5, border = border_style)       # at column 5\n\nmy_table\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\nFont and alignment\nWe centre-align all columns aside from the left-most column with the hospital names, using the align() function from flextable.\n\nmy_table &lt;- my_table %&gt;% \n   flextable::align(align = \"center\", j = c(2:8), part = \"all\") \nmy_table\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nAdditionally, we can increase the header font size and change then to bold. We can also change the total row to bold.\n\nmy_table &lt;-  my_table %&gt;%  \n  fontsize(i = 1, size = 12, part = \"header\") %&gt;%   # adjust font size of header\n  bold(i = 1, bold = TRUE, part = \"header\") %&gt;%     # adjust bold face of header\n  bold(i = 7, bold = TRUE, part = \"body\")           # adjust bold face of total row (row 7 of body)\n\nmy_table\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nWe can ensure that the proportion columns display only one decimal place using the function colformat_num(). Note this could also have been done at data management stage with the round() function.\n\nmy_table &lt;- colformat_num(my_table, j = c(4,7), digits = 1)\nmy_table\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\nMerge cells\nJust as we merge cells horizontally in the header row, we can also merge cells vertically using merge_at() and specifying the rows (i) and column (j). Here we merge the “Hospital” and “Total cases with known outcome” values vertically to give them more space.\n\nmy_table &lt;- my_table %&gt;% \n  merge_at(i = 1:2, j = 1, part = \"header\") %&gt;% \n  merge_at(i = 1:2, j = 2, part = \"header\")\n\nmy_table\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\nBackground color\nTo distinguish the content of the table from the headers, we may want to add additional formatting. e.g. changing the background color. In this example we change the table body to gray.\n\nmy_table &lt;- my_table %&gt;% \n    bg(part = \"body\", bg = \"gray95\")  \n\nmy_table \n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Tables for presentation</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_presentation.html#conditional-formatting",
    "href": "new_pages/tables_presentation.html#conditional-formatting",
    "title": "29  Tables for presentation",
    "section": "29.3 Conditional formatting",
    "text": "29.3 Conditional formatting\nWe can highlight all values in a column that meet a certain rule, e.g. where more than 55% of cases died. Simply put the criteria to the i = or j = argument, preceded by a tilde ~. Reference the column in the data frame, not the display heading values.\n\nmy_table %&gt;% \n  bg(j = 7, i = ~ Pct_Death &gt;= 55, part = \"body\", bg = \"red\") \n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nOr, we can highlight the entire row meeting a certain criterion, such as a hospital of interest. To do this we just remove the column (j) specification so the criteria apply to all columns.\n\nmy_table %&gt;% \n  bg(., i= ~ hospital == \"Military Hospital\", part = \"body\", bg = \"#91c293\") \n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Tables for presentation</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_presentation.html#tbl_pres_all",
    "href": "new_pages/tables_presentation.html#tbl_pres_all",
    "title": "29  Tables for presentation",
    "section": "29.4 All code together",
    "text": "29.4 All code together\nBelow we show all the code from the above sections together.\n\nborder_style = officer::fp_border(color=\"black\", width=1)\n\npacman::p_load(\n  rio,            # import/export\n  here,           # file pathways\n  flextable,      # make HTML tables \n  officer,        # helper functions for tables\n  tidyverse)      # data management, summary, and visualization\n\ntable &lt;- linelist %&gt;% \n\n  # Get summary values per hospital-outcome group\n  ###############################################\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T)) %&gt;%           # median CT value per group\n  \n  # add totals\n  ############\n  bind_rows(                                           # Bind the previous table with this mini-table of totals\n    linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # Number of rows for whole dataset     \n        ct_value = median(ct_blood, na.rm=T))) %&gt;%     # Median CT for whole dataset\n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known) %&gt;%                                 # Arrange rows from lowest to highest (Total row at bottom)\n\n  # formatting\n  ############\n  flextable() %&gt;%              # table is piped in from above\n  add_header_row(\n    top = TRUE,                # New header goes on top of existing header row\n    values = c(\"Hospital\",     # Header values for each column below\n               \"Total cases with known outcome\", \n               \"Recovered\",    # This will be the top-level header for this and two next columns\n               \"\",\n               \"\",\n               \"Died\",         # This will be the top-level header for this and two next columns\n               \"\",             # Leave blank, as it will be merged with \"Died\"\n               \"\")) %&gt;% \n    set_header_labels(         # Rename the columns in original header row\n      hospital = \"\", \n      N_Known = \"\",                  \n      N_Recover = \"Total\",\n      Pct_Recover = \"% of cases\",\n      ct_value_Recover = \"Median CT values\",\n      N_Death = \"Total\",\n      Pct_Death = \"% of cases\",\n      ct_value_Death = \"Median CT values\")  %&gt;% \n  merge_at(i = 1, j = 3:5, part = \"header\") %&gt;% # Horizontally merge columns 3 to 5 in new header row\n  merge_at(i = 1, j = 6:8, part = \"header\") %&gt;%  \n  border_remove() %&gt;%  \n  theme_booktabs() %&gt;% \n  vline(part = \"all\", j = 2, border = border_style) %&gt;%   # at column 2 \n  vline(part = \"all\", j = 5, border = border_style) %&gt;%   # at column 5\n  merge_at(i = 1:2, j = 1, part = \"header\") %&gt;% \n  merge_at(i = 1:2, j = 2, part = \"header\") %&gt;% \n  width(j=1, width = 2.7) %&gt;% \n  width(j=2, width = 1.5) %&gt;% \n  width(j=c(4,5,7,8), width = 1) %&gt;% \n  flextable::align(., align = \"center\", j = c(2:8), part = \"all\") %&gt;% \n  bg(., part = \"body\", bg = \"gray95\")  %&gt;% \n  bg(., j=c(1:8), i= ~ hospital == \"Military Hospital\", part = \"body\", bg = \"#91c293\") %&gt;% \n  colformat_num(., j = c(4,7), digits = 1) %&gt;%\n  bold(i = 1, bold = TRUE, part = \"header\") %&gt;% \n  bold(i = 7, bold = TRUE, part = \"body\")\n\n`summarise()` has grouped output by 'hospital'. You can override using the\n`.groups` argument.\n\ntable\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Tables for presentation</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_presentation.html#saving-your-table",
    "href": "new_pages/tables_presentation.html#saving-your-table",
    "title": "29  Tables for presentation",
    "section": "29.5 Saving your table",
    "text": "29.5 Saving your table\nThere are different ways the table can be integrated into your output.\n\nSave single table\nYou can export the tables to Word, PowerPoint or HTML or as an image (PNG) files. To do this, use one of the following functions:\n\nsave_as_docx()\n\nsave_as_pptx()\n\nsave_as_image()\n\nsave_as_html()\n\nFor instance below we save our table as a word document. Note the syntax of the first argument - you can just provide the name of your flextable object e.g. my_table, or you can give is a “name” as shown below (the name is “my table”). If name, this will appear as the title of the table in Word. We also demonstrate code to save as PNG image.\n\n# Edit the 'my table' as needed for the title of table.  \nsave_as_docx(\"my table\" = my_table, path = \"file.docx\")\n\nsave_as_image(my_table, path = \"file.png\")\n\nNote the packages webshot or webshot2 are required to save a flextable as an image. Images may come out with transparent backgrounds.\nIf you want to view a ‘live’ version of the flextable output in the intended document format, use print() and specify one of the below to preview =. The document will “pop-up” open on your computer in the specified software program, but will not be saved. This can be useful to check if the table fits in one page/slide or so you can quickly copy it into another document, you can use the print method with the argument preview set to “pptx” or “docx”.\n\nprint(my_table, preview = \"docx\") # Word document example\nprint(my_table, preview = \"pptx\") # Powerpoint example\n\n\n\nPrint table in R markdown\nThis table can be integrated into your an automated document, an R markdown output, if the table object is called within the R markdown chunk. This means the table can be updated as part of a report where the data might change, so the numbers can be refreshed.\nSee detail in the Reports with R Markdown page of this handbook.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Tables for presentation</span>"
    ]
  },
  {
    "objectID": "new_pages/tables_presentation.html#resources",
    "href": "new_pages/tables_presentation.html#resources",
    "title": "29  Tables for presentation",
    "section": "29.6 Resources",
    "text": "29.6 Resources\nThe full flextable book is here: https://ardata-fr.github.io/flextable-book/ The Github site is here\nA manual of all the flextable functions can be found here\nA gallery of beautiful example flextable tables with code can be accessed here",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Tables for presentation</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html",
    "href": "new_pages/ggplot_basics.html",
    "title": "30  ggplot basics",
    "section": "",
    "text": "30.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#preparation",
    "href": "new_pages/ggplot_basics.html#preparation",
    "title": "30  ggplot basics",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  tidyverse,      # includes ggplot2 and other data management tools\n  janitor,        # cleaning and summary tables\n  ggforce,        # ggplot extras\n  rio,            # import/export\n  here,           # file locator\n  stringr         # working with characters   \n)\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\n\nlinelist &lt;- rio::import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below. We will focus on the continuous variables age, wt_kg (weight in kilos), ct_blood (CT values), and days_onset_hosp (difference between onset date and hospitalisation).\n\n\n\n\n\n\n\n\nGeneral cleaning\nWhen preparing data to plot, it is best to make the data adhere to “tidy” data standards as much as possible. How to achieve this is expanded on in the data management pages of this handbook, such as Cleaning data and core functions.\nSome simple ways we can prepare our data to make it better for plotting can include making the contents of the data better for display - which does not necessarily equate to better for data manipulation. For example:\n\nReplace NA values in a character column with the character string “Unknown”\n\nConsider converting column to class factor so their values have prescribed ordinal levels\n\nClean some columns so that their “data friendly” values with underscores etc are changed to normal text or title case (see Characters and strings)\n\nHere are some examples of this in action:\n\n# make display version of columns with more friendly names\nlinelist &lt;- linelist %&gt;%\n  mutate(\n    gender_disp = case_when(gender == \"m\" ~ \"Male\",        # m to Male \n                            gender == \"f\" ~ \"Female\",      # f to Female,\n                            is.na(gender) ~ \"Unknown\"),    # NA to Unknown\n    \n    outcome_disp = replace_na(outcome, \"Unknown\")          # replace NA outcome with \"unknown\"\n  )\n\n\n\nPivoting longer\nAs a matter of data structure, for ggplot2 we often also want to pivot our data into longer formats. Read more about this is the page on Pivoting data.\n\n\n\n\n\n\n\n\n\nFor example, say that we want to plot data that are in a “wide” format, such as for each case in the linelist and their symptoms. Below we create a mini-linelist called symptoms_data that contains only the case_id and symptoms columns.\n\nsymptoms_data &lt;- linelist %&gt;% \n  select(c(case_id, fever, chills, cough, aches, vomit))\n\nHere is how the first 50 rows of this mini-linelist look - see how they are formatted “wide” with each symptom as a column:\n\n\n\n\n\n\nIf we wanted to plot the number of cases with specific symptoms, we are limited by the fact that each symptom is a specific column. However, we can pivot the symptoms columns to a longer format like this:\n\nsymptoms_data_long &lt;- symptoms_data %&gt;%    # begin with \"mini\" linelist called symptoms_data\n  \n  pivot_longer(\n    cols = -case_id,                       # pivot all columns except case_id (all the symptoms columns)\n    names_to = \"symptom_name\",             # assign name for new column that holds the symptoms\n    values_to = \"symptom_is_present\") %&gt;%  # assign name for new column that holds the values (yes/no)\n  \n  mutate(symptom_is_present = replace_na(symptom_is_present, \"unknown\")) # convert NA to \"unknown\"\n\nHere are the first 50 rows. Note that case has 5 rows - one for each possible symptom. The new columns symptom_name and symptom_is_present are the result of the pivot. Note that this format may not be very useful for other operations, but is useful for plotting.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#basics-of-ggplot",
    "href": "new_pages/ggplot_basics.html#basics-of-ggplot",
    "title": "30  ggplot basics",
    "section": "30.2 Basics of ggplot",
    "text": "30.2 Basics of ggplot\n“Grammar of graphics” - ggplot2\nPlotting with ggplot2 is based on “adding” plot layers and design elements on top of one another, with each command added to the previous ones with a plus symbol (+). The result is a multi-layer plot object that can be saved, modified, printed, exported, etc.\nggplot objects can be highly complex, but the basic order of layers will usually look like this:\n\nBegin with the baseline ggplot() command - this “opens” the ggplot and allow subsequent functions to be added with +. Typically the dataset is also specified in this command\n\nAdd “geom” layers - these functions visualize the data as geometries (shapes), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with geom_ as a prefix.\n\nAdd design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation\n\nA simple example of skeleton code is as follows. We will explain each component in the sections below.\n\n# plot data from my_data columns as red points\nggplot(data = my_data)+                   # use the dataset \"my_data\"\n  geom_point(                             # add a layer of points (dots)\n    mapping = aes(x = col1, y = col2),    # \"map\" data column to axes\n    color = \"red\")+                       # other specification for the geom\n  labs()+                                 # here you add titles, axes labels, etc.\n  theme()                                 # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.)",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#ggplot",
    "href": "new_pages/ggplot_basics.html#ggplot",
    "title": "30  ggplot basics",
    "section": "30.3 ggplot()",
    "text": "30.3 ggplot()\nThe opening command of any ggplot2 plot is ggplot(). This command simply creates a blank canvas upon which to add layers. It “opens” the way for further layers to be added with a + symbol.\nTypically, the command ggplot() includes the data = argument for the plot. This sets the default dataset to be used for subsequent layers of the plot.\nThis command will end with a + after its closing parentheses. This leaves the command “open”. The ggplot will only execute/appear when the full command includes a final layer without a + at the end.\n\n# This will create plot that is a blank canvas\nggplot(data = linelist)",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#geoms",
    "href": "new_pages/ggplot_basics.html#geoms",
    "title": "30  ggplot basics",
    "section": "30.4 Geoms",
    "text": "30.4 Geoms\nA blank canvas is certainly not sufficient - we need to create geometries (shapes) from our data (e.g. bar plots, histograms, scatter plots, box plots).\nThis is done by adding layers “geoms” to the initial ggplot() command. There are many ggplot2 functions that create “geoms”. Each of these functions begins with “geom_”, so we will refer to them generically as geom_XXXX(). There are over 40 geoms in ggplot2 and many others created by fans. View them at the ggplot2 gallery. Some common geoms are listed below:\n\nHistograms - geom_histogram()\n\nBar charts - geom_bar() or geom_col() (see “Bar plot” section)\n\nBox plots - geom_boxplot()\n\nPoints (e.g. scatter plots) - geom_point()\n\nLine graphs - geom_line() or geom_path()\n\nTrend lines - geom_smooth()\n\nIn one plot you can display one or multiple geoms. Each is added to previous ggplot2 commands with a +, and they are plotted sequentially such that later geoms are plotted on top of previous ones.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#ggplot_basics_mapping",
    "href": "new_pages/ggplot_basics.html#ggplot_basics_mapping",
    "title": "30  ggplot basics",
    "section": "30.5 Mapping data to the plot",
    "text": "30.5 Mapping data to the plot\nMost geom functions must be told what to use to create their shapes - so you must tell them how they should map (assign) columns in your data to components of the plot like the axes, shape colors, shape sizes, etc. For most geoms, the essential components that must be mapped to columns in the data are the x-axis, and (if necessary) the y-axis.\nThis “mapping” occurs with the mapping = argument. The mappings you provide to mapping must be wrapped in the aes() function, so you would write something like mapping = aes(x = col1, y = col2), as shown below.\nBelow, in the ggplot() command the data are set as the case linelist. In the mapping = aes() argument the column age is mapped to the x-axis, and the column wt_kg is mapped to the y-axis.\nAfter a +, the plotting commands continue. A shape is created with the “geom” function geom_point(). This geom inherits the mappings from the ggplot() command above - it knows the axis-column assignments and proceeds to visualize those relationships as points on the canvas.\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+\n  geom_point()\n\n\n\n\n\n\n\n\nAs another example, the following commands utilize the same data, a slightly different mapping, and a different geom. The geom_histogram() function only requires a column mapped to the x-axis, as the counts y-axis is generated automatically.\n\nggplot(data = linelist, mapping = aes(x = age))+\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nPlot aesthetics\nIn ggplot terminology a plot “aesthetic” has a specific meaning. It refers to a visual property of plotted data. Note that “aesthetic” here refers to the data being plotted in geoms/shapes - not the surrounding display such as titles, axis labels, background color, that you might associate with the word “aesthetics” in common English. In ggplot those details are called “themes” and are adjusted within a theme() command (see this section).\nTherefore, plot object aesthetics can be colors, sizes, transparencies, placement, etc. of the plotted data. Not all geoms will have the same aesthetic options, but many can be used by most geoms. Here are some examples:\n\nshape = Display a point with geom_point() as a dot, star, triangle, or square…\n\nfill = The interior color (e.g. of a bar or boxplot)\n\ncolor = The exterior line of a bar, boxplot, etc., or the point color if using geom_point()\n\nsize = Size (e.g. line thickness, point size)\n\nalpha = Transparency (1 = opaque, 0 = invisible)\n\nbinwidth = Width of histogram bins\n\nwidth = Width of “bar plot” columns\n\nlinetype = Line type (e.g. solid, dashed, dotted)\n\nThese plot object aesthetics can be assigned values in two ways:\n\nAssigned a static value (e.g. color = \"blue\") to apply across all plotted observations\n\nAssigned to a column of the data (e.g. color = hospital) such that display of each observation depends on its value in that column\n\n\n\n\nSet to a static value\nIf you want the plot object aesthetic to be static, that is - to be the same for every observation in the data, you write its assignment within the geom but outside of any mapping = aes() statement. These assignments could look like size = 1 or color = \"blue\". Here are two examples:\n\nIn the first example, the mapping = aes() is in the ggplot() command and the axes are mapped to age and weight columns in the data. The plot aesthetics color =, size =, and alpha = (transparency) are assigned to static values. For clarity, this is done in the geom_point() function, as you may add other geoms afterward that would take different values for their plot aesthetics.\n\nIn the second example, the histogram requires only the x-axis mapped to a column. The histogram binwidth =, color =, fill = (internal color), and alpha = are again set within the geom to static values.\n\n\n# scatterplot\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  # set data and axes mapping\n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)         # set static point aesthetics\n\n# histogram\nggplot(data = linelist, mapping = aes(x = age))+       # set data and axes\n  geom_histogram(              # display histogram\n    binwidth = 7,                # width of bins\n    color = \"red\",               # bin line color\n    fill = \"blue\",               # bin interior color\n    alpha = 0.1)                 # bin transparency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaled to column values\nThe alternative is to scale the plot object aesthetic by the values in a column. In this approach, the display of this aesthetic will depend on that observation’s value in that column of the data. If the column values are continuous, the display scale (legend) for that aesthetic will be continuous. If the column values are discrete, the legend will display each value and the plotted data will appear as distinctly “grouped” (read more in the grouping section of this page).\nTo achieve this, you map that plot aesthetic to a column name (not in quotes). This must be done within a mapping = aes() function (note: there are several places in the code you can make these mapping assignments, as discussed below).\nTwo examples are below.\n\nIn the first example, the color = aesthetic (of each point) is mapped to the column age - and a scale has appeared in a legend! For now just note that the scale exists - we will show how to modify it in later sections.\n\nIn the second example two new plot aesthetics are also mapped to columns (color = and size =), while the plot aesthetics shape = and alpha = are mapped to static values outside of any mapping = aes() function.\n\n\n# scatterplot\nggplot(data = linelist,   # set data\n       mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age)\n       )+     # map color to age\n  geom_point()         # display data as points \n\n# scatterplot\nggplot(data = linelist,   # set data\n       mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age,       # map color to age\n         size = age))+      # map size to age\n  geom_point(             # display data as points\n    shape = \"diamond\",      # points display as diamonds\n    alpha = 0.3)            # point transparency at 30%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Axes assignments are always assigned to columns in the data (not to static values), and this is always done within mapping = aes().\nIt becomes important to keep track of your plot layers and aesthetics when making more complex plots - for example plots with multiple geoms. In the example below, the size = aesthetic is assigned twice - once for geom_point() and once for geom_smooth() - both times as a static value.\n\nggplot(data = linelist,\n       mapping = aes(           # map aesthetics to columns\n         x = age,\n         y = wt_kg,\n         color = age_years)\n       ) + \n  geom_point(                   # add points for each row of data\n    size = 1,\n    alpha = 0.5) +  \n  geom_smooth(                  # add a trend line \n    method = \"lm\",              # with linear method\n    size = 2)                   # size (width of line) of 2\n\n\n\n\n\n\n\n\n\n\nWhere to make mapping assignments\nAesthetic mapping within mapping = aes() can be written in several places in your plotting commands and can even be written more than once. This can be written in the top ggplot() command, and/or for each individual geom beneath. The nuances include:\n\nMapping assignments made in the top ggplot() command will be inherited as defaults across any geom below, like how x = and y = are inherited\nMapping assignments made within one geom apply only to that geom\n\nLikewise, data = specified in the top ggplot() will apply by default to any geom below, but you could also specify data for each geom (but this is more difficult).\nThus, each of the following commands will create the same plot:\n\n# These commands will produce the exact same plot\nggplot(data = linelist, mapping = aes(x = age))+\n  geom_histogram()\n\nggplot(data = linelist)+\n  geom_histogram(mapping = aes(x = age))\n\nggplot()+\n  geom_histogram(data = linelist, mapping = aes(x = age))\n\n\n\nGroups\nYou can easily group the data and “plot by group”. In fact, you have already done this!\nAssign the “grouping” column to the appropriate plot aesthetic, within a mapping = aes(). Above, we demonstrated this using continuous values when we assigned point size = to the column age. However this works the same way for discrete/categorical columns.\nFor example, if you want points to be displayed by gender, you would set mapping = aes(color = gender). A legend automatically appears. This assignment can be made within the mapping = aes() in the top ggplot() command (and be inherited by the geom), or it could be set in a separate mapping = aes() within the geom. Both approaches are shown below:\n\nggplot(data = linelist,\n       mapping = aes(x = age, y = wt_kg, color = gender))+\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# This alternative code produces the same plot\nggplot(data = linelist,\n       mapping = aes(x = age, y = wt_kg))+\n  geom_point(\n    mapping = aes(color = gender),\n    alpha = 0.5)\n\nNote that depending on the geom, you will need to use different arguments to group the data. For geom_point() you will most likely use color =, shape = or size =. Whereas for geom_bar() you are more likely to use fill =. This just depends on the geom and what plot aesthetic you want to reflect the groupings.\nFor your information - the most basic way of grouping the data is by using only the group = argument within mapping = aes(). However, this by itself will not change the colors, fill, or shapes. Nor will it create a legend. Yet the data are grouped, so statistical displays may be affected.\nTo adjust the order of groups in a plot, see the ggplot tips page or the page on Factors. There are many examples of grouped plots in the sections below on plotting continuous and categorical data.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#ggplot_basics_facet",
    "href": "new_pages/ggplot_basics.html#ggplot_basics_facet",
    "title": "30  ggplot basics",
    "section": "30.6 Facets / Small-multiples",
    "text": "30.6 Facets / Small-multiples\nFacets, or “small-multiples”, are used to split one plot into a multi-panel figure, with one panel (“facet”) per group of data. The same type of plot is created multiple times, each one using a sub-group of the same dataset.\nFaceting is a functionality that comes with ggplot2, so the legends and axes of the facet “panels” are automatically aligned. There are other packages discussed in the ggplot tips page that are used to combine completely different plots (cowplot and patchwork) into one figure.\nFaceting is done with one of the following ggplot2 functions:\n\nfacet_wrap() To show a different panel for each level of a single variable. One example of this could be showing a different epidemic curve for each hospital in a region. Facets are ordered alphabetically, unless the variable is a factor with other ordering defined.\n\n\n\nYou can invoke certain options to determine the layout of the facets, e.g. nrow = 1 or ncol = 1 to control the number of rows or columns that the faceted plots are arranged within.\n\n\nfacet_grid() This is used when you want to bring a second variable into the faceting arrangement. Here each panel of a grid shows the intersection between values in two columns. For example, epidemic curves for each hospital-age group combination with hospitals along the top (columns) and age groups along the sides (rows).\n\n\n\nnrow and ncol are not relevant, as the subgroups are presented in a grid\n\nEach of these functions accept a formula syntax to specify the column(s) for faceting. Both accept up to two columns, one on each side of a tilde ~.\n\nFor facet_wrap() most often you will write only one column preceded by a tilde ~ like facet_wrap(~hospital). However you can write two columns facet_wrap(outcome ~ hospital) - each unique combination will display in a separate panel, but they will not be arranged in a grid. The headings will show combined terms and these won’t be specific logic to the columns vs. rows. If you are providing only one faceting variable, a period . is used as a placeholder on the other side of the formula - see the code examples.\nFor facet_grid() you can also specify one or two columns to the formula (grid rows ~ columns). If you only want to specify one, you can place a period . on the other side of the tilde like facet_grid(. ~ hospital) or facet_grid(hospital ~ .).\n\nFacets can quickly contain an overwhelming amount of information - its good to ensure you don’t have too many levels of each variable that you choose to facet by. Here are some quick examples with the malaria dataset (see Download handbook and data) which consists of daily case counts of malaria for facilities, by age group.\nBelow we import and do some quick modifications for simplicity:\n\n# These data are daily counts of malaria cases, by facility-day\nmalaria_data &lt;- import(here(\"data\", \"malaria_facility_count_data.rds\")) %&gt;%  # import\n  select(-submitted_date, -Province, -newid)                                 # remove unneeded columns\n\nThe first 50 rows of the malaria data are below. Note there is a column malaria_tot, but also columns for counts by age group (these will be used in the second, facet_grid() example).\n\n\n\n\n\n\n\nfacet_wrap()\nFor the moment, let’s focus on the columns malaria_tot and District. Ignore the age-specific count columns for now. We will plot epidemic curves with geom_col(), which produces a column for each day at the specified y-axis height given in column malaria_tot (the data are already daily counts, so we use geom_col() - see the “Bar plot” section below).\nWhen we add the command facet_wrap(), we specify a tilde and then the column to facet on (District in this case). You can place another column on the left side of the tilde, - this will create one facet for each combination - but we recommend you do this with facet_grid() instead. In this use case, one facet is created for each unique value of District.\n\n# A plot with facets by district\nggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1, fill = \"darkred\") +       # plot the count data as columns\n  theme_minimal()+                              # simplify the background panels\n  labs(                                         # add plot labels, title, etc.\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district\") +\n  facet_wrap(~District)                       # the facets are created\n\n\n\n\n\n\n\n\n\n\nfacet_grid()\nWe can use a facet_grid() approach to cross two variables. Let’s say we want to cross District and age. Well, we need to do some data transformations on the age columns to get these data into ggplot-preferred “long” format. The age groups all have their own columns - we want them in a single column called age_group and another called num_cases. See the page on Pivoting data for more information on this process.\n\nmalaria_age &lt;- malaria_data %&gt;%\n  select(-malaria_tot) %&gt;% \n  pivot_longer(\n    cols = c(starts_with(\"malaria_rdt_\")),  # choose columns to pivot longer\n    names_to = \"age_group\",      # column names become age group\n    values_to = \"num_cases\"      # values to a single column (num_cases)\n  ) %&gt;%\n  mutate(\n    age_group = str_replace(age_group, \"malaria_rdt_\", \"\"),\n    age_group = forcats::fct_relevel(age_group, \"5-14\", after = 1))\n\nNow the first 50 rows of data look like this:\n\n\n\n\n\n\nWhen you pass the two variables to facet_grid(), easiest is to use formula notation (e.g. x ~ y) where x is rows and y is columns. Here is the plot, using facet_grid() to show the plots for each combination of the columns age_group and District.\n\nggplot(malaria_age, aes(x = data_date, y = num_cases)) +\n  geom_col(fill = \"darkred\", width = 1) +\n  theme_minimal()+\n  labs(\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district and age group\"\n  ) +\n  facet_grid(District ~ age_group)\n\n\n\n\n\n\n\n\n\n\nFree or fixed axes\nThe axes scales displayed when faceting are by default the same (fixed) across all the facets. This is helpful for cross-comparison, but not always appropriate.\nWhen using facet_wrap() or facet_grid(), we can add scales = \"free_y\" to “free” or release the y-axes of the panels to scale appropriately to their data subset. This is particularly useful if the actual counts are small for one of the subcategories and trends are otherwise hard to see. Instead of “free_y” we can also write “free_x” to do the same for the x-axis (e.g. for dates) or “free” for both axes. Note that in facet_grid, the y scales will be the same for facets in the same row, and the x scales will be the same for facets in the same column.\nWhen using facet_grid only, we can add space = \"free_y\" or space = \"free_x\" so that the actual height or width of the facet is weighted to the values of the figure within. This only works if scales = \"free\" (y or x) is already applied.\n\n# Free y-axis\nggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1, fill = \"darkred\") +       # plot the count data as columns\n  theme_minimal()+                              # simplify the background panels\n  labs(                                         # add plot labels, title, etc.\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district - 'free' x and y axes\") +\n  facet_wrap(~District, scales = \"free\")        # the facets are created\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor level order in facets\nSee this post on how to re-order factor levels within facets.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#storing-plots",
    "href": "new_pages/ggplot_basics.html#storing-plots",
    "title": "30  ggplot basics",
    "section": "30.7 Storing plots",
    "text": "30.7 Storing plots\n\nSaving plots\nBy default when you run a ggplot() command, the plot will be printed to the Plots RStudio pane. However, you can also save the plot as an object by using the assignment operator &lt;- and giving it a name. Then it will not print unless the object name itself is run. You can also print it by wrapping the plot name with print(), but this is only necessary in certain circumstances such as if the plot is created inside a for loop used to print multiple plots at once (see Iteration, loops, and lists page).\n\n# define plot\nage_by_wt &lt;- ggplot(data = linelist, mapping = aes(x = age_years, y = wt_kg, color = age_years))+\n  geom_point(alpha = 0.1)\n\n# print\nage_by_wt    \n\n\n\n\n\n\n\n\n\n\nModifying saved plots\nOne nice thing about ggplot2 is that you can define a plot (as above), and then add layers to it starting with its name. You do not have to repeat all the commands that created the original plot!\nFor example, to modify the plot age_by_wt that was defined above, to include a vertical line at age 50, we would just add a + and begin adding additional layers to the plot.\n\nage_by_wt+\n  geom_vline(xintercept = 50)\n\n\n\n\n\n\n\n\n\n\nExporting plots\nExporting ggplots is made easy with the ggsave() function from ggplot2. It can work in two ways, either:\n\nSpecify the name of the plot object, then the file path and name with extension\n\nFor example: ggsave(my_plot, here(\"plots\", \"my_plot.png\"))\n\n\nRun the command with only a file path, to save the last plot that was printed\n\nFor example: ggsave(here(\"plots\", \"my_plot.png\"))\n\n\nYou can export as png, pdf, jpeg, tiff, bmp, svg, or several other file types, by specifying the file extension in the file path.\nYou can also specify the arguments width =, height =, and units = (either “in”, “cm”, or “mm”). You can also specify dpi = with a number for plot resolution (e.g. 300). See the function details by entering ?ggsave or reading the documentation online.\nRemember that you can use here() syntax to provide the desired file path. see the Import and export page for more information.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#labels",
    "href": "new_pages/ggplot_basics.html#labels",
    "title": "30  ggplot basics",
    "section": "30.8 Labels",
    "text": "30.8 Labels\nSurely you will want to add or adjust the plot’s labels. These are most easily done within the labs() function which is added to the plot with + just as the geoms were.\nWithin labs() you can provide character strings to these arguements:\n\nx = and y = The x-axis and y-axis title (labels)\n\ntitle = The main plot title\n\nsubtitle = The subtitle of the plot, in smaller text below the title\n\ncaption = The caption of the plot, in bottom-right by default\n\nHere is a plot we made earlier, but with nicer labels:\n\nage_by_wt &lt;- ggplot(\n  data = linelist,   # set data\n  mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age))+     # map color to age\n  geom_point()+           # display data as points\n  labs(\n    title = \"Age and weight distribution\",\n    subtitle = \"Fictional Ebola outbreak, 2014\",\n    x = \"Age in years\",\n    y = \"Weight in kilos\",\n    color = \"Age\",\n    caption = stringr::str_glue(\"Data as of {max(linelist$date_hospitalisation, na.rm=T)}\"))\n\nage_by_wt\n\n\n\n\n\n\n\n\nNote how in the caption assignment we used str_glue() from the stringr package to implant dynamic R code within the string text. The caption will show the “Data as of:” date that reflects the maximum hospitalization date in the linelist. Read more about this in the page on Characters and strings.\nA note on specifying the legend title: There is no one “legend title” argument, as you could have multiple scales in your legend. Within labs(), you can write the argument for the plot aesthetic used to create the legend, and provide the title this way. For example, above we assigned color = age to create the legend. Therefore, we provide color = to labs() and assign the legend title desired (“Age” with capital A). If you create the legend with aes(fill = COLUMN), then in labs() you would write fill = to adjust the title of that legend. The section on color scales in the ggplot tips page provides more details on editing legends, and an alternative approach using scales_() functions.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#ggplot_basics_themes",
    "href": "new_pages/ggplot_basics.html#ggplot_basics_themes",
    "title": "30  ggplot basics",
    "section": "30.9 Themes",
    "text": "30.9 Themes\nOne of the best parts of ggplot2 is the amount of control you have over the plot - you can define anything! As mentioned above, the design of the plot that is not related to the data shapes/geometries are adjusted within the theme() function. For example, the plot background color, presence/absence of gridlines, and the font/size/color/alignment of text (titles, subtitles, captions, axis text…). These adjustments can be done in one of two ways:\n\nAdd a complete theme theme_() function to make sweeping adjustments - these include theme_classic(), theme_minimal(), theme_dark(), theme_light() theme_grey(), theme_bw() among others\n\nAdjust each tiny aspect of the plot individually within theme()\n\n\nComplete themes\nAs they are quite straight-forward, we will demonstrate the complete theme functions below and will not describe them further here. Note that any micro-adjustments with theme() should be made after use of a complete theme.\nWrite them with empty parentheses.\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme classic\")+\n  theme_classic()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme bw\")+\n  theme_bw()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme minimal\")+\n  theme_minimal()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme gray\")+\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModify theme\nThe theme() function can take a large number of arguments, each of which edits a very specific aspect of the plot. There is no way we could cover all of the arguments, but we will describe the general pattern for them and show you how to find the argument name that you need. The basic syntax is this:\n\nWithin theme() write the argument name for the plot element you want to edit, like plot.title =\n\nProvide an element_() function to the argument\n\n\n\nMost often, use element_text(), but others include element_rect() for canvas background colors, or element_blank() to remove plot elements\n\n\n\nWithin the element_() function, write argument assignments to make the fine adjustments you desire\n\nSo, that description was quite abstract, so here are some examples.\nThe below plot looks quite silly, but it serves to show you a variety of the ways you can adjust your plot.\n\nWe begin with the plot age_by_wt defined just above and add theme_classic()\n\nFor finer adjustments we add theme() and include one argument for each plot element to adjust\n\nIt can be nice to organize the arguments in logical sections. To describe just some of those used below:\n\nlegend.position = is unique in that it accepts simple values like “bottom”, “top”, “left”, and “right”. But generally, text-related arguments require that you place the details within element_text().\n\nTitle size with element_text(size = 30)\n\nThe caption horizontal alignment with element_text(hjust = 0) (from right to left)\n\nThe subtitle is italicized with element_text(face = \"italic\")\n\n\nage_by_wt + \n  theme_classic()+                                 # pre-defined theme adjustments\n  theme(\n    legend.position = \"bottom\",                    # move legend to bottom\n    \n    plot.title = element_text(size = 30),          # size of title to 30\n    plot.caption = element_text(hjust = 0),        # left-align caption\n    plot.subtitle = element_text(face = \"italic\"), # italicize subtitle\n    \n    axis.text.x = element_text(color = \"red\", size = 15, angle = 90), # adjusts only x-axis text\n    axis.text.y = element_text(size = 15),         # adjusts only y-axis text\n    \n    axis.title = element_text(size = 20)           # adjusts both axes titles\n    )     \n\n\n\n\n\n\n\n\nHere are some especially common theme() arguments. You will recognize some patterns, such as appending .x or .y to apply the change only to one axis.\n\n\n\n\n\n\n\ntheme() argument\nWhat it adjusts\n\n\n\n\nplot.title = element_text()\nThe title\n\n\nplot.subtitle = element_text()\nThe subtitle\n\n\nplot.caption = element_text()\nThe caption (family, face, color, size, angle, vjust, hjust…)\n\n\naxis.title = element_text()\nAxis titles (both x and y) (size, face, angle, color…)\n\n\naxis.title.x = element_text()\nAxis title x-axis only (use .y for y-axis only)\n\n\naxis.text = element_text()\nAxis text (both x and y)\n\n\naxis.text.x = element_text()\nAxis text x-axis only (use .y for y-axis only)\n\n\naxis.ticks = element_blank()\nRemove axis ticks\n\n\naxis.line = element_line()\nAxis lines (colour, size, linetype: solid dashed dotted etc)\n\n\nstrip.text = element_text()\nFacet strip text (colour, face, size, angle…)\n\n\nstrip.background = element_rect()\nfacet strip (fill, colour, size…)\n\n\n\nBut there are so many theme arguments! How could I remember them all? Do not worry - it is impossible to remember them all. Luckily there are a few tools to help you:\nThe tidyverse documentation on modifying theme, which has a complete list.\nTIP: Run theme_get() from ggplot2 to print a list of all 90+ theme() arguments to the console.\nTIP: If you ever want to remove an element of a plot, you can also do it through theme(). Just pass element_blank() to an argument to have it disappear completely. For legends, set legend.position = \"none\".",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#colors",
    "href": "new_pages/ggplot_basics.html#colors",
    "title": "30  ggplot basics",
    "section": "30.10 Colors",
    "text": "30.10 Colors\nPlease see this section on color scales of the ggplot tips page.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#piping-into-ggplot2",
    "href": "new_pages/ggplot_basics.html#piping-into-ggplot2",
    "title": "30  ggplot basics",
    "section": "30.11 Piping into ggplot2",
    "text": "30.11 Piping into ggplot2\nWhen using pipes to clean and transform your data, it is easy to pass the transformed data into ggplot().\nThe pipes that pass the dataset from function-to-function will transition to + once the ggplot() function is called. Note that in this case, there is no need to specify the data = argument, as this is automatically defined as the piped-in dataset.\nThis is how that might look:\n\nlinelist %&gt;%                                                     # begin with linelist\n  select(c(case_id, fever, chills, cough, aches, vomit)) %&gt;%     # select columns\n  pivot_longer(                                                  # pivot longer\n    cols = -case_id,                                  \n    names_to = \"symptom_name\",\n    values_to = \"symptom_is_present\") %&gt;%\n  mutate(                                                        # replace missing values\n    symptom_is_present = replace_na(symptom_is_present, \"unknown\")) %&gt;% \n  \n  ggplot(                                                        # begin ggplot!\n    mapping = aes(x = symptom_name, fill = symptom_is_present))+\n  geom_bar(position = \"fill\", col = \"black\") +                    \n  theme_classic() +\n  labs(\n    x = \"Symptom\",\n    y = \"Symptom status (proportion)\"\n  )",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#plot-continuous-data",
    "href": "new_pages/ggplot_basics.html#plot-continuous-data",
    "title": "30  ggplot basics",
    "section": "30.12 Plot continuous data",
    "text": "30.12 Plot continuous data\nThroughout this page, you have already seen many examples of plotting continuous data. Here we briefly consolidate these and present a few variations.\nVisualisations covered here include:\n\nPlots for one continuous variable:\n\nHistogram, a classic graph to present the distribution of a continuous variable.\nBox plot (also called box and whisker), to show the 25th, 50th, and 75th percentiles, tail ends of the distribution, and outliers (important limitations).\n\nJitter plot, to show all values as points that are ‘jittered’ so they can (mostly) all be seen, even where two have the same value.\n\nViolin plot, show the distribution of a continuous variable based on the symmetrical width of the ‘violin’.\nSina plot, are a combination of jitter and violin plots, where individual points are shown but in the symmetrical shape of the distribution (via ggforce package).\n\n\nScatter plot for two continuous variables.\n\nHeat plots for three continuous variables (linked to Heat plots page)\n\n\nHistograms\nHistograms may look like bar charts, but are distinct because they measure the distribution of a continuous variable. There are no spaces between the “bars”, and only one column is provided to geom_histogram().\nBelow is code for generating histograms, which group continuous data into ranges and display in adjacent bars of varying height. This is done using geom_histogram(). See the “Bar plot” section of the ggplot basics page to understand difference between geom_histogram(), geom_bar(), and geom_col().\nWe will show the distribution of ages of cases. Within mapping = aes() specify which column you want to see the distribution of. You can assign this column to either the x or the y axis.\nThe rows will be assigned to “bins” based on their numeric age, and these bins will be graphically represented by bars. If you specify a number of bins with the bins = plot aesthetic, the break points are evenly spaced between the minimum and maximum values of the histogram. If bins = is unspecified, an appropriate number of bins will be guessed and this message displayed after the plot:\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nIf you do not want to specify a number of bins to bins =, you could alternatively specify binwidth = in the units of the axis. We give a few examples showing different bins and bin widths:\n\n# A) Regular histogram\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram()+\n  labs(title = \"A) Default histogram (30 bins)\")\n\n# B) More bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(bins = 50)+\n  labs(title = \"B) Set to 50 bins\")\n\n# C) Fewer bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(bins = 5)+\n  labs(title = \"C) Set to 5 bins\")\n\n\n# D) More bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(binwidth = 1)+\n  labs(title = \"D) binwidth of 1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo get smoothed proportions, you can use geom_density():\n\n# Frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age)) +\n  geom_density(size = 2, alpha = 0.2)+\n  labs(title = \"Proportional density\")\n\n# Stacked frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age, fill = gender)) +\n  geom_density(size = 2, alpha = 0.2, position = \"stack\")+\n  labs(title = \"'Stacked' proportional densities\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo get a “stacked” histogram (of a continuous column of data), you can do one of the following:\n\nUse geom_histogram() with the fill = argument within aes() and assigned to the grouping column, or\n\nUse geom_freqpoly(), which is likely easier to read (you can still set binwidth =)\n\nTo see proportions of all values, set the y = after_stat(density) (use this syntax exactly - not changed for your data). Note: these proportions will show per group.\n\nEach is shown below (*note use of color = vs. fill = in each):\n\n# \"Stacked\" histogram\nggplot(data = linelist, mapping = aes(x = age, fill = gender)) +\n  geom_histogram(binwidth = 2)+\n  labs(title = \"'Stacked' histogram\")\n\n# Frequency \nggplot(data = linelist, mapping = aes(x = age, color = gender)) +\n  geom_freqpoly(binwidth = 2, size = 2)+\n  labs(title = \"Freqpoly\")\n\n# Frequency with proportion axis\nggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +\n  geom_freqpoly(binwidth = 5, size = 2)+\n  labs(title = \"Proportional freqpoly\")\n\n# Frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +\n  geom_density(size = 2, alpha = 0.2)+\n  labs(title = \"Proportional, smoothed with geom_density()\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to have some fun, try geom_density_ridges from the ggridges package (vignette here.\nRead more in detail about histograms at the tidyverse page on geom_histogram().\n\n\nBox plots\nBox plots are common, but have important limitations. They can obscure the actual distribution - e.g. a bi-modal distribution. See this R graph gallery and this data-to-viz article for more details. However, they do nicely display the inter-quartile range and outliers - so they can be overlaid on top of other types of plots that show the distribution in more detail.\nBelow we remind you of the various components of a boxplot:\n\n\n\n\n\n\n\n\n\nWhen using geom_boxplot() to create a box plot, you generally map only one axis (x or y) within aes(). The axis specified determines if the plots are horizontal or vertical.\nIn most geoms, you create a plot per group by mapping an aesthetic like color = or fill = to a column within aes(). However, for box plots achieve this by assigning the grouping column to the un-assigned axis (x or y). Below is code for a boxplot of all age values in the dataset, and second is code to display one box plot for each (non-missing) gender in the dataset. Note that NA (missing) values will appear as a separate box plot unless removed. In this example we also set the fill to the column outcome so each plot is a different color - but this is not necessary.\n\n# A) Overall boxplot\nggplot(data = linelist)+  \n  geom_boxplot(mapping = aes(y = age))+   # only y axis mapped (not x)\n  labs(title = \"A) Overall boxplot\")\n\n# B) Box plot by group\nggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + \n  geom_boxplot()+                     \n  theme(legend.position = \"none\")+   # remove legend (redundant)\n  labs(title = \"B) Boxplot by gender\")      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor code to add a box plot to the edges of a scatter plot (“marginal” plots) see the page ggplot tips.\n\n\nViolin, jitter, and sina plots\nBelow is code for creating violin plots (geom_violin) and jitter plots (geom_jitter) to show distributions. You can specify that the fill or color is also determined by the data, by inserting these options within aes().\n\n# A) Jitter plot by group\nggplot(data = linelist %&gt;% drop_na(outcome),      # remove missing values\n       mapping = aes(y = age,                     # Continuous variable\n           x = outcome,                           # Grouping variable\n           color = outcome))+                     # Color variable\n  geom_jitter()+                                  # Create the violin plot\n  labs(title = \"A) jitter plot by gender\")     \n\n\n\n# B) Violin plot by group\nggplot(data = linelist %&gt;% drop_na(outcome),       # remove missing values\n       mapping = aes(y = age,                      # Continuous variable\n           x = outcome,                            # Grouping variable\n           fill = outcome))+                       # fill variable (color)\n  geom_violin()+                                   # create the violin plot\n  labs(title = \"B) violin plot by gender\")    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can combine the two using the geom_sina() function from the ggforce package. The sina plots the jitter points in the shape of the violin plot. When overlaid on the violin plot (adjusting the transparencies) this can be easier to visually interpret.\n\n# A) Sina plot by group\nggplot(\n  data = linelist %&gt;% drop_na(outcome), \n  aes(y = age,           # numeric variable\n      x = outcome)) +    # group variable\n  geom_violin(\n    aes(fill = outcome), # fill (color of violin background)\n    color = \"white\",     # white outline\n    alpha = 0.2)+        # transparency\n  geom_sina(\n    size=1,                # Change the size of the jitter\n    aes(color = outcome))+ # color (color of dots)\n  scale_fill_manual(       # Define fill for violin background by death/recover\n    values = c(\"Death\" = \"#bf5300\", \n              \"Recover\" = \"#11118c\")) + \n  scale_color_manual(      # Define colours for points by death/recover\n    values = c(\"Death\" = \"#bf5300\", \n              \"Recover\" = \"#11118c\")) + \n  theme_minimal() +                                # Remove the gray background\n  theme(legend.position = \"none\") +                # Remove unnecessary legend\n  labs(title = \"B) violin and sina plot by gender, with extra formatting\")      \n\n\n\n\n\n\n\n\n\n\nTwo continuous variables\nFollowing similar syntax, geom_point() will allow you to plot two continuous variables against each other in a scatter plot. This is useful for showing actual values rather than their distributions. A basic scatter plot of age vs weight is shown in (A). In (B) we again use facet_grid() to show the relationship between two continuous variables in the linelist.\n\n# Basic scatter plot of weight and age\nggplot(data = linelist, \n       mapping = aes(y = wt_kg, x = age))+\n  geom_point() +\n  labs(title = \"A) Scatter plot of weight and age\")\n\n# Scatter plot of weight and age by gender and Ebola outcome\nggplot(data = linelist %&gt;% drop_na(gender, outcome), # filter retains non-missing gender/outcome\n       mapping = aes(y = wt_kg, x = age))+\n  geom_point() +\n  labs(title = \"B) Scatter plot of weight and age faceted by gender and outcome\")+\n  facet_grid(gender ~ outcome) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree continuous variables\nYou can display three continuous variables by utilizing the fill = argument to create a heat plot. The color of each “cell” will reflect the value of the third continuous column of data. See the ggplot tips page and the page on on [Heat plots] for more details and several examples.\nThere are ways to make 3D plots in R, but for applied epidemiology these are often difficult to interpret and therefore less useful for decision-making.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#plot-categorical-data",
    "href": "new_pages/ggplot_basics.html#plot-categorical-data",
    "title": "30  ggplot basics",
    "section": "30.13 Plot categorical data",
    "text": "30.13 Plot categorical data\nCategorical data can be character values, could be logical (TRUE/FALSE), or factors (see the [Factors] page).\n\nPreparation\n\nData structure\nThe first thing to understand about your categorical data is whether it exists as raw observations like a linelist of cases, or as a summary or aggregate data frame that holds counts or proportions. The state of your data will impact which plotting function you use:\n\nIf your data are raw observations with one row per observation, you will likely use geom_bar()\n\nIf your data are already aggregated into counts or proportions, you will likely use geom_col()\n\n\n\nColumn class and value ordering\nNext, examine the class of the columns you want to plot. We look at hospital, first with class() from base R, and with tabyl() from janitor.\n\n# View class of hospital column - we can see it is a character\nclass(linelist$hospital)\n\n[1] \"character\"\n\n# Look at values and proportions within hospital column\nlinelist %&gt;% \n  tabyl(hospital)\n\n                             hospital    n    percent\n                     Central Hospital  454 0.07710598\n                    Military Hospital  896 0.15217391\n                              Missing 1469 0.24949049\n                                Other  885 0.15030571\n                        Port Hospital 1762 0.29925272\n St. Mark's Maternity Hospital (SMMH)  422 0.07167120\n\n\nWe can see the values within are characters, as they are hospital names, and by default they are ordered alphabetically. There are ‘other’ and ‘missing’ values, which we would prefer to be the last subcategories when presenting breakdowns. So we change this column into a factor and re-order it. This is covered in more detail in the Factors page.\n\n# Convert to factor and define level order so \"Other\" and \"Missing\" are last\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    hospital = fct_relevel(hospital, \n      \"St. Mark's Maternity Hospital (SMMH)\",\n      \"Port Hospital\", \n      \"Central Hospital\",\n      \"Military Hospital\",\n      \"Other\",\n      \"Missing\"))\n\n\nlevels(linelist$hospital)\n\n[1] \"St. Mark's Maternity Hospital (SMMH)\"\n[2] \"Port Hospital\"                       \n[3] \"Central Hospital\"                    \n[4] \"Military Hospital\"                   \n[5] \"Other\"                               \n[6] \"Missing\"                             \n\n\n\n\n\ngeom_bar()\nUse geom_bar() if you want bar height (or the height of stacked bar components) to reflect the number of relevant rows in the data. These bars will have gaps between them, unless the width = plot aesthetic is adjusted.\n\nProvide only one axis column assignment (typically x-axis). If you provide x and y, you will get Error: stat_count() can only have an x or y aesthetic.\n\nYou can create stacked bars by adding a fill = column assignment within mapping = aes()\n\nThe opposite axis will be titled “count” by default, because it represents the number of rows\n\nBelow, we have assigned outcome to the y-axis, but it could just as easily be on the x-axis. If you have longer character values, it can sometimes look better to flip the bars sideways and put the legend on the bottom. This may impact how your factor levels are ordered - in this case we reverse them with fct_rev() to put missing and other at the bottom.\n\n# A) Outcomes in all cases\nggplot(linelist %&gt;% drop_na(outcome)) + \n  geom_bar(aes(y = fct_rev(hospital)), width = 0.7) +\n  theme_minimal()+\n  labs(title = \"A) Number of cases by hospital\",\n       y = \"Hospital\")\n\n\n# B) Outcomes in all cases by hosptial\nggplot(linelist %&gt;% drop_na(outcome)) + \n  geom_bar(aes(y = fct_rev(hospital), fill = outcome), width = 0.7) +\n  theme_minimal()+\n  theme(legend.position = \"bottom\") +\n  labs(title = \"B) Number of recovered and dead Ebola cases, by hospital\",\n       y = \"Hospital\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeom_col()\nUse geom_col() if you want bar height (or height of stacked bar components) to reflect pre-calculated values that exists in the data. Often, these are summary or “aggregated” counts, or proportions.\nProvide column assignments for both axes to geom_col(). Typically your x-axis column is discrete and your y-axis column is numeric.\nLet’s say we have this dataset outcomes:\n\n\n# A tibble: 2 × 3\n  outcome     n proportion\n  &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 Death    1022       56.2\n2 Recover   796       43.8\n\n\nBelow is code using geom_col for creating simple bar charts to show the distribution of Ebola patient outcomes. With geom_col, both x and y need to be specified. Here x is the categorical variable along the x axis, and y is the generated proportions column proportion.\n\n# Outcomes in all cases\nggplot(outcomes) + \n  geom_col(aes(x=outcome, y = proportion)) +\n  labs(subtitle = \"Number of recovered and dead Ebola cases\")\n\n\n\n\n\n\n\n\nTo show breakdowns by hospital, we would need our table to contain more information, and to be in “long” format. We create this table with the frequencies of the combined categories outcome and hospital (see Grouping data page for grouping tips).\n\noutcomes2 &lt;- linelist %&gt;% \n  drop_na(outcome) %&gt;% \n  count(hospital, outcome) %&gt;%  # get counts by hospital and outcome\n  group_by(hospital) %&gt;%        # Group so proportions are out of hospital total\n  mutate(proportion = n/sum(n)*100) # calculate proportions of hospital total\n\nhead(outcomes2) # Preview data\n\n# A tibble: 6 × 4\n# Groups:   hospital [3]\n  hospital                             outcome     n proportion\n  &lt;fct&gt;                                &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 St. Mark's Maternity Hospital (SMMH) Death     199       61.2\n2 St. Mark's Maternity Hospital (SMMH) Recover   126       38.8\n3 Port Hospital                        Death     785       57.6\n4 Port Hospital                        Recover   579       42.4\n5 Central Hospital                     Death     193       53.9\n6 Central Hospital                     Recover   165       46.1\n\n\nWe then create the ggplot with some added formatting:\n\nAxis flip: Swapped the axis around with coord_flip() so that we can read the hospital names.\nColumns side-by-side: Added a position = \"dodge\" argument so that the bars for death and recover are presented side by side rather than stacked. Note stacked bars are the default.\nColumn width: Specified ‘width’, so the columns are half as thin as the full possible width.\nColumn order: Reversed the order of the categories on the y axis so that ‘Other’ and ‘Missing’ are at the bottom, with scale_x_discrete(limits=rev). Note that we used that rather than scale_y_discrete because hospital is stated in the x argument of aes(), even if visually it is on the y axis. We do this because Ggplot seems to present categories backwards unless we tell it not to.\n\nOther details: Labels/titles and colours added within labs and scale_fill_color respectively.\n\n\n# Outcomes in all cases by hospital\nggplot(outcomes2) +  \n  geom_col(\n    mapping = aes(\n      x = proportion,                 # show pre-calculated proportion values\n      y = fct_rev(hospital),          # reverse level order so missing/other at bottom\n      fill = outcome),                # stacked by outcome\n    width = 0.5)+                    # thinner bars (out of 1)\n  theme_minimal() +                  # Minimal theme \n  theme(legend.position = \"bottom\")+\n  labs(subtitle = \"Number of recovered and dead Ebola cases, by hospital\",\n       fill = \"Outcome\",             # legend title\n       y = \"Count\",                  # y axis title\n       x = \"Hospital of admission\")+ # x axis title\n  scale_fill_manual(                 # adding colors manually\n    values = c(\"Death\"= \"#3B1c8C\",\n               \"Recover\" = \"#21908D\" )) \n\n\n\n\n\n\n\n\nNote that the proportions are binary, so we may prefer to drop ‘recover’ and just show the proportion who died. This is just for illustration purposes.\nIf using geom_col() with dates data (e.g. an epicurve from aggregated data) - you will want to adjust the width = argument to remove the “gap” lines between the bars. If using daily data set width = 1. If weekly, width = 7. Months are not possible because each month has a different number of days.\n\n\ngeom_histogram()\nHistograms may look like bar charts, but are distinct because they measure the distribution of a continuous variable. There are no spaces between the “bars”, and only one column is provided to geom_histogram(). There are arguments specific to histograms such as bin_width = and breaks = to specify how the data should be binned. The section above on continuous data and the page on Epidemic curves provide additional detail.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_basics.html#resources",
    "href": "new_pages/ggplot_basics.html#resources",
    "title": "30  ggplot basics",
    "section": "30.14 Resources",
    "text": "30.14 Resources\nThere is a huge amount of help online, especially with ggplot. See:\n\nggplot2 cheat sheet\nanother cheat sheet\ntidyverse ggplot basics page\n\nplotting continuous variables\n\nR for Data Science pages on data visualization\ngraphics for communicaton",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>ggplot basics</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html",
    "href": "new_pages/ggplot_tips.html",
    "title": "31  ggplot tips",
    "section": "",
    "text": "31.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#preparation",
    "href": "new_pages/ggplot_tips.html#preparation",
    "title": "31  ggplot tips",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  tidyverse,      # includes ggplot2 and other\n  rio,            # import/export\n  here,           # file locator\n  stringr,        # working with characters   \n  scales,         # transform numbers\n  ggrepel,        # smartly-placed labels\n  gghighlight,    # highlight one part of plot\n  RColorBrewer    # color scales\n)\n\n\n\nImport data\nFor this page, we import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\nlinelist &lt;- rio::import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#ggplot_tips_colors",
    "href": "new_pages/ggplot_tips.html#ggplot_tips_colors",
    "title": "31  ggplot tips",
    "section": "31.2 Scales for color, fill, axes, etc.",
    "text": "31.2 Scales for color, fill, axes, etc.\nIn ggplot2, when aesthetics of plotted data (e.g. size, color, shape, fill, plot axis) are mapped to columns in the data, the exact display can be adjusted with the corresponding “scale” command. In this section we explain some common scale adjustments.\n\n31.2.1 Color schemes\nOne thing that can initially be difficult to understand with ggplot2 is control of color schemes. Note that this section discusses the color of plot objects (geoms/shapes) such as points, bars, lines, tiles, etc. To adjust color of accessory text, titles, or background color see the Themes section of the [ggplot basics] page.\nTo control “color” of plot objects you will be adjusting either the color = aesthetic (the exterior color) or the fill = aesthetic (the interior color). One exception to this pattern is geom_point(), where you really only get to control color =, which controls the color of the point (interior and exterior).\nWhen setting colour or fill you can use colour names recognized by R like \"red\" (see complete list or enter ?colors), or a specific hex colour such as \"#ff0505\".\n\n# histogram - \nggplot(data = linelist, mapping = aes(x = age))+       # set data and axes\n  geom_histogram(              # display histogram\n    binwidth = 7,                # width of bins\n    color = \"red\",               # bin line color\n    fill = \"lightblue\")          # bin interior color (fill) \n\n\n\n\n\n\n\n\nAs explained the ggplot basics section on mapping data to the plot, aesthetics such as fill = and color = can be defined either outside of a mapping = aes() statement or inside of one. If outside the aes(), the assigned value should be static (e.g. color = \"blue\") and will apply for all data plotted by the geom. If inside, the aesthetic should be mapped to a column, like color = hospital, and the expression will vary by the value for that row in the data. A few examples:\n\n# Static color for points and for line\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(color = \"purple\")+\n  geom_vline(xintercept = 50, color = \"orange\")+\n  labs(title = \"Static color for points and line\")\n\n# Color mapped to continuous column\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(mapping = aes(color = temp))+         \n  labs(title = \"Color mapped to continuous column\")\n\n# Color mapped to discrete column\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(mapping = aes(color = gender))+         \n  labs(title = \"Color mapped to discrete column\")\n\n# bar plot, fill to discrete column, color to static value\nggplot(data = linelist, mapping = aes(x = hospital))+     \n  geom_bar(mapping = aes(fill = gender), color = \"yellow\")+         \n  labs(title = \"Fill mapped to discrete column, static color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScales\nOnce you map a column to a plot aesthetic (e.g. x =, y =, fill =, color =…), your plot will gain a scale/legend. See above how the scale can be continuous, discrete, date, etc. values depending on the class of the assigned column. If you have multiple aesthetics mapped to columns, your plot will have multiple scales.\nYou can control the scales with the appropriate scales_() function. The scale functions of ggplot() have 3 parts that are written like this: scale_AESTHETIC_METHOD().\n\nThe first part, scale_(), is fixed.\n\nThe second part, the AESTHETIC, should be the aesthetic that you want to adjust the scale for (_fill_, _shape_, _color_, _size_, _alpha_…) - the options here also include _x_ and _y_.\n\nThe third part, the METHOD, will be either _discrete(), continuous(), _date(), _gradient(), or _manual() depending on the class of the column and how you want to control it. There are others, but these are the most-often used.\n\nBe sure that you use the correct function for the scale! Otherwise your scale command will not appear to change anything. If you have multiple scales, you may use multiple scale functions to adjust them! For example:\n\n\nScale arguments\nEach kind of scale has its own arguments, though there is some overlap. Query the function like ?scale_color_discrete in the R console to see the function argument documentation.\nFor continuous scales, use breaks = to provide a sequence of values with seq() (take to =, from =, and by = as shown in the example below. Set expand = c(0,0) to eliminate padding space around the axes (this can be used on any _x_ or _y_ scale.\nFor discrete scales, you can adjust the order of level appearance with breaks =, and how the values display with the labels = argument. Provide a character vector to each of those (see example below). You can also drop NA easily by setting na.translate = FALSE.\nThe nuances of date scales are covered more extensively in the Epidemic curves page.\n\n\nManual adjustments\nOne of the most useful tricks is using “manual” scaling functions to explicitly assign colors as you desire. These are functions with the syntax scale_xxx_manual() (e.g. scale_colour_manual() or scale_fill_manual()). Each of the below arguments are demonstrated in the code example below.\n\nAssign colors to data values with the values = argument\n\nSpecify a color for NA with na.value =\n\nChange how the values are written in the legend with the labels = argument\n\nChange the legend title with name =\n\nBelow, we create a bar plot and show how it appears by default, and then with three scales adjusted - the continuous y-axis scale, the discrete x-axis scale, and manual adjustment of the fill (interior bar color).\n\n# BASELINE - no scale adjustment\nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  labs(title = \"Baseline - no scale adjustments\")\n\n\n\n\n\n\n\n# SCALES ADJUSTED\nggplot(data = linelist)+\n  \n  geom_bar(mapping = aes(x = outcome, fill = gender), color = \"black\")+\n  \n  theme_minimal()+                   # simplify background\n  \n  scale_y_continuous(                # continuous scale for y-axis (counts)\n    expand = c(0,0),                 # no padding\n    breaks = seq(from = 0,\n                 to = 3000,\n                 by = 500))+\n  \n  scale_x_discrete(                   # discrete scale for x-axis (gender)\n    expand = c(0,0),                  # no padding\n    drop = FALSE,                     # show all factor levels (even if not in data)\n    na.translate = FALSE,             # remove NA outcomes from plot\n    labels = c(\"Died\", \"Recovered\"))+ # Change display of values\n    \n  \n  scale_fill_manual(                  # Manually specify fill (bar interior color)\n    values = c(\"m\" = \"violetred\",     # reference values in data to assign colors\n               \"f\" = \"aquamarine\"),\n    labels = c(\"m\" = \"Male\",          # re-label the legend (use \"=\" assignment to avoid mistakes)\n              \"f\" = \"Female\",\n              \"Missing\"),\n    name = \"Gender\",                  # title of legend\n    na.value = \"grey\"                 # assign a color for missing values\n  )+\n  labs(title = \"Adjustment of scales\") # Adjust the title of the fill legend\n\n\n\n\n\n\n\n\n\n\nContinuous axes scales\nWhen data are mapping to the plot axes, these too can be adjusted with scales commands. A common example is adjusting the display of an axis (e.g. y-axis) that is mapped to a column with continuous data.\nWe may want to adjust the breaks or display of the values in the ggplot using scale_y_continuous(). As noted above, use the argument breaks = to provide a sequence of values that will serve as “breaks” along the scale. These are the values at which numbers will display. To this argument, you can provide a c() vector containing the desired break values, or you can provide a regular sequence of numbers using the base R function seq(). This seq() function accepts to =, from =, and by =.\n\n# BASELINE - no scale adjustment\nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  labs(title = \"Baseline - no scale adjustments\")\n\n# \nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  scale_y_continuous(\n    breaks = seq(\n      from = 0,\n      to = 3000,\n      by = 100)\n  )+\n  labs(title = \"Adjusted y-axis breaks\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplay percents\nIf your original data values are proportions, you can easily display them as percents with “%” by providing labels = scales::percent in your scales command, as shown below.\nWhile an alternative would be to convert the values to character and add a “%” character to the end, this approach will cause complications because your data will no longer be continuous numeric values.\n\n# Original y-axis proportions\n#############################\nlinelist %&gt;%                                   # start with linelist\n  group_by(hospital) %&gt;%                       # group data by hospital\n  summarise(                                   # create summary columns\n    n = n(),                                     # total number of rows in group\n    deaths = sum(outcome == \"Death\", na.rm=T),   # number of deaths in group\n    prop_death = deaths/n) %&gt;%                   # proportion deaths in group\n  ggplot(                                      # begin plotting\n    mapping = aes(\n      x = hospital,\n      y = prop_death))+ \n  geom_col()+\n  theme_minimal()+\n  labs(title = \"Display y-axis original proportions\")\n\n\n\n# Display y-axis proportions as percents\n########################################\nlinelist %&gt;%         \n  group_by(hospital) %&gt;% \n  summarise(\n    n = n(),\n    deaths = sum(outcome == \"Death\", na.rm=T),\n    prop_death = deaths/n) %&gt;% \n  ggplot(\n    mapping = aes(\n      x = hospital,\n      y = prop_death))+\n  geom_col()+\n  theme_minimal()+\n  labs(title = \"Display y-axis as percents (%)\")+\n  scale_y_continuous(\n    labels = scales::percent                    # display proportions as percents\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog scale\nTo transform a continuous axis to log scale, add trans = \"log2\" to the scale command. For purposes of example, we create a data frame of regions with their respective preparedness_index and cumulative cases values.\n\nplot_data &lt;- data.frame(\n  region = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"),\n  preparedness_index = c(8.8, 7.5, 3.4, 3.6, 2.1, 7.9, 7.0, 5.6, 1.0),\n  cases_cumulative = c(15, 45, 80, 20, 21, 7, 51, 30, 1442)\n)\n\nplot_data\n\n  region preparedness_index cases_cumulative\n1      A                8.8               15\n2      B                7.5               45\n3      C                3.4               80\n4      D                3.6               20\n5      E                2.1               21\n6      F                7.9                7\n7      G                7.0               51\n8      H                5.6               30\n9      I                1.0             1442\n\n\nThe cumulative cases for region “I” are dramatically greater than all the other regions. In circumstances like this, you may elect to display the y-axis using a log scale so the reader can see differences between the regions with fewer cumulative cases.\n\n# Original y-axis\npreparedness_plot &lt;- ggplot(data = plot_data,  \n       mapping = aes(\n         x = preparedness_index,\n         y = cases_cumulative))+\n  geom_point(size = 2)+            # points for each region \n  geom_text(\n    mapping = aes(label = region),\n    vjust = 1.5)+                  # add text labels\n  theme_minimal()\n\npreparedness_plot                  # print original plot\n\n\n# print with y-axis transformed\npreparedness_plot+                   # begin with plot saved above\n  scale_y_continuous(trans = \"log2\") # add transformation for y-axis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGradient scales\nFill gradient scales can involve additional nuance. The defaults are usually quite pleasing, but you may want to adjust the values, cutoffs, etc.\nTo demonstrate how to adjust a continuous color scale, we’ll use a data set from the Contact tracing page that contains the ages of cases and of their source cases.\n\ncase_source_relationships &lt;- rio::import(here::here(\"data\", \"godata\", \"relationships_clean.rds\")) %&gt;% \n  select(source_age, target_age) \n\nBelow, we produce a “raster” heat tile density plot. We won’t elaborate how (see the link in paragraph above) but we will focus on how we can adjust the color scale. Read more about the stat_density2d() ggplot2 function here. Note how the fill scale is continuous.\n\ntrans_matrix &lt;- ggplot(\n    data = case_source_relationships,\n    mapping = aes(x = source_age, y = target_age))+\n  stat_density2d(\n    geom = \"raster\",\n    mapping = aes(fill = after_stat(density)),\n    contour = FALSE)+\n  theme_minimal()\n\nNow we show some variations on the fill scale:\n\ntrans_matrix\ntrans_matrix + scale_fill_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we show some examples of actually adjusting the break points of the scale:\n\nscale_fill_gradient() accepts two colors (high/low)\n\nscale_fill_gradientn() accepts a vector of any length of colors to values = (intermediate values will be interpolated)\n\nUse scales::rescale() to adjust how colors are positioned along the gradient; it rescales your vector of positions to be between 0 and 1.\n\n\ntrans_matrix + \n  scale_fill_gradient(     # 2-sided gradient scale\n    low = \"aquamarine\",    # low value\n    high = \"purple\",       # high value\n    na.value = \"grey\",     # value for NA\n    name = \"Density\")+     # Legend title\n  labs(title = \"Manually specify high/low colors\")\n\n# 3+ colors to scale\ntrans_matrix + \n  scale_fill_gradientn(    # 3-color scale (low/mid/high)\n    colors = c(\"blue\", \"yellow\",\"red\") # provide colors in vector\n  )+\n  labs(title = \"3-color scale\")\n\n# Use of rescale() to adjust placement of colors along scale\ntrans_matrix + \n  scale_fill_gradientn(    # provide any number of colors\n    colors = c(\"blue\", \"yellow\",\"red\", \"black\"),\n    values = scales::rescale(c(0, 0.05, 0.07, 0.10, 0.15, 0.20, 0.3, 0.5)) # positions for colors are rescaled between 0 and 1\n    )+\n  labs(title = \"Colors not evenly positioned\")\n\n# use of limits to cut-off values that get fill color\ntrans_matrix + \n  scale_fill_gradientn(    \n    colors = c(\"blue\", \"yellow\",\"red\"),\n    limits = c(0, 0.0002))+\n  labs(title = \"Restrict value limits, resulting in grey space\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalettes\n\nColorbrewer and Viridis\nMore generally, if you want predefined palettes, you can use the scale_xxx_brewer or scale_xxx_viridis_y functions.\nThe ‘brewer’ functions can draw from colorbrewer.org palettes.\nThe ‘viridis’ functions draw from viridis (colourblind friendly!) palettes, which “provide colour maps that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness.” (read more here and here). Define if the palette is discrete, continuous, or binned by specifying this at the end of the function (e.g. discrete is scale_xxx_viridis_d).\nIt is advised that you test your plot in this color blindness simulator. If you have a red/green color scheme, try a “hot-cold” (red-blue) scheme instead as described here\nHere is an example from the ggplot basics page, using various color schemes.\n\nsymp_plot &lt;- linelist %&gt;%                                         # begin with linelist\n  select(c(case_id, fever, chills, cough, aches, vomit)) %&gt;%     # select columns\n  pivot_longer(                                                  # pivot longer\n    cols = -case_id,                                  \n    names_to = \"symptom_name\",\n    values_to = \"symptom_is_present\") %&gt;%\n  mutate(                                                        # replace missing values\n    symptom_is_present = replace_na(symptom_is_present, \"unknown\")) %&gt;% \n  ggplot(                                                        # begin ggplot!\n    mapping = aes(x = symptom_name, fill = symptom_is_present))+\n  geom_bar(position = \"fill\", col = \"black\") +                    \n  theme_classic() +\n  theme(legend.position = \"bottom\")+\n  labs(\n    x = \"Symptom\",\n    y = \"Symptom status (proportion)\"\n  )\n\nsymp_plot  # print with default colors\n\n#################################\n# print with manually-specified colors\nsymp_plot +\n  scale_fill_manual(\n    values = c(\"yes\" = \"black\",         # explicitly define colours\n               \"no\" = \"white\",\n               \"unknown\" = \"grey\"),\n    breaks = c(\"yes\", \"no\", \"unknown\"), # order the factors correctly\n    name = \"\"                           # set legend to no title\n\n  ) \n\n#################################\n# print with viridis discrete colors\nsymp_plot +\n  scale_fill_viridis_d(\n    breaks = c(\"yes\", \"no\", \"unknown\"),\n    name = \"\"\n  )",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#change-order-of-discrete-variables",
    "href": "new_pages/ggplot_tips.html#change-order-of-discrete-variables",
    "title": "31  ggplot tips",
    "section": "31.3 Change order of discrete variables",
    "text": "31.3 Change order of discrete variables\nChanging the order that discrete variables appear in is often difficult to understand for people who are new to ggplot2 graphs. It’s easy to understand how to do this however once you understand how ggplot2 handles discrete variables under the hood. Generally speaking, if a discrete varaible is used, it is automatically converted to a factor type - which orders factors by alphabetical order by default. To handle this, you simply have to reorder the factor levels to reflect the order you would like them to appear in the chart. For more detailed information on how to reorder factor objects, see the factor section of the guide.\nWe can look at a common example using age groups - by default the 5-9 age group will be placed in the middle of the age groups (given alphanumeric order), but we can move it behind the 0-4 age group of the chart by releveling the factors.\n\nggplot(\n  data = linelist %&gt;% drop_na(age_cat5),                         # remove rows where age_cat5 is missing\n  mapping = aes(x = fct_relevel(age_cat5, \"5-9\", after = 1))) +  # relevel factor\n\n  geom_bar() +\n  \n  labs(x = \"Age group\", y = \"Number of hospitalisations\",\n       title = \"Total hospitalisations by age group\") +\n  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n31.3.0.1 ggthemr\nAlso consider using the ggthemr package. You can download this package from Github using the instructions here. It offers palettes that are very aesthetically pleasing, but be aware that these typically have a maximum number of values that can be limiting if you want more than 7 or 8 colors.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#contour-lines",
    "href": "new_pages/ggplot_tips.html#contour-lines",
    "title": "31  ggplot tips",
    "section": "31.4 Contour lines",
    "text": "31.4 Contour lines\nContour plots are helpful when you have many points that might cover each other (“overplotting”). The case-source data used above are again plotted, but more simply using stat_density2d() and stat_density2d_filled() to produce discrete contour levels - like a topographical map. Read more about the statistics here.\n\ncase_source_relationships %&gt;% \n  ggplot(aes(x = source_age, y = target_age))+\n  stat_density2d()+\n  geom_point()+\n  theme_minimal()+\n  labs(title = \"stat_density2d() + geom_point()\")\n\n\ncase_source_relationships %&gt;% \n  ggplot(aes(x = source_age, y = target_age))+\n  stat_density2d_filled()+\n  theme_minimal()+\n  labs(title = \"stat_density2d_filled()\")",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#marginal-distributions",
    "href": "new_pages/ggplot_tips.html#marginal-distributions",
    "title": "31  ggplot tips",
    "section": "31.5 Marginal distributions",
    "text": "31.5 Marginal distributions\nTo show the distributions on the edges of a geom_point() scatterplot, you can use the ggExtra package and its function ggMarginal(). Save your original ggplot as an object, then pass it to ggMarginal() as shown below. Here are the key arguments:\n\nYou must specify the type = as either “histogram”, “density” “boxplot”, “violin”, or “densigram”.\n\nBy default, marginal plots will appear for both axes. You can set margins = to “x” or “y” if you only want one.\n\nOther optional arguments include fill = (bar color), color = (line color), size = (plot size relative to margin size, so larger number makes the marginal plot smaller).\n\nYou can provide other axis-specific arguments to xparams = and yparams =. For example, to have different histogram bin sizes, as shown below.\n\nYou can have the marginal plots reflect groups (columns that have been assigned to color = in your ggplot() mapped aesthetics). If this is the case, set the ggMarginal() argument groupColour = or groupFill = to TRUE, as shown below.\nRead more at this vignette, in the R Graph Gallery or the function R documentation ?ggMarginal.\n\n# Install/load ggExtra\npacman::p_load(ggExtra)\n\n# Basic scatter plot of weight and age\nscatter_plot &lt;- ggplot(data = linelist)+\n  geom_point(mapping = aes(y = wt_kg, x = age)) +\n  labs(title = \"Scatter plot of weight and age\")\n\nTo add marginal histograms use type = \"histogram\". You can optionally set groupFill = TRUE to get stacked histograms.\n\n# with histograms\nggMarginal(\n  scatter_plot,                     # add marginal histograms\n  type = \"histogram\",               # specify histograms\n  fill = \"lightblue\",               # bar fill\n  xparams = list(binwidth = 10),    # other parameters for x-axis marginal\n  yparams = list(binwidth = 5))     # other parameters for y-axis marginal\n\n\n\n\n\n\n\n\nMarginal density plot with grouped/colored values:\n\n# Scatter plot, colored by outcome\n# Outcome column is assigned as color in ggplot. groupFill in ggMarginal set to TRUE\nscatter_plot_color &lt;- ggplot(data = linelist %&gt;% drop_na(gender))+\n  geom_point(mapping = aes(y = wt_kg, x = age, color = gender)) +\n  labs(title = \"Scatter plot of weight and age\")+\n  theme(legend.position = \"bottom\")\n\nggMarginal(scatter_plot_color, type = \"density\", groupFill = TRUE)\n\n\n\n\n\n\n\n\nSet the size = arguemnt to adjust the relative size of the marginal plot. Smaller number makes a larger marginal plot. You also set color =. Below are is a marginal boxplot, with demonstration of the margins = argument so it appears on only one axis:\n\n# with boxplot \nggMarginal(\n  scatter_plot,\n  margins = \"x\",      # only show x-axis marginal plot\n  type = \"boxplot\")",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#smart-labeling",
    "href": "new_pages/ggplot_tips.html#smart-labeling",
    "title": "31  ggplot tips",
    "section": "31.6 Smart Labeling",
    "text": "31.6 Smart Labeling\nIn ggplot2, it is also possible to add text to plots. However, this comes with the notable limitation where text labels often clash with data points in a plot, making them look messy or hard to read. There is no ideal way to deal with this in the base package, but there is a ggplot2 add-on, known as ggrepel that makes dealing with this very simple!\nThe ggrepel package provides two new functions, geom_label_repel() and geom_text_repel(), which replace geom_label() and geom_text(). Simply use these functions instead of the base functions to produce neat labels. Within the function, map the aesthetics aes() as always, but include the argument label = to which you provide a column name containing the values you want to display (e.g. patient id, or name, etc.). You can make more complex labels by combining columns and newlines (\\n) within str_glue() as shown below.\nA few tips:\n\nUse min.segment.length = 0 to always draw line segments, or min.segment.length = Inf to never draw them\n\nUse size = outside of aes() to set text size\n\nUse force = to change the degree of repulsion between labels and their respective points (default is 1)\n\nInclude fill = within aes() to have label colored by value\n\nA letter “a” may appear in the legend - add guides(fill = guide_legend(override.aes = aes(color = NA)))+ to remove it\n\n\nSee this is very in-depth tutorial for more.\n\npacman::p_load(ggrepel)\n\nlinelist %&gt;%                                               # start with linelist\n  group_by(hospital) %&gt;%                                   # group by hospital\n  summarise(                                               # create new dataset with summary values per hospital\n    n_cases = n(),                                           # number of cases per hospital\n    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # mean delay per hospital\n  ) %&gt;% \n  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # send data frame to ggplot\n  geom_point(size = 2)+                                    # add points\n  geom_label_repel(                                        # add point labels\n    mapping = aes(\n      label = stringr::str_glue(\n        \"{hospital}\\n{n_cases} cases, {delay_mean} days\")  # how label displays\n      ), \n    size = 3,                                              # text size in labels\n    min.segment.length = 0)+                               # show all line segments                \n  labs(                                                    # add axes labels\n    title = \"Mean delay to admission, by hospital\",\n    x = \"Number of cases\",\n    y = \"Mean delay (days)\")\n\n\n\n\n\n\n\n\nYou can label only a subset of the data points - by using standard ggplot() syntax to provide different data = for each geom layer of the plot. Below, All cases are plotted, but only a few are labeled.\n\nggplot()+\n  # All points in grey\n  geom_point(\n    data = linelist,                                   # all data provided to this layer\n    mapping = aes(x = ht_cm, y = wt_kg),\n    color = \"grey\",\n    alpha = 0.5)+                                              # grey and semi-transparent\n  \n  # Few points in black\n  geom_point(\n    data = linelist %&gt;% filter(days_onset_hosp &gt; 15),  # filtered data provided to this layer\n    mapping = aes(x = ht_cm, y = wt_kg),\n    alpha = 1)+                                                # default black and not transparent\n  \n  # point labels for few points\n  geom_label_repel(\n    data = linelist %&gt;% filter(days_onset_hosp &gt; 15),  # filter the data for the labels\n    mapping = aes(\n      x = ht_cm,\n      y = wt_kg,\n      fill = outcome,                                          # label color by outcome\n      label = stringr::str_glue(\"Delay: {days_onset_hosp}d\")), # label created with str_glue()\n    min.segment.length = 0) +                                  # show line segments for all\n  \n  # remove letter \"a\" from inside legend boxes\n  guides(fill = guide_legend(override.aes = aes(color = NA)))+\n  \n  # axis labels\n  labs(\n    title = \"Cases with long delay to admission\",\n    y = \"weight (kg)\",\n    x = \"height(cm)\")",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#time-axes",
    "href": "new_pages/ggplot_tips.html#time-axes",
    "title": "31  ggplot tips",
    "section": "31.7 Time axes",
    "text": "31.7 Time axes\nWorking with time axes in ggplot can seem daunting, but is made very easy with a few key functions. Remember that when working with time or date that you should ensure that the correct variables are formatted as date or datetime class - see the Working with dates page for more information on this, or [Epidemic curves] page (ggplot section) for examples.\nThe single most useful set of functions for working with dates in ggplot2 are the scale functions (scale_x_date(), scale_x_datetime(), and their cognate y-axis functions). These functions let you define how often you have axis labels, and how to format axis labels. To find out how to format dates, see the working with dates section again! You can use the date_breaks and date_labels arguments to specify how dates should look:\n\ndate_breaks allows you to specify how often axis breaks occur - you can pass a string here (e.g. \"3 months\", or “2 days\")\ndate_labels allows you to define the format dates are shown in. You can pass a date format string to these arguments (e.g. \"%b-%d-%Y\"):\n\n\n# make epi curve by date of onset when available\nggplot(linelist, aes(x = date_onset)) +\n  geom_histogram(binwidth = 7) +\n  scale_x_date(\n    # 1 break every 1 month\n    date_breaks = \"1 months\",\n    # labels should show month then date\n    date_labels = \"%b %d\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\nOne easy solution to efficient date labels on the x-axis is to to assign the labels = argument in scale_x_date() to the function label_date_short() from the package scales. This function will automatically construct efficient date labels (read more here). An additional benefit of this function is that the labels will automatically adjust as your data expands over time, from days, to weeks, to months and years.\nSee a complete example in the Epicurves page section on multi-level date labels, but a quick example is shown below for reference:\n\nggplot(linelist, aes(x = date_onset)) +\n  geom_histogram(binwidth = 7) +\n  scale_x_date(\n    labels = scales::label_date_short()  # automatically efficient date labels\n  )+\n  theme_classic()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#highlighting",
    "href": "new_pages/ggplot_tips.html#highlighting",
    "title": "31  ggplot tips",
    "section": "31.8 Highlighting",
    "text": "31.8 Highlighting\nHighlighting specific elements in a chart is a useful way to draw attention to a specific instance of a variable while also providing information on the dispersion of the full dataset. While this is not easily done in base ggplot2, there is an external package that can help to do this known as gghighlight. This is easy to use within the ggplot syntax.\nThe gghighlight package uses the gghighlight() function to achieve this effect. To use this function, supply a logical statement to the function - this can have quite flexible outcomes, but here we’ll show an example of the age distribution of cases in our linelist, highlighting them by outcome.\n\n# load gghighlight\nlibrary(gghighlight)\n\n# replace NA values with unknown in the outcome variable\nlinelist &lt;- linelist %&gt;%\n  mutate(outcome = replace_na(outcome, \"Unknown\"))\n\n# produce a histogram of all cases by age\nggplot(\n  data = linelist,\n  mapping = aes(x = age_years, fill = outcome)) +\n  geom_histogram() + \n  gghighlight::gghighlight(outcome == \"Death\")     # highlight instances where the patient has died.\n\n\n\n\n\n\n\n\nThis also works well with faceting functions - it allows the user to produce facet plots with the background data highlighted that doesn’t apply to the facet! Below we count cases by week and plot the epidemic curves by hospital (color = and facet_wrap() set to hospital column).\n\n# produce a histogram of all cases by age\nlinelist %&gt;% \n  count(week = lubridate::floor_date(date_hospitalisation, \"week\"),\n        hospital) %&gt;% \n  ggplot()+\n  geom_line(aes(x = week, y = n, color = hospital))+\n  theme_minimal()+\n  gghighlight::gghighlight() +                      # highlight instances where the patient has died\n  facet_wrap(~hospital)                              # make facets by outcome",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#plotting-multiple-datasets",
    "href": "new_pages/ggplot_tips.html#plotting-multiple-datasets",
    "title": "31  ggplot tips",
    "section": "31.9 Plotting multiple datasets",
    "text": "31.9 Plotting multiple datasets\nNote that properly aligning axes to plot from multiple datasets in the same plot can be difficult. Consider one of the following strategies:\n\nMerge the data prior to plotting, and convert to “long” format with a column reflecting the dataset\n\nUse cowplot or a similar package to combine two plots (see below)",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#combine-plots",
    "href": "new_pages/ggplot_tips.html#combine-plots",
    "title": "31  ggplot tips",
    "section": "31.10 Combine plots",
    "text": "31.10 Combine plots\nTwo packages that are very useful for combining plots are cowplot and patchwork. In this page we will mostly focus on cowplot, with occassional use of patchwork.\nHere is the online introduction to cowplot. You can read the more extensive documentation for each function online here. We will cover a few of the most common use cases and functions below.\nThe cowplot package works in tandem with ggplot2 - essentially, you use it to arrange and combine ggplots and their legends into compound figures. It can also accept base R graphics.\n\npacman::p_load(\n  tidyverse,      # data manipulation and visualisation\n  cowplot,        # combine plots\n  patchwork       # combine plots\n)\n\nWhile faceting (described in the ggplot basics page) is a convenient approach to plotting, sometimes its not possible to get the results you want from its relatively restrictive approach. Here, you may choose to combine plots by sticking them together into a larger plot. There are three well known packages that are great for this - cowplot, gridExtra, and patchwork. However, these packages largely do the same things, so we’ll focus on cowplot for this section.\n\nplot_grid()\nThe cowplot package has a fairly wide range of functions, but the easiest use of it can be achieved through the use of plot_grid(). This is effectively a way to arrange predefined plots in a grid formation. We can work through another example with the malaria dataset - here we can plot the total cases by district, and also show the epidemic curve over time.\n\nmalaria_data &lt;- rio::import(here::here(\"data\", \"malaria_facility_count_data.rds\")) \n\n# bar chart of total cases by district\np1 &lt;- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    x = \"District\",\n    y = \"Total number of cases\",\n    title = \"Total malaria cases by district\"\n  ) +\n  theme_minimal()\n\n# epidemic curve over time\np2 &lt;- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1) +\n  labs(\n    x = \"Date of data submission\",\n    y =  \"number of cases\"\n  ) +\n  theme_minimal()\n\ncowplot::plot_grid(p1, p2,\n                  # 1 column and two rows - stacked on top of each other\n                   ncol = 1,\n                   nrow = 2,\n                   # top plot is 2/3 as tall as second\n                   rel_heights = c(2, 3))\n\n\n\n\n\n\n\n\n\n\nCombine legends\nIf your plots have the same legend, combining them is relatively straight-forward. Simple use the cowplot approach above to combine the plots, but remove the legend from one of them (de-duplicate).\nIf your plots have different legends, you must use an alternative approach:\n\nCreate and save your plots without legends using theme(legend.position = \"none\")\n\nExtract the legends from each plot using get_legend() as shown below - but extract legends from the plots modified to actually show the legend\n\nCombine the legends into a legends panel\n\nCombine the plots and legends panel\n\nFor demonstration we show the two plots separately, and then arranged in a grid with their own legends showing (ugly and inefficient use of space):\n\np1 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n  scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Cases by outcome\")\n\n\np2 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, age_cat) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(axis.text.y = element_blank())+\n  labs(title = \"Cases by age\")\n\nHere is how the two plots look when combined using plot_grid() without combining their legends:\n\ncowplot::plot_grid(p1, p2, rel_widths = c(0.3))\n\n\n\n\n\n\n\n\nAnd now we show how to combine the legends. Essentially what we do is to define each plot without its legend (theme(legend.position = \"none\"), and then we define each plot’s legend separately, using the get_legend() function from cowplot. When we extract the legend from the saved plot, we need to add + the legend back in, including specifying the placement (“right”) and smaller adjustments for alignment of the legends and their titles. Then, we combine the legends together vertically, and then combine the two plots with the newly-combined legends. Voila!\n\n# Define plot 1 without legend\np1 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n  scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  labs(title = \"Cases by outcome\")\n\n\n# Define plot 2 without legend\np2 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, age_cat) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(\n    legend.position = \"none\",\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank()\n  )+\n  labs(title = \"Cases by age\")\n\n\n# extract legend from p1 (from p1 + legend)\nleg_p1 &lt;- cowplot::get_legend(p1 +\n                                theme(legend.position = \"right\",        # extract vertical legend\n                                      legend.justification = c(0,0.5))+ # so legends  align\n                                labs(fill = \"Outcome\"))                 # title of legend\n# extract legend from p2 (from p2 + legend)\nleg_p2 &lt;- cowplot::get_legend(p2 + \n                                theme(legend.position = \"right\",         # extract vertical legend   \n                                      legend.justification = c(0,0.5))+  # so legends align\n                                labs(fill = \"Age Category\"))             # title of legend\n\n# create a blank plot for legend alignment\n#blank_p &lt;- patchwork::plot_spacer() + theme_void()\n\n# create legends panel, can be one on top of the other (or use spacer commented above)\nlegends &lt;- cowplot::plot_grid(leg_p1, leg_p2, nrow = 2, rel_heights = c(.3, .7))\n\n# combine two plots and the combined legends panel\ncombined &lt;- cowplot::plot_grid(p1, p2, legends, ncol = 3, rel_widths = c(.4, .4, .2))\n\ncombined  # print\n\n\n\n\n\n\n\n\nThis solution was learned from this post with a minor fix to align legends from this post.\nTIP: Fun note - the “cow” in cowplot comes from the creator’s name - Claus O. Wilke.\n\n\nInset plots\nYou can inset one plot in another using cowplot. Here are things to be aware of:\n\nDefine the main plot with theme_half_open() from cowplot; it may be best to have the legend either on top or bottom\n\nDefine the inset plot. Best is to have a plot where you do not need a legend. You can remove plot theme elements with element_blank() as shown below.\n\nCombine them by applying ggdraw() to the main plot, then adding draw_plot() on the inset plot and specifying the coordinates (x and y of lower left corner), height and width as proportion of the whole main plot.\n\n\n# Define main plot\nmain_plot &lt;- ggplot(data = linelist)+\n  geom_histogram(aes(x = date_onset, fill = hospital))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+ \n  theme_half_open()+\n  theme(legend.position = \"bottom\")+\n  labs(title = \"Epidemic curve and outcomes by hospital\")\n\n\n# Define inset plot\ninset_plot &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n    geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n    scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n    coord_flip()+\n    theme_minimal()+\n    theme(legend.position = \"none\",\n          axis.title.y = element_blank())+\n    labs(title = \"Cases by outcome\") \n\n\n# Combine main with inset\ncowplot::ggdraw(main_plot)+\n     draw_plot(inset_plot,\n               x = .6, y = .55,    #x = .07, y = .65,\n               width = .4, height = .4)\n\n\n\n\n\n\n\n\nThis technique is explained more in these two vignettes:\nWilke lab\ndraw_plot() documentation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#dual-axes",
    "href": "new_pages/ggplot_tips.html#dual-axes",
    "title": "31  ggplot tips",
    "section": "31.11 Dual axes",
    "text": "31.11 Dual axes\nA secondary y-axis is often a requested addition to a ggplot2 graph. While there is a robust debate about the validity of such graphs in the data visualization community, and they are often not recommended, your manager may still want them. Below, we present one method to achieve them: using the cowplot package to combine two separate plots.\nThis approach involves creating two separate plots - one with a y-axis on the left, and the other with y-axis on the right. Both will use a specific theme_cowplot() and must have the same x-axis. Then in a third command the two plots are aligned and overlaid on top of each other. The functionalities of cowplot, of which this is only one, are described in depth at this site.\nTo demonstrate this technique we will overlay the epidemic curve with a line of the weekly percent of patients who died. We use this example because the alignment of dates on the x-axis is more complex than say, aligning a bar chart with another plot. Some things to note:\n\nThe epicurve and the line are aggregated into weeks prior to plotting and the date_breaks and date_labels are identical - we do this so that the x-axes of the two plots are the same when they are overlaid.\n\nThe y-axis is moved to the right-side for plot 2 with the position = argument of scale_y_continuous().\n\nBoth plots make use of theme_cowplot()\n\nNote there is another example of this technique in the Epidemic curves page - overlaying cumulative incidence on top of the epicurve.\nMake plot 1\nThis is essentially the epicurve. We use geom_area() just to demonstrate its use (area under a line, by default)\n\npacman::p_load(cowplot)            # load/install cowplot\n\np1 &lt;- linelist %&gt;%                 # save plot as object\n     count(\n       epiweek = lubridate::floor_date(date_onset, \"week\")) %&gt;% \n     ggplot()+\n          geom_area(aes(x = epiweek, y = n), fill = \"grey\")+\n          scale_x_date(\n               date_breaks = \"month\",\n               date_labels = \"%b\")+\n     theme_cowplot()+\n     labs(\n       y = \"Weekly cases\"\n     )\n\np1                                      # view plot \n\n\n\n\n\n\n\n\nMake plot 2\nCreate the second plot showing a line of the weekly percent of cases who died.\n\np2 &lt;- linelist %&gt;%         # save plot as object\n     group_by(\n       epiweek = lubridate::floor_date(date_onset, \"week\")) %&gt;% \n     summarise(\n       n = n(),\n       pct_death = 100*sum(outcome == \"Death\", na.rm=T) / n) %&gt;% \n     ggplot(aes(x = epiweek, y = pct_death))+\n          geom_line()+\n          scale_x_date(\n               date_breaks = \"month\",\n               date_labels = \"%b\")+\n          scale_y_continuous(\n               position = \"right\")+\n          theme_cowplot()+\n          labs(\n            x = \"Epiweek of symptom onset\",\n            y = \"Weekly percent of deaths\",\n            title = \"Weekly case incidence and percent deaths\"\n          )\n\np2     # view plot\n\n\n\n\n\n\n\n\nNow we align the plot using the function align_plots(), specifying horizontal and vertical alignment (“hv”, could also be “h”, “v”, “none”). We specify alignment of all axes as well (top, bottom, left, and right) with “tblr”. The output is of class list (2 elements).\nThen we draw the two plots together using ggdraw() (from cowplot) and referencing the two parts of the aligned_plots object.\n\naligned_plots &lt;- cowplot::align_plots(p1, p2, align=\"hv\", axis=\"tblr\")         # align the two plots and save them as list\naligned_plotted &lt;- ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])  # overlay them and save the visual plot\naligned_plotted                                                                # print the overlayed plots",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#packages-to-help-you",
    "href": "new_pages/ggplot_tips.html#packages-to-help-you",
    "title": "31  ggplot tips",
    "section": "31.12 Packages to help you",
    "text": "31.12 Packages to help you\nThere are some really neat R packages specifically designed to help you navigate ggplot2:\n\nPoint-and-click ggplot2 with equisse\n“This addin allows you to interactively explore your data by visualizing it with the ggplot2 package. It allows you to draw bar plots, curves, scatter plots, histograms, boxplot and sf objects, then export the graph or retrieve the code to reproduce the graph.”\nInstall and then launch the addin via the RStudio menu or with esquisse::esquisser().\nSee the Github page\nDocumentation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#miscellaneous",
    "href": "new_pages/ggplot_tips.html#miscellaneous",
    "title": "31  ggplot tips",
    "section": "31.13 Miscellaneous",
    "text": "31.13 Miscellaneous\n\nNumeric display\nYou can disable scientific notation by running this command prior to plotting.\n\noptions(scipen=999)\n\nOr apply number_format() from the scales package to a specific value or column, as shown below.\nUse functions from the package scales to easily adjust how numbers are displayed. These can be applied to columns in your data frame, but are shown on individual numbers for purpose of example.\n\nscales::number(6.2e5)\n\n[1] \"620 000\"\n\nscales::number(1506800.62,  accuracy = 0.1,)\n\n[1] \"1 506 800.6\"\n\nscales::comma(1506800.62, accuracy = 0.01)\n\n[1] \"1,506,800.62\"\n\nscales::comma(1506800.62, accuracy = 0.01,  big.mark = \".\" , decimal.mark = \",\")\n\n[1] \"1.506.800,62\"\n\nscales::percent(0.1)\n\n[1] \"10%\"\n\nscales::dollar(56)\n\n[1] \"$56\"\n\nscales::scientific(100000)\n\n[1] \"1e+05\"",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/ggplot_tips.html#resources",
    "href": "new_pages/ggplot_tips.html#resources",
    "title": "31  ggplot tips",
    "section": "31.14 Resources",
    "text": "31.14 Resources\nInspiration ggplot graph gallery\nPresentation of data European Centre for Disease Prevention and Control Guidelines of presentation of surveillance data\nFacets and labellers Using labellers for facet strips Labellers\nAdjusting order with factors fct_reorder\nfct_inorder\nHow to reorder a boxplot\nReorder a variable in ggplot2\nR for Data Science - Factors\nLegends\nAdjust legend order\nCaptions Caption alignment\nLabels\nggrepel\nCheatsheets\nBeautiful plotting with ggplot2",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>ggplot tips</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html",
    "href": "new_pages/epicurves.html",
    "title": "32  Epidemic curves",
    "section": "",
    "text": "32.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html#preparation",
    "href": "new_pages/epicurves.html#preparation",
    "title": "32  Epidemic curves",
    "section": "",
    "text": "Packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,          # file import/export\n  here,         # relative filepaths \n  lubridate,    # working with dates/epiweeks\n  aweek,        # alternative package for working with dates/epiweeks\n  incidence2,   # epicurves of linelist data\n  i2extras,     # supplement to incidence2\n  stringr,      # search and manipulate character strings\n  forcats,      # working with factors\n  RColorBrewer, # Color palettes from colorbrewer2.org\n  tidyverse     # data management + ggplot2 graphics\n) \n\n\n\nImport data\nTwo example datasets are used in this section:\n\nLinelist of individual cases from a simulated epidemic\n\nAggregated counts by hospital from the same simulated epidemic\n\nThe datasets are imported using the import() function from the rio package. See the page on Import and export for various ways to import data.\nCase linelist\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instruction in the Download handbook and data page. We assume the file is in the working directory so no sub-folders are specified in this file path.\n\nlinelist &lt;- import(\"linelist_cleaned.xlsx\")\n\nThe first 50 rows are displayed below.\n\n\n\n\n\n\nCase counts aggregated by hospital\nFor the purposes of the handbook, the dataset of weekly aggregated counts by hospital is created from the linelist with the following code.\n\n# import the counts data into R\ncount_data &lt;- linelist %&gt;% \n  group_by(hospital, date_hospitalisation) %&gt;% \n  summarize(n_cases = dplyr::n()) %&gt;% \n  filter(date_hospitalisation &gt; as.Date(\"2013-06-01\")) %&gt;% \n  ungroup()\n\nThe first 50 rows are displayed below:\n\n\n\n\n\n\n\n\nSet parameters\nFor production of a report, you may want to set editable parameters such as the date for which the data is current (the “data date”). You can then reference the object data_date in your code when applying filters or in dynamic captions.\n\n## set the report date for the report\n## note: can be set to Sys.Date() for the current date\ndata_date &lt;- as.Date(\"2015-05-15\")\n\n\n\nVerify dates\nVerify that each relevant date column is class Date and has an appropriate range of values. You can do this simply using hist() for histograms, or range() with na.rm=TRUE, or with ggplot() as below.\n\n# check range of onset dates\nggplot(data = linelist)+\n  geom_histogram(aes(x = date_onset))",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html#epicurves-with-ggplot2",
    "href": "new_pages/epicurves.html#epicurves-with-ggplot2",
    "title": "32  Epidemic curves",
    "section": "32.2 Epicurves with ggplot2",
    "text": "32.2 Epicurves with ggplot2\nUsing ggplot() to build your epicurve allows for flexibility and customization, but requires more effort and understanding of how ggplot() works.\nYou must manually control the aggregation of the cases by time (into weeks, months, etc) and the intervals of the labels on the date axis. This must be carefully managed.\nThese examples use a subset of the linelist dataset - only the cases from Central Hospital.\n\ncentral_data &lt;- linelist %&gt;% \n  filter(hospital == \"Central Hospital\")\n\nTo produce an epicurve with ggplot() there are three main elements:\n\nA histogram, with linelist cases aggregated into “bins” distinguished by specific “break” points\n\nScales for the axes and their labels\n\nThemes for the plot appearance, including titles, labels, captions, etc.\n\n\nSpecify case bins\nHere we show how to specify how cases will be aggregated into histogram bins (“bars”). It is important to recognize that the aggregation of cases into histogram bins is not necessarily the same intervals as the dates that will appear on the x-axis.\nBelow is perhaps the most simple code to produce daily and weekly epicurves.\nIn the over-arching ggplot() command the dataset is provided to data =. Onto this foundation, the geometry of a histogram is added with a +. Within the geom_histogram(), we map the aesthetics such that the column date_onset is mapped to the x-axis. Also within the geom_histogram() but not within aes() we set the binwidth = of the histogram bins, in days. If this ggplot2 syntax is confusing, review the page on ggplot basics.\nCAUTION: Plotting weekly cases by using binwidth = 7 starts the first 7-day bin at the first case, which could be any day of the week! To create specific weeks, see section below .\n\n# daily \nggplot(data = central_data) +          # set data\n  geom_histogram(                      # add histogram\n    mapping = aes(x = date_onset),     # map date column to x-axis\n    binwidth = 1)+                     # cases binned by 1 day \n  labs(title = \"Central Hospital - Daily\")                # title\n\n# weekly\nggplot(data = central_data) +          # set data \n  geom_histogram(                      # add histogram\n      mapping = aes(x = date_onset),   # map date column to x-axis\n      binwidth = 7)+                   # cases binned every 7 days, starting from first case (!) \n  labs(title = \"Central Hospital - 7-day bins, starting at first case\") # title\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us note that the first case in this Central Hospital dataset had symptom onset on:\n\nformat(min(central_data$date_onset, na.rm=T), \"%A %d %b, %Y\")\n\n[1] \"Thursday 01 May, 2014\"\n\n\nTo manually specify the histogram bin breaks, do not use the binwidth = argument, and instead supply a vector of dates to breaks =.\nCreate the vector of dates with the base R function seq.Date(). This function expects arguments to =, from =, and by =. For example, the command below returns monthly dates starting at Jan 15 and ending by June 28.\n\nmonthly_breaks &lt;- seq.Date(from = as.Date(\"2014-02-01\"),\n                           to = as.Date(\"2015-07-15\"),\n                           by = \"months\")\n\nmonthly_breaks   # print\n\n [1] \"2014-02-01\" \"2014-03-01\" \"2014-04-01\" \"2014-05-01\" \"2014-06-01\"\n [6] \"2014-07-01\" \"2014-08-01\" \"2014-09-01\" \"2014-10-01\" \"2014-11-01\"\n[11] \"2014-12-01\" \"2015-01-01\" \"2015-02-01\" \"2015-03-01\" \"2015-04-01\"\n[16] \"2015-05-01\" \"2015-06-01\" \"2015-07-01\"\n\n\nThis vector can be provided to geom_histogram() as breaks =:\n\n# monthly \nggplot(data = central_data) +  \n  geom_histogram(\n    mapping = aes(x = date_onset),\n    breaks = monthly_breaks)+         # provide the pre-defined vector of breaks                    \n  labs(title = \"Monthly case bins\")   # title\n\n\n\n\n\n\n\n\nA simple weekly date sequence can be returned by setting by = \"week\". For example:\n\nweekly_breaks &lt;- seq.Date(from = as.Date(\"2014-02-01\"),\n                          to = as.Date(\"2015-07-15\"),\n                          by = \"week\")\n\nAn alternative to supplying specific start and end dates is to write dynamic code so that weekly bins begin the Monday before the first case. We will use these date vectors throughout the examples below.\n\n# Sequence of weekly Monday dates for CENTRAL HOSPITAL\nweekly_breaks_central &lt;- seq.Date(\n  from = floor_date(min(central_data$date_onset, na.rm=T),   \"week\", week_start = 1), # monday before first case\n  to   = ceiling_date(max(central_data$date_onset, na.rm=T), \"week\", week_start = 1), # monday after last case\n  by   = \"week\")\n\nLet’s unpack the rather daunting code above:\n\nThe “from” value (earliest date of the sequence) is created as follows: the minimum date value (min() with na.rm=TRUE) in the column date_onset is fed to floor_date() from the lubridate package. floor_date() set to “week” returns the start date of that cases’s “week”, given that the start day of each week is a Monday (week_start = 1).\n\nLikewise, the “to” value (end date of the sequence) is created using the inverse function ceiling_date() to return the Monday after the last case.\n\nThe “by” argument of seq.Date() can be set to any number of days, weeks, or months.\n\nUse week_start = 7 for Sunday weeks\n\nAs we will use these date vectors throughout this page, we also define one for the whole outbreak (the above is for Central Hospital only).\n\n# Sequence for the entire outbreak\nweekly_breaks_all &lt;- seq.Date(\n  from = floor_date(min(linelist$date_onset, na.rm=T),   \"week\", week_start = 1), # monday before first case\n  to   = ceiling_date(max(linelist$date_onset, na.rm=T), \"week\", week_start = 1), # monday after last case\n  by   = \"week\")\n\nThese seq.Date() outputs can be used to create histogram bin breaks, but also the breaks for the date labels, which may be independent from the bins. Read more about the date labels in later sections.\nTIP: For a more simple ggplot() command, save the bin breaks and date label breaks as named vectors in advance, and simply provide their names to breaks =.\n\n\nWeekly epicurve example\nBelow is detailed example code to produce weekly epicurves for Monday weeks, with aligned bars, date labels, and vertical gridlines. This section is for the user who needs code quickly. To understand each aspect (themes, date labels, etc.) in-depth, continue to the subsequent sections. Of note:\n\nThe histogram bin breaks are defined with seq.Date() as explained above to begin the Monday before the earliest case and to end the Monday after the last case\n\nThe interval of date labels is specified by date_breaks = within scale_x_date()\n\nThe interval of minor vertical gridlines between date labels is specified to date_minor_breaks =\n\nWe use closed = \"left\" in the geom_histogram() to ensure the date are counted in the correct bins\n\nexpand = c(0,0) in the x and y scales removes excess space on each side of the axes, which also ensures the date labels begin from the first bar.\n\n\n# TOTAL MONDAY WEEK ALIGNMENT\n#############################\n# Define sequence of weekly breaks\nweekly_breaks_central &lt;- seq.Date(\n      from = floor_date(min(central_data$date_onset, na.rm=T),   \"week\", week_start = 1), # Monday before first case\n      to   = ceiling_date(max(central_data$date_onset, na.rm=T), \"week\", week_start = 1), # Monday after last case\n      by   = \"week\")    # bins are 7-days \n\n\nggplot(data = central_data) + \n  \n  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case\n  geom_histogram(\n    \n    # mapping aesthetics\n    mapping = aes(x = date_onset),  # date column mapped to x-axis\n    \n    # histogram bin breaks\n    breaks = weekly_breaks_central, # histogram bin breaks defined previously\n      \n    closed = \"left\",  # count cases from start of breakpoint\n    \n    # bars\n    color = \"darkblue\",     # color of lines around bars\n    fill = \"lightblue\"      # color of fill within bars\n  )+ \n    \n  # x-axis labels\n  scale_x_date(\n    expand            = c(0,0),           # remove excess x-axis space before and after case bars\n    date_breaks       = \"4 weeks\",        # date labels and major vertical gridlines appear every 3 Monday weeks\n    date_minor_breaks = \"week\",           # minor vertical lines appear every Monday week\n    date_labels       = \"%a\\n%d %b\\n%Y\")+ # date labels format\n  \n  # y-axis\n  scale_y_continuous(\n    expand = c(0,0))+             # remove excess y-axis space below 0 (align histogram flush with x-axis)\n  \n  # aesthetic themes\n  theme_minimal()+                # simplify plot background\n  \n  theme(\n    plot.caption = element_text(hjust = 0,        # caption on left side\n                                face = \"italic\"), # caption in italics\n    axis.title = element_text(face = \"bold\"))+    # axis titles in bold\n  \n  # labels including dynamic caption\n  labs(\n    title    = \"Weekly incidence of cases (Monday weeks)\",\n    subtitle = \"Note alignment of bars, vertical gridlines, and axis labels on Monday weeks\",\n    x        = \"Week of symptom onset\",\n    y        = \"Weekly incident cases reported\",\n    caption  = stringr::str_glue(\"n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\\n{nrow(central_data %&gt;% filter(is.na(date_onset)))} cases missing date of onset and not shown\"))\n\n\n\n\n\n\n\n\n\nSunday weeks\nTo achieve the above plot for Sunday weeks a few modifications are needed, because the date_breaks = \"weeks\" only work for Monday weeks.\n\nThe break points of the histogram bins must be set to Sundays (week_start = 7)\n\nWithin scale_x_date(), the similar date breaks should be provided to breaks = and minor_breaks = to ensure the date labels and vertical gridlines align on Sundays.\n\nFor example, the scale_x_date() command for Sunday weeks could look like this:\n\nscale_x_date(\n    expand = c(0,0),\n    \n    # specify interval of date labels and major vertical gridlines\n    breaks = seq.Date(\n      from = floor_date(min(central_data$date_onset, na.rm=T),   \"week\", week_start = 7), # Sunday before first case\n      to   = ceiling_date(max(central_data$date_onset, na.rm=T), \"week\", week_start = 7), # Sunday after last case\n      by   = \"4 weeks\"),\n    \n    # specify interval of minor vertical gridline \n    minor_breaks = seq.Date(\n      from = floor_date(min(central_data$date_onset, na.rm=T),   \"week\", week_start = 7), # Sunday before first case\n      to   = ceiling_date(max(central_data$date_onset, na.rm=T), \"week\", week_start = 7), # Sunday after last case\n      by   = \"week\"),\n   \n    # date label format\n    #date_labels = \"%a\\n%d %b\\n%Y\")+         # day, above month abbrev., above 2-digit year\n    label = scales::label_date_short())+ # automatic label formatting\n\n\n\n\nGroup/color by value\nThe histogram bars can be colored by group and “stacked”. To designate the grouping column, make the following changes. See the ggplot basics page for details.\n\nWithin the histogram aesthetic mapping aes(), map the column name to the group = and fill = arguments\n\nRemove any fill = argument outside of aes(), as it will override the one inside\n\nArguments inside aes() will apply by group, whereas any outside will apply to all bars (e.g. you may still want color = outside, so each bar has the same border)\n\nHere is what the aes() command would look like to group and color the bars by gender:\n\naes(x = date_onset, group = gender, fill = gender)\n\nHere it is applied:\n\nggplot(data = linelist) +     # begin with linelist (many hospitals)\n  \n  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case\n  geom_histogram(\n    mapping = aes(\n      x = date_onset,\n      group = hospital,       # set data to be grouped by hospital\n      fill = hospital),       # bar fill (inside color) by hospital\n    \n    # bin breaks are Monday weeks\n    breaks = weekly_breaks_all,   # sequence of weekly Monday bin breaks for whole outbreak, defined in previous code       \n    \n    closed = \"left\",          # count cases from start of breakpoint\n\n    # Color around bars\n    color = \"black\")\n\n\n\n\n\n\n\n\n\n\nAdjust colors\n\nTo manually set the fill for each group, use scale_fill_manual() (note: scale_color_manual() is different!).\n\nUse the values = argument to apply a vector of colors.\n\nUse na.value = to specify a color for NA values.\n\nUse the labels = argument to change the text of legend items. To be safe, provide as a named vector like c(\"old\" = \"new\", \"old\" = \"new\") or adjust the values in the data itself.\n\nUse name = to give a proper title to the legend\n\n\nFor more tips on color scales and palettes, see the page on ggplot basics.\n\n\nggplot(data = linelist)+           # begin with linelist (many hospitals)\n  \n  # make histogram\n  geom_histogram(\n    mapping = aes(x = date_onset,\n        group = hospital,          # cases grouped by hospital\n        fill = hospital),          # bar fill by hospital\n    \n    # bin breaks\n    breaks = weekly_breaks_all,    # sequence of weekly Monday bin breaks, defined in previous code\n    \n    closed = \"left\",               # count cases from start of breakpoint\n\n    # Color around bars\n    color = \"black\")+              # border color of each bar\n  \n  # manual specification of colors\n  scale_fill_manual(\n    values = c(\"black\", \"orange\", \"grey\", \"beige\", \"blue\", \"brown\"),\n    labels = c(\"St. Mark's Maternity Hospital (SMMH)\" = \"St. Mark's\"),\n    name = \"Hospital\") # specify fill colors (\"values\") - attention to order!\n\n\n\n\n\n\n\n\n\n\nAdjust level order\nThe order in which grouped bars are stacked is best adjusted by classifying the grouping column as class Factor. You can then designate the factor level order (and their display labels). See the page on Factors or ggplot tips for details.\nBefore making the plot, use the fct_relevel() function from forcats package to convert the grouping column to class factor and manually adjust the level order, as detailed in the page on Factors.\n\n# load forcats package for working with factors\npacman::p_load(forcats)\n\n# Define new dataset with hospital as factor\nplot_data &lt;- linelist %&gt;% \n  mutate(hospital = fct_relevel(hospital, c(\"Missing\", \"Other\"))) # Convert to factor and set \"Missing\" and \"Other\" as top levels to appear on epicurve top\n\nlevels(plot_data$hospital) # print levels in order\n\n[1] \"Missing\"                             \n[2] \"Other\"                               \n[3] \"Central Hospital\"                    \n[4] \"Military Hospital\"                   \n[5] \"Port Hospital\"                       \n[6] \"St. Mark's Maternity Hospital (SMMH)\"\n\n\nIn the below plot, the only differences from previous is that column hospital has been consolidated as above, and we use guides() to reverse the legend order, so that “Missing” is on the bottom of the legend.\n\nggplot(plot_data) +                     # Use NEW dataset with hospital as re-ordered factor\n  \n  # make histogram\n  geom_histogram(\n    mapping = aes(x = date_onset,\n        group = hospital,               # cases grouped by hospital\n        fill = hospital),               # bar fill (color) by hospital\n    \n    breaks = weekly_breaks_all,         # sequence of weekly Monday bin breaks for whole outbreak, defined at top of ggplot section\n    \n    closed = \"left\",                    # count cases from start of breakpoint\n\n    color = \"black\")+                   # border color around each bar\n    \n  # x-axis labels\n  scale_x_date(\n    expand            = c(0,0),           # remove excess x-axis space before and after case bars\n    date_breaks       = \"3 weeks\",        # labels appear every 3 Monday weeks\n    date_minor_breaks = \"week\",           # vertical lines appear every Monday week\n    label = scales::label_date_short()) + # efficient label formatting\n  \n  # y-axis\n  scale_y_continuous(\n    expand = c(0,0))+                   # remove excess y-axis space below 0\n  \n  # manual specification of colors, ! attention to order\n  scale_fill_manual(\n    values = c(\"grey\", \"beige\", \"black\", \"orange\", \"blue\", \"brown\"),\n    labels = c(\"St. Mark's Maternity Hospital (SMMH)\" = \"St. Mark's\"),\n    name = \"Hospital\")+ \n  \n  # aesthetic themes\n  theme_minimal()+                      # simplify plot background\n  \n  theme(\n    plot.caption = element_text(face = \"italic\", # caption on left side in italics\n                                hjust = 0), \n    axis.title = element_text(face = \"bold\"))+   # axis titles in bold\n  \n  # labels\n  labs(\n    title    = \"Weekly incidence of cases by hospital\",\n    subtitle = \"Hospital as re-ordered factor\",\n    x        = \"Week of symptom onset\",\n    y        = \"Weekly cases\")\n\n\n\n\n\n\n\n\nTIP: To reverse the order of the legend only, add this ggplot2 command: guides(fill = guide_legend(reverse = TRUE)).\n\n\nAdjust legend\nRead more about legends and scales in the ggplot tips page. Here are a few highlights:\n\nEdit legend title either in the scale function or with labs(fill = \"Legend title\") (if your are using color = aesthetic, then use labs(color = \"\"))\n\ntheme(legend.title = element_blank()) to have no legend title\n\ntheme(legend.position = \"top\") (“bottom”, “left”, “right”, or “none” to remove the legend)\ntheme(legend.direction = \"horizontal\") horizontal legend\nguides(fill = guide_legend(reverse = TRUE)) to reverse order of the legend\n\n\n\nBars side-by-side\nSide-by-side display of group bars (as opposed to stacked) is specified within geom_histogram() with position = \"dodge\" placed outside of aes().\nIf there are more than two value groups, these can become difficult to read. Consider instead using a faceted plot (small multiples). To improve readability in this example, missing gender values are removed.\n\nggplot(central_data %&gt;% drop_na(gender))+   # begin with Central Hospital cases dropping missing gender\n    geom_histogram(\n        mapping = aes(\n          x = date_onset,\n          group = gender,         # cases grouped by gender\n          fill = gender),         # bars filled by gender\n        \n        # histogram bin breaks\n        breaks = weekly_breaks_central,   # sequence of weekly dates for Central outbreak - defined at top of ggplot section\n        \n        closed = \"left\",          # count cases from start of breakpoint\n        \n        color = \"black\",          # bar edge color\n        \n        position = \"dodge\")+      # SIDE-BY-SIDE bars\n                      \n  \n  # The labels on the x-axis\n  scale_x_date(expand            = c(0,0),          # remove excess x-axis space below and after case bars\n               date_breaks       = \"3 weeks\",       # labels appear every 3 Monday weeks\n               date_minor_breaks = \"week\",          # vertical lines appear every Monday week\n               label = scales::label_date_short())+ # efficient date labels\n  \n  # y-axis\n  scale_y_continuous(expand = c(0,0))+             # removes excess y-axis space between bottom of bars and the labels\n  \n  #scale of colors and legend labels\n  scale_fill_manual(values = c(\"brown\", \"orange\"),  # specify fill colors (\"values\") - attention to order!\n                    na.value = \"grey\" )+     \n\n  # aesthetic themes\n  theme_minimal()+                                               # a set of themes to simplify plot\n  theme(plot.caption = element_text(face = \"italic\", hjust = 0), # caption on left side in italics\n        axis.title = element_text(face = \"bold\"))+               # axis titles in bold\n  \n  # labels\n  labs(title    = \"Weekly incidence of cases, by gender\",\n       subtitle = \"Subtitle\",\n       fill     = \"Gender\",                                      # provide new title for legend\n       x        = \"Week of symptom onset\",\n       y        = \"Weekly incident cases reported\")\n\n\n\n\n\n\n\n\n\n\nAxis limits\nThere are two ways to limit the extent of axis values.\nGenerally the preferred way is to use the command coord_cartesian(), which accepts xlim = c(min, max) and ylim = c(min, max) (where you provide the min and max values). This acts as a “zoom” without actually removing any data, which is important for statistics and summary measures.\nAlternatively, you can set maximum and minimum date values using limits = c() within scale_x_date(). For example:\n\nscale_x_date(limits = c(as.Date(\"2014-04-01\"), NA)) # sets a minimum date but leaves the maximum open.  \n\nLikewise, if you want to the x-axis to extend to a specific date (e.g. current date), even if no new cases have been reported, you can use:\n\nscale_x_date(limits = c(NA, Sys.Date()) # ensures date axis will extend until current date  \n\nDANGER: Be cautious setting the y-axis scale breaks or limits (e.g. 0 to 30 by 5: seq(0, 30, 5)). Such static numbers can cut-off your plot too short if the data changes to exceed the limit!.\n\n\nDate-axis labels/gridlines\nTIP: Remember that date-axis labels are independent from the aggregation of the data into bars, but visually it can be important to align bins, date labels, and vertical grid lines.\nTo modify the date labels and grid lines, use scale_x_date() in one of these ways:\n\nIf your histogram bins are days, Monday weeks, months, or years:\n\nUse date_breaks = to specify the interval of labels and major gridlines (e.g. “day”, “week”, “3 weeks”, “month”, or “year”)\nUse date_minor_breaks = to specify interval of minor vertical gridlines (between date labels)\n\nAdd expand = c(0,0) to begin the labels at the first bar\n\nUse date_labels = to specify format of date labels - see the Dates page for tips (use \\n for a new line)\n\n\nIf your histogram bins are Sunday weeks:\n\nUse breaks = and minor_breaks = by providing a sequence of date breaks for each\nYou can still use date_labels = and expand = for formatting as described above\n\n\nSome notes:\n\nSee the opening ggplot section for instructions on how to create a sequence of dates using seq.Date().\n\nSee this page or the Working with dates page for tips on creating date labels.\n\n\nDemonstrations\nBelow is a demonstration of plots where the bins and the plot labels/grid lines are aligned and not aligned:\n\n# 7-day bins + Monday labels\n#############################\nggplot(central_data) +\n  geom_histogram(\n    mapping = aes(x = date_onset),\n    binwidth = 7,                 # 7-day bins with start at first case\n    color = \"darkblue\",\n    fill = \"lightblue\") +\n  \n  scale_x_date(\n    expand = c(0,0),                     # remove excess x-axis space below and after case bars\n    date_breaks = \"3 weeks\",             # Monday every 3 weeks\n    date_minor_breaks = \"week\",          # Monday weeks\n    label = scales::label_date_short())+ # automatic label formatting\n  \n  scale_y_continuous(\n    expand = c(0,0))+              # remove excess space under x-axis, make flush\n  \n  labs(\n    title = \"MISALIGNED\",\n    subtitle = \"! CAUTION: 7-day bars start Thursdays at first case\\nDate labels and gridlines on Mondays\\nNote how ticks don't align with bars\")\n\n\n\n# 7-day bins + Months\n#####################\nggplot(central_data) +\n  geom_histogram(\n    mapping = aes(x = date_onset),\n    binwidth = 7,\n    color = \"darkblue\",\n    fill = \"lightblue\") +\n  \n  scale_x_date(\n    expand = c(0,0),                     # remove excess x-axis space below and after case bars\n    date_breaks = \"months\",              # 1st of month\n    date_minor_breaks = \"week\",          # Monday weeks\n    label = scales::label_date_short())+ # automatic label formatting\n  \n  scale_y_continuous(\n    expand = c(0,0))+                 # remove excess space under x-axis, make flush \n  \n  labs(\n    title = \"MISALIGNED\",\n    subtitle = \"! CAUTION: 7-day bars start Thursdays with first case\\nMajor gridlines and date labels at 1st of each month\\nMinor gridlines weekly on Mondays\\nNote uneven spacing of some gridlines and ticks unaligned with bars\")\n\n\n# TOTAL MONDAY ALIGNMENT: specify manual bin breaks to be mondays\n#################################################################\nggplot(central_data) + \n  geom_histogram(\n    mapping = aes(x = date_onset),\n    \n    # histogram breaks set to 7 days beginning Monday before first case\n    breaks = weekly_breaks_central,    # defined earlier in this page\n    \n    closed = \"left\",                   # count cases from start of breakpoint\n    \n    color = \"darkblue\",\n    \n    fill = \"lightblue\") + \n  \n  scale_x_date(\n    expand = c(0,0),                     # remove excess x-axis space below and after case bars\n    date_breaks = \"4 weeks\",             # Monday every 4 weeks\n    date_minor_breaks = \"week\",          # Monday weeks \n    label = scales::label_date_short())+ # label formatting\n  \n  scale_y_continuous(\n    expand = c(0,0))+                  # remove excess space under x-axis, make flush \n  \n  labs(\n    title = \"ALIGNED Mondays\",\n    subtitle = \"7-day bins manually set to begin Monday before first case (28 Apr)\\nDate labels and gridlines on Mondays as well\")\n\n\n# TOTAL MONDAY ALIGNMENT WITH MONTHS LABELS:\n############################################\nggplot(central_data) + \n  geom_histogram(\n    mapping = aes(x = date_onset),\n    \n    # histogram breaks set to 7 days beginning Monday before first case\n    breaks = weekly_breaks_central,    # defined earlier in this page\n    \n    closed = \"left\",                   # count cases from start of breakpoint\n    \n    color = \"darkblue\",\n    \n    fill = \"lightblue\") + \n  \n  scale_x_date(\n    expand = c(0,0),                     # remove excess x-axis space below and after case bars\n    date_breaks = \"months\",              # Monday every 4 weeks\n    date_minor_breaks = \"week\",          # Monday weeks \n    label = scales::label_date_short())+ # label formatting\n  \n  scale_y_continuous(\n    expand = c(0,0))+                  # remove excess space under x-axis, make flush \n  \n  theme(panel.grid.major = element_blank())+  # Remove major gridlines (fall on 1st of month)\n          \n  labs(\n    title = \"ALIGNED Mondays with MONTHLY labels\",\n    subtitle = \"7-day bins manually set to begin Monday before first case (28 Apr)\\nDate labels on 1st of Month\\nMonthly major gridlines removed\")\n\n\n# TOTAL SUNDAY ALIGNMENT: specify manual bin breaks AND labels to be Sundays\n############################################################################\nggplot(central_data) + \n  geom_histogram(\n    mapping = aes(x = date_onset),\n    \n    # histogram breaks set to 7 days beginning Sunday before first case\n    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   \"week\", week_start = 7),\n                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), \"week\", week_start = 7),\n                      by   = \"7 days\"),\n    \n    closed = \"left\",                    # count cases from start of breakpoint\n\n    color = \"darkblue\",\n    \n    fill = \"lightblue\") + \n  \n  scale_x_date(\n    expand = c(0,0),\n    # date label breaks and major gridlines set to every 3 weeks beginning Sunday before first case\n    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   \"week\", week_start = 7),\n                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), \"week\", week_start = 7),\n                      by   = \"3 weeks\"),\n    \n    # minor gridlines set to weekly beginning Sunday before first case\n    minor_breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   \"week\", week_start = 7),\n                            to   = ceiling_date(max(central_data$date_onset, na.rm=T), \"week\", week_start = 7),\n                            by   = \"7 days\"),\n    \n    label = scales::label_date_short())+ # label formatting\n  \n  scale_y_continuous(\n    expand = c(0,0))+                # remove excess space under x-axis, make flush \n  \n  labs(title = \"ALIGNED Sundays\",\n       subtitle = \"7-day bins manually set to begin Sunday before first case (27 Apr)\\nDate labels and gridlines manually set to Sundays as well\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregated data\nOften instead of a linelist, you begin with aggregated counts from facilities, districts, etc. You can make an epicurve with ggplot() but the code will be slightly different. This section will utilize the count_data dataset that was imported earlier, in the data preparation section. This dataset is the linelist aggregated to day-hospital counts. The first 50 rows are displayed below.\n\n\n\n\n\n\n\nPlotting daily counts\nWe can plot a daily epicurve from these daily counts. Here are the differences to the code:\n\nWithin the aesthetic mapping aes(), specify y = as the counts column (in this case, the column name is n_cases)\nAdd the argument stat = \"identity\" within geom_histogram(), which specifies that bar height should be the y = value, not the number of rows as is the default\n\nAdd the argument width = to avoid vertical white lines between the bars. For daily data set to 1. For weekly count data set to 7. For monthly count data, white lines are an issue (each month has different number of days) - consider transforming your x-axis to a categorical ordered factor (months) and using geom_col().\n\n\nggplot(data = count_data)+\n  geom_histogram(\n   mapping = aes(x = date_hospitalisation, y = n_cases),\n   stat = \"identity\",\n   width = 1)+                # for daily counts, set width = 1 to avoid white space between bars\n  labs(\n    x = \"Date of report\", \n    y = \"Number of cases\",\n    title = \"Daily case incidence, from daily count data\")\n\n\n\n\n\n\n\n\n\n\nPlotting weekly counts\nIf your data are already case counts by week, they might look like this dataset (called count_data_weekly):\nThe first 50 rows of count_data_weekly are displayed below. You can see that the counts have been aggregated into weeks. Each week is displayed by the first day of the week (Monday by default).\n\n\n\n\n\n\nNow plot so that x = the epiweek column. Remember to add y = the counts column to the aesthetic mapping, and add stat = \"identity\" as explained above.\n\nggplot(data = count_data_weekly)+\n  \n  geom_histogram(\n    mapping = aes(\n      x = epiweek,           # x-axis is epiweek (as class Date)\n      y = n_cases_weekly,    # y-axis height in the weekly case counts\n      group = hospital,      # we are grouping the bars and coloring by hospital\n      fill = hospital),\n    stat = \"identity\")+      # this is also required when plotting count data\n     \n  # labels for x-axis\n  scale_x_date(\n    date_breaks = \"2 months\",      # labels every 2 months \n    date_minor_breaks = \"1 month\", # gridlines every month\n    label = scales::label_date_short())+ # label formatting\n     \n  # Choose color palette (uses RColorBrewer package)\n  scale_fill_brewer(palette = \"Pastel2\")+ \n  \n  theme_minimal()+\n  \n  labs(\n    x = \"Week of onset\", \n    y = \"Weekly case incidence\",\n    fill = \"Hospital\",\n    title = \"Weekly case incidence, from aggregated count data by hospital\")\n\n\n\n\n\n\n\n\n\n\n\nMoving averages\nSee the page on Moving averages for a detailed description and several options. Below is one option for calculating moving averages with the package slider. In this approach, the moving average is calculated in the dataset prior to plotting:\n\nAggregate the data into counts as necessary (daily, weekly, etc.) (see Grouping data page)\n\nCreate a new column to hold the moving average, created with slide_index() from slider package\n\nPlot the moving average as a geom_line() on top of (after) the epicurve histogram\n\nSee the helpful online vignette for the slider package\n\n# load package\npacman::p_load(slider)  # slider used to calculate rolling averages\n\n# make dataset of daily counts and 7-day moving average\n#######################################################\nll_counts_7day &lt;- linelist %&gt;%    # begin with linelist\n  \n  ## count cases by date\n  count(date_onset, name = \"new_cases\") %&gt;%   # name new column with counts as \"new_cases\"\n  drop_na(date_onset) %&gt;%                     # remove cases with missing date_onset\n  \n  ## calculate the average number of cases in 7-day window\n  mutate(\n    avg_7day = slider::slide_index(    # create new column\n      new_cases,                       # calculate based on value in new_cases column\n      .i = date_onset,                 # index is date_onset col, so non-present dates are included in window \n      .f = ~mean(.x, na.rm = TRUE),    # function is mean() with missing values removed\n      .before = 6,                     # window is the day and 6-days before\n      .complete = FALSE),              # must be FALSE for unlist() to work in next step\n    avg_7day = unlist(avg_7day))       # convert class list to class numeric\n\n\n# plot\n######\nggplot(data = ll_counts_7day) +  # begin with new dataset defined above \n    geom_histogram(              # create epicurve histogram\n      mapping = aes(\n        x = date_onset,          # date column as x-axis\n        y = new_cases),          # height is number of daily new cases\n        stat = \"identity\",       # height is y value\n        fill=\"#92a8d1\",          # cool color for bars\n        colour = \"#92a8d1\",      # same color for bar border\n        )+ \n    geom_line(                   # make line for rolling average\n      mapping = aes(\n        x = date_onset,          # date column for x-axis\n        y = avg_7day,            # y-value set to rolling average column\n        lty = \"7-day \\nrolling avg\"), # name of line in legend\n      color=\"red\",               # color of line\n      size = 1) +                # width of line\n    scale_x_date(                # date scale\n      date_breaks = \"1 month\",\n      label = scales::label_date_short(), # label formatting\n      expand = c(0,0)) +\n    scale_y_continuous(          # y-axis scale\n      expand = c(0,0),\n      limits = c(0, NA)) +       \n    labs(\n      x=\"\",\n      y =\"Number of confirmed cases\",\n      fill = \"Legend\")+ \n    theme_minimal()+\n    theme(legend.title = element_blank())  # removes title of legend\n\n\n\n\n\n\n\n\n\n\nFaceting/small-multiples\nAs with other ggplots, you can create facetted plots (“small multiples”). As explained in the ggplot tips page of this handbook, you can use either facet_wrap() or facet_grid(). Here we demonstrate with facet_wrap(). For epicurves, facet_wrap() is typically easier as it is likely that you only need to facet on one column.\nThe general syntax is facet_wrap(rows ~ cols), where to the left of the tilde (~) is the name of a column to be spread across the “rows” of the facetted plot, and to the right of the tilde is the name of a column to be spread across the “columns” of the facetted plot. Most simply, just use one column name, to the right of the tilde: facet_wrap(~age_cat).\nFree axes\nYou will need to decide whether the scales of the axes for each facet are “fixed” to the same dimensions (default), or “free” (meaning they will change based on the data within the facet). Do this with the scales = argument within facet_wrap() by specifying “free_x” or “free_y”, or “free”.\nNumber of cols and rows of facets\nThis can be specified with ncol = and nrow = within facet_wrap().\nOrder of panels\nTo change the order of appearance, change the underlying order of the levels of the factor column used to create the facets.\nAesthetics\nFont size and face, strip color, etc. can be modified through theme() with arguments like:\n\nstrip.text = element_text() (size, colour, face, angle…)\nstrip.background = element_rect() (e.g. element_rect(fill=“grey”))\n\nstrip.position = (position of the strip “bottom”, “top”, “left”, or “right”)\n\nStrip labels\nLabels of the facet plots can be modified through the “labels” of the column as a factor, or by the use of a “labeller”.\nMake a labeller like this, using the function as_labeller() from ggplot2. Then provide the labeller to the labeller = argument of facet_wrap() as shown below.\n\nmy_labels &lt;- as_labeller(c(\n     \"0-4\"   = \"Ages 0-4\",\n     \"5-9\"   = \"Ages 5-9\",\n     \"10-14\" = \"Ages 10-14\",\n     \"15-19\" = \"Ages 15-19\",\n     \"20-29\" = \"Ages 20-29\",\n     \"30-49\" = \"Ages 30-49\",\n     \"50-69\" = \"Ages 50-69\",\n     \"70+\"   = \"Over age 70\"))\n\nAn example facetted plot - facetted by column age_cat.\n\n# make plot\n###########\nggplot(central_data) + \n  \n  geom_histogram(\n    mapping = aes(\n      x = date_onset,\n      group = age_cat,\n      fill = age_cat),    # arguments inside aes() apply by group\n      \n    color = \"black\",      # arguments outside aes() apply to all data\n        \n    # histogram breaks\n    breaks = weekly_breaks_central, # pre-defined date vector (see earlier in this page)\n    closed = \"left\" # count cases from start of breakpoint\n    )+  \n                      \n  # The labels on the x-axis\n  scale_x_date(\n    expand            = c(0,0),          # remove excess x-axis space below and after case bars\n    date_breaks       = \"2 months\",      # labels appear every 2 months\n    date_minor_breaks = \"1 month\",       # vertical lines appear every 1 month \n    label = scales::label_date_short())+ # label formatting\n  \n  # y-axis\n  scale_y_continuous(expand = c(0,0))+                       # removes excess y-axis space between bottom of bars and the labels\n  \n  # aesthetic themes\n  theme_minimal()+                                           # a set of themes to simplify plot\n  theme(\n    plot.caption = element_text(face = \"italic\", hjust = 0), # caption on left side in italics\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"bottom\",\n    strip.text = element_text(face = \"bold\", size = 10),\n    strip.background = element_rect(fill = \"grey\"))+         # axis titles in bold\n  \n  # create facets\n  facet_wrap(\n    ~age_cat,\n    ncol = 4,\n    strip.position = \"top\",\n    labeller = my_labels)+             \n  \n  # labels\n  labs(\n    title    = \"Weekly incidence of cases, by age category\",\n    subtitle = \"Subtitle\",\n    fill     = \"Age category\",                                      # provide new title for legend\n    x        = \"Week of symptom onset\",\n    y        = \"Weekly incident cases reported\",\n    caption  = stringr::str_glue(\"n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\\n{nrow(central_data %&gt;% filter(is.na(date_onset)))} cases missing date of onset and not shown\"))\n\n\n\n\n\n\n\n\nSee this link for more information on labellers.\n\nTotal epidemic in facet background\nTo show the total epidemic in the background of each facet, add the function gghighlight() with empty parentheses to the ggplot. This is from the package gghighlight. Note that the y-axis maximum in all facets is now based on the peak of the entire epidemic. There are more examples of this package in the ggplot tips page.\n\nggplot(central_data) + \n  \n  # epicurves by group\n  geom_histogram(\n    mapping = aes(\n      x = date_onset,\n      group = age_cat,\n      fill = age_cat),  # arguments inside aes() apply by group\n    \n    color = \"black\",    # arguments outside aes() apply to all data\n    \n    # histogram breaks\n    breaks = weekly_breaks_central, # pre-defined date vector (see earlier in this page)\n    \n    closed = \"left\", # count cases from start of breakpoint\n    )+     # pre-defined date vector (see top of ggplot section)                \n  \n  # add grey epidemic in background to each facet\n  gghighlight::gghighlight()+\n  \n  # labels on x-axis\n  scale_x_date(\n    expand            = c(0,0),          # remove excess x-axis space below and after case bars\n    date_breaks       = \"2 months\",      # labels appear every 2 months\n    date_minor_breaks = \"1 month\",       # vertical lines appear every 1 month \n    label = scales::label_date_short())+ # label formatting\n  \n  # y-axis\n  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space below 0\n  \n  # aesthetic themes\n  theme_minimal()+                                           # a set of themes to simplify plot\n  theme(\n    plot.caption = element_text(face = \"italic\", hjust = 0), # caption on left side in italics\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"bottom\",\n    strip.text = element_text(face = \"bold\", size = 10),\n    strip.background = element_rect(fill = \"white\"))+        # axis titles in bold\n  \n  # create facets\n  facet_wrap(\n    ~age_cat,                          # each plot is one value of age_cat\n    ncol = 4,                          # number of columns\n    strip.position = \"top\",            # position of the facet title/strip\n    labeller = my_labels)+             # labeller defines above\n  \n  # labels\n  labs(\n    title    = \"Weekly incidence of cases, by age category\",\n    subtitle = \"Subtitle\",\n    fill     = \"Age category\",                                      # provide new title for legend\n    x        = \"Week of symptom onset\",\n    y        = \"Weekly incident cases reported\",\n    caption  = stringr::str_glue(\"n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\\n{nrow(central_data %&gt;% filter(is.na(date_onset)))} cases missing date of onset and not shown\"))\n\n\n\n\n\n\n\n\n\n\nOne facet with data\nIf you want to have one facet box that contains all the data, duplicate the entire dataset and treat the duplicates as one faceting value. A “helper” function CreateAllFacet() below can assist with this (thanks to this blog post). When it is run, the number of rows doubles and there will be a new column called facet in which the duplicated rows will have the value “all”, and the original rows have the their original value of the faceting colum. Now you just have to facet on the facet column.\nHere is the helper function. Run it so that it is available to you.\n\n# Define helper function\nCreateAllFacet &lt;- function(df, col){\n     df$facet &lt;- df[[col]]\n     temp &lt;- df\n     temp$facet &lt;- \"all\"\n     merged &lt;-rbind(temp, df)\n     \n     # ensure the facet value is a factor\n     merged[[col]] &lt;- as.factor(merged[[col]])\n     \n     return(merged)\n}\n\nNow apply the helper function to the dataset, on column age_cat:\n\n# Create dataset that is duplicated and with new column \"facet\" to show \"all\" age categories as another facet level\ncentral_data2 &lt;- CreateAllFacet(central_data, col = \"age_cat\") %&gt;%\n  \n  # set factor levels\n  mutate(facet = fct_relevel(facet, \"all\", \"0-4\", \"5-9\",\n                             \"10-14\", \"15-19\", \"20-29\",\n                             \"30-49\", \"50-69\", \"70+\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `facet = fct_relevel(...)`.\nCaused by warning:\n! 1 unknown level in `f`: 70+\n\n# check levels\ntable(central_data2$facet, useNA = \"always\")\n\n\n  all   0-4   5-9 10-14 15-19 20-29 30-49 50-69  &lt;NA&gt; \n  454    84    84    82    58    73    57     7     9 \n\n\nNotable changes to the ggplot() command are:\n\nThe data used is now central_data2 (double the rows, with new column “facet”)\nLabeller will need to be updated, if used\n\nOptional: to achieve vertically stacked facets: the facet column is moved to rows side of equation and on right is replaced by “.” (facet_wrap(facet~.)), and ncol = 1. You may also need to adjust the width and height of the saved png plot image (see ggsave() in ggplot tips).\n\n\nggplot(central_data2) + \n  \n  # actual epicurves by group\n  geom_histogram(\n        mapping = aes(\n          x = date_onset,\n          group = age_cat,\n          fill = age_cat),  # arguments inside aes() apply by group\n        color = \"black\",    # arguments outside aes() apply to all data\n        \n        # histogram breaks\n        breaks = weekly_breaks_central, # pre-defined date vector (see earlier in this page)\n        \n        closed = \"left\", # count cases from start of breakpoint\n        )+    # pre-defined date vector (see top of ggplot section)\n                     \n  # Labels on x-axis\n  scale_x_date(\n    expand            = c(0,0),          # remove excess x-axis space below and after case bars\n    date_breaks       = \"2 months\",      # labels appear every 2 months\n    date_minor_breaks = \"1 month\",       # vertical lines appear every 1 month \n    label = scales::label_date_short())+ # automatic label formatting\n  \n  # y-axis\n  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space between bottom of bars and the labels\n  \n  # aesthetic themes\n  theme_minimal()+                                           # a set of themes to simplify plot\n  theme(\n    plot.caption = element_text(face = \"italic\", hjust = 0), # caption on left side in italics\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"bottom\")+               \n  \n  # create facets\n  facet_wrap(facet~. ,                            # each plot is one value of facet\n             ncol = 1)+            \n\n  # labels\n  labs(title    = \"Weekly incidence of cases, by age category\",\n       subtitle = \"Subtitle\",\n       fill     = \"Age category\",                                      # provide new title for legend\n       x        = \"Week of symptom onset\",\n       y        = \"Weekly incident cases reported\",\n       caption  = stringr::str_glue(\"n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\\n{nrow(central_data %&gt;% filter(is.na(date_onset)))} cases missing date of onset and not shown\"))",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html#tentative-data",
    "href": "new_pages/epicurves.html#tentative-data",
    "title": "32  Epidemic curves",
    "section": "32.3 Tentative data",
    "text": "32.3 Tentative data\nThe most recent data shown in epicurves should often be marked as tentative, or subject to reporting delays. This can be done in by adding a vertical line and/or rectangle over a specified number of days. Here are two options:\n\nUse annotate():\n\nFor a line use annotate(geom = \"segment\"). Provide x, xend, y, and yend. Adjust size, linetype (lty), and color.\n\nFor a rectangle use annotate(geom = \"rect\"). Provide xmin/xmax/ymin/ymax. Adjust color and alpha.\n\n\nGroup the data by tentative status and color those bars differently\n\nCAUTION: You might try geom_rect() to draw a rectangle, but adjusting the transparency does not work in a linelist context. This function overlays one rectangle for each observation/row!. Use either a very low alpha (e.g. 0.01), or another approach. \n\nUsing annotate()\n\nWithin annotate(geom = \"rect\"), the xmin and xmax arguments must be given inputs of class Date.\n\nNote that because these data are aggregated into weekly bars, and the last bar extends to the Monday after the last data point, the shaded region may appear to cover 4 weeks\n\nHere is an annotate() online example\n\n\nggplot(central_data) + \n  \n  # histogram\n  geom_histogram(\n    mapping = aes(x = date_onset),\n    \n    breaks = weekly_breaks_central,   # pre-defined date vector - see top of ggplot section\n    \n    closed = \"left\", # count cases from start of breakpoint\n    \n    color = \"darkblue\",\n    \n    fill = \"lightblue\") +\n\n  # scales\n  scale_y_continuous(expand = c(0,0))+\n  scale_x_date(\n    expand = c(0,0),                     # remove excess x-axis space below and after case bars\n    date_breaks = \"1 month\",             # 1st of month\n    date_minor_breaks = \"1 month\",       # 1st of month\n    label = scales::label_date_short())+ # automatic label formatting\n  \n  # labels and theme\n  labs(\n    title = \"Using annotate()\\nRectangle and line showing that data from last 21-days are tentative\",\n    x = \"Week of symptom onset\",\n    y = \"Weekly case indicence\")+ \n  theme_minimal()+\n  \n  # add semi-transparent red rectangle to tentative data\n  annotate(\n    \"rect\",\n    xmin  = as.Date(max(central_data$date_onset, na.rm = T) - 21), # note must be wrapped in as.Date()\n    xmax  = as.Date(Inf),                                          # note must be wrapped in as.Date()\n    ymin  = 0,\n    ymax  = Inf,\n    alpha = 0.2,          # alpha easy and intuitive to adjust using annotate()\n    fill  = \"red\")+\n  \n  # add black vertical line on top of other layers\n  annotate(\n    \"segment\",\n    x     = max(central_data$date_onset, na.rm = T) - 21, # 21 days before last data\n    xend  = max(central_data$date_onset, na.rm = T) - 21, \n    y     = 0,         # line begins at y = 0\n    yend  = Inf,       # line to top of plot\n    size  = 2,         # line size\n    color = \"black\",\n    lty   = \"solid\")+   # linetype e.g. \"solid\", \"dashed\"\n\n  # add text in rectangle\n  annotate(\n    \"text\",\n    x = max(central_data$date_onset, na.rm = T) - 15,\n    y = 15,\n    label = \"Subject to reporting delays\",\n    angle = 90)\n\n\n\n\n\n\n\n\nThe same black vertical line can be achieved with the code below, but using geom_vline() you lose the ability to control the height:\n\ngeom_vline(xintercept = max(central_data$date_onset, na.rm = T) - 21,\n           size = 2,\n           color = \"black\")\n\n\n\nBars color\nAn alternative approach could be to adjust the color or display of the tentative bars of data themselves. You could create a new column in the data preparation stage and use it to group the data, such that the aes(fill = ) of tentative data can be a different color or alpha than the other bars.\n\n# add column\n############\nplot_data &lt;- central_data %&gt;% \n  mutate(tentative = case_when(\n    date_onset &gt;= max(date_onset, na.rm=T) - 7 ~ \"Tentative\", # tenative if in last 7 days\n    TRUE                                       ~ \"Reliable\")) # all else reliable\n\n# plot\n######\nggplot(plot_data, aes(x = date_onset, fill = tentative)) + \n  \n  # histogram\n  geom_histogram(\n    breaks = weekly_breaks_central,   # pre-defined data vector, see top of ggplot page\n    closed = \"left\", # count cases from start of breakpoint\n    color = \"black\") +\n\n  # scales\n  scale_y_continuous(expand = c(0,0))+\n  scale_fill_manual(values = c(\"lightblue\", \"grey\"))+\n  scale_x_date(\n    expand = c(0,0),                     # remove excess x-axis space below and after case bars\n    date_breaks = \"3 weeks\",             # Monday every 3 weeks\n    date_minor_breaks = \"week\",          # Monday weeks \n    label = scales::label_date_short())+ # automatic label formatting\n  \n  # labels and theme\n  labs(title = \"Show days that are tentative reporting\",\n    subtitle = \"\")+ \n  theme_minimal()+\n  theme(legend.title = element_blank())                 # remove title of legend",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html#multi-level-date-labels",
    "href": "new_pages/epicurves.html#multi-level-date-labels",
    "title": "32  Epidemic curves",
    "section": "32.4 Multi-level date labels",
    "text": "32.4 Multi-level date labels\nIf you want multi-level date labels (e.g. month and year) without duplicating the lower label levels, consider one of the approaches below:\nRemember - you can can use tools like \\n within the date_labels or labels arguments to put parts of each label on a new line below. However, the codes below help you take years or months (for example) on a lower line and only once.\nThe easiest method is to assign the labels = argument in scale_x_date() to the function label_date_short() from the package scales (note: don’t forget to include empty parentheses (), as shown below). This function will automatically construct efficient date labels (read more here). An additional benefit of this function is that the labels will automatically adjust as your data expands over time: from days, to weeks, to months and years.\n\nggplot(central_data) + \n  \n  # histogram\n  geom_histogram(\n    mapping = aes(x = date_onset),\n    breaks = weekly_breaks_central,   # pre-defined date vector - see top of ggplot section\n    closed = \"left\",                  # count cases from start of breakpoint\n    color = \"darkblue\",\n    fill = \"lightblue\") +\n\n  # y-axis scale as before \n  scale_y_continuous(expand = c(0,0))+\n  \n  # x-axis scale sets efficient date labels\n  scale_x_date(\n    expand = c(0,0),                      # remove excess x-axis space below and after case bars\n    labels = scales::label_date_short())+ # auto efficient date labels\n  \n  # labels and theme\n  labs(\n    title = \"Using label_date_short()\\nTo make automatic and efficient date labels\",\n    x = \"Week of symptom onset\",\n    y = \"Weekly case indicence\")+ \n  theme_minimal()\n\n\n\n\n\n\n\n\nA second option is to use faceting. Below:\n\nCase counts are aggregated into weeks for aesthetic reasons. See Epicurves page (aggregated data tab) for details.\n\nA geom_area() line is used instead of a histogram, as the faceting approach below does not work well with histograms.\n\nAggregate to weekly counts\n\n# Create dataset of case counts by week\n#######################################\ncentral_weekly &lt;- linelist %&gt;%\n  filter(hospital == \"Central Hospital\") %&gt;%   # filter linelist\n  mutate(week = lubridate::floor_date(date_onset, unit = \"weeks\")) %&gt;%  \n  count(week) %&gt;%                              # summarize weekly case counts\n  drop_na(week) %&gt;%                            # remove cases with missing onset_date\n  complete(                                    # fill-in all weeks with no cases reported\n    week = seq.Date(\n      from = min(week),   \n      to   = max(week),\n      by   = \"week\"),\n    fill = list(n = 0))                        # convert new NA values to 0 counts\n\nMake plots\n\n# plot with no facet box border\n#################################\nggplot(central_weekly,\n       aes(x = week, y = n)) +              # establish x and y for entire plot\n  geom_line(stat = \"identity\",              # make line, line height is count number\n            color = \"#69b3a2\") +            # line color\n  geom_point(size=1, color=\"#69b3a2\") +     # make points at the weekly data points\n  geom_area(fill = \"#69b3a2\",               # fill area below line\n            alpha = 0.4)+                   # fill transparency\n  scale_x_date(date_labels=\"%b\",            # date label format show month \n               date_breaks=\"month\",         # date labels on 1st of each month\n               expand=c(0,0)) +             # remove excess space\n  scale_y_continuous(\n    expand  = c(0,0))+                      # remove excess space below x-axis\n  facet_grid(~lubridate::year(week),        # facet on year (of Date class column)\n             space=\"free_x\",                \n             scales=\"free_x\",               # x-axes adapt to data range (not \"fixed\")\n             switch=\"x\") +                  # facet labels (year) on bottom\n  theme_bw() +\n  theme(strip.placement = \"outside\",                  # facet label placement\n          strip.background = element_blank(),         # no facet lable background\n          panel.grid.minor.x = element_blank(),          \n          panel.border = element_blank(),             # no border for facet panel\n          panel.spacing=unit(0,\"cm\"))+                # No space between facet panels\n  labs(title = \"Nested year labels - points, shaded, no label border\")\n\n\n\n\n\n\n\n\nThe above technique for faceting was adapted from this and this post on stackoverflow.com.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html#dual-axis",
    "href": "new_pages/epicurves.html#dual-axis",
    "title": "32  Epidemic curves",
    "section": "32.5 Dual-axis",
    "text": "32.5 Dual-axis\nAlthough there are fierce discussions about the validity of dual axes within the data visualization community, many epi supervisors still want to see an epicurve or similar chart with a percent overlaid with a second axis. This is discussed more extensively in the ggplot tips page, but one example using the cowplot method is shown below:\n\nTwo distinct plots are made, and then combined with cowplot package.\n\nThe plots must have the exact same x-axis (set limits) or else the data and labels will not align\n\nEach uses theme_cowplot() and one has the y-axis moved to the right side of the plot\n\n\n#load package\npacman::p_load(cowplot)\n\n# Make first plot of epicurve histogram\n#######################################\nplot_cases &lt;- linelist %&gt;% \n  \n  # plot cases per week\n  ggplot()+\n  \n  # create histogram  \n  geom_histogram(\n    \n    mapping = aes(x = date_onset),\n    \n    # bin breaks every week beginning monday before first case, going to monday after last case\n    breaks = weekly_breaks_all)+  # pre-defined vector of weekly dates (see top of ggplot section)\n        \n  # specify beginning and end of date axis to align with other plot\n  scale_x_date(\n    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram\n  \n  # labels\n  labs(\n      y = \"Daily cases\",\n      x = \"Date of symptom onset\"\n    )+\n  theme_cowplot()\n\n\n# make second plot of percent died per week\n###########################################\nplot_deaths &lt;- linelist %&gt;%                        # begin with linelist\n  group_by(week = floor_date(date_onset, \"week\")) %&gt;%  # create week column\n  \n  # summarise to get weekly percent of cases who died\n  summarise(n_cases = n(),\n            died = sum(outcome == \"Death\", na.rm=T),\n            pct_died = 100*died/n_cases) %&gt;% \n  \n  # begin plot\n  ggplot()+\n  \n  # line of weekly percent who died\n  geom_line(                                # create line of percent died\n    mapping = aes(x = week, y = pct_died),  # specify y-height as pct_died column\n    stat = \"identity\",                      # set line height to the value in pct_death column, not the number of rows (which is default)\n    size = 2,\n    color = \"black\")+\n  \n  # Same date-axis limits as the other plot - perfect alignment\n  scale_x_date(\n    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram\n  \n  \n  # y-axis adjustments\n  scale_y_continuous(                # adjust y-axis\n    breaks = seq(0,100, 10),         # set break intervals of percent axis\n    limits = c(0, 100),              # set extent of percent axis\n    position = \"right\")+             # move percent axis to the right\n  \n  # Y-axis label, no x-axis label\n  labs(x = \"\",\n       y = \"Percent deceased\")+      # percent axis label\n  \n  theme_cowplot()                   # add this to make the two plots merge together nicely\n\nNow use cowplot to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of theme_cowplot().\n\naligned_plots &lt;- cowplot::align_plots(plot_cases, plot_deaths, align=\"hv\", axis=\"tblr\")\nggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html#cumulative-incidence",
    "href": "new_pages/epicurves.html#cumulative-incidence",
    "title": "32  Epidemic curves",
    "section": "32.6 Cumulative Incidence",
    "text": "32.6 Cumulative Incidence\nIf beginning with a case linelist, create a new column containing the cumulative number of cases per day in an outbreak using cumsum() from base R:\n\ncumulative_case_counts &lt;- linelist %&gt;% \n  count(date_onset) %&gt;%                # count of rows per day (returned in column \"n\")   \n  mutate(                         \n    cumulative_cases = cumsum(n)       # new column of the cumulative number of rows at each date\n    )\n\nThe first 10 rows are shown below:\n\n\n\n\n\n\nThis cumulative column can then be plotted against date_onset, using geom_line():\n\nplot_cumulative &lt;- ggplot()+\n  geom_line(\n    data = cumulative_case_counts,\n    aes(x = date_onset, y = cumulative_cases),\n    size = 2,\n    color = \"blue\")\n\nplot_cumulative\n\n\n\n\n\n\n\n\nIt can also be overlaid onto the epicurve, with dual-axis using the cowplot method described above and in the ggplot tips page:\n\n#load package\npacman::p_load(cowplot)\n\n# Make first plot of epicurve histogram\nplot_cases &lt;- ggplot()+\n  geom_histogram(          \n    data = linelist,\n    aes(x = date_onset),\n    binwidth = 1)+\n  labs(\n    y = \"Daily cases\",\n    x = \"Date of symptom onset\"\n  )+\n  theme_cowplot()\n\n# make second plot of cumulative cases line\nplot_cumulative &lt;- ggplot()+\n  geom_line(\n    data = cumulative_case_counts,\n    aes(x = date_onset, y = cumulative_cases),\n    size = 2,\n    color = \"blue\")+\n  scale_y_continuous(\n    position = \"right\")+\n  labs(x = \"\",\n       y = \"Cumulative cases\")+\n  theme_cowplot()+\n  theme(\n    axis.line.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks = element_blank())\n\nNow use cowplot to overlay the two plots. Attention has been paid to the x-axis alignment, side of the y-axis, and use of theme_cowplot().\n\naligned_plots &lt;- cowplot::align_plots(plot_cases, plot_cumulative, align=\"hv\", axis=\"tblr\")\nggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/epicurves.html#resources",
    "href": "new_pages/epicurves.html#resources",
    "title": "32  Epidemic curves",
    "section": "32.7 Resources",
    "text": "32.7 Resources",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Epidemic curves</span>"
    ]
  },
  {
    "objectID": "new_pages/age_pyramid.html",
    "href": "new_pages/age_pyramid.html",
    "title": "33  Demographic pyramids and Likert-scales",
    "section": "",
    "text": "33.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Demographic pyramids and Likert-scales</span>"
    ]
  },
  {
    "objectID": "new_pages/age_pyramid.html#preparation",
    "href": "new_pages/age_pyramid.html#preparation",
    "title": "33  Demographic pyramids and Likert-scales",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(rio,       # to import data\n               here,      # to locate files\n               tidyverse, # to clean, handle, and plot the data (includes ggplot2 package)\n               apyramid,  # a package dedicated to creating age pyramids\n               janitor,   # tables and cleaning data\n               stringr)   # working with strings for titles, captions, etc.\n\n\n\nImport data\nTo begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import case linelist \nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.\n\n\n\n\n\n\n\n\nCleaning\nTo make a traditional age/gender demographic pyramid, the data must first be cleaned in the following ways:\n\nThe gender column must be cleaned.\n\nDepending on your method, age should be stored as either a numeric or in an age category column.\n\nIf using age categories, the column values should be corrected ordered, either by default alpha-numeric or intentionally set by converting to class factor.\nBelow we use tabyl() from janitor to inspect the columns gender and age_cat5.\n\nlinelist %&gt;% \n  tabyl(age_cat5, gender)\n\n age_cat5   f   m NA_\n      0-4 640 416  39\n      5-9 641 412  42\n    10-14 518 383  40\n    15-19 359 364  20\n    20-24 305 316  17\n    25-29 163 259  13\n    30-34 104 213   9\n    35-39  42 157   3\n    40-44  25 107   1\n    45-49   8  80   5\n    50-54   2  37   1\n    55-59   0  30   0\n    60-64   0  12   0\n    65-69   0  12   1\n    70-74   0   4   0\n    75-79   0   0   1\n    80-84   0   1   0\n      85+   0   0   0\n     &lt;NA&gt;   0   0  86\n\n\nWe also run a quick histogram on the age column to ensure it is clean and correctly classified:\n\nhist(linelist$age)",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Demographic pyramids and Likert-scales</span>"
    ]
  },
  {
    "objectID": "new_pages/age_pyramid.html#apyramid-package",
    "href": "new_pages/age_pyramid.html#apyramid-package",
    "title": "33  Demographic pyramids and Likert-scales",
    "section": "33.2 apyramid package",
    "text": "33.2 apyramid package\nThe package apyramid is a product of the R4Epis project. You can read more about this package here. It allows you to quickly make an age pyramid. For more nuanced situations, see the section below using ggplot(). You can read more about the apyramid package in its Help page by entering ?age_pyramid in your R console.\n\nLinelist data\nUsing the cleaned linelist dataset, we can create an age pyramid with one simple age_pyramid() command. In this command:\n\nThe data = argument is set as the linelist data frame\n\nThe age_group = argument (for y-axis) is set to the name of the categorical age column (in quotes)\n\nThe split_by = argument (for x-axis) is set to the gender column\n\n\napyramid::age_pyramid(data = linelist,\n                      age_group = \"age_cat5\",\n                      split_by = \"gender\")\n\n\n\n\n\n\n\n\nThe pyramid can be displayed with percent of all cases on the x-axis, instead of counts, by including proportional = TRUE.\n\napyramid::age_pyramid(data = linelist,\n                      age_group = \"age_cat5\",\n                      split_by = \"gender\",\n                      proportional = TRUE)\n\n\n\n\n\n\n\n\nWhen using agepyramid package, if the split_by column is binary (e.g. male/female, or yes/no), then the result will appear as a pyramid. However if there are more than two values in the split_by column (not including NA), the pyramid will appears as a faceted bar plot with grey bars in the “background” indicating the range of the un-faceted data for that age group. In this case, values of split_by = will appear as labels at top of each facet panel. For example, below is what occurs if the split_by = is assigned the column hospital.\n\napyramid::age_pyramid(data = linelist,\n                      age_group = \"age_cat5\",\n                      split_by = \"hospital\")  \n\n\n\n\n\n\n\n\n\nMissing values\nRows that have NA missing values in the split_by = or age_group = columns, if coded as NA, will not trigger the faceting shown above. By default these rows will not be shown. However you can specify that they appear, in an adjacent barplot and as a separate age group at the top, by specifying na.rm = FALSE.\n\napyramid::age_pyramid(data = linelist,\n                      age_group = \"age_cat5\",\n                      split_by = \"gender\",\n                      na.rm = FALSE)         # show patients missing age or gender\n\n\n\n\n\n\n\n\n\n\nProportions, colors, & aesthetics\nBy default, the bars display counts (not %), a dashed mid-line for each group is shown, and the colors are green/purple. Each of these parameters can be adjusted, as shown below:\nYou can also add additional ggplot() commands to the plot using the standard ggplot() “+” syntax, such as aesthetic themes and label adjustments:\n\napyramid::age_pyramid(\n  data = linelist,\n  age_group = \"age_cat5\",\n  split_by = \"gender\",\n  proportional = TRUE,              # show percents, not counts\n  show_midpoint = FALSE,            # remove bar mid-point line\n  #pal = c(\"orange\", \"purple\")      # can specify alt. colors here (but not labels)\n  )+                 \n  \n  # additional ggplot commands\n  theme_minimal()+                               # simplfy background\n  scale_fill_manual(                             # specify colors AND labels\n    values = c(\"orange\", \"purple\"),              \n    labels = c(\"m\" = \"Male\", \"f\" = \"Female\"))+\n  labs(y = \"Percent of all cases\",              # note x and y labs are switched\n       x = \"Age categories\",                          \n       fill = \"Gender\", \n       caption = \"My data source and caption here\",\n       title = \"Title of my plot\",\n       subtitle = \"Subtitle with \\n a second line...\")+\n  theme(\n    legend.position = \"bottom\",                          # legend to bottom\n    axis.text = element_text(size = 10, face = \"bold\"),  # fonts/sizes\n    axis.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\nAggregated data\nThe examples above assume your data are in a linelist format, with one row per observation. If your data are already aggregated into counts by age category, you can still use the apyramid package, as shown below.\nFor demonstration, we aggregate the linelist data into counts by age category and gender, into a “wide” format. This will simulate as if your data were in counts to begin with. Learn more about Grouping data and Pivoting data in their respective pages.\n\ndemo_agg &lt;- linelist %&gt;% \n  count(age_cat5, gender, name = \"cases\") %&gt;% \n  pivot_wider(\n    id_cols = age_cat5,\n    names_from = gender,\n    values_from = cases) %&gt;% \n  rename(`missing_gender` = `NA`)\n\n…which makes the dataset looks like this: with columns for age category, and male counts, female counts, and missing counts.\n\n\n\n\n\n\nTo set-up these data for the age pyramid, we will pivot the data to be “long” with the pivot_longer() function from dplyr. This is because ggplot() generally prefers “long” data, and apyramid is using ggplot().\n\n# pivot the aggregated data into long format\ndemo_agg_long &lt;- demo_agg %&gt;% \n  pivot_longer(\n    col = c(f, m, missing_gender),            # cols to elongate\n    names_to = \"gender\",                # name for new col of categories\n    values_to = \"counts\") %&gt;%           # name for new col of counts\n  mutate(\n    gender = na_if(gender, \"missing_gender\")) # convert \"missing_gender\" to NA\n\n\n\n\n\n\n\nThen use the split_by = and count = arguments of age_pyramid() to specify the respective columns in the data:\n\napyramid::age_pyramid(data = demo_agg_long,\n                      age_group = \"age_cat5\",# column name for age category\n                      split_by = \"gender\",   # column name for gender\n                      count = \"counts\")      # column name for case counts\n\n\n\n\n\n\n\n\nNote in the above, that the factor order of “m” and “f” is different (pyramid reversed). To adjust the order you must re-define gender in the aggregated data as a Factor and order the levels as desired. See the Factors page.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Demographic pyramids and Likert-scales</span>"
    ]
  },
  {
    "objectID": "new_pages/age_pyramid.html#demo_pyr_gg",
    "href": "new_pages/age_pyramid.html#demo_pyr_gg",
    "title": "33  Demographic pyramids and Likert-scales",
    "section": "33.3 ggplot()",
    "text": "33.3 ggplot()\nUsing ggplot() to build your age pyramid allows for more flexibility, but requires more effort and understanding of how ggplot() works. It is also easier to accidentally make mistakes.\nTo use ggplot() to make demographic pyramids, you create two bar plots (one for each gender), convert the values in one plot to negative, and finally flip the x and y axes to display the bar plots vertically, their bases meeting in the plot middle.\n\nPreparation\nThis approach uses the numeric age column, not the categorical column of age_cat5. So we will check to ensure the class of this column is indeed numeric.\n\nclass(linelist$age)\n\n[1] \"numeric\"\n\n\nYou could use the same logic below to build a pyramid from categorical data using geom_col() instead of geom_histogram().\n\n\n\nConstructing the plot\nFirst, understand that to make such a pyramid using ggplot() the approach is as follows:\n\nWithin the ggplot(), create two histograms using the numeric age column. Create one for each of the two grouping values (in this case genders male and female). To do this, the data for each histogram are specified within their respective geom_histogram() commands, with the respective filters applied to linelist.\nOne graph will have positive count values, while the other will have its counts converted to negative values - this creates the “pyramid” with the 0 value in the middle of the plot. The negative values are created using a special ggplot2 term ..count.. and multiplying by -1.\nThe command coord_flip() switches the X and Y axes, resulting in the graphs turning vertical and creating the pyramid.\nLastly, the counts-axis value labels must be altered so they appear as “positive” counts on both sides of the pyramid (despite the underlying values on one side being negative).\n\nA simple version of this, using geom_histogram(), is below:\n\n  # begin ggplot\n  ggplot(mapping = aes(x = age, fill = gender)) +\n  \n  # female histogram\n  geom_histogram(data = linelist %&gt;% filter(gender == \"f\"),\n                 breaks = seq(0,85,5),\n                 colour = \"white\") +\n  \n  # male histogram (values converted to negative)\n  geom_histogram(data = linelist %&gt;% filter(gender == \"m\"),\n                 breaks = seq(0,85,5),\n                 mapping = aes(y = ..count..*(-1)),\n                 colour = \"white\") +\n  \n  # flip the X and Y axes\n  coord_flip() +\n  \n  # adjust counts-axis scale\n  scale_y_continuous(limits = c(-600, 900),\n                     breaks = seq(-600,900,100),\n                     labels = abs(seq(-600, 900, 100)))\n\n\n\n\n\n\n\n\nDANGER: If the limits of your counts axis are set too low, and a counts bar exceeds them, the bar will disappear entirely or be artificially shortened! Watch for this if analyzing data which is routinely updated. Prevent it by having your count-axis limits auto-adjust to your data, as below.\nThere are many things you can change/add to this simple version, including:\n\nAuto adjust counts-axis scale to your data (avoid errors discussed in warning below)\n\nManually specify colors and legend labels\n\nConvert counts to percents\nTo convert counts to percents (of total), do this in your data prior to plotting. Below, we get the age-gender counts, then ungroup(), and then mutate() to create new percent columns. If you want percents by gender, skip the ungroup step.\n\n# create dataset with proportion of total\npyramid_data &lt;- linelist %&gt;%\n  count(age_cat5,\n        gender,\n        name = \"counts\") %&gt;% \n  ungroup() %&gt;%                 # ungroup so percents are not by group\n  mutate(percent = round(100*(counts / sum(counts, na.rm=T)), digits = 1), \n         percent = case_when(\n            gender == \"f\" ~ percent,\n            gender == \"m\" ~ -percent,     # convert male to negative\n            TRUE          ~ NA_real_))    # NA val must by numeric as well\n\nImportantly, we save the max and min values so we know what the limits of the scale should be. These will be used in the ggplot() command below.\n\nmax_per &lt;- max(pyramid_data$percent, na.rm=T)\nmin_per &lt;- min(pyramid_data$percent, na.rm=T)\n\nmax_per\n\n[1] 10.9\n\nmin_per\n\n[1] -7.1\n\n\nFinally we make the ggplot() on the percent data. We specify scale_y_continuous() to extend the pre-defined lengths in each direction (positive and “negative”). We use floor() and ceiling() to round decimals the appropriate direction (down or up) for the side of the axis.\n\n# begin ggplot\n  ggplot()+  # default x-axis is age in years;\n\n  # case data graph\n  geom_col(data = pyramid_data,\n           mapping = aes(\n             x = age_cat5,\n             y = percent,\n             fill = gender),         \n           colour = \"white\")+       # white around each bar\n  \n  # flip the X and Y axes to make pyramid vertical\n  coord_flip()+\n  \n\n  # adjust the axes scales\n  # scale_x_continuous(breaks = seq(0,100,5), labels = seq(0,100,5)) +\n  scale_y_continuous(\n    limits = c(min_per, max_per),\n    breaks = seq(from = floor(min_per),                # sequence of values, by 2s\n                 to = ceiling(max_per),\n                 by = 2),\n    labels = paste0(abs(seq(from = floor(min_per),     # sequence of absolute values, by 2s, with \"%\"\n                            to = ceiling(max_per),\n                            by = 2)),\n                    \"%\"))+  \n\n  # designate colors and legend labels manually\n  scale_fill_manual(\n    values = c(\"f\" = \"orange\",\n               \"m\" = \"darkgreen\"),\n    labels = c(\"Female\", \"Male\")) +\n  \n  # label values (remember X and Y flipped now)\n  labs(\n    title = \"Age and gender of cases\",\n    x = \"Age group\",\n    y = \"Percent of total\",\n    fill = NULL,\n    caption = stringr::str_glue(\"Data are from linelist \\nn = {nrow(linelist)} (age or sex missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases) \\nData as of: {format(Sys.Date(), '%d %b %Y')}\")) +\n  \n  # display themes\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(colour = \"black\"),\n    plot.title = element_text(hjust = 0.5), \n    plot.caption = element_text(hjust=0, size=11, face = \"italic\")\n    )\n\n\n\n\n\n\n\n\n\n\n\nCompare to baseline\nWith the flexibility of ggplot(), you can have a second layer of bars in the background that represent the “true” or “baseline” population pyramid. This can provide a nice visualization to compare the observed with the baseline.\nImport and view the population data (see Download handbook and data page):\n\n# import the population demographics data\npop &lt;- rio::import(\"country_demographics.csv\")\n\n\n\n\n\n\n\nFirst some data management steps:\nHere we record the order of age categories that we want to appear. Due to some quirks the way the ggplot() is implemented, in this specific scenario it is easiest to store these as a character vector and use them later in the plotting function.\n\n# record correct age cat levels\nage_levels &lt;- c(\"0-4\",\"5-9\", \"10-14\", \"15-19\", \"20-24\",\n                \"25-29\",\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                \"50-54\", \"55-59\", \"60-64\", \"65-69\", \"70-74\",\n                \"75-79\", \"80-84\", \"85+\")\n\nCombine the population and case data through the dplyr function bind_rows():\n\nFirst, ensure they have the exact same column names, age categories values, and gender values\n\nMake them have the same data structure: columns of age category, gender, counts, and percent of total\n\nBind them together, one on-top of the other (bind_rows())\n\n\n# create/transform populaton data, with percent of total\n########################################################\npop_data &lt;- pop %&gt;% \n  pivot_longer(      # pivot gender columns longer\n    cols = c(m, f),\n    names_to = \"gender\",\n    values_to = \"counts\") %&gt;% \n  \n  mutate(\n    percent  = round(100*(counts / sum(counts, na.rm=T)),1),  # % of total\n    percent  = case_when(                                                        \n     gender == \"f\" ~ percent,\n     gender == \"m\" ~ -percent,               # if male, convert % to negative\n     TRUE          ~ NA_real_))\n\nReview the changed population dataset\n\n\n\n\n\n\nNow implement the same for the case linelist. Slightly different because it begins with case-rows, not counts.\n\n# create case data by age/gender, with percent of total\n#######################################################\ncase_data &lt;- linelist %&gt;%\n  count(age_cat5, gender, name = \"counts\") %&gt;%  # counts by age-gender groups\n  ungroup() %&gt;% \n  mutate(\n    percent = round(100*(counts / sum(counts, na.rm=T)),1),  # calculate % of total for age-gender groups\n    percent = case_when(                                     # convert % to negative if male\n      gender == \"f\" ~ percent,\n      gender == \"m\" ~ -percent,\n      TRUE          ~ NA_real_))\n\nReview the changed case dataset\n\n\n\n\n\n\nNow the two data frames are combined, one on top of the other (they have the same column names). We can “name” each of the data frame, and use the .id = argument to create a new column “data_source” that will indicate which data frame each row originated from. We can use this column to filter in the ggplot().\n\n# combine case and population data (same column names, age_cat values, and gender values)\npyramid_data &lt;- bind_rows(\"cases\" = case_data, \"population\" = pop_data, .id = \"data_source\")\n\nStore the maximum and minimum percent values, used in the plotting function to define the extent of the plot (and not cut short any bars!)\n\n# Define extent of percent axis, used for plot limits\nmax_per &lt;- max(pyramid_data$percent, na.rm=T)\nmin_per &lt;- min(pyramid_data$percent, na.rm=T)\n\nNow the plot is made with ggplot():\n\nOne bar graph of population data (wider, more transparent bars)\nOne bar graph of case data (small, more solid bars)\n\n\n# begin ggplot\n##############\nggplot()+  # default x-axis is age in years;\n\n  # population data graph\n  geom_col(\n    data = pyramid_data %&gt;% filter(data_source == \"population\"),\n    mapping = aes(\n      x = age_cat5,\n      y = percent,\n      fill = gender),\n    colour = \"black\",                               # black color around bars\n    alpha = 0.2,                                    # more transparent\n    width = 1)+                                     # full width\n  \n  # case data graph\n  geom_col(\n    data = pyramid_data %&gt;% filter(data_source == \"cases\"), \n    mapping = aes(\n      x = age_cat5,                               # age categories as original X axis\n      y = percent,                                # % as original Y-axis\n      fill = gender),                             # fill of bars by gender\n    colour = \"black\",                               # black color around bars\n    alpha = 1,                                      # not transparent \n    width = 0.3)+                                   # half width\n  \n  # flip the X and Y axes to make pyramid vertical\n  coord_flip()+\n  \n  # manually ensure that age-axis is ordered correctly\n  scale_x_discrete(limits = age_levels)+     # defined in chunk above\n  \n  # set percent-axis \n  scale_y_continuous(\n    limits = c(min_per, max_per),                                          # min and max defined above\n    breaks = seq(floor(min_per), ceiling(max_per), by = 2),                # from min% to max% by 2 \n    labels = paste0(                                                       # for the labels, paste together... \n              abs(seq(floor(min_per), ceiling(max_per), by = 2)), \"%\"))+                                                  \n\n  # designate colors and legend labels manually\n  scale_fill_manual(\n    values = c(\"f\" = \"orange\",         # assign colors to values in the data\n               \"m\" = \"darkgreen\"),\n    labels = c(\"f\" = \"Female\",\n               \"m\"= \"Male\"),      # change labels that appear in legend, note order\n  ) +\n\n  # plot labels, titles, caption    \n  labs(\n    title = \"Case age and gender distribution,\\nas compared to baseline population\",\n    subtitle = \"\",\n    x = \"Age category\",\n    y = \"Percent of total\",\n    fill = NULL,\n    caption = stringr::str_glue(\"Cases shown on top of country demographic baseline\\nCase data are from linelist, n = {nrow(linelist)}\\nAge or gender missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases\\nCase data as of: {format(max(linelist$date_onset, na.rm=T), '%d %b %Y')}\")) +\n  \n  # optional aesthetic themes\n  theme(\n    legend.position = \"bottom\",                             # move legend to bottom\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(colour = \"black\"),\n    plot.title = element_text(hjust = 0), \n    plot.caption = element_text(hjust=0, size=11, face = \"italic\"))",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Demographic pyramids and Likert-scales</span>"
    ]
  },
  {
    "objectID": "new_pages/age_pyramid.html#likert-scale",
    "href": "new_pages/age_pyramid.html#likert-scale",
    "title": "33  Demographic pyramids and Likert-scales",
    "section": "33.4 Likert scale",
    "text": "33.4 Likert scale\nThe techniques used to make a population pyramid with ggplot() can also be used to make plots of Likert-scale survey data.\nImport the data (see Download handbook and data page if desired).\n\n# import the likert survey response data\nlikert_data &lt;- rio::import(\"likert_data.csv\")\n\nStart with data that looks like this, with a categorical classification of each respondent (status) and their answers to 8 questions on a 4-point Likert-type scale (“Very poor”, “Poor”, “Good”, “Very good”).\n\n\n\n\n\n\nFirst, some data management steps:\n\nPivot the data longer\n\nCreate new column direction depending on whether response was generally “positive” or “negative”\n\nSet the Factor level order for the status column and the Response column\n\nStore the max count value so limits of plot are appropriate\n\n\nmelted &lt;- likert_data %&gt;% \n  pivot_longer(\n    cols = Q1:Q8,\n    names_to = \"Question\",\n    values_to = \"Response\") %&gt;% \n  mutate(\n    \n    direction = case_when(\n      Response %in% c(\"Poor\",\"Very Poor\")  ~ \"Negative\",\n      Response %in% c(\"Good\", \"Very Good\") ~ \"Positive\",\n      TRUE                                 ~ \"Unknown\"),\n    \n    status = fct_relevel(status, \"Junior\", \"Intermediate\", \"Senior\"),\n    \n    # must reverse 'Very Poor' and 'Poor' for ordering to work\n    Response = fct_relevel(Response, \"Very Good\", \"Good\", \"Very Poor\", \"Poor\")) \n\n# get largest value for scale limits\nmelted_max &lt;- melted %&gt;% \n  count(status, Question) %&gt;% # get counts\n  pull(n) %&gt;%                 # column 'n'\n  max(na.rm=T)                # get max\n\nNow make the plot. As in the age pyramids above, we are creating two bar plots and inverting the values of one of them to negative.\nWe use geom_bar() because our data are one row per observation, not aggregated counts. We use the special ggplot2 term ..count.. in one of the bar plots to invert the values negative (*-1), and we set position = \"stack\" so the values stack on top of each other.\n\n# make plot\nggplot()+\n     \n  # bar graph of the \"negative\" responses \n     geom_bar(\n       data = melted %&gt;% filter(direction == \"Negative\"),\n       mapping = aes(\n         x = status,\n         y = ..count..*(-1),    # counts inverted to negative\n         fill = Response),\n       color = \"black\",\n       closed = \"left\",\n       position = \"stack\")+\n     \n     # bar graph of the \"positive responses\n     geom_bar(\n       data = melted %&gt;% filter(direction == \"Positive\"),\n       mapping = aes(\n         x = status,\n         fill = Response),\n       colour = \"black\",\n       closed = \"left\",\n       position = \"stack\")+\n     \n     # flip the X and Y axes\n     coord_flip()+\n  \n     # Black vertical line at 0\n     geom_hline(yintercept = 0, color = \"black\", size=1)+\n     \n    # convert labels to all positive numbers\n    scale_y_continuous(\n      \n      # limits of the x-axis scale\n      limits = c(-ceiling(melted_max/10)*11,    # seq from neg to pos by 10, edges rounded outward to nearest 5\n                 ceiling(melted_max/10)*10),   \n      \n      # values of the x-axis scale\n      breaks = seq(from = -ceiling(melted_max/10)*10,\n                   to = ceiling(melted_max/10)*10,\n                   by = 10),\n      \n      # labels of the x-axis scale\n      labels = abs(unique(c(seq(-ceiling(melted_max/10)*10, 0, 10),\n                            seq(0, ceiling(melted_max/10)*10, 10))))) +\n     \n    # color scales manually assigned \n    scale_fill_manual(\n      values = c(\"Very Good\"  = \"green4\", # assigns colors\n                \"Good\"      = \"green3\",\n                \"Poor\"      = \"yellow\",\n                \"Very Poor\" = \"red3\"),\n      breaks = c(\"Very Good\", \"Good\", \"Poor\", \"Very Poor\"))+ # orders the legend\n     \n    \n     \n    # facet the entire plot so each question is a sub-plot\n    facet_wrap( ~ Question, ncol = 3)+\n     \n    # labels, titles, caption\n    labs(\n      title = str_glue(\"Likert-style responses\\nn = {nrow(likert_data)}\"),\n      x = \"Respondent status\",\n      y = \"Number of responses\",\n      fill = \"\")+\n\n     # display adjustments \n     theme_minimal()+\n     theme(axis.text = element_text(size = 12),\n           axis.title = element_text(size = 14, face = \"bold\"),\n           strip.text = element_text(size = 14, face = \"bold\"),  # facet sub-titles\n           plot.title = element_text(size = 20, face = \"bold\"),\n           panel.background = element_rect(fill = NA, color = \"black\")) # black box around each facet",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Demographic pyramids and Likert-scales</span>"
    ]
  },
  {
    "objectID": "new_pages/age_pyramid.html#resources",
    "href": "new_pages/age_pyramid.html#resources",
    "title": "33  Demographic pyramids and Likert-scales",
    "section": "33.5 Resources",
    "text": "33.5 Resources\napyramid documentation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Demographic pyramids and Likert-scales</span>"
    ]
  },
  {
    "objectID": "new_pages/heatmaps.html",
    "href": "new_pages/heatmaps.html",
    "title": "34  Heat plots",
    "section": "",
    "text": "34.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Heat plots</span>"
    ]
  },
  {
    "objectID": "new_pages/heatmaps.html#preparation",
    "href": "new_pages/heatmaps.html#preparation",
    "title": "34  Heat plots",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  tidyverse,       # data manipulation and visualization\n  rio,             # importing data \n  lubridate        # working with dates\n  )\n\nDatasets\nThis page utilizes the case linelist of a simulated outbreak for the transmission matrix section, and a separate dataset of daily malaria case counts by facility for the metrics tracking section. They are loaded and cleaned in their individual sections.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Heat plots</span>"
    ]
  },
  {
    "objectID": "new_pages/heatmaps.html#transmission-matrix",
    "href": "new_pages/heatmaps.html#transmission-matrix",
    "title": "34  Heat plots",
    "section": "34.2 Transmission matrix",
    "text": "34.2 Transmission matrix\nHeat tiles can be useful to visualize matrices. One example is to display “who-infected-whom” in an outbreak. This assumes that you have information on transmission events.\nNote that the [Contact tracing] page contains another example of making a heat tile contact matrix, using a different (perhaps more simple) dataset where the ages of cases and their sources are neatly aligned in the same row of the data frame. This same data is used to make a density map in the [ggplot tips] page. This example below begins from a case linelist and so involves considerable data manipulation prior to achieving a plotable data frame. So there are many scenarios to chose from…\nWe begin from the case linelist of a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import your data with the import() function from the rio package (it accepts many file types like .xlsx, .rds, .csv - see the Import and export page for details).\nThe first 50 rows of the linelist are shown below for demonstration:\n\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nIn this linelist:\n\nThere is one row per case, as identified by case_id\n\nThere is a later column infector that contains the case_id of the infector, who is also a case in the linelist\n\n\n\n\n\n\n\n\nData preparation\nObjective: We need to achieve a “long”-style data frame that contains one row per possible age-to-age transmission route, with a numeric column containing that row’s proportion of all observed transmission events in the linelist.\nThis will take several data manuipulation steps to achieve:\n\nMake cases data frame\nTo begin, we create a data frame of the cases, their ages, and their infectors - we call the data frame case_ages. The first 50 rows are displayed below.\n\ncase_ages &lt;- linelist %&gt;% \n  select(case_id, infector, age_cat) %&gt;% \n  rename(\"case_age_cat\" = \"age_cat\")\n\n\n\n\n\n\n\n\n\nMake infectors data frame\nNext, we create a data frame of the infectors - at the moment it consists of a single column. These are the infector IDs from the linelist. Not every case has a known infector, so we remove missing values. The first 50 rows are displayed below.\n\ninfectors &lt;- linelist %&gt;% \n  select(infector) %&gt;% \n  drop_na(infector)\n\n\n\n\n\n\n\nNext, we use joins to procure the ages of the infectors. This is not simple, because in the linelist, the infector’s ages are not listed as such. We achieve this result by joining the case linelist to the infectors. We begin with the infectors, and left_join() (add) the case linelist such that the infector id column left-side “baseline” data frame joins to the case_id column in the right-side linelist data frame.\nThus, the data from the infector’s case record in the linelist (including age) is added to the infector row. The 50 first rows are displayed below.\n\ninfector_ages &lt;- infectors %&gt;%             # begin with infectors\n  left_join(                               # add the linelist data to each infector  \n    linelist,\n    by = c(\"infector\" = \"case_id\")) %&gt;%    # match infector to their information as a case\n  select(infector, age_cat) %&gt;%            # keep only columns of interest\n  rename(\"infector_age_cat\" = \"age_cat\")   # rename for clarity\n\n\n\n\n\n\n\nThen, we combine the cases and their ages with the infectors and their ages. Each of these data frame has the column infector, so it is used for the join. The first rows are displayed below:\n\nages_complete &lt;- case_ages %&gt;%  \n  left_join(\n    infector_ages,\n    by = \"infector\") %&gt;%        # each has the column infector\n  drop_na()                     # drop rows with any missing data\n\nWarning in left_join(., infector_ages, by = \"infector\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 6 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\nBelow, a simple cross-tabulation of counts between the case and infector age groups. Labels added for clarity.\n\ntable(cases = ages_complete$case_age_cat,\n      infectors = ages_complete$infector_age_cat)\n\n       infectors\ncases   0-4 5-9 10-14 15-19 20-29 30-49 50-69 70+\n  0-4   105 156   105   114   143   117    13   0\n  5-9   102 132   110   102   117    96    12   5\n  10-14 104 109    91    79   120    80    12   4\n  15-19  85 105    82    39    75    69     7   5\n  20-29 101 127   109    80   143   107    22   4\n  30-49  72  97    56    54    98    61     4   5\n  50-69   5   6    15     9     7     5     2   0\n  70+     1   0     2     0     0     0     0   0\n\n\nWe can convert this table to a data frame with data.frame() from base R, which also automatically converts it to “long” format, which is desired for the ggplot(). The first rows are shown below.\n\nlong_counts &lt;- data.frame(table(\n    cases     = ages_complete$case_age_cat,\n    infectors = ages_complete$infector_age_cat))\n\n\n\n\n\n\n\nNow we do the same, but apply prop.table() from base R to the table so instead of counts we get proportions of the total. The first 50 rows are shown below.\n\nlong_prop &lt;- data.frame(prop.table(table(\n    cases = ages_complete$case_age_cat,\n    infectors = ages_complete$infector_age_cat)))\n\n\n\n\n\n\n\n\n\n\nCreate heat plot\nNow finally we can create the heat plot with ggplot2 package, using the geom_tile() function. See the ggplot tips page to learn more extensively about color/fill scales, especially the scale_fill_gradient() function.\n\nIn the aesthetics aes() of geom_tile() set the x and y as the case age and infector age\n\nAlso in aes() set the argument fill = to the Freq column - this is the value that will be converted to a tile color\n\nSet a scale color with scale_fill_gradient() - you can specify the high/low colors\n\nNote that scale_color_gradient() is different! In this case you want the fill\n\n\nBecause the color is made via “fill”, you can use the fill = argument in labs() to change the legend title\n\n\nggplot(data = long_prop)+       # use long data, with proportions as Freq\n  geom_tile(                    # visualize it in tiles\n    aes(\n      x = cases,         # x-axis is case age\n      y = infectors,     # y-axis is infector age\n      fill = Freq))+            # color of the tile is the Freq column in the data\n  scale_fill_gradient(          # adjust the fill color of the tiles\n    low = \"blue\",\n    high = \"orange\")+\n  labs(                         # labels\n    x = \"Case age\",\n    y = \"Infector age\",\n    title = \"Who infected whom\",\n    subtitle = \"Frequency matrix of transmission events\",\n    fill = \"Proportion of all\\ntranmsission events\"     # legend title\n  )",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Heat plots</span>"
    ]
  },
  {
    "objectID": "new_pages/heatmaps.html#reporting-metrics-over-time",
    "href": "new_pages/heatmaps.html#reporting-metrics-over-time",
    "title": "34  Heat plots",
    "section": "34.3 Reporting metrics over time",
    "text": "34.3 Reporting metrics over time\nOften in public health, one objective is to assess trends over time for many entities (facilities, jurisdictions, etc.). One way to visualize such trends over time is a heat plot where the x-axis is time and on the y-axis are the many entities.\n\nData preparation\nWe begin by importing a dataset of daily malaria reports from many facilities. The reports contain a date, province, district, and malaria counts. See the page on Download handbook and data for information on how to download these data. Below are the first 30 rows:\n\nfacility_count_data &lt;- import(\"malaria_facility_count_data.rds\")\n\n\n\n\n\n\n\n\nAggregate and summarize\nThe objective in this example is to transform the daily facility total malaria case counts (seen in previous tab) into weekly summary statistics of facility reporting performance - in this case the proportion of days per week that the facility reported any data. For this example we will show data only for Spring District.\nTo achieve this we will do the following data management steps:\n\nFilter the data as appropriate (by place, date)\n\nCreate a week column using floor_date() from package lubridate\n\nThis function returns the start-date of a given date’s week, using a specified start date of each week (e.g. “Mondays”)\n\n\nThe data are grouped by columns “location” and “week” to create analysis units of “facility-week”\n\nThe function summarise() creates new columns to reflecting summary statistics per facility-week group:\n\nNumber of days per week (7 - a static value)\n\nNumber of reports received from the facility-week (could be more than 7!)\n\nSum of malaria cases reported by the facility-week (just for interest)\n\nNumber of unique days in the facility-week for which there is data reported\n\nPercent of the 7 days per facility-week for which data was reported\n\n\nThe data frame is joined with right_join() to a comprehensive list of all possible facility-week combinations, to make the dataset complete. The matrix of all possible combinations is created by applying expand() to those two columns of the data frame as it is at that moment in the pipe chain (represented by .). Because a right_join() is used, all rows in the expand() data frame are kept, and added to agg_weeks if necessary. These new rows appear with NA (missing) summarized values.\n\nBelow we demonstrate step-by-step:\n\n# Create weekly summary dataset\nagg_weeks &lt;- facility_count_data %&gt;% \n  \n  # filter the data as appropriate\n  filter(\n    District == \"Spring\",\n    data_date &lt; as.Date(\"2020-08-01\")) \n\nNow the dataset has nrow(agg_weeks) rows, when it previously had nrow(facility_count_data).\nNext we create a week column reflecting the start date of the week for each record. This is achieved with the lubridate package and the function floor_date(), which is set to “week” and for the weeks to begin on Mondays (day 1 of the week - Sundays would be 7). The top rows are shown below.\n\nagg_weeks &lt;- agg_weeks %&gt;% \n  # Create week column from data_date\n  mutate(\n    week = lubridate::floor_date(                     # create new column of weeks\n      data_date,                                      # date column\n      unit = \"week\",                                  # give start of the week\n      week_start = 1))                                # weeks to start on Mondays \n\nThe new week column can be seen on the far right of the data frame\n\n\n\n\n\n\nNow we group the data into facility-weeks and summarise them to produce statistics per facility-week. See the page on Descriptive tables for tips. The grouping itself doesn’t change the data frame, but it impacts how the subsequent summary statistics are calculated.\nThe top rows are shown below. Note how the columns have completely changed to reflect the desired summary statistics. Each row reflects one facility-week.\n\nagg_weeks &lt;- agg_weeks %&gt;%   \n\n  # Group into facility-weeks\n  group_by(location_name, week) %&gt;%\n  \n  # Create summary statistics columns on the grouped data\n  summarize(\n    n_days          = 7,                                          # 7 days per week           \n    n_reports       = dplyr::n(),                                 # number of reports received per week (could be &gt;7)\n    malaria_tot     = sum(malaria_tot, na.rm = T),                # total malaria cases reported\n    n_days_reported = length(unique(data_date)),                  # number of unique days reporting per week\n    p_days_reported = round(100*(n_days_reported / n_days))) %&gt;%  # percent of days reporting\n\n  ungroup(location_name, week)                                    # ungroup so expand() works in next step\n\n\n\n\n\n\n\nFinally, we run the command below to ensure that ALL possible facility-weeks are present in the data, even if they were missing before.\nWe are using a right_join() on itself (the dataset is represented by “.”) but having been expanded to include all possible combinations of the columns week and location_name. See documentation on the expand() function in the page on Pivoting. Before running this code the dataset contains nrow(agg_weeks) rows.\n\n# Create data frame of every possible facility-week\nexpanded_weeks &lt;- agg_weeks %&gt;% \n  tidyr::expand(location_name, week)  # expand data frame to include all possible facility-week combinations\n\nHere is expanded_weeks, with 180 rows:\n\n\n\n\n\n\nBefore running this code, agg_weeks contains 107 rows.\n\n# Use a right-join with the expanded facility-week list to fill-in the missing gaps in the data\nagg_weeks &lt;- agg_weeks %&gt;%      \n  right_join(expanded_weeks) %&gt;%                            # Ensure every possible facility-week combination appears in the data\n  mutate(p_days_reported = replace_na(p_days_reported, 0))  # convert missing values to 0                           \n\nJoining with `by = join_by(location_name, week)`\n\n\nAfter running this code, agg_weeks contains nrow(agg_weeks) rows.\n\n\n\n\nCreate heat plot\nThe ggplot() is made using geom_tile() from the ggplot2 package:\n\nWeeks on the x-axis is transformed to dates, allowing use of scale_x_date()\n\nlocation_name on the y-axis will show all facility names\n\nThe fill is p_days_reported, the performance for that facility-week (numeric)\n\nscale_fill_gradient() is used on the numeric fill, specifying colors for high, low, and NA\n\nscale_x_date() is used on the x-axis specifying labels every 2 weeks and their format\n\nDisplay themes and labels can be adjusted as necessary\n\n\n\n\nBasic\nA basic heat plot is produced below, using the default colors, scales, etc. As explained above, within the aes() for geom_tile() you must provide an x-axis column, y-axis column, and a column for the the fill =. The fill is the numeric value that presents as tile color.\n\nggplot(data = agg_weeks)+\n  geom_tile(\n    aes(x = week,\n        y = location_name,\n        fill = p_days_reported))\n\n\n\n\n\n\n\n\n\n\nCleaned plot\nWe can make this plot look better by adding additional ggplot2 functions, as shown below. See the page on ggplot tips for details.\n\nggplot(data = agg_weeks)+ \n  \n  # show data as tiles\n  geom_tile(\n    aes(x = week,\n        y = location_name,\n        fill = p_days_reported),      \n    color = \"white\")+                 # white gridlines\n  \n  scale_fill_gradient(\n    low = \"orange\",\n    high = \"darkgreen\",\n    na.value = \"grey80\")+\n  \n  # date axis\n  scale_x_date(\n    expand = c(0,0),             # remove extra space on sides\n    date_breaks = \"2 weeks\",     # labels every 2 weeks\n    date_labels = \"%d\\n%b\")+     # format is day over month (\\n in newline)\n  \n  # aesthetic themes\n  theme_minimal()+                                  # simplify background\n  \n  theme(\n    legend.title = element_text(size=12, face=\"bold\"),\n    legend.text  = element_text(size=10, face=\"bold\"),\n    legend.key.height = grid::unit(1,\"cm\"),           # height of legend key\n    legend.key.width  = grid::unit(0.6,\"cm\"),         # width of legend key\n    \n    axis.text.x = element_text(size=12),              # axis text size\n    axis.text.y = element_text(vjust=0.2),            # axis text alignment\n    axis.ticks = element_line(size=0.4),               \n    axis.title = element_text(size=12, face=\"bold\"),  # axis title size and bold\n    \n    plot.title = element_text(hjust=0,size=14,face=\"bold\"),  # title right-aligned, large, bold\n    plot.caption = element_text(hjust = 0, face = \"italic\")  # caption right-aligned and italic\n    )+\n  \n  # plot labels\n  labs(x = \"Week\",\n       y = \"Facility name\",\n       fill = \"Reporting\\nperformance (%)\",           # legend title, because legend shows fill\n       title = \"Percent of days per week that facility reported data\",\n       subtitle = \"District health facilities, May-July 2020\",\n       caption = \"7-day weeks beginning on Mondays.\")\n\n\n\n\n\n\n\n\n\n\n\nOrdered y-axis\nCurrently, the facilities are ordered “alpha-numerically” from the bottom to the top. If you want to adjust the order the y-axis facilities, convert them to class factor and provide the order. See the page on Factors for tips.\nSince there are many facilities and we don’t want to write them all out, we will try another approach - ordering the facilities in a data frame and using the resulting column of names as the factor level order. Below, the column location_name is converted to a factor, and the order of its levels is set based on the total number of reporting days filed by the facility across the whole time-span.\nTo do this, we create a data frame which represents the total number of reports per facility, arranged in ascending order. We can use this vector to order the factor levels in the plot.\n\nfacility_order &lt;- agg_weeks %&gt;% \n  group_by(location_name) %&gt;% \n  summarize(tot_reports = sum(n_days_reported, na.rm=T)) %&gt;% \n  arrange(tot_reports) # ascending order\n\nSee the data frame below:\n\n\n\n\n\n\nNow use a column from the above data frame (facility_order$location_name) to be the order of the factor levels of location_name in the data frame agg_weeks:\n\n# load package \npacman::p_load(forcats)\n\n# create factor and define levels manually\nagg_weeks &lt;- agg_weeks %&gt;% \n  mutate(location_name = fct_relevel(\n    location_name, facility_order$location_name)\n    )\n\nAnd now the data are re-plotted, with location_name being an ordered factor:\n\nggplot(data = agg_weeks)+ \n  \n  # show data as tiles\n  geom_tile(\n    aes(x = week,\n        y = location_name,\n        fill = p_days_reported),      \n    color = \"white\")+                 # white gridlines\n  \n  scale_fill_gradient(\n    low = \"orange\",\n    high = \"darkgreen\",\n    na.value = \"grey80\")+\n  \n  # date axis\n  scale_x_date(\n    expand = c(0,0),             # remove extra space on sides\n    date_breaks = \"2 weeks\",     # labels every 2 weeks\n    date_labels = \"%d\\n%b\")+     # format is day over month (\\n in newline)\n  \n  # aesthetic themes\n  theme_minimal()+                                  # simplify background\n  \n  theme(\n    legend.title = element_text(size=12, face=\"bold\"),\n    legend.text  = element_text(size=10, face=\"bold\"),\n    legend.key.height = grid::unit(1,\"cm\"),           # height of legend key\n    legend.key.width  = grid::unit(0.6,\"cm\"),         # width of legend key\n    \n    axis.text.x = element_text(size=12),              # axis text size\n    axis.text.y = element_text(vjust=0.2),            # axis text alignment\n    axis.ticks = element_line(size=0.4),               \n    axis.title = element_text(size=12, face=\"bold\"),  # axis title size and bold\n    \n    plot.title = element_text(hjust=0,size=14,face=\"bold\"),  # title right-aligned, large, bold\n    plot.caption = element_text(hjust = 0, face = \"italic\")  # caption right-aligned and italic\n    )+\n  \n  # plot labels\n  labs(x = \"Week\",\n       y = \"Facility name\",\n       fill = \"Reporting\\nperformance (%)\",           # legend title, because legend shows fill\n       title = \"Percent of days per week that facility reported data\",\n       subtitle = \"District health facilities, May-July 2020\",\n       caption = \"7-day weeks beginning on Mondays.\")\n\n\n\n\n\n\n\n\n\n\n\nDisplay values\nYou can add a geom_text() layer on top of the tiles, to display the actual numbers of each tile. Be aware this may not look pretty if you have many small tiles!\nThe following code has been added: geom_text(aes(label = p_days_reported)). This adds text onto every tile. The text displayed is the value assigned to the argument label =, which in this case has been set to the same numeric column p_days_reported that is also used to create the color gradient.\n\nggplot(data = agg_weeks)+ \n  \n  # show data as tiles\n  geom_tile(\n    aes(x = week,\n        y = location_name,\n        fill = p_days_reported),      \n    color = \"white\")+                 # white gridlines\n  \n  # text\n  geom_text(\n    aes(\n      x = week,\n      y = location_name,\n      label = p_days_reported))+      # add text on top of tile\n  \n  # fill scale\n  scale_fill_gradient(\n    low = \"orange\",\n    high = \"darkgreen\",\n    na.value = \"grey80\")+\n  \n  # date axis\n  scale_x_date(\n    expand = c(0,0),             # remove extra space on sides\n    date_breaks = \"2 weeks\",     # labels every 2 weeks\n    date_labels = \"%d\\n%b\")+     # format is day over month (\\n in newline)\n  \n  # aesthetic themes\n  theme_minimal()+                                    # simplify background\n  \n  theme(\n    legend.title = element_text(size=12, face=\"bold\"),\n    legend.text  = element_text(size=10, face=\"bold\"),\n    legend.key.height = grid::unit(1,\"cm\"),           # height of legend key\n    legend.key.width  = grid::unit(0.6,\"cm\"),         # width of legend key\n    \n    axis.text.x = element_text(size=12),              # axis text size\n    axis.text.y = element_text(vjust=0.2),            # axis text alignment\n    axis.ticks = element_line(size=0.4),               \n    axis.title = element_text(size=12, face=\"bold\"),  # axis title size and bold\n    \n    plot.title = element_text(hjust=0,size=14,face=\"bold\"),  # title right-aligned, large, bold\n    plot.caption = element_text(hjust = 0, face = \"italic\")  # caption right-aligned and italic\n    )+\n  \n  # plot labels\n  labs(x = \"Week\",\n       y = \"Facility name\",\n       fill = \"Reporting\\nperformance (%)\",           # legend title, because legend shows fill\n       title = \"Percent of days per week that facility reported data\",\n       subtitle = \"District health facilities, May-July 2020\",\n       caption = \"7-day weeks beginning on Mondays.\")",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Heat plots</span>"
    ]
  },
  {
    "objectID": "new_pages/heatmaps.html#resources",
    "href": "new_pages/heatmaps.html#resources",
    "title": "34  Heat plots",
    "section": "34.4 Resources",
    "text": "34.4 Resources\nscale_fill_gradient()\nR graph gallery - heatmap",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Heat plots</span>"
    ]
  },
  {
    "objectID": "new_pages/diagrams.html",
    "href": "new_pages/diagrams.html",
    "title": "35  Diagrams and charts",
    "section": "",
    "text": "35.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Diagrams and charts</span>"
    ]
  },
  {
    "objectID": "new_pages/diagrams.html#preparation",
    "href": "new_pages/diagrams.html#preparation",
    "title": "35  Diagrams and charts",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  DiagrammeR,     # for flow diagrams\n  networkD3,      # For alluvial/Sankey diagrams\n  tidyverse)      # data management and visualization\n\n\n\nImport data\nMost of the content in this page does not require a dataset. However, in the Sankey diagram section, we will use the case linelist from a simulated Ebola epidemic. If you want to follow along for this part, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Diagrams and charts</span>"
    ]
  },
  {
    "objectID": "new_pages/diagrams.html#flow-diagrams",
    "href": "new_pages/diagrams.html#flow-diagrams",
    "title": "35  Diagrams and charts",
    "section": "35.2 Flow diagrams",
    "text": "35.2 Flow diagrams\nOne can use the R package DiagrammeR to create charts/flow charts. They can be static, or they can adjust somewhat dynamically based on changes in a dataset.\nTools\nThe function grViz() is used to create a “Graphviz” diagram. This function accepts a character string input containing instructions for making the diagram. Within that string, the instructions are written in a different language, called DOT - it is quite easy to learn the basics.\nBasic structure\n\nOpen the instructions grViz(\"\n\nSpecify directionality and name of the graph, and open brackets, e.g. digraph my_flow_chart {\nGraph statement (layout, rank direction)\n\nNodes statements (create nodes)\nEdges statements (gives links between nodes)\n\nClose the instructions }\")\n\n\nSimple examples\nBelow are two simple examples\nA very minimal example:\n\n# A minimal plot\nDiagrammeR::grViz(\"digraph {\n  \ngraph[layout = dot, rankdir = LR]\n\na\nb\nc\n\na -&gt; b -&gt; c\n}\")\n\n\n\n\n\nAn example with perhaps a bit more applied public health context:\n\ngrViz(\"                           # All instructions are within a large character string\ndigraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name \n  \n  # graph statement\n  #################\n  graph [layout = dot,\n         rankdir = TB,\n         overlap = true,\n         fontsize = 10]\n  \n  # nodes\n  #######\n  node [shape = circle,           # shape = circle\n       fixedsize = true\n       width = 1.3]               # width of circles\n  \n  Primary                         # names of nodes\n  Secondary\n  Tertiary\n\n  # edges\n  #######\n  Primary   -&gt; Secondary [label = ' case transfer']\n  Secondary -&gt; Tertiary [label = ' case transfer']\n}\n\")\n\n\n\n\n\n\n\nSyntax\nBasic syntax\nNode names, or edge statements, can be separated with spaces, semicolons, or newlines.\nRank direction\nA plot can be re-oriented to move left-to-right by adjusting the rankdir argument within the graph statement. The default is TB (top-to-bottom), but it can be LR (left-to-right), RL, or BT.\nNode names\nNode names can be single words, as in the simple example above. To use multi-word names or special characters (e.g. parentheses, dashes), put the node name within single quotes (’ ’). It may be easier to have a short node name, and assign a label, as shown below within brackets [ ]. If you want to have a newline within the node’s name, you must do it via a label - use \\n in the node label within single quotes, as shown below.\nSubgroups\nWithin edge statements, subgroups can be created on either side of the edge with curly brackets ({ }). The edge then applies to all nodes in the bracket - it is a shorthand.\nLayouts\n\ndot (set rankdir to either TB, LR, RL, BT, )\nneato\n\ntwopi\n\ncirco\n\nNodes - editable attributes\n\nlabel (text, in single quotes if multi-word)\n\nfillcolor (many possible colors)\n\nfontcolor\n\nalpha (transparency 0-1)\n\nshape (ellipse, oval, diamond, egg, plaintext, point, square, triangle)\n\nstyle\n\nsides\n\nperipheries\n\nfixedsize (h x w)\n\nheight\n\nwidth\n\ndistortion\n\npenwidth (width of shape border)\n\nx (displacement left/right)\n\ny (displacement up/down)\n\nfontname\n\nfontsize\n\nicon\n\nEdges - editable attributes\n\narrowsize\n\narrowhead (normal, box, crow, curve, diamond, dot, inv, none, tee, vee)\n\narrowtail\n\ndir (direction, )\n\nstyle (dashed, …)\n\ncolor\n\nalpha\n\nheadport (text in front of arrowhead)\n\ntailport (text in behind arrowtail)\n\nfontname\n\nfontsize\n\nfontcolor\n\npenwidth (width of arrow)\n\nminlen (minimum length)\n\nColor names: hexadecimal values or ‘X11’ color names, see here for X11 details\n\n\nComplex examples\nThe example below expands on the surveillance_diagram, adding complex node names, grouped edges, colors and styling\nDiagrammeR::grViz(\"               # All instructions are within a large character string\ndigraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name \n  \n  # graph statement\n  #################\n  graph [layout = dot,\n         rankdir = TB,            # layout top-to-bottom\n         fontsize = 10]\n  \n\n  # nodes (circles)\n  #################\n  node [shape = circle,           # shape = circle\n       fixedsize = true\n       width = 1.3]                      \n  \n  Primary   [label = 'Primary\\nFacility'] \n  Secondary [label = 'Secondary\\nFacility'] \n  Tertiary  [label = 'Tertiary\\nFacility'] \n  SC        [label = 'Surveillance\\nCoordination',\n             fontcolor = darkgreen] \n  \n  # edges\n  #######\n  Primary   -&gt; Secondary [label = ' case transfer',\n                          fontcolor = red,\n                          color = red]\n  Secondary -&gt; Tertiary [label = ' case transfer',\n                          fontcolor = red,\n                          color = red]\n  \n  # grouped edge\n  {Primary Secondary Tertiary} -&gt; SC [label = 'case reporting',\n                                      fontcolor = darkgreen,\n                                      color = darkgreen,\n                                      style = dashed]\n}\n\")\n\n\n\n\n\n\nSub-graph clusters\nTo group nodes into boxed clusters, put them within the same named subgraph (subgraph name {}). To have each subgraph identified within a bounding box, begin the name of the subgraph with “cluster”, as shown with the 4 boxes below.\nDiagrammeR::grViz(\"             # All instructions are within a large character string\ndigraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name \n  \n  # graph statement\n  #################\n  graph [layout = dot,\n         rankdir = TB,            \n         overlap = true,\n         fontsize = 10]\n  \n\n  # nodes (circles)\n  #################\n  node [shape = circle,                  # shape = circle\n       fixedsize = true\n       width = 1.3]                      # width of circles\n  \n  subgraph cluster_passive {\n    Primary   [label = 'Primary\\nFacility'] \n    Secondary [label = 'Secondary\\nFacility'] \n    Tertiary  [label = 'Tertiary\\nFacility'] \n    SC        [label = 'Surveillance\\nCoordination',\n               fontcolor = darkgreen] \n  }\n  \n  # nodes (boxes)\n  ###############\n  node [shape = box,                     # node shape\n        fontname = Helvetica]            # text font in node\n  \n  subgraph cluster_active {\n    Active [label = 'Active\\nSurveillance'] \n    HCF_active [label = 'HCF\\nActive Search']\n  }\n  \n  subgraph cluster_EBD {\n    EBS [label = 'Event-Based\\nSurveillance (EBS)'] \n    'Social Media'\n    Radio\n  }\n  \n  subgraph cluster_CBS {\n    CBS [label = 'Community-Based\\nSurveillance (CBS)']\n    RECOs\n  }\n\n  \n  # edges\n  #######\n  {Primary Secondary Tertiary} -&gt; SC [label = 'case reporting']\n\n  Primary   -&gt; Secondary [label = 'case transfer',\n                          fontcolor = red]\n  Secondary -&gt; Tertiary [label = 'case transfer',\n                          fontcolor = red]\n  \n  HCF_active -&gt; Active\n  \n  {'Social Media' Radio} -&gt; EBS\n  \n  RECOs -&gt; CBS\n}\n\")\n\n\n\n\n\n\n\nNode shapes\nThe example below, borrowed from this tutorial, shows applied node shapes and a shorthand for serial edge connections\n\nDiagrammeR::grViz(\"digraph {\n\ngraph [layout = dot, rankdir = LR]\n\n# define the global styles of the nodes. We can override these in box if we wish\nnode [shape = rectangle, style = filled, fillcolor = Linen]\n\ndata1 [label = 'Dataset 1', shape = folder, fillcolor = Beige]\ndata2 [label = 'Dataset 2', shape = folder, fillcolor = Beige]\nprocess [label =  'Process \\n Data']\nstatistical [label = 'Statistical \\n Analysis']\nresults [label= 'Results']\n\n# edge definitions with the node IDs\n{data1 data2}  -&gt; process -&gt; statistical -&gt; results\n}\")\n\n\n\n\n\n\n\nOutputs\nHow to handle and save outputs\n\nOutputs will appear in RStudio’s Viewer pane, by default in the lower-right alongside Files, Plots, Packages, and Help.\n\nTo export you can “Save as image” or “Copy to clipboard” from the Viewer. The graphic will adjust to the specified size.\n\n\n\nParameterized figures\nHere is a quote from this tutorial: https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/\n“Parameterized figures: A great benefit of designing figures within R is that we are able to connect the figures directly with our analysis by reading R values directly into our flowcharts. For example, suppose you have created a filtering process which removes values after each stage of a process, you can have a figure show the number of values left in the dataset after each stage of your process. To do this we, you can use the @@X symbol directly within the figure, then refer to this in the footer of the plot using [X]:, where X is the a unique numeric index.”\nWe encourage you to review this tutorial if parameterization is something you are interested in.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Diagrams and charts</span>"
    ]
  },
  {
    "objectID": "new_pages/diagrams.html#alluvialsankey-diagrams",
    "href": "new_pages/diagrams.html#alluvialsankey-diagrams",
    "title": "35  Diagrams and charts",
    "section": "35.3 Alluvial/Sankey Diagrams",
    "text": "35.3 Alluvial/Sankey Diagrams\n\nLoad packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\nWe load the networkD3 package to produce the diagram, and also tidyverse for the data preparation steps.\n\npacman::p_load(\n  networkD3,\n  tidyverse)\n\n\n\nPlotting from dataset\nPlotting the connections in a dataset. Below we demonstrate using this package on the case linelist. Here is an online tutorial.\nWe begin by getting the case counts for each unique age category and hospital combination. We’ve removed values with missing age category for clarity. We also re-label the hospital and age_cat columns as source and target respectively. These will be the two sides of the alluvial diagram.\n\n# counts by hospital and age category\nlinks &lt;- linelist %&gt;% \n  drop_na(age_cat) %&gt;% \n  select(hospital, age_cat) %&gt;%\n  count(hospital, age_cat) %&gt;% \n  rename(source = hospital,\n         target = age_cat)\n\nThe dataset now look like this:\n\n\n\n\n\n\nNow we create a data frame of all the diagram nodes, under the column name. This consists of all the values for hospital and age_cat. Note that we ensure they are all class Character before combining them. and adjust the ID columns to be numbers instead of labels:\n\n# The unique node names\nnodes &lt;- data.frame(\n  name=c(as.character(links$source), as.character(links$target)) %&gt;% \n    unique()\n  )\n\nnodes  # print\n\n                                   name\n1                      Central Hospital\n2                     Military Hospital\n3                               Missing\n4                                 Other\n5                         Port Hospital\n6  St. Mark's Maternity Hospital (SMMH)\n7                                   0-4\n8                                   5-9\n9                                 10-14\n10                                15-19\n11                                20-29\n12                                30-49\n13                                50-69\n14                                  70+\n\n\nThe we edit the links data frame, which we created above with count(). We add two numeric columns IDsource and IDtarget which will actually reflect/create the links between the nodes. These columns will hold the rownumbers (position) of the source and target nodes. 1 is subtracted so that these position numbers begin at 0 (not 1).\n\n# match to numbers, not names\nlinks$IDsource &lt;- match(links$source, nodes$name)-1 \nlinks$IDtarget &lt;- match(links$target, nodes$name)-1\n\nThe links dataset now looks like this:\n\n\n\n\n\n\nNow plot the Sankey diagram with sankeyNetwork(). You can read more about each argument by running ?sankeyNetwork in the console. Note that unless you set iterations = 0 the order of your nodes may not be as expected.\n\n# plot\n######\np &lt;- sankeyNetwork(\n  Links = links,\n  Nodes = nodes,\n  Source = \"IDsource\",\n  Target = \"IDtarget\",\n  Value = \"n\",\n  NodeID = \"name\",\n  units = \"TWh\",\n  fontSize = 12,\n  nodeWidth = 30,\n  iterations = 0)        # ensure node order is as in data\np\n\n\n\n\n\nHere is an example where the patient Outcome is included as well. Note in the data preparation step we have to calculate the counts of cases between age and hospital, and separately between hospital and outcome - and then bind all these counts together with bind_rows().\n\n# counts by hospital and age category\nage_hosp_links &lt;- linelist %&gt;% \n  drop_na(age_cat) %&gt;% \n  select(hospital, age_cat) %&gt;%\n  count(hospital, age_cat) %&gt;% \n  rename(source = age_cat,          # re-name\n         target = hospital)\n\nhosp_out_links &lt;- linelist %&gt;% \n    drop_na(age_cat) %&gt;% \n    select(hospital, outcome) %&gt;% \n    count(hospital, outcome) %&gt;% \n    rename(source = hospital,       # re-name\n           target = outcome)\n\n# combine links\nlinks &lt;- bind_rows(age_hosp_links, hosp_out_links)\n\n# The unique node names\nnodes &lt;- data.frame(\n  name=c(as.character(links$source), as.character(links$target)) %&gt;% \n    unique()\n  )\n\n# Create id numbers\nlinks$IDsource &lt;- match(links$source, nodes$name)-1 \nlinks$IDtarget &lt;- match(links$target, nodes$name)-1\n\n# plot\n######\np &lt;- sankeyNetwork(Links = links,\n                   Nodes = nodes,\n                   Source = \"IDsource\",\n                   Target = \"IDtarget\",\n                   Value = \"n\",\n                   NodeID = \"name\",\n                   units = \"TWh\",\n                   fontSize = 12,\n                   nodeWidth = 30,\n                   iterations = 0)\np\n\n\n\n\n\nhttps://www.displayr.com/sankey-diagrams-r/",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Diagrams and charts</span>"
    ]
  },
  {
    "objectID": "new_pages/diagrams.html#event-timelines",
    "href": "new_pages/diagrams.html#event-timelines",
    "title": "35  Diagrams and charts",
    "section": "35.4 Event timelines",
    "text": "35.4 Event timelines\nTo make a timeline showing specific events, you can use the vistime package.\nSee this vignette\n\n# load package\npacman::p_load(vistime,  # make the timeline\n               plotly    # for interactive visualization\n               )\n\nHere is the events dataset we begin with:\n\n\n\n\n\n\n\np &lt;- vistime(data)    # apply vistime\n\nlibrary(plotly)\n\n# step 1: transform into a list\npp &lt;- plotly_build(p)\n\n# step 2: Marker size\nfor(i in 1:length(pp$x$data)){\n  if(pp$x$data[[i]]$mode == \"markers\") pp$x$data[[i]]$marker$size &lt;- 10\n}\n\n# step 3: text size\nfor(i in 1:length(pp$x$data)){\n  if(pp$x$data[[i]]$mode == \"text\") pp$x$data[[i]]$textfont$size &lt;- 10\n}\n\n\n# step 4: text position\nfor(i in 1:length(pp$x$data)){\n  if(pp$x$data[[i]]$mode == \"text\") pp$x$data[[i]]$textposition &lt;- \"right\"\n}\n\n#print\npp",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Diagrams and charts</span>"
    ]
  },
  {
    "objectID": "new_pages/diagrams.html#dags",
    "href": "new_pages/diagrams.html#dags",
    "title": "35  Diagrams and charts",
    "section": "35.5 DAGs",
    "text": "35.5 DAGs\nYou can build a DAG manually using the DiagammeR package and DOT language as described above.\nAlternatively, there are packages like ggdag and daggity\nIntroduction to DAGs ggdag vignette\nCausal inference with dags in R",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Diagrams and charts</span>"
    ]
  },
  {
    "objectID": "new_pages/diagrams.html#resources",
    "href": "new_pages/diagrams.html#resources",
    "title": "35  Diagrams and charts",
    "section": "35.6 Resources",
    "text": "35.6 Resources\nMuch of the above regarding the DOT language is adapted from the tutorial at this site\nAnother more in-depth tutorial on DiagammeR\nThis page on Sankey diagrams",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Diagrams and charts</span>"
    ]
  },
  {
    "objectID": "new_pages/combination_analysis.html",
    "href": "new_pages/combination_analysis.html",
    "title": "36  Combinations analysis",
    "section": "",
    "text": "36.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Combinations analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/combination_analysis.html#preparation",
    "href": "new_pages/combination_analysis.html#preparation",
    "title": "36  Combinations analysis",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  tidyverse,     # data management and visualization\n  UpSetR,        # special package for combination plots\n  ggupset)       # special package for combination plots\n\n\n\n\nImport data\nTo begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import case linelist \nlinelist_sym &lt;- import(\"linelist_cleaned.rds\")\n\nThis linelist includes five “yes/no” variables on reported symptoms. We will need to transform these variables a bit to use the ggupset package to make our plot. View the data (scroll to the right to see the symptoms variables).\n\n\n\n\n\n\n\n\n\nRe-format values\nTo align with the format expected by ggupset we convert the “yes” and “no” the the actual symptom name, using case_when() from dplyr. If “no”, we set the value as blank, so the values are either NA or the symptom.\n\n# create column with the symptoms named, separated by semicolons\nlinelist_sym_1 &lt;- linelist_sym %&gt;% \n\n  # convert the \"yes\" and \"no\" values into the symptom name itself\n  # if old value is \"yes\", new value is \"fever\", otherwise set to missing (NA)\nmutate(fever = ifelse(fever == \"yes\", \"fever\", NA), \n       chills = ifelse(chills == \"yes\", \"chills\", NA),\n       cough = ifelse(cough == \"yes\", \"cough\", NA),\n       aches = ifelse(aches == \"yes\", \"aches\", NA),\n       vomit = ifelse(vomit == \"yes\", \"vomit\", NA))\n\nNow we make two final columns:\n\nConcatenating (gluing together) all the symptoms of the patient (a character column)\n\nConvert the above column to class list, so it can be accepted by ggupset to make the plot\n\nSee the page on Characters and strings to learn more about the unite() function from stringr\n\nlinelist_sym_1 &lt;- linelist_sym_1 %&gt;% \n  unite(col = \"all_symptoms\",\n        c(fever, chills, cough, aches, vomit), \n        sep = \"; \",\n        remove = TRUE,\n        na.rm = TRUE) %&gt;% \n  mutate(\n    # make a copy of all_symptoms column, but of class \"list\" (which is required to use ggupset() in next step)\n    all_symptoms_list = as.list(strsplit(all_symptoms, \"; \"))\n    )\n\nView the new data. Note the two columns towards the right end - the pasted combined values, and the list",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Combinations analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/combination_analysis.html#ggupset",
    "href": "new_pages/combination_analysis.html#ggupset",
    "title": "36  Combinations analysis",
    "section": "36.2 ggupset",
    "text": "36.2 ggupset\nLoad the package\n\npacman::p_load(ggupset)\n\nCreate the plot. We begin with a ggplot() and geom_bar(), but then we add the special function scale_x_upset() from the ggupset.\n\nggplot(\n  data = linelist_sym_1,\n  mapping = aes(x = all_symptoms_list)) +\ngeom_bar() +\nscale_x_upset(\n  reverse = FALSE,\n  n_intersections = 10,\n  sets = c(\"fever\", \"chills\", \"cough\", \"aches\", \"vomit\"))+\nlabs(\n  title = \"Signs & symptoms\",\n  subtitle = \"10 most frequent combinations of signs and symptoms\",\n  caption = \"Caption here.\",\n  x = \"Symptom combination\",\n  y = \"Frequency in dataset\")\n\n\n\n\n\n\n\n\nMore information on ggupset can be found online or offline in the package documentation in your RStudio Help tab ?ggupset.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Combinations analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/combination_analysis.html#upsetr",
    "href": "new_pages/combination_analysis.html#upsetr",
    "title": "36  Combinations analysis",
    "section": "36.3 UpSetR",
    "text": "36.3 UpSetR\nThe UpSetR package allows more customization of the plot, but it can be more difficult to execute:\nLoad package\n\npacman::p_load(UpSetR)\n\nData cleaning\nWe must convert the linelist symptoms values to 1 / 0.\n\nlinelist_sym_2 &lt;- linelist_sym %&gt;% \n     # convert the \"yes\" and \"no\" values into 1s and 0s\n     mutate(fever = ifelse(fever == \"yes\", 1, 0), \n            chills = ifelse(chills == \"yes\", 1, 0),\n            cough = ifelse(cough == \"yes\", 1, 0),\n            aches = ifelse(aches == \"yes\", 1, 0),\n            vomit = ifelse(vomit == \"yes\", 1, 0))\n\nIf you are interested in a more efficient command, you can take advantage of the +() function, which converts to 1s and 0s based on a logical statement. This command utilizes the across() function to change multiple columns at once (read more in Cleaning data and core functions).\n\n# Efficiently convert \"yes\" to 1 and 0\nlinelist_sym_2 &lt;- linelist_sym %&gt;% \n  \n  # convert the \"yes\" and \"no\" values into 1s and 0s\n  mutate(across(c(fever, chills, cough, aches, vomit), .fns = ~+(.x == \"yes\")))\n\nNow make the plot using the custom function upset() - using only the symptoms columns. You must designate which “sets” to compare (the names of the symptom columns). Alternatively, use nsets = and order.by = \"freq\" to only show the top X combinations.\n\n# Make the plot\nlinelist_sym_2 %&gt;% \n  UpSetR::upset(\n       sets = c(\"fever\", \"chills\", \"cough\", \"aches\", \"vomit\"),\n       order.by = \"freq\",\n       sets.bar.color = c(\"blue\", \"red\", \"yellow\", \"darkgreen\", \"orange\"), # optional colors\n       empty.intersections = \"on\",\n       # nsets = 3,\n       number.angles = 0,\n       point.size = 3.5,\n       line.size = 2, \n       mainbar.y.label = \"Symptoms Combinations\",\n       sets.x.label = \"Patients with Symptom\")",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Combinations analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/combination_analysis.html#resources",
    "href": "new_pages/combination_analysis.html#resources",
    "title": "36  Combinations analysis",
    "section": "36.4 Resources",
    "text": "36.4 Resources\nThe github page on UpSetR\nA Shiny App version - you can upload your own data\n*documentation - difficult to interpret",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Combinations analysis</span>"
    ]
  },
  {
    "objectID": "new_pages/transmission_chains.html",
    "href": "new_pages/transmission_chains.html",
    "title": "37  Transmission chains",
    "section": "",
    "text": "37.1 Overview\nThe primary tool to handle, analyse and visualise transmission chains and contact tracing data is the package epicontacts, developed by the folks at RECON. Try out the interactive plot below by hovering over the nodes for more information, dragging them to move them and clicking on them to highlight downstream cases.\nWarning in epicontacts::make_epicontacts(linelist = linelist, contacts =\ncontacts, : Cycle(s) detected in the contact network: this may be unwanted",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Transmission chains</span>"
    ]
  },
  {
    "objectID": "new_pages/transmission_chains.html#preparation",
    "href": "new_pages/transmission_chains.html#preparation",
    "title": "37  Transmission chains",
    "section": "37.2 Preparation",
    "text": "37.2 Preparation\n\nLoad packages\nFirst load the standard packages required for data import and manipulation. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n   rio,          # File import\n   here,         # File locator\n   tidyverse,    # Data management + ggplot2 graphics\n   remotes       # Package installation from github\n)\n\nYou will require the development version of epicontacts, which can be installed from github using the p_install_github() function from pacman. You only need to run this command below once, not every time you use the package (thereafter, you can use p_load() as usual).\n\npacman::p_install_gh(\"reconhub/epicontacts@timeline\")\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the Download handbook and data page. The dataset is imported using the import() function from the rio package. See the page on Import and export for various ways to import data.\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.xlsx\")\n\nThe first 50 rows of the linelist are displayed below. Of particular interest are the columns case_id, generation, infector, and source.\n\n\n\n\n\n\n\n\nCreating an epicontacts object\nWe then need to create an epicontacts object, which requires two types of data:\n\na linelist documenting cases where columns are variables and rows correspond to unique cases\na list of edges defining links between cases on the basis of their unique IDs (these can be contacts, transmission events, etc.)\n\nAs we already have a linelist, we just need to create a list of edges between cases, more specifically between their IDs. We can extract transmission links from the linelist by linking the infector column with the case_id column. At this point we can also add “edge properties”, by which we mean any variable describing the link between the two cases, not the cases themselves. For illustration, we will add a location variable describing the location of the transmission event, and a duration variable describing the duration of the contact in days.\nIn the code below, the dplyr function transmute is similar to mutate, except it only keeps the columns we have specified within the function. The drop_na function will filter out any rows where the specified columns have an NA value; in this case, we only want to keep the rows where the infector is known.\n\n## generate contacts\ncontacts &lt;- linelist %&gt;%\n  transmute(\n    infector = infector,\n    case_id = case_id,\n    location = sample(c(\"Community\", \"Nosocomial\"), n(), TRUE),\n    duration = sample.int(10, n(), TRUE)\n  ) %&gt;%\n  drop_na(infector)\n\nWe can now create the epicontacts object using the make_epicontacts function. We need to specify which column in the linelist points to the unique case identifier, as well as which columns in the contacts point to the unique identifiers of the cases involved in each link. These links are directional in that infection is going from the infector to the case, so we need to specify the from and to arguments accordingly. We therefore also set the directed argument to TRUE, which will affect future operations.\n\n## generate epicontacts object\nepic &lt;- make_epicontacts(\n  linelist = linelist,\n  contacts = contacts,\n  id = \"case_id\",\n  from = \"infector\",\n  to = \"case_id\",\n  directed = TRUE\n)\n\nWarning in make_epicontacts(linelist = linelist, contacts = contacts, id =\n\"case_id\", : Cycle(s) detected in the contact network: this may be unwanted\n\n\nUpon examining the epicontacts objects, we can see that the case_id column in the linelist has been renamed to id and the case_id and infector columns in the contacts have been renamed to from and to. This ensures consistency in subsequent handling, visualisation and analysis operations.\n\n## view epicontacts object\nepic\n\n\n/// Epidemiological Contacts //\n\n  // class: epicontacts\n  // 5,888 cases in linelist; 3,800 contacts; directed \n\n  // linelist\n\n# A tibble: 5,888 × 30\n   id     generation date_infection date_onset date_hospitalisation date_outcome\n   &lt;chr&gt;       &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;     &lt;date&gt;               &lt;date&gt;      \n 1 5fe599          4 2014-05-08     2014-05-13 2014-05-15           NA          \n 2 8689b7          4 NA             2014-05-13 2014-05-14           2014-05-18  \n 3 11f8ea          2 NA             2014-05-16 2014-05-18           2014-05-30  \n 4 b8812a          3 2014-05-04     2014-05-18 2014-05-20           NA          \n 5 893f25          3 2014-05-18     2014-05-21 2014-05-22           2014-05-29  \n 6 be99c8          3 2014-05-03     2014-05-22 2014-05-23           2014-05-24  \n 7 07e3e8          4 2014-05-22     2014-05-27 2014-05-29           2014-06-01  \n 8 369449          4 2014-05-28     2014-06-02 2014-06-03           2014-06-07  \n 9 f393b4          4 NA             2014-06-05 2014-06-06           2014-06-18  \n10 1389ca          4 NA             2014-06-05 2014-06-07           2014-06-09  \n# ℹ 5,878 more rows\n# ℹ 24 more variables: outcome &lt;chr&gt;, gender &lt;chr&gt;, age &lt;dbl&gt;, age_unit &lt;chr&gt;,\n#   age_years &lt;dbl&gt;, age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;, hospital &lt;chr&gt;, lon &lt;dbl&gt;,\n#   lat &lt;dbl&gt;, infector &lt;chr&gt;, source &lt;chr&gt;, wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;,\n#   ct_blood &lt;dbl&gt;, fever &lt;chr&gt;, chills &lt;chr&gt;, cough &lt;chr&gt;, aches &lt;chr&gt;,\n#   vomit &lt;chr&gt;, temp &lt;dbl&gt;, time_admission &lt;chr&gt;, bmi &lt;dbl&gt;,\n#   days_onset_hosp &lt;dbl&gt;\n\n  // contacts\n\n# A tibble: 3,800 × 4\n   from   to     location   duration\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;         &lt;int&gt;\n 1 f547d6 5fe599 Community         2\n 2 f90f5f b8812a Nosocomial        1\n 3 11f8ea 893f25 Nosocomial        5\n 4 aec8ec be99c8 Community         3\n 5 893f25 07e3e8 Nosocomial        4\n 6 133ee7 369449 Community        10\n 7 996f3a 2978ac Nosocomial        7\n 8 133ee7 57a565 Community         8\n 9 37a6f6 fc15ef Community         2\n10 9f6884 2eaa9a Nosocomial        2\n# ℹ 3,790 more rows",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Transmission chains</span>"
    ]
  },
  {
    "objectID": "new_pages/transmission_chains.html#handling",
    "href": "new_pages/transmission_chains.html#handling",
    "title": "37  Transmission chains",
    "section": "37.3 Handling",
    "text": "37.3 Handling\n\nSubsetting\nThe subset() method for epicontacts objects allows for, among other things, filtering of networks based on properties of the linelist (“node attributes”) and the contacts database (“edge attributes”). These values must be passed as named lists to the respective argument. For example, in the code below we are keeping only the male cases in the linelist that have an infection date between April and July 2014 (dates are specified as ranges), and transmission links that occured in the hospital.\n\nsub_attributes &lt;- subset(\n  epic,\n  node_attribute = list(\n    gender = \"m\",\n    date_infection = as.Date(c(\"2014-04-01\", \"2014-07-01\"))\n  ), \n  edge_attribute = list(location = \"Nosocomial\")\n)\nsub_attributes\n\n\n/// Epidemiological Contacts //\n\n  // class: epicontacts\n  // 69 cases in linelist; 1,912 contacts; directed \n\n  // linelist\n\n# A tibble: 69 × 30\n   id     generation date_infection date_onset date_hospitalisation date_outcome\n   &lt;chr&gt;       &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;     &lt;date&gt;               &lt;date&gt;      \n 1 5fe599          4 2014-05-08     2014-05-13 2014-05-15           NA          \n 2 893f25          3 2014-05-18     2014-05-21 2014-05-22           2014-05-29  \n 3 2978ac          4 2014-05-30     2014-06-06 2014-06-08           2014-06-15  \n 4 57a565          4 2014-05-28     2014-06-13 2014-06-15           NA          \n 5 fc15ef          6 2014-06-14     2014-06-16 2014-06-17           2014-07-09  \n 6 99e8fa          7 2014-06-24     2014-06-28 2014-06-29           2014-07-09  \n 7 f327be          6 2014-06-14     2014-07-12 2014-07-13           2014-07-14  \n 8 90e5fe          5 2014-06-18     2014-07-13 2014-07-14           2014-07-16  \n 9 a47529          5 2014-06-13     2014-07-17 2014-07-18           2014-07-26  \n10 da8ecb          5 2014-06-20     2014-07-18 2014-07-20           2014-08-01  \n# ℹ 59 more rows\n# ℹ 24 more variables: outcome &lt;chr&gt;, gender &lt;chr&gt;, age &lt;dbl&gt;, age_unit &lt;chr&gt;,\n#   age_years &lt;dbl&gt;, age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;, hospital &lt;chr&gt;, lon &lt;dbl&gt;,\n#   lat &lt;dbl&gt;, infector &lt;chr&gt;, source &lt;chr&gt;, wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;,\n#   ct_blood &lt;dbl&gt;, fever &lt;chr&gt;, chills &lt;chr&gt;, cough &lt;chr&gt;, aches &lt;chr&gt;,\n#   vomit &lt;chr&gt;, temp &lt;dbl&gt;, time_admission &lt;chr&gt;, bmi &lt;dbl&gt;,\n#   days_onset_hosp &lt;dbl&gt;\n\n  // contacts\n\n# A tibble: 1,912 × 4\n   from   to     location   duration\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;         &lt;int&gt;\n 1 f90f5f b8812a Nosocomial        1\n 2 11f8ea 893f25 Nosocomial        5\n 3 893f25 07e3e8 Nosocomial        4\n 4 996f3a 2978ac Nosocomial        7\n 5 9f6884 2eaa9a Nosocomial        2\n 6 a75c7f 7f5a01 Nosocomial        1\n 7 ab634e 99e8fa Nosocomial        7\n 8 b799eb bc2adf Nosocomial        1\n 9 a15e13 f327be Nosocomial        8\n10 ea3740 90e5fe Nosocomial        2\n# ℹ 1,902 more rows\n\n\nWe can use the thin function to either filter the linelist to include cases that are found in the contacts by setting the argument what = \"linelist\", or filter the contacts to include cases that are found in the linelist by setting the argument what = \"contacts\". In the code below, we are further filtering the epicontacts object to keep only the transmission links involving the male cases infected between April and July which we had filtered for above. We can see that only two known transmission links fit that specification.\n\nsub_attributes &lt;- thin(sub_attributes, what = \"contacts\")\nnrow(sub_attributes$contacts)\n\n[1] 5\n\n\nIn addition to subsetting by node and edge attributes, networks can be pruned to only include components that are connected to certain nodes. The cluster_id argument takes a vector of case IDs and returns the linelist of individuals that are linked, directly or indirectly, to those IDs. In the code below, we can see that a total of 13 linelist cases are involved in the clusters containing 2ae019 and 71577a.\n\nsub_id &lt;- subset(epic, cluster_id = c(\"2ae019\",\"71577a\"))\nnrow(sub_id$linelist)\n\n[1] 13\n\n\nThe subset() method for epicontacts objects also allows filtering by cluster size using the cs, cs_min and cs_max arguments. In the code below, we are keeping only the cases linked to clusters of 10 cases or larger, and can see that 271 linelist cases are involved in such clusters.\n\nsub_cs &lt;- subset(epic, cs_min = 10)\nnrow(sub_cs$linelist)\n\n[1] 271\n\n\n\n\nAccessing IDs\nThe get_id() function retrieves information on case IDs in the dataset, and can be parameterized as follows:\n\nlinelist: IDs in the line list data\ncontacts: IDs in the contact dataset (“from” and “to” combined)\nfrom: IDs in the “from” column of contact datset\nto IDs in the “to” column of contact dataset\nall: IDs that appear anywhere in either dataset\ncommon: IDs that appear in both contacts dataset and line list\n\nFor example, what are the first ten IDs in the contacts dataset?\n\ncontacts_ids &lt;- get_id(epic, \"contacts\")\nhead(contacts_ids, n = 10)\n\n [1] \"f547d6\" \"f90f5f\" \"11f8ea\" \"aec8ec\" \"893f25\" \"133ee7\" \"996f3a\" \"37a6f6\"\n [9] \"9f6884\" \"4802b1\"\n\n\nHow many IDs are found in both the linelist and the contacts?\n\nlength(get_id(epic, \"common\"))\n\n[1] 4352",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Transmission chains</span>"
    ]
  },
  {
    "objectID": "new_pages/transmission_chains.html#visualization",
    "href": "new_pages/transmission_chains.html#visualization",
    "title": "37  Transmission chains",
    "section": "37.4 Visualization",
    "text": "37.4 Visualization\n\nBasic plotting\nAll visualisations of epicontacts objects are handled by the plot function. We will first filter the epicontacts object to include only the cases with onset dates in June 2014 using the subset function, and only include the contacts linked to those cases using the thin function.\n\n## subset epicontacts object\nsub &lt;- epic %&gt;%\n  subset(\n    node_attribute = list(date_onset = c(as.Date(c(\"2014-06-30\", \"2014-06-01\"))))\n  ) %&gt;%\n thin(\"contacts\")\n\nWe can then create the basic, interactive plot very simply as follows:\n\n## plot epicontacts object\nplot(\n  sub,\n  width = 700,\n  height = 700\n)\n\n\n\n\n\nYou can move the nodes around by dragging them, hover over them for more information and click on them to highlight connected cases.\nThere are a large number of arguments to further modify this plot. We will cover the main ones here, but check out the documentation via ?vis_epicontacts (the function called when using plot on an epicontacts object) to get a full description of the function arguments.\n\nVisualising node attributes\nNode color, node shape and node size can be mapped to a given column in the linelist using the node_color, node_shape and node_size arguments. This is similar to the aes syntax you may recognise from ggplot2.\nThe specific colors, shapes and sizes of nodes can be specified as follows:\n\nColors via the col_pal argument, either by providing a name list for manual specification of each color as done below, or by providing a color palette function such as colorRampPalette(c(\"black\", \"red\", \"orange\")), which would provide a gradient of colours between the ones specified.\nShapes by passing a named list to the shapes argument, specifying one shape for each unique element in the linelist column specified by the node_shape argument. See codeawesome for available shapes.\nSize by passing a size range of the nodes to the size_range argument.\n\nHere an example, where color represents the outcome, shape the gender and size the age:\n\nplot(\n  sub, \n  node_color = \"outcome\",\n  node_shape = \"gender\",\n  node_size = \"age\",\n  col_pal = c(Death = \"firebrick\", Recover = \"green\"),\n  shapes = c(f = \"female\", m = \"male\"),\n  size_range = c(40, 60),\n  height = 700,\n  width = 700\n)\n\n\n\n\n\n\n\nVisualising edge attributes\nEdge color, width and linetype can be mapped to a given column in the contacts dataframe using the edge_color, edge_width and edge_linetype arguments. The specific colors and widths of the edges can be specified as follows:\n\nColors via the edge_col_pal argument, in the same manner used for col_pal.\nWidths by passing a size range of the nodes to the width_range argument.\n\nHere an example:\n\nplot(\n  sub, \n  node_color = \"outcome\",\n  node_shape = \"gender\",\n  node_size = 'age',\n  col_pal = c(Death = \"firebrick\", Recover = \"green\"),\n  shapes = c(f = \"female\", m = \"male\"),\n  size_range = c(40, 60),\n  edge_color = 'location',\n  edge_linetype = 'location',\n  edge_width = 'duration',\n  edge_col_pal = c(Community = \"orange\", Nosocomial = \"purple\"),\n  width_range = c(1, 3),\n  height = 700,\n  width = 700\n)\n\n\n\n\n\n\n\n\nTemporal axis\nWe can also visualise the network along a temporal axis by mapping the x_axis argument to a column in the linelist. In the example below, the x-axis represents the date of symptom onset. We have also specified the arrow_size argument to ensure the arrows are not too large, and set label = FALSE to make the figure less cluttered.\n\nplot(\n  sub,\n  x_axis = \"date_onset\",\n  node_color = \"outcome\",\n  col_pal = c(Death = \"firebrick\", Recover = \"green\"),\n  arrow_size = 0.5,\n  node_size = 13,\n  label = FALSE,\n  height = 700,\n  width = 700\n)\n\n\n\n\n\nThere are a large number of additional arguments to futher specify how this network is visualised along a temporal axis, which you can check out via ?vis_temporal_interactive (the function called when using plot on an epicontacts object with x_axis specified). We’ll go through some below.\n\nSpecifying transmission tree shape\nThere are two main shapes that the transmission tree can assume, specified using the network_shape argument. The first is a branching shape as shown above, where a straight edge connects any two nodes. This is the most intuitive representation, however can result in overlapping edges in a densely connected network. The second shape is rectangle, which produces a tree resembling a phylogeny. For example:\n\nplot(\n  sub,\n  x_axis = \"date_onset\",\n  network_shape = \"rectangle\",\n  node_color = \"outcome\",\n  col_pal = c(Death = \"firebrick\", Recover = \"green\"),\n  arrow_size = 0.5,\n  node_size = 13,\n  label = FALSE,\n  height = 700,\n  width = 700\n)\n\n\n\n\n\nEach case node can be assigned a unique vertical position by toggling the position_dodge argument. The position of unconnected cases (i.e. with no reported contacts) is specified using the unlinked_pos argument.\n\nplot(\n  sub,\n  x_axis = \"date_onset\",\n  network_shape = \"rectangle\",\n  node_color = \"outcome\",\n  col_pal = c(Death = \"firebrick\", Recover = \"green\"),\n  position_dodge = TRUE,\n  unlinked_pos = \"bottom\",\n  arrow_size = 0.5,\n  node_size = 13,\n  label = FALSE,\n  height = 700,\n  width = 700\n)\n\n\n\n\n\nThe position of the parent node relative to the children nodes can be specified using the parent_pos argument. The default option is to place the parent node in the middle, however it can be placed at the bottom (parent_pos = 'bottom') or at the top (parent_pos = 'top').\n\nplot(\n  sub,\n  x_axis = \"date_onset\",\n  network_shape = \"rectangle\",\n  node_color = \"outcome\",\n  col_pal = c(Death = \"firebrick\", Recover = \"green\"),\n  parent_pos = \"top\",\n  arrow_size = 0.5,\n  node_size = 13,\n  label = FALSE,\n  height = 700,\n  width = 700\n)\n\n\n\n\n\n\n\nSaving plots and figures\nYou can save a plot as an interactive, self-contained html file with the visSave function from the VisNetwork package:\n\nplot(\n  sub,\n  x_axis = \"date_onset\",\n  network_shape = \"rectangle\",\n  node_color = \"outcome\",\n  col_pal = c(Death = \"firebrick\", Recover = \"green\"),\n  parent_pos = \"top\",\n  arrow_size = 0.5,\n  node_size = 13,\n  label = FALSE,\n  height = 700,\n  width = 700\n) %&gt;%\n  visNetwork::visSave(\"network.html\")\n\nSaving these network outputs as an image is unfortunately less easy and requires you to save the file as an html and then take a screenshot of this file using the webshot package. In the code below, we are converting the html file saved above into a PNG:\n\nwebshot(url = \"network.html\", file = \"network.png\")\n\n\n\n\nTimelines\nYou can also case timelines to the network, which are represented on the x-axis of each case. This can be used to visualise case locations, for example, or time to outcome. To generate a timeline, we need to create a data.frame of at least three columns indicating the case ID, the start date of the “event” and the end of date of the “event”. You can also add any number of other columns which can then be mapped to node and edge properties of the timeline. In the code below, we generate a timeline going from the date of symptom onset to the date of outcome, and keep the outcome and hospital variables which we use to define the node shape and colour. Note that you can have more than one timeline row/event per case, for example if a case is transferred between multiple hospitals.\n\n## generate timeline\ntimeline &lt;- linelist %&gt;%\n  transmute(\n    id = case_id,\n    start = date_onset,\n    end = date_outcome,\n    outcome = outcome,\n    hospital = hospital\n  )\n\nWe then pass the timeline element to the timeline argument. We can map timeline attributes to timeline node colours, shapes and sizes in the same way defined in previous sections, except that we have two nodes: the start and end node of each timeline, which have seperate arguments. For example, tl_start_node_color defines which timeline column is mapped to the colour of the start node, while tl_end_node_shape defines which timeline column is mapped to the shape of the end node. We can also map colour, width, linetype and labels to the timeline edge via the tl_edge_* arguments.\nSee ?vis_temporal_interactive (the function called when plotting an epicontacts object) for detailed documentation on the arguments. Each argument is annotated in the code below too:\n\n## define shapes\nshapes &lt;- c(\n  f = \"female\",\n  m = \"male\",\n  Death = \"user-times\",\n  Recover = \"heartbeat\",\n  \"NA\" = \"question-circle\"\n)\n\n## define colours\ncolours &lt;- c(\n  Death = \"firebrick\",\n  Recover = \"green\",\n  \"NA\" = \"grey\"\n)\n\n## make plot\nplot(\n  sub,\n  ## max x coordinate to date of onset\n  x_axis = \"date_onset\",\n  ## use rectangular network shape\n  network_shape = \"rectangle\",\n  ## mape case node shapes to gender column\n  node_shape = \"gender\",\n  ## we don't want to map node colour to any columns - this is important as the\n  ## default value is to map to node id, which will mess up the colour scheme\n  node_color = NULL,\n  ## set case node size to 30 (as this is not a character, node_size is not\n  ## mapped to a column but instead interpreted as the actual node size)\n  node_size = 30,\n  ## set transmission link width to 4 (as this is not a character, edge_width is\n  ## not mapped to a column but instead interpreted as the actual edge width)\n  edge_width = 4,\n  ## provide the timeline object\n  timeline = timeline,\n  ## map the shape of the end node to the outcome column in the timeline object\n  tl_end_node_shape = \"outcome\",\n  ## set the size of the end node to 15 (as this is not a character, this\n  ## argument is not mapped to a column but instead interpreted as the actual\n  ## node size)\n  tl_end_node_size = 15,\n  ## map the colour of the timeline edge to the hospital column\n  tl_edge_color = \"hospital\",\n  ## set the width of the timeline edge to 2 (as this is not a character, this\n  ## argument is not mapped to a column but instead interpreted as the actual\n  ## edge width)\n  tl_edge_width = 2,\n  ## map edge labels to the hospital variable\n  tl_edge_label = \"hospital\",\n  ## specify the shape for everyone node attribute (defined above)\n  shapes = shapes,\n  ## specify the colour palette (defined above)\n  col_pal = colours,\n  ## set the size of the arrow to 0.5\n  arrow_size = 0.5,\n  ## use two columns in the legend\n  legend_ncol = 2,\n  ## set font size\n  font_size = 15,\n  ## define formatting for dates\n  date_labels = c(\"%d %b %Y\"),\n  ## don't plot the ID labels below nodes\n  label = FALSE,\n  ## specify height\n  height = 1000,\n  ## specify width\n  width = 1200,\n  ## ensure each case node has a unique y-coordinate - this is very important\n  ## when using timelines, otherwise you will have overlapping timelines from\n  ## different cases\n  position_dodge = TRUE\n)\n\nWarning in assert_timeline(timeline, x, x_axis): 5865 timeline row(s) removed\nas ID not found in linelist or start/end date is NA",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Transmission chains</span>"
    ]
  },
  {
    "objectID": "new_pages/transmission_chains.html#analysis",
    "href": "new_pages/transmission_chains.html#analysis",
    "title": "37  Transmission chains",
    "section": "37.5 Analysis",
    "text": "37.5 Analysis\n\nSummarising\nWe can get an overview of some of the network properties using the summary function.\n\n## summarise epicontacts object\nsummary(epic)\n\n\n/// Overview //\n  // number of unique IDs in linelist: 5888\n  // number of unique IDs in contacts: 5511\n  // number of unique IDs in both: 4352\n  // number of contacts: 3800\n  // contacts with both cases in linelist: 56.868 %\n\n/// Degrees of the network //\n  // in-degree summary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  1.0000  0.5392  1.0000  1.0000 \n\n  // out-degree summary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.5392  1.0000  6.0000 \n\n  // in and out degree summary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.000   1.000   1.078   1.000   7.000 \n\n/// Attributes //\n  // attributes in linelist:\n generation date_infection date_onset date_hospitalisation date_outcome outcome gender age age_unit age_years age_cat age_cat5 hospital lon lat infector source wt_kg ht_cm ct_blood fever chills cough aches vomit temp time_admission bmi days_onset_hosp\n\n  // attributes in contacts:\n location duration\n\n\nFor example, we can see that only 57% of contacts have both cases in the linelist; this means that the we do not have linelist data on a significant number of cases involved in these transmission chains.\n\n\nPairwise characteristics\nThe get_pairwise() function allows processing of variable(s) in the line list according to each pair in the contact dataset. For the following example, date of onset of disease is extracted from the line list in order to compute the difference between disease date of onset for each pair. The value that is produced from this comparison represents the serial interval (si).\n\nsi &lt;- get_pairwise(epic, \"date_onset\")   \nsummary(si)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    5.00    9.00   11.01   15.00   99.00    1820 \n\ntibble(si = si) %&gt;%\n  ggplot(aes(si)) +\n  geom_histogram() +\n  labs(\n    x = \"Serial interval\",\n    y = \"Frequency\"\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1820 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe get_pairwise() will interpret the class of the column being used for comparison, and will adjust its method of comparing the values accordingly. For numbers and dates (like the si example above), the function will subtract the values. When applied to columns that are characters or categorical, get_pairwise() will paste values together. Because the function also allows for arbitrary processing (see “f” argument), these discrete combinations can be easily tabulated and analyzed.\n\nhead(get_pairwise(epic, \"gender\"), n = 10)\n\n [1] \"f -&gt; m\" NA       \"m -&gt; m\" NA       \"m -&gt; f\" \"f -&gt; f\" NA       \"f -&gt; m\"\n [9] NA       \"m -&gt; f\"\n\nget_pairwise(epic, \"gender\", f = table)\n\n           values.to\nvalues.from   f   m\n          f 464 516\n          m 510 468\n\nfisher.test(get_pairwise(epic, \"gender\", f = table))\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  get_pairwise(epic, \"gender\", f = table)\np-value = 0.03758\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.6882761 0.9892811\nsample estimates:\nodds ratio \n 0.8252575 \n\n\nHere, we see a significant association between transmission links and gender.\n\n\nIdentifying clusters\nThe get_clusters() function can be used for to identify connected components in an epicontacts object. First, we use it to retrieve a data.frame containing the cluster information:\n\nclust &lt;- get_clusters(epic, output = \"data.frame\")\ntable(clust$cluster_size)\n\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14 \n1536 1680 1182  784  545  342  308  208  171  100   99   24   26   42 \n\nggplot(clust, aes(cluster_size)) +\n  geom_bar() +\n  labs(\n    x = \"Cluster size\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nLet us look at the largest clusters. For this, we add cluster information to the epicontacts object and then subset it to keep only the largest clusters:\n\nepic &lt;- get_clusters(epic)\nmax_size &lt;- max(epic$linelist$cluster_size)\nplot(subset(epic, cs = max_size))\n\n\n\n\n\n\n\nCalculating degrees\nThe degree of a node corresponds to its number of edges or connections to other nodes. get_degree() provides an easy method for calculating this value for epicontacts networks. A high degree in this context indicates an individual who was in contact with many others. The type argument indicates that we want to count both the in-degree and out-degree, the only_linelist argument indicates that we only want to calculate the degree for cases in the linelist.\n\ndeg_both &lt;- get_degree(epic, type = \"both\", only_linelist = TRUE)\n\nWhich individuals have the ten most contacts?\n\nhead(sort(deg_both, decreasing = TRUE), 10)\n\n916d0a 858426 6833d7 f093ea 11f8ea 3a4372 38fc71 c8c4d5 a127a7 02d8fd \n     7      6      6      6      5      5      5      5      5      5 \n\n\nWhat is the mean number of contacts?\n\nmean(deg_both)\n\n[1] 1.078473",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Transmission chains</span>"
    ]
  },
  {
    "objectID": "new_pages/transmission_chains.html#resources",
    "href": "new_pages/transmission_chains.html#resources",
    "title": "37  Transmission chains",
    "section": "37.6 Resources",
    "text": "37.6 Resources\nThe epicontacts page provides an overview of the package functions and includes some more in-depth vignettes.\nThe github page can be used to raise issues and request features.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Transmission chains</span>"
    ]
  },
  {
    "objectID": "new_pages/phylogenetic_trees.html",
    "href": "new_pages/phylogenetic_trees.html",
    "title": "38  Phylogenetic trees",
    "section": "",
    "text": "38.1 Overview\nPhylogenetic trees are used to visualize and describe the relatedness and evolution of organisms based on the sequence of their genetic code.\nThey can be constructed from genetic sequences using distance-based methods (such as neighbor-joining method) or character-based methods (such as maximum likelihood and Bayesian Markov Chain Monte Carlo method). Next-generation sequencing (NGS) has become more affordable and is becoming more widely used in public health to describe pathogens causing infectious diseases. Portable sequencing devices decrease the turn around time and hold promises to make data available for the support of outbreak investigation in real-time. NGS data can be used to identify the origin or source of an outbreak strain and its propagation, as well as determine presence of antimicrobial resistance genes. To visualize the genetic relatedness between samples a phylogenetic tree is constructed.\nIn this page we will learn how to use the ggtree package, which allows for combined visualization of phylogenetic trees with additional sample data in form of a dataframe. This will enable us to observe patterns and improve understanding of the outbreak dynamic.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "new_pages/phylogenetic_trees.html#preparation",
    "href": "new_pages/phylogenetic_trees.html#preparation",
    "title": "38  Phylogenetic trees",
    "section": "38.2 Preparation",
    "text": "38.2 Preparation\n\nLoad packages\nThis code chunk shows the loading of required packages. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,             # import/export\n  here,            # relative file paths\n  tidyverse,       # general data management and visualization\n  ape,             # to import and export phylogenetic files\n  ggtree,          # to visualize phylogenetic files\n  treeio,          # to visualize phylogenetic files\n  ggnewscale)      # to add additional layers of color schemes\n\n\n\nImport data\nThe data for this page can be downloaded with the instructions on the Download handbook and data page.\nThere are several different formats in which a phylogenetic tree can be stored (eg. Newick, NEXUS, Phylip). A common one is the Newick file format (.nwk), which is the standard for representing trees in computer-readable form. This means an entire tree can be expressed in a string format such as “((t2:0.04,t1:0.34):0.89,(t5:0.37,(t4:0.03,t3:0.67):0.9):0.59);”, listing all nodes and tips and their relationship (branch length) to each other.\nNote: It is important to understand that the phylogenetic tree file in itself does not contain sequencing data, but is merely the result of the genetic distances between the sequences. We therefore cannot extract sequencing data from a tree file.\nFirst, we use the read.tree() function from ape package to import a Newick phylogenetic tree file in .txt format, and store it in a list object of class “phylo”. If necessary, use the here() function from the here package to specify the relative file path.\nNote: In this case the newick tree is saved as a .txt file for easier handling and downloading from Github.\n\ntree &lt;- ape::read.tree(\"Shigella_tree.txt\")\n\nWe inspect our tree object and see it contains 299 tips (or samples) and 236 nodes.\n\ntree\n\n\nPhylogenetic tree with 299 tips and 236 internal nodes.\n\nTip labels:\n  SRR5006072, SRR4192106, S18BD07865, S18BD00489, S17BD08906, S17BD05939, ...\nNode labels:\n  17, 29, 100, 67, 100, 100, ...\n\nRooted; includes branch lengths.\n\n\nSecond, we import a table stored as a .csv file with additional information for each sequenced sample, such as gender, country of origin and attributes for antimicrobial resistance, using the import() function from the rio package:\n\nsample_data &lt;- import(\"sample_data_Shigella_tree.csv\")\n\nBelow are the first 50 rows of the data:\n\n\n\n\n\n\n\n\nClean and inspect\nWe clean and inspect our data: In order to assign the correct sample data to the phylogenetic tree, the values in the column Sample_ID in the sample_data data frame need to match the tip.labels values in the tree file:\nWe check the formatting of the tip.labels in the tree file by looking at the first 6 entries using with head() from base R.\n\nhead(tree$tip.label) \n\n[1] \"SRR5006072\" \"SRR4192106\" \"S18BD07865\" \"S18BD00489\" \"S17BD08906\"\n[6] \"S17BD05939\"\n\n\nWe also make sure the first column in our sample_data data frame is Sample_ID. We look at the column names of our dataframe using colnames() from base R.\n\ncolnames(sample_data)   \n\n [1] \"Sample_ID\"                  \"serotype\"                  \n [3] \"Country\"                    \"Continent\"                 \n [5] \"Travel_history\"             \"Year\"                      \n [7] \"Belgium\"                    \"Source\"                    \n [9] \"Gender\"                     \"gyrA_mutations\"            \n[11] \"macrolide_resistance_genes\" \"MIC_AZM\"                   \n[13] \"MIC_CIP\"                   \n\n\nWe look at the Sample_IDs in the data frame to make sure the formatting is the same than in the tip.label (eg. letters are all capitals, no extra underscores _ between letters and numbers, etc.)\n\nhead(sample_data$Sample_ID) # we again inspect only the first 6 using head()\n\n[1] \"S17BD05944\" \"S15BD07413\" \"S18BD07247\" \"S19BD07384\" \"S18BD07338\"\n[6] \"S18BD02657\"\n\n\nWe can also compare if all samples are present in the tree file and vice versa by generating a logical vector of TRUE or FALSE where they do or do not match. These are not printed here, for simplicity.\n\nsample_data$Sample_ID %in% tree$tip.label\n\ntree$tip.label %in% sample_data$Sample_ID\n\nWe can use these vectors to show any sample IDs that are not on the tree (there are none).\n\nsample_data$Sample_ID[!tree$tip.label %in% sample_data$Sample_ID]\n\ncharacter(0)\n\n\nUpon inspection we can see that the format of Sample_ID in the dataframe corresponds to the format of sample names at the tip.labels. These do not have to be sorted in the same order to be matched.\nWe are ready to go!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "new_pages/phylogenetic_trees.html#simple-tree-visualization",
    "href": "new_pages/phylogenetic_trees.html#simple-tree-visualization",
    "title": "38  Phylogenetic trees",
    "section": "38.3 Simple tree visualization",
    "text": "38.3 Simple tree visualization\n\nDifferent tree layouts\nggtree offers many different layout formats and some may be more suitable for your specific purpose than others. Below are a few demonstrations. For other options see this online book.\nHere are some example tree layouts:\n\nggtree(tree)                                            # simple linear tree\nggtree(tree,  branch.length = \"none\")                   # simple linear tree with all tips aligned\nggtree(tree, layout=\"circular\")                         # simple circular tree\nggtree(tree, layout=\"circular\", branch.length = \"none\") # simple circular tree with all tips aligned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple tree plus sample data\nThe %&lt;+% operator is used to connect the sample_data data frame to the tree file. The most easy annotation of your tree is the addition of the sample names at the tips, as well as coloring of tip points and if desired the branches:\nHere is an example of a circular tree:\n\nggtree(tree, layout = \"circular\", branch.length = 'none') %&lt;+% sample_data + # %&lt;+% adds dataframe with sample data to tree\n  aes(color = Belgium)+                       # color the branches according to a variable in your dataframe\n  scale_color_manual(\n    name = \"Sample Origin\",                      # name of your color scheme (will show up in the legend like this)\n    breaks = c(\"Yes\", \"No\"),                     # the different options in your variable\n    labels = c(\"NRCSS Belgium\", \"Other\"),        # how you want the different options named in your legend, allows for formatting\n    values = c(\"blue\", \"black\"),                  # the color you want to assign to the variable \n    na.value = \"black\") +                        # color NA values in black as well\n  new_scale_color()+                             # allows to add an additional color scheme for another variable\n    geom_tippoint(\n      mapping = aes(color = Continent),          # tip color by continent. You may change shape adding \"shape = \"\n      size = 1.5)+                               # define the size of the point at the tip\n  scale_color_brewer(\n    name = \"Continent\",                    # name of your color scheme (will show up in the legend like this)\n    palette = \"Set1\",                      # we choose a set of colors coming with the brewer package\n    na.value = \"grey\") +                    # for the NA values we choose the color grey\n  geom_tiplab(                             # adds name of sample to tip of its branch \n    color = 'black',                       # (add as many text lines as you wish with + , but you may need to adjust offset value to place them next to each other)\n    offset = 1,\n    size = 1,\n    geom = \"text\",\n    align = TRUE)+    \n  ggtitle(\"Phylogenetic tree of Shigella sonnei\")+       # title of your graph\n  theme(\n    axis.title.x = element_blank(), # removes x-axis title\n    axis.title.y = element_blank(), # removes y-axis title\n    legend.title = element_text(    # defines font size and format of the legend title\n      face = \"bold\",\n      size = 12),   \n    legend.text=element_text(       # defines font size and format of the legend text\n      face = \"bold\",\n      size = 10),  \n    plot.title = element_text(      # defines font size and format of the plot title\n      size = 12,\n      face = \"bold\"),  \n    legend.position = \"bottom\",     # defines placement of the legend\n    legend.box = \"vertical\",        # defines placement of the legend\n    legend.margin = margin())   \n\n\n\n\n\n\n\n\nYou can export your tree plot with ggsave() as you would any other ggplot object. Written this way, ggsave() saves the last image produced to the file path you specify. Remember that you can use here() and relative file paths to easily save in subfolders, etc.\n\nggsave(\"example_tree_circular_1.png\", width = 12, height = 14)",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "new_pages/phylogenetic_trees.html#tree-manipulation",
    "href": "new_pages/phylogenetic_trees.html#tree-manipulation",
    "title": "38  Phylogenetic trees",
    "section": "38.4 Tree manipulation",
    "text": "38.4 Tree manipulation\nSometimes you may have a very large phylogenetic tree and you are only interested in one part of the tree. For example, if you produced a tree including historical or international samples to get a large overview of where your dataset might fit in the bigger picture. But then to look closer at your data you want to inspect only that portion of the bigger tree.\nSince the phylogenetic tree file is just the output of sequencing data analysis, we can not manipulate the order of the nodes and branches in the file itself. These have already been determined in previous analysis from the raw NGS data. We are able though to zoom into parts, hide parts and even subset part of the tree.\n\nZoom in\nIf you don’t want to “cut” your tree, but only inspect part of it more closely you can zoom in to view a specific part.\nFirst, we plot the entire tree in linear format and add numeric labels to each node in the tree.\n\np &lt;- ggtree(tree,) %&lt;+% sample_data +\n  geom_tiplab(size = 1.5) +                # labels the tips of all branches with the sample name in the tree file\n  geom_text2(\n    mapping = aes(subset = !isTip,\n                  label = node),\n    size = 5,\n    color = \"darkred\",\n    hjust = 1,\n    vjust = 1)                            # labels all the nodes in the tree\n\np  # print\n\n\n\n\n\n\n\n\nTo zoom in to one particular branch (sticking out to the right), use viewClade() on the ggtree object p and provide the node number to get a closer look:\n\nviewClade(p, node = 452)\n\n\n\n\n\n\n\n\n\n\nCollapsing branches\nHowever, we may want to ignore this branch and can collapse it at that same node (node nr. 452) using collapse(). This tree is defined as p_collapsed.\n\np_collapsed &lt;- collapse(p, node = 452)\np_collapsed\n\n\n\n\n\n\n\n\nFor clarity, when we print p_collapsed, we add a geom_point2() (a blue diamond) at the node of the collapsed branch.\n\np_collapsed + \ngeom_point2(aes(subset = (node == 452)),  # we assign a symbol to the collapsed node\n            size = 5,                     # define the size of the symbol\n            shape = 23,                   # define the shape of the symbol\n            fill = \"steelblue\")           # define the color of the symbol\n\n\n\n\n\n\n\n\n\n\nSubsetting a tree\nIf we want to make a more permanent change and create a new, reduced tree to work with we can subset part of it with tree_subset(). Then you can save it as new newick tree file or .txt file.\nFirst, we inspect the tree nodes and tip labels in order to decide what to subset.\n\nggtree(\n  tree,\n  branch.length = 'none',\n  layout = 'circular') %&lt;+% sample_data +               # we add the asmple data using the %&lt;+% operator\n  geom_tiplab(size = 1)+                                # label tips of all branches with sample name in tree file\n  geom_text2(\n    mapping = aes(subset = !isTip, label = node),\n    size = 3,\n    color = \"darkred\") +                                # labels all the nodes in the tree\n theme(\n   legend.position = \"none\",                            # removes the legend all together\n   axis.title.x = element_blank(),\n   axis.title.y = element_blank(),\n   plot.title = element_text(size = 12, face=\"bold\"))\n\n\n\n\n\n\n\n\nNow, say we have decided to subset the tree at node 528 (keep only tips within this branch after node 528) and we save it as a new sub_tree1 object:\n\nsub_tree1 &lt;- tree_subset(\n  tree,\n  node = 528)                                            # we subset the tree at node 528\n\nLets have a look at the subset tree 1:\n\nggtree(sub_tree1) +\n  geom_tiplab(size = 3) +\n  ggtitle(\"Subset tree 1\")\n\n\n\n\n\n\n\n\nYou can also subset based on one particular sample, specifying how many nodes “backwards” you want to include. Let’s subset the same part of the tree based on a sample, in this case S17BD07692, going back 9 nodes and we save it as a new sub_tree2 object:\n\nsub_tree2 &lt;- tree_subset(\n  tree,\n  \"S17BD07692\",\n  levels_back = 9) # levels back defines how many nodes backwards from the sample tip you want to go\n\nLets have a look at the subset tree 2:\n\nggtree(sub_tree2) +\n  geom_tiplab(size =3)  +\n  ggtitle(\"Subset tree 2\")\n\n\n\n\n\n\n\n\nYou can also save your new tree either as a Newick type or even a text file using the write.tree() function from ape package:\n\n# to save in .nwk format\nape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.nwk')\n\n# to save in .txt format\nape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.txt')\n\n\n\nRotating nodes in a tree\nAs mentioned before we cannot change the order of tips or nodes in the tree, as this is based on their genetic relatedness and is not subject to visual manipulation. But we can rote branches around nodes if that eases our visualization.\nFirst, we plot our new subset tree 2 with node labels to choose the node we want to manipulate and store it an a ggtree plot object p.\n\np &lt;- ggtree(sub_tree2) +  \n  geom_tiplab(size = 4) +\n  geom_text2(aes(subset=!isTip, label=node), # labels all the nodes in the tree\n             size = 5,\n             color = \"darkred\", \n             hjust = 1, \n             vjust = 1) \np\n\n\n\n\n\n\n\n\nWe can then manipulate nodes by applying ggtree::rotate() or ggtree::flip(): Note: to illustrate which nodes we are manipulating we first apply the geom_hilight() function from ggtree to highlight the samples in the nodes we are interested in and store that ggtree plot object in a new object p1.\n\np1 &lt;- p + geom_hilight(  # highlights node 39 in blue, \"extend =\" allows us to define the length of the color block\n  node = 39,\n  fill = \"steelblue\",\n  extend = 0.0017) +  \ngeom_hilight(            # highlights the node 37 in yellow\n  node = 37,\n  fill = \"yellow\",\n  extend = 0.0017) +               \nggtitle(\"Original tree\")\n\n\np1 # print\n\n\n\n\n\n\n\n\nNow we can rotate node 37 in object p1 so that the samples on node 38 move to the top. We store the rotated tree in a new object p2.\n\np2 &lt;- ggtree::rotate(p1, 37) + \n      ggtitle(\"Rotated Node 37\")\n\n\np2   # print\n\n\n\n\n\n\n\n\nOr we can use the flip command to rotate node 36 in object p1 and switch node 37 to the top and node 39 to the bottom. We store the flipped tree in a new object p3.\n\np3 &lt;-  flip(p1, 39, 37) +\n      ggtitle(\"Rotated Node 36\")\n\n\np3   # print\n\n\n\n\n\n\n\n\n\n\nExample subtree with sample data annotation\nLets say we are investigating the cluster of cases with clonal expansion which occurred in 2017 and 2018 at node 39 in our sub-tree. We add the year of strain isolation as well as travel history and color by country to see origin of other closely related strains:\n\nggtree(sub_tree2) %&lt;+% sample_data +     # we use th %&lt;+% operator to link to the sample_data\n  geom_tiplab(                          # labels the tips of all branches with the sample name in the tree file\n    size = 2.5,\n    offset = 0.001,\n    align = TRUE) + \n  theme_tree2()+\n  xlim(0, 0.015)+                       # set the x-axis limits of our tree\n  geom_tippoint(aes(color=Country),     # color the tip point by continent\n                size = 1.5)+ \n  scale_color_brewer(\n    name = \"Country\", \n    palette = \"Set1\", \n    na.value = \"grey\")+\n  geom_tiplab(                          # add isolation year as a text label at the tips\n    aes(label = Year),\n    color = 'blue',\n    offset = 0.0045,\n    size = 3,\n    linetype = \"blank\" ,\n    geom = \"text\",\n    align = TRUE)+ \n  geom_tiplab(                          # add travel history as a text label at the tips, in red color\n    aes(label = Travel_history),\n    color = 'red',\n    offset = 0.006,\n    size = 3,\n    linetype = \"blank\",\n    geom = \"text\",\n    align = TRUE)+ \n  ggtitle(\"Phylogenetic tree of Belgian S. sonnei strains with travel history\")+  # add plot title\n  xlab(\"genetic distance (0.001 = 4 nucleotides difference)\")+                    # add a label to the x-axis \n  theme(\n    axis.title.x = element_text(size = 10),\n    axis.title.y = element_blank(),\n    legend.title = element_text(face = \"bold\", size = 12),\n    legend.text = element_text(face = \"bold\", size = 10),\n    plot.title = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nOur observation points towards an import event of strains from Asia, which then circulated in Belgium over the years and seem to have caused our latest outbreak.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "new_pages/phylogenetic_trees.html#more-complex-trees-adding-heatmaps-of-sample-data",
    "href": "new_pages/phylogenetic_trees.html#more-complex-trees-adding-heatmaps-of-sample-data",
    "title": "38  Phylogenetic trees",
    "section": "More complex trees: adding heatmaps of sample data",
    "text": "More complex trees: adding heatmaps of sample data\nWe can add more complex information, such as categorical presence of antimicrobial resistance genes and numeric values for actually measured resistance to antimicrobials in form of a heatmap using the ggtree::gheatmap() function.\nFirst we need to plot our tree (this can be either linear or circular) and store it in a new ggtree plot object p: We will use the sub_tree from part 3.)\n\np &lt;- ggtree(sub_tree2, branch.length='none', layout='circular') %&lt;+% sample_data +\n  geom_tiplab(size =3) + \n theme(\n   legend.position = \"none\",\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    plot.title = element_text(\n      size = 12,\n      face = \"bold\",\n      hjust = 0.5,\n      vjust = -15))\np\n\n\n\n\n\n\n\n\nSecond, we prepare our data. To visualize different variables with new color schemes, we subset our dataframe to the desired variable. It is important to add the Sample_ID as rownames otherwise it cannot match the data to the tree tip.labels:\nIn our example we want to look at gender and mutations that could confer resistance to Ciprofloxacin, an important first line antibiotic used to treat Shigella infections.\nWe create a dataframe for gender:\n\ngender &lt;- data.frame(\"gender\" = sample_data[,c(\"Gender\")])\nrownames(gender) &lt;- sample_data$Sample_ID\n\nWe create a dataframe for mutations in the gyrA gene, which confer Ciprofloxacin resistance:\n\ncipR &lt;- data.frame(\"cipR\" = sample_data[,c(\"gyrA_mutations\")])\nrownames(cipR) &lt;- sample_data$Sample_ID\n\nWe create a dataframe for the measured minimum inhibitory concentration (MIC) for Ciprofloxacin from the laboratory:\n\nMIC_Cip &lt;- data.frame(\"mic_cip\" = sample_data[,c(\"MIC_CIP\")])\nrownames(MIC_Cip) &lt;- sample_data$Sample_ID\n\nWe create a first plot adding a binary heatmap for gender to the phylogenetic tree and storing it in a new ggtree plot object h1:\n\nh1 &lt;-  gheatmap(p, gender,                                 # we add a heatmap layer of the gender dataframe to our tree plot\n                offset = 10,                               # offset shifts the heatmap to the right,\n                width = 0.10,                              # width defines the width of the heatmap column,\n                color = NULL,                              # color defines the boarder of the heatmap columns\n         colnames = FALSE) +                               # hides column names for the heatmap\n  scale_fill_manual(name = \"Gender\",                       # define the coloring scheme and legend for gender\n                    values = c(\"#00d1b1\", \"purple\"),\n                    breaks = c(\"Male\", \"Female\"),\n                    labels = c(\"Male\", \"Female\")) +\n   theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 12),\n        legend.text = element_text(size = 10),\n        legend.box = \"vertical\", legend.margin = margin())\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nh1\n\n\n\n\n\n\n\n\nThen we add information on mutations in the gyrA gene, which confer resistance to Ciprofloxacin:\nNote: The presence of chromosomal point mutations in WGS data was prior determined using the PointFinder tool developed by Zankari et al. (see reference in the additional references section)\nFirst, we assign a new color scheme to our existing plot object h1 and store it in a now object h2. This enables us to define and change the colors for our second variable in the heatmap.\n\nh2 &lt;- h1 + new_scale_fill() \n\nThen we add the second heatmap layer to h2 and store the combined plots in a new object h3:\n\nh3 &lt;- gheatmap(h2, cipR,         # adds the second row of heatmap describing Ciprofloxacin resistance mutations\n               offset = 12, \n               width = 0.10, \n               colnames = FALSE) +\n  scale_fill_manual(name = \"Ciprofloxacin resistance \\n conferring mutation\",\n                    values = c(\"#fe9698\",\"#ea0c92\"),\n                    breaks = c( \"gyrA D87Y\", \"gyrA S83L\"),\n                    labels = c( \"gyrA d87y\", \"gyrA s83l\")) +\n   theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 12),\n        legend.text = element_text(size = 10),\n        legend.box = \"vertical\", legend.margin = margin())+\n  guides(fill = guide_legend(nrow = 2,byrow = TRUE))\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nh3\n\n\n\n\n\n\n\n\nWe repeat the above process, by first adding a new color scale layer to our existing object h3, and then adding the continuous data on the minimum inhibitory concentration (MIC) of Ciprofloxacin for each strain to the resulting object h4 to produce the final object h5:\n\n# First we add the new coloring scheme:\nh4 &lt;- h3 + new_scale_fill()\n\n# then we combine the two into a new plot:\nh5 &lt;- gheatmap(h4, MIC_Cip,  \n               offset = 14, \n               width = 0.10,\n                colnames = FALSE)+\n  scale_fill_continuous(name = \"MIC for Ciprofloxacin\",  # here we define a gradient color scheme for the continuous variable of MIC\n                      low = \"yellow\", high = \"red\",\n                      breaks = c(0, 0.50, 1.00),\n                      na.value = \"white\") +\n   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+\n   theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 12),\n        legend.text = element_text(size = 10),\n        legend.box = \"vertical\", legend.margin = margin())\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nh5\n\n\n\n\n\n\n\n\nWe can do the same exercise for a linear tree:\n\np &lt;- ggtree(sub_tree2) %&lt;+% sample_data +\n  geom_tiplab(size = 3) + # labels the tips\n  theme_tree2()+\n  xlab(\"genetic distance (0.001 = 4 nucleotides difference)\")+\n  xlim(0, 0.015)+\n theme(legend.position = \"none\",\n      axis.title.y = element_blank(),\n      plot.title = element_text(size = 12, \n                                face = \"bold\",\n                                hjust = 0.5,\n                                vjust = -15))\np\n\n\n\n\n\n\n\n\nFirst we add gender:\n\nh1 &lt;-  gheatmap(p, gender, \n                offset = 0.003,\n                width = 0.1, \n                color=\"black\", \n         colnames = FALSE)+\n  scale_fill_manual(name = \"Gender\",\n                    values = c(\"#00d1b1\", \"purple\"),\n                    breaks = c(\"Male\", \"Female\"),\n                    labels = c(\"Male\", \"Female\"))+\n   theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 12),\n        legend.text = element_text(size = 10),\n        legend.box = \"vertical\", legend.margin = margin())\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nh1\n\n\n\n\n\n\n\n\nThen we add Ciprofloxacin resistance mutations after adding another color scheme layer:\n\nh2 &lt;- h1 + new_scale_fill()\nh3 &lt;- gheatmap(h2, cipR,   \n               offset = 0.004, \n               width = 0.1,\n               color = \"black\",\n                colnames = FALSE)+\n  scale_fill_manual(name = \"Ciprofloxacin resistance \\n conferring mutation\",\n                    values = c(\"#fe9698\",\"#ea0c92\"),\n                    breaks = c( \"gyrA D87Y\", \"gyrA S83L\"),\n                    labels = c( \"gyrA d87y\", \"gyrA s83l\"))+\n   theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 12),\n        legend.text = element_text(size = 10),\n        legend.box = \"vertical\", legend.margin = margin())+\n  guides(fill = guide_legend(nrow = 2,byrow = TRUE))\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n h3\n\n\n\n\n\n\n\n\nThen we add the minimum inhibitory concentration determined by the laboratory (MIC):\n\nh4 &lt;- h3 + new_scale_fill()\nh5 &lt;- gheatmap(h4, MIC_Cip, \n               offset = 0.005,  \n               width = 0.1,\n               color = \"black\", \n                colnames = FALSE)+\n  scale_fill_continuous(name = \"MIC for Ciprofloxacin\",\n                      low = \"yellow\", high = \"red\",\n                      breaks = c(0,0.50,1.00),\n                      na.value = \"white\")+\n   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+\n   theme(legend.position = \"bottom\",\n        legend.title = element_text(size = 10),\n        legend.text = element_text(size = 8),\n        legend.box = \"horizontal\", legend.margin = margin())+\n  guides(shape = guide_legend(override.aes = list(size = 2)))\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\nh5",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "new_pages/phylogenetic_trees.html#resources",
    "href": "new_pages/phylogenetic_trees.html#resources",
    "title": "38  Phylogenetic trees",
    "section": "38.5 Resources",
    "text": "38.5 Resources\nhttp://hydrodictyon.eeb.uconn.edu/eebedia/index.php/Ggtree# Clade_Colors https://bioconductor.riken.jp/packages/3.2/bioc/vignettes/ggtree/inst/doc/treeManipulation.html https://guangchuangyu.github.io/ggtree-book/chapter-ggtree.html https://bioconductor.riken.jp/packages/3.8/bioc/vignettes/ggtree/inst/doc/treeManipulation.html\nEa Zankari, Rosa Allesøe, Katrine G Joensen, Lina M Cavaco, Ole Lund, Frank M Aarestrup, PointFinder: a novel web tool for WGS-based detection of antimicrobial resistance associated with chromosomal point mutations in bacterial pathogens, Journal of Antimicrobial Chemotherapy, Volume 72, Issue 10, October 2017, Pages 2764–2768, https://doi.org/10.1093/jac/dkx217",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Phylogenetic trees</span>"
    ]
  },
  {
    "objectID": "new_pages/interactive_plots.html",
    "href": "new_pages/interactive_plots.html",
    "title": "39  Interactive plots",
    "section": "",
    "text": "39.1 Preparation",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Interactive plots</span>"
    ]
  },
  {
    "objectID": "new_pages/interactive_plots.html#preparation",
    "href": "new_pages/interactive_plots.html#preparation",
    "title": "39  Interactive plots",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,       # import/export\n  here,      # filepaths\n  lubridate, # working with dates\n  plotly,    # interactive plots\n  scales,    # quick percents\n  tidyverse  # data management and visualization\n  ) \n\n\n\nStart with a ggplot()\nIn this page we assume that you are beginning with a ggplot() plot that you want to convert to be interactive. We will build several of these plots in this page, using the case linelist used in many pages of this handbook.\n\n\nImport data\nTo begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import case linelist \nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Interactive plots</span>"
    ]
  },
  {
    "objectID": "new_pages/interactive_plots.html#plot-with-ggplotly",
    "href": "new_pages/interactive_plots.html#plot-with-ggplotly",
    "title": "39  Interactive plots",
    "section": "39.2 Plot with ggplotly()",
    "text": "39.2 Plot with ggplotly()\nThe function ggplotly() from the plotly package makes it easy to convert a ggplot() to be interactive. Simply save your ggplot() and then pipe it to the ggplotly() function.\nBelow, we plot a simple line representing the proportion of cases who died in a given week:\nWe begin by creating a summary dataset of each epidemiological week, and the percent of cases with a known outcome that died.\n\nweekly_deaths &lt;- linelist %&gt;%\n  group_by(epiweek = floor_date(date_onset, \"week\")) %&gt;%  # create and group data by epiweek column\n  summarise(                                              # create new summary data frame:\n    n_known_outcome = sum(!is.na(outcome), na.rm=T),      # number of cases per group with known outcome\n    n_death  = sum(outcome == \"Death\", na.rm=T),          # number of cases per group who died\n    pct_death = 100*(n_death / n_known_outcome)           # percent of cases with known outcome who died\n  )\n\nHere is the first 50 rows of the weekly_deaths dataset.\n\n\n\n\n\n\nThen we create the plot with ggplot2, using geom_line().\n\ndeaths_plot &lt;- ggplot(data = weekly_deaths)+            # begin with weekly deaths data\n  geom_line(mapping = aes(x = epiweek, y = pct_death))  # make line \n\ndeaths_plot   # print\n\n\n\n\n\n\n\n\nWe can make this interactive by simply passing this plot to ggplotly(), as below. Hover your mouse over the line to show the x and y values. You can zoom in on the plot, and drag it around. You can also see icons in the upper-right of the plot. In order, they allow you to:\n\nDownload the current view as a PNG image\n\nZoom in with a select box\n\n“Pan”, or move across the plot by clicking and dragging the plot\n\nZoom in, zoom out, or return to default zoom\n\nReset axes to defaults\n\nToggle on/off “spike lines” which are dotted lines from the interactive point extending to the x and y axes\n\nAdjustments to whether data show when you are not hovering on the line\n\n\ndeaths_plot %&gt;% plotly::ggplotly()\n\n\n\n\n\nGrouped data work with ggplotly() as well. Below, a weekly epicurve is made, grouped by outcome. The stacked bars are interactive. Try clicking on the different items in the legend (they will appear/disappear).\n\n# Make epidemic curve with incidence2 pacakge\np &lt;- incidence2::incidence(\n  linelist,\n  date_index = date_onset,\n  interval = \"weeks\",\n  groups = outcome) %&gt;% plot(fill = outcome)\n\n\n# Plot interactively  \np %&gt;% plotly::ggplotly()",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Interactive plots</span>"
    ]
  },
  {
    "objectID": "new_pages/interactive_plots.html#modifications",
    "href": "new_pages/interactive_plots.html#modifications",
    "title": "39  Interactive plots",
    "section": "39.3 Modifications",
    "text": "39.3 Modifications\n\nFile size\nWhen exporting in an R Markdown generated HTML (like this book!) you want to make the plot as small data size as possible (with no negative side effects in most cases). For this, just pipe the interactive plot to partial_bundle(), also from plotly.\n\np &lt;- p %&gt;% \n  plotly::ggplotly() %&gt;%\n  plotly::partial_bundle()\n\n\n\nButtons\nSome of the buttons on a standard plotly are superfluous and can be distracting, so you can remove them. You can do this simply by piping the output into config() from plotly and specifying which buttons to remove. In the below example we specify in advance the names of the buttons to remove, and provide them to the argument modeBarButtonsToRemove =. We also set displaylogo = FALSE to remove the plotly logo.\n\n## these buttons are distracting and we want to remove them\nplotly_buttons_remove &lt;- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',\n                              'zoomOut2d','autoScale2d','hoverClosestCartesian',\n                              'toggleSpikelines','hoverCompareCartesian')\n\np &lt;- p %&gt;%          # re-define interactive plot without these buttons\n  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Interactive plots</span>"
    ]
  },
  {
    "objectID": "new_pages/interactive_plots.html#heat-tiles",
    "href": "new_pages/interactive_plots.html#heat-tiles",
    "title": "39  Interactive plots",
    "section": "39.4 Heat tiles",
    "text": "39.4 Heat tiles\nYou can make almost any ggplot() plot interactive, including heat tiles. In the page on Heat plots you can read about how to make the below plot, which displays the proportion of days per week that certain facilities reported data to their province.\nHere is the code, although we will not describe it in depth here.\n\n# import data\nfacility_count_data &lt;- rio::import(here::here(\"data\", \"malaria_facility_count_data.rds\"))\n\n# aggregate data into Weeks for Spring district\nagg_weeks &lt;- facility_count_data %&gt;% \n  filter(District == \"Spring\",\n         data_date &lt; as.Date(\"2020-08-01\")) %&gt;% \n  mutate(week = aweek::date2week(\n    data_date,\n    start_date = \"Monday\",\n    floor_day = TRUE,\n    factor = TRUE)) %&gt;% \n  group_by(location_name, week, .drop = F) %&gt;%\n  summarise(\n    n_days          = 7,\n    n_reports       = n(),\n    malaria_tot     = sum(malaria_tot, na.rm = T),\n    n_days_reported = length(unique(data_date)),\n    p_days_reported = round(100*(n_days_reported / n_days))) %&gt;% \n  ungroup(location_name, week) %&gt;% \n  right_join(tidyr::expand(., week, location_name)) %&gt;% \n  mutate(week = aweek::week2date(week))\n\n# create plot\nmetrics_plot &lt;- ggplot(agg_weeks,\n       aes(x = week,\n           y = location_name,\n           fill = p_days_reported))+\n  geom_tile(colour=\"white\")+\n  scale_fill_gradient(low = \"orange\", high = \"darkgreen\", na.value = \"grey80\")+\n  scale_x_date(expand = c(0,0),\n               date_breaks = \"2 weeks\",\n               date_labels = \"%d\\n%b\")+\n  theme_minimal()+ \n  theme(\n    legend.title = element_text(size=12, face=\"bold\"),\n    legend.text  = element_text(size=10, face=\"bold\"),\n    legend.key.height = grid::unit(1,\"cm\"),\n    legend.key.width  = grid::unit(0.6,\"cm\"),\n    axis.text.x = element_text(size=12),\n    axis.text.y = element_text(vjust=0.2),\n    axis.ticks = element_line(size=0.4),\n    axis.title = element_text(size=12, face=\"bold\"),\n    plot.title = element_text(hjust=0,size=14,face=\"bold\"),\n    plot.caption = element_text(hjust = 0, face = \"italic\")\n    )+\n  labs(x = \"Week\",\n       y = \"Facility name\",\n       fill = \"Reporting\\nperformance (%)\",\n       title = \"Percent of days per week that facility reported data\",\n       subtitle = \"District health facilities, April-May 2019\",\n       caption = \"7-day weeks beginning on Mondays.\")\n\nmetrics_plot # print\n\n\n\n\n\n\n\n\nBelow, we make it interactive and modify it for simple buttons and file size.\n\nmetrics_plot %&gt;% \n  plotly::ggplotly() %&gt;% \n  plotly::partial_bundle() %&gt;% \n  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Interactive plots</span>"
    ]
  },
  {
    "objectID": "new_pages/interactive_plots.html#resources",
    "href": "new_pages/interactive_plots.html#resources",
    "title": "39  Interactive plots",
    "section": "39.5 Resources",
    "text": "39.5 Resources\nPlotly is not just for R, but also works well with Python (and really any data science language as it’s built in JavaScript). You can read more about it on the plotly website",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Interactive plots</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html",
    "href": "new_pages/rmarkdown.html",
    "title": "40  Reports with R Markdown",
    "section": "",
    "text": "40.1 Preparation\nBackground to R Markdown\nTo explain some of the concepts and packages involved:\nIn sum, the process that happens in the background (you do not need to know all these steps!) involves feeding the .Rmd file to knitr, which executes the R code chunks and creates a new .md (markdown) file which includes the R code and its rendered output. The .md file is then processed by pandoc to create the finished product: a Microsoft Word document, HTML file, powerpoint document, pdf, etc.\n(source: https://rmarkdown.rstudio.com/authoring_quick_tour.html):\nInstallation\nTo create a R Markdown output, you need to have the following installed:\npacman::p_load(tinytex)     # install tinytex package\ntinytex::install_tinytex()  # R command to install TinyTeX software",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#preparation",
    "href": "new_pages/rmarkdown.html#preparation",
    "title": "40  Reports with R Markdown",
    "section": "",
    "text": "Markdown is a “language” that allows you to write a document using plain text, that can be converted to html and other formats. It is not specific to R. Files written in Markdown have a ‘.md’ extension.\nR Markdown: is a variation on markdown that is specific to R - it allows you to write a document using markdown to produce text and to embed R code and display their outputs. R Markdown files have ‘.Rmd’ extension.\n\nrmarkdown - the package: This is used by R to render the .Rmd file into the desired output. It’s focus is converting the markdown (text) syntax, so we also need…\nknitr: This R package will read the code chunks, execute it, and ‘knit’ it back into the document. This is how tables and graphs are included alongside the text.\nPandoc: Finally, pandoc actually convert the output into word/pdf/powerpoint etc. It is a software separate from R but is installed automatically with RStudio.\n\n\n\n\n\n\n\nThe rmarkdown package (knitr will also be installed automatically)\n\nPandoc, which should come installed with RStudio. If you are not using RStudio, you can download Pandoc here: http://pandoc.org.\nIf you want to generate PDF output (a bit trickier), you will need to install LaTeX. For R Markdown users who have not installed LaTeX before, we recommend that you install TinyTeX (https://yihui.name/tinytex/). You can use the following commands:",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#getting-started",
    "href": "new_pages/rmarkdown.html#getting-started",
    "title": "40  Reports with R Markdown",
    "section": "40.2 Getting started",
    "text": "40.2 Getting started\n\nInstall rmarkdown R package\nInstall the rmarkdown R package. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(rmarkdown)\n\n\n\nStarting a new Rmd file\nIn RStudio, open a new R markdown file, starting with ‘File’, then ‘New file’ then ‘R markdown…’.\n\n\n\n\n\n\n\n\n\nR Studio will give you some output options to pick from. In the example below we select “HTML” because we want to create an html document. The title and the author names are not important. If the output document type you want is not one of these, don’t worry - you can just pick any one and change it in the script later.\n\n\n\n\n\n\n\n\n\nThis will open up a new .Rmd script.\n\n\nImportant to know\nThe working directory\nThe working directory of a markdown file is wherever the Rmd file itself is saved. For instance, if the R project is within ~/Documents/projectX and the Rmd file itself is in a subfolder ~/Documents/projectX/markdownfiles/markdown.Rmd, the code read.csv(“data.csv”) within the markdown will look for a csv file in the markdownfiles folder, and not the root project folder where scripts within projects would normally automatically look.\nTo refer to files elsewhere, you will either need to use the full file path or use the here package. The here package sets the working directory to the root folder of the R project and is explained in detail in the R projects and Import and export pages of this handbook. For instance, to import a file called “data.csv” from within the projectX folder, the code would be import(here(“data.csv”)).\nNote that use of setwd() in R Markdown scripts is not recommended – it only applies to the code chunk that it is written in.\nWorking on a drive vs your computer\nBecause R Markdown can run into pandoc issues when running on a shared network drive, it is recommended that your folder is on your local machine, e.g. in a project within ‘My Documents’. If you use Git (much recommended!), this will be familiar. For more details, see the handbook pages on R on network drives and Errors and help.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#r-markdown-components",
    "href": "new_pages/rmarkdown.html#r-markdown-components",
    "title": "40  Reports with R Markdown",
    "section": "40.3 R Markdown components",
    "text": "40.3 R Markdown components\nAn R Markdown document can be edited in RStudio just like a standard R script. When you start a new R Markdown script, RStudio tries to be helpful by showing a template which explains the different section of an R Markdown script.\nThe below is what appears when starting a new Rmd script intended to produce an html output (as per previous section).\n\n\n\n\n\n\n\n\n\nAs you can see, there are three basic components to an Rmd file: YAML, Markdown text, and R code chunks.\nThese will create and become your document output. See the diagram below:\n\n\n\n\n\n\n\n\n\n\nYAML metadata\nReferred to as the ‘YAML metadata’ or just ‘YAML’, this is at the top of the R Markdown document. This section of the script will tell your Rmd file what type of output to produce, formatting preferences, and other metadata such as document title, author, and date. There are other uses not mentioned here (but referred to in ‘Producing an output’). Note that indentation matters; tabs are not accepted but spaces are.\nThis section must begin with a line containing just three dashes --- and must close with a line containing just three dashes ---. YAML parameters comes in key:value pairs. The placement of colons in YAML is important - the key:value pairs are separated by colons (not equals signs!).\nThe YAML should begin with metadata for the document. The order of these primary YAML parameters (not indented) does not matter. For example:\ntitle: \"My document\"\nauthor: \"Me\"\ndate: \"2024-06-19\"\nYou can use R code in YAML values by writing it as in-line code (preceded by r within back-ticks) but also within quotes (see above example for date:).\nIn the image above, because we clicked that our default output would be an html file, we can see that the YAML says output: html_document. However we can also change this to say powerpoint_presentation or word_document or even pdf_document.\n\n\nText\nThis is the narrative of your document, including the titles and headings. It is written in the “markdown” language, which is used across many different software.\nBelow are the core ways to write this text. See more extensive documentation available on R Markdown “cheatsheet” at the RStudio website.\n\nNew lines\nUniquely in R Markdown, to initiate a new line, enter *two spaces** at the end of the previous line and then Enter/Return.\n\n\nCase\nSurround your normal text with these character to change how it appears in the output.\n\nUnderscores (_text_) or single asterisk (*text*) to italicise\nDouble asterisks (**text**) for bold text\nBack-ticks (text) to display text as code\n\nThe actual appearance of the font can be set by using specific templates (specified in the YAML metadata; see example tabs).\n\n\nColor\nThere is no simple mechanism to change the color of text in R Markdown. One work-around, IF your output is an HTML file, is to add an HTML line into the markdown text. The below HTML code will print a line of text in bold red.\n&lt;span style=\"color: red;\"&gt;**_DANGER:_** This is a warning.&lt;/span&gt;  \nDANGER: This is a warning.\n\n\nTitles and headings\nA hash symbol in a text portion of a R Markdown script creates a heading. This is different than in a chunk of R code in the script, in which a hash symbol is a mechanism to comment/annotate/de-activate, as in a normal R script.\nDifferent heading levels are established with different numbers of hash symbols at the start of a new line. One hash symbol is a title or primary heading. Two hash symbols are a second-level heading. Third- and fourth-level headings can be made with successively more hash symbols.\n# First-level heading / title\n\n## Second level heading  \n\n### Third-level heading\n\n\nBullets and numbering\nUse asterisks (*) to created a bullets list. Finish the previous sentence, enter two spaces, Enter/Return twice, and then start your bullets. Include a space between the asterisk and your bullet text. After each bullet enter two spaces and then Enter/Return. Sub-bullets work the same way but are indented. Numbers work the same way but instead of an asterisk, write 1), 2), etc. Below is how your R Markdown script text might look.\nHere are my bullets (there are two spaces after this colon):  \n\n* Bullet 1 (followed by two spaces and Enter/Return)  \n* Bullet 2 (followed by two spaces and Enter/Return)  \n  * Sub-bullet 1 (followed by two spaces and Enter/Return)  \n  * Sub-bullet 2 (followed by two spaces and Enter/Return)  \n  \n\n\nComment out text\nYou can “comment out” R Markdown text just as you can use the “#” to comment out a line of R code in an R chunk. Simply highlight the text and press Ctrl+Shift+c (Cmd+Shift+c for Mac). The text will be surrounded by arrows and turn green. It will not appear in your output.\n\n\n\n\n\n\n\n\n\n\n\n\nCode chunks\nSections of the script that are dedicated to running R code are called “chunks”. This is where you may load packages, import data, and perform the actual data management and visualisation. There may be many code chunks, so they can help you organize your R code into parts, perhaps interspersed with text. To note: These ‘chunks’ will appear to have a slightly different background colour from the narrative part of the document.\nEach chunk is opened with a line that starts with three back-ticks, and curly brackets that contain parameters for the chunk ({ }). The chunk ends with three more back-ticks.\nYou can create a new chunk by typing it out yourself, by using the keyboard shortcut “Ctrl + Alt + i” (or Cmd + Shift + r in Mac), or by clicking the green ‘insert a new code chunk’ icon at the top of your script editor.\nSome notes about the contents of the curly brackets { }:\n\nThey start with ‘r’ to indicate that the language name within the chunk is R\nAfter the r you can optionally write a chunk “name” – these are not necessary but can help you organise your work. Note that if you name your chunks, you should ALWAYS use unique names or else R will complain when you try to render.\n\nThe curly brackets can include other options too, written as tag=value, such as:\n\neval = FALSE to not run the R code\n\necho = FALSE to not print the chunk’s R source code in the output document\n\nwarning = FALSE to not print warnings produced by the R code\n\nmessage = FALSE to not print any messages produced by the R code\n\ninclude = either TRUE/FALSE whether to include chunk outputs (e.g. plots) in the document\nout.width = and out.height = - provide in style out.width = \"75%\"\n\nfig.align = \"center\" adjust how a figure is aligned across the page\n\nfig.show='hold' if your chunk prints multiple figures and you want them printed next to each other (pair with out.width = c(\"33%\", \"67%\"). Can also set as fig.show='asis' to show them below the code that generates them, 'hide' to hide, or 'animate' to concatenate multiple into an animation.\n\nA chunk header must be written in one line\n\nTry to avoid periods, underscores, and spaces. Use hyphens ( - ) instead if you need a separator.\n\nRead more extensively about the knitr options here.\nSome of the above options can be configured with point-and-click using the setting buttons at the top right of the chunk. Here, you can specify which parts of the chunk you want the rendered document to include, namely the code, the outputs, and the warnings. This will come out as written preferences within the curly brackets, e.g. echo=FALSE if you specify you want to ‘Show output only’.\n\n\n\n\n\n\n\n\n\nThere are also two arrows at the top right of each chunk, which are useful to run code within a chunk, or all code in prior chunks. Hover over them to see what they do.\nFor global options to be applied to all chunks in the script, you can set this up within your very first R code chunk in the script. For instance, so that only the outputs are shown for each code chunk and not the code itself, you can include this command in the R code chunk:\n\nknitr::opts_chunk$set(echo = FALSE) \n\n\nIn-text R code\nYou can also include minimal R code within back-ticks. Within the back-ticks, begin the code with “r” and a space, so RStudio knows to evaluate the code as R code. See the example below.\nThe example below shows multiple heading levels, bullets, and uses R code for the current date (Sys.Date()) to evaluate into a printed date.\n\n\n\n\n\n\n\n\n\nThe example above is simple (showing the current date), but using the same syntax you can display values produced by more complex R code (e.g. to calculate the min, median, max of a column). You can also integrate R objects or values that were created in R code chunks earlier in the script.\nAs an example, the script below calculates the proportion of cases that are aged less than 18 years old, using tidyverse functions, and creates the objects less18, total, and less18prop. This dynamic value is inserted into subsequent text. We see how it looks when knitted to a word document.\n\n\n\n\n\n\n\n\n\n\n\n\nImages\nYou can include images in your R Markdown one of two ways:\n\n![](\"path/to/image.png\")  \n\nIf the above does not work, try using knitr::include_graphics()\n\nknitr::include_graphics(\"path/to/image.png\")\n\n(remember, your file path could be written using the here package)\n\nknitr::include_graphics(here::here(\"path\", \"to\", \"image.png\"))\n\n\n\nTables\nCreate a table using hyphens ( - ) and bars ( | ). The number of hyphens before/between bars allow the number of spaces in the cell before the text begins to wrap.\nColumn 1 |Column  2 |Column 3\n---------|----------|--------\nCell A   |Cell B    |Cell C\nCell D   |Cell E    |Cell F\nThe above code produces the table below:\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nCell A\nCell B\nCell C\n\n\nCell D\nCell E\nCell F\n\n\n\n\n\nTabbed sections\nFor HTML outputs, you can arrange the sections into “tabs”. Simply add .tabset in the curly brackets { } that are placed after a heading. Any sub-headings beneath that heading (until another heading of the same level) will appear as tabs that the user can click through. Read more here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can add an additional option .tabset-pills after .tabset to give the tabs themselves a “pilled” appearance. Be aware that when viewing the tabbed HTML output, the Ctrl+f search functionality will only search “active” tabs, not hidden tabs.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#file-structure",
    "href": "new_pages/rmarkdown.html#file-structure",
    "title": "40  Reports with R Markdown",
    "section": "40.4 File structure",
    "text": "40.4 File structure\nThere are several ways to structure your R Markdown and any associated R scripts. Each has advantages and disadvantages:\n\nSelf-contained R Markdown - everything needed for the report is imported or created within the R Markdown\n\nSource other files - You can run external R scripts with the source() command and use their outputs in the Rmd\n\nChild scripts - an alternate mechanism for source()\n\n\nUtilize a “runfile” - Run commands in an R script prior to rendering the R Markdown\n\n\nSelf-contained Rmd\nFor a relatively simple report, you may elect to organize your R Markdown script such that it is “self-contained” and does not involve any external scripts.\nEverything you need to run the R markdown is imported or created within the Rmd file, including all the code chunks and package loading. This “self-contained” approach is appropriate when you do not need to do much data processing (e.g. it brings in a clean or semi-clean data file) and the rendering of the R Markdown will not take too long.\nIn this scenario, one logical organization of the R Markdown script might be:\n\nSet global knitr options\n\nLoad packages\n\nImport data\n\nProcess data\n\nProduce outputs (tables, plots, etc.)\n\nSave outputs, if applicable (.csv, .png, etc.)\n\n\nSource other files\nOne variation of the “self-contained” approach is to have R Markdown code chunks “source” (run) other R scripts. This can make your R Markdown script less cluttered, more simple, and easier to organize. It can also help if you want to display final figures at the beginning of the report. In this approach, the final R Markdown script simply combines pre-processed outputs into a document.\nOne way to do this is by providing the R scripts (file path and name with extension) to the base R command source().\n\nsource(\"your-script.R\", local = knitr::knit_global())\n# or sys.source(\"your-script.R\", envir = knitr::knit_global())\n\nNote that when using source() within the R Markdown, the external files will still be run during the course of rendering your Rmd file. Therefore, each script is run every time you render the report. Thus, having these source() commands within the R Markdown does not speed up your run time, nor does it greatly assist with de-bugging, as error produced will still be printed when producing the R Markdown.\nAn alternative is to utilize the child = knitr option. EXPLAIN MORE TO DO\nYou must be aware of various R environments. Objects created within an environment will not necessarily be available to the environment used by the R Markdown.\n\n\n\nRunfile\nThis approach involves utilizing the R script that contains the render() command(s) to pre-process objects that feed into the R markdown.\nFor instance, you can load the packages, load and clean the data, and even create the graphs of interest prior to render(). These steps can occur in the R script, or in other scripts that are sourced. As long as these commands occur in the same RStudio session and objects are saved to the environment, the objects can then be called within the Rmd content. Then the R markdown itself will only be used for the final step - to produce the output with all the pre-processed objects. This is much easier to de-bug if something goes wrong.\nThis approach is helpful for the following reasons:\n\nMore informative error messages - these messages will be generated from the R script, not the R Markdown. R Markdown errors tend to tell you which chunk had a problem, but will not tell you which line.\n\nIf applicable, you can run long processing steps in advance of the render() command - they will run only once.\n\nIn the example below, we have a separate R script in which we pre-process a data object into the R Environment and then render the “create_output.Rmd” using render().\n\ndata &lt;- import(\"datafile.csv\") %&gt;%       # Load data and save to environment\n  select(age, hospital, weight)          # Select limited columns\n\nrmarkdown::render(input = \"create_output.Rmd\")   # Create Rmd file\n\n\n\nFolder strucutre\nWorkflow also concerns the overall folder structure, such as having an ‘output’ folder for created documents and figures, and ‘data’ or ‘inputs’ folders for cleaned data. We do not go into further detail here, but check out the Organizing routine reports page.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#producing-the-document",
    "href": "new_pages/rmarkdown.html#producing-the-document",
    "title": "40  Reports with R Markdown",
    "section": "40.5 Producing the document",
    "text": "40.5 Producing the document\nYou can produce the document in the following ways:\n\nManually by pressing the “Knit” button at the top of the RStudio script editor (fast and easy)\n\nRun the render() command (executed outside the R Markdown script)\n\n\nOption 1: “Knit” button\nWhen you have the Rmd file open, press the ‘Knit’ icon/button at the top of the file.\nR Studio will you show the progress within an ‘R Markdown’ tab near your R console. The document will automatically open when complete.\nThe document will be saved in the same folder as your R markdown script, and with the same file name (aside from the extension). This is obviously not ideal for version control (it will be over-written each tim you knit, unless moved manually), as you may then need to rename the file yourself (e.g. add a date).\nThis is RStudio’s shortcut button for the render() function from rmarkdown. This approach only compatible with a self-contained R markdown, where all the needed components exist or are sourced within the file.\n\n\n\n\n\n\n\n\n\n\n\nOption 2: render() command\nAnother way to produce your R Markdown output is to run the render() function (from the rmarkdown package). You must execute this command outside the R Markdown script - so either in a separate R script (often called a “run file”), or as a stand-alone command in the R Console.\n\nrmarkdown::render(input = \"my_report.Rmd\")\n\nAs with “knit”, the default settings will save the Rmd output to the same folder as the Rmd script, with the same file name (aside from the file extension). For instance “my_report.Rmd” when knitted will create “my_report.docx” if you are knitting to a word document. However, by using render() you have the option to use different settings. render() can accept arguments including:\n\noutput_format = This is the output format to convert to (e.g. \"html_document\", \"pdf_document\", \"word_document\", or \"all\"). You can also specify this in the YAML inside the R Markdown script.\n\noutput_file = This is the name of the output file (and file path). This can be created via R functions like here() or str_glue() as demonstrated below.\n\noutput_dir = This is an output directory (folder) to save the file. This allows you to chose an alternative other than the directory the Rmd file is saved to.\n\noutput_options = You can provide a list of options that will override those in the script YAML (e.g. )\noutput_yaml = You can provide path to a .yml file that contains YAML specifications\n\nparams = See the section on parameters below\n\nSee the complete list here\n\nAs one example, to improve version control, the following command will save the output file within an ‘outputs’ sub-folder, with the current date in the file name. To create the file name, the function str_glue() from the stringr package is use to ‘glue’ together static strings (written plainly) with dynamic R code (written in curly brackets). For instance if it is April 10th 2021, the file name from below will be “Report_2021-04-10.docx”. See the page on Characters and strings for more details on str_glue().\n\nrmarkdown::render(\n  input = \"create_output.Rmd\",\n  output_file = stringr::str_glue(\"outputs/Report_{Sys.Date()}.docx\")) \n\nAs the file renders, the RStudio Console will show you the rendering progress up to 100%, and a final message to indicate that the rendering is complete.\n\n\nOptions 3: reportfactory package\nThe R package reportfactory offers an alternative method of organising and compiling R Markdown reports catered to scenarios where you run reports routinely (e.g. daily, weekly…). It eases the compilation of multiple R Markdown files and the organization of their outputs. In essence, it provides a “factory” from which you can run the R Markdown reports, get automatically date- and time-stamped folders for the outputs, and have “light” version control.\nRead more about this work flow in the page on Organizing routine reports.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#parameterised-reports",
    "href": "new_pages/rmarkdown.html#parameterised-reports",
    "title": "40  Reports with R Markdown",
    "section": "40.6 Parameterised reports",
    "text": "40.6 Parameterised reports\nYou can use parameterisation to make a report dynamic, such that it can be run with specific setting (e.g. a specific date or place or with certain knitting options). Below, we focus on the basics, but there is more detail online about parameterized reports.\nUsing the Ebola linelist as an example, let’s say we want to run a standard surveillance report for each hospital each day. We show how one can do this using parameters.\nImportant: dynamic reports are also possible without the formal parameter structure (without params:), using simple R objects in an adjacent R script. This is explained at the end of this section.\n\nSetting parameters\nYou have several options for specifying parameter values for your R Markdown output.\n\nOption 1: Set parameters within YAML\nEdit the YAML to include a params: option, with indented statements for each parameter you want to define. In this example we create parameters date and hospital, for which we specify values. These values are subject to change each time the report is run. If you use the “Knit” button to produce the output, the parameters will have these default values. Likewise, if you use render() the parameters will have these default values unless otherwise specified in the render() command.\n---\ntitle: Surveillance report\noutput: html_document\nparams:\n date: 2021-04-10\n hospital: Central Hospital\n---\nIn the background, these parameter values are contained within a read-only list called params. Thus, you can insert the parameter values in R code as you would another R object/value in your environment. Simply type params$ followed by the parameter name. For example params$hospital to represent the hospital name (“Central Hospital” by default).\nNote that parameters can also hold values true or false, and so these can be included in your knitr options for a R chunk. For example, you can set {r, eval=params$run} instead of {r, eval=FALSE}, and now whether the chunk runs or not depends on the value of a parameter run:.\nNote that for parameters that are dates, they will be input as a string. So for params$date to be interpreted in R code it will likely need to be wrapped with as.Date() or a similar function to convert to class Date.\n\n\nOption 2: Set parameters within render()\nAs mentioned above, as alternative to pressing the “Knit” button to produce the output is to execute the render() function from a separate script. In this later case, you can specify the parameters to be used in that rendering to the params = argument of render().\nNote than any parameter values provided here will overwrite their default values if written within the YAML. We write the values in quotation marks as in this case they should be defined as character/string values.\nThe below command renders “surveillance_report.Rmd”, specifies a dynamic output file name and folder, and provides a list() of two parameters and their values to the argument params =.\n\nrmarkdown::render(\n  input = \"surveillance_report.Rmd\",  \n  output_file = stringr::str_glue(\"outputs/Report_{Sys.Date()}.docx\"),\n  params = list(date = \"2021-04-10\", hospital  = \"Central Hospital\"))\n\n\n\nOption 3: Set parameters using a Graphical User Interface\nFor a more interactive feel, you can also use the Graphical User Interface (GUI) to manually select values for parameters. To do this we can click the drop-down menu next to the ‘Knit’ button and choose ‘Knit with parameters’.\nA pop-up will appear allowing you to type in values for the parameters that are established in the document’s YAML.\n\n\n\n\n\n\n\n\n\nYou can achieve the same through a render() command by specifying params = \"ask\", as demonstrated below.\n\nrmarkdown::render(\n  input = \"surveillance_report.Rmd\",  \n  output_file = stringr::str_glue(\"outputs/Report_{Sys.Date()}.docx\"),\n  params = “ask”)\n\nHowever, typing values into this pop-up window is subject to error and spelling mistakes. You may prefer to add restrictions to the values that can be entered through drop-down menus. You can do this by adding in the YAML several specifications for each params: entry.\n\nlabel: is how the title for that particular drop-down menu\n\nvalue: is the default (starting) value\n\ninput: set to select for drop-down menu\n\nchoices: Give the eligible values in the drop-down menu\n\nBelow, these specifications are written for the hospital parameter.\n---\ntitle: Surveillance report\noutput: html_document\nparams:\n date: 2021-04-10\n hospital: \n  label: “Town:”\n  value: Central Hospital\n  input: select\n  choices: [Central Hospital, Military Hospital, Port Hospital, St. Mark's Maternity Hospital (SMMH)]\n---\nWhen knitting (either via the ‘knit with parameters’ button or by render()), the pop-up window will have drop-down options to select from.\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized example\nThe following code creates parameters for date and hospital, which are used in the R Markdown as params$date and params$hospital, respectively.\nIn the resulting report output, see how the data are filtered to the specific hospital, and the plot title refers to the correct hospital and date. We use the “linelist_cleaned.rds” file here, but it would be particularly appropriate if the linelist itself also had a datestamp within it to align with parameterised date.\n\n\n\n\n\n\n\n\n\nKnitting this produces the final output with the default font and layout.\n\n\n\n\n\n\n\n\n\n\n\nParameterisation without params\nIf you are rendering a R Markdown file with render() from a separate script, you can actually create the impact of parameterization without using the params: functionality.\nFor instance, in the R script that contains the render() command, you can simply define hospital and date as two R objects (values) before the render() command. In the R Markdown, you would not need to have a params: section in the YAML, and we would refer to the date object rather than params$date and hospital rather than params$hospital.\n\n# This is a R script that is separate from the R Markdown\n\n# define R objects\nhospital &lt;- \"Central Hospital\"\ndate &lt;- \"2021-04-10\"\n\n# Render the R markdown\nrmarkdown::render(input = \"create_output.Rmd\") \n\nFollowing this approach means means you can not “knit with parameters”, use the GUI, or include knitting options within the parameters. However it allows for simpler code, which may be advantageous.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#looping-reports",
    "href": "new_pages/rmarkdown.html#looping-reports",
    "title": "40  Reports with R Markdown",
    "section": "40.7 Looping reports",
    "text": "40.7 Looping reports\nWe may want to run a report multiple times, varying the input parameters, to produce a report for each jurisdictions/unit. This can be done using tools for iteration, which are explained in detail in the page on Iteration, loops, and lists. Options include the purrr package, or use of a for loop as explained below.\nBelow, we use a simple for loop to generate a surveillance report for all hospitals of interest. This is done with one command (instead of manually changing the hospital parameter one-at-a-time). The command to render the reports must exist in a separate script outside the report Rmd. This script will also contain defined objects to “loop through” - today’s date, and a vector of hospital names to loop through.\n\nhospitals &lt;- c(\"Central Hospital\",\n                \"Military Hospital\", \n                \"Port Hospital\",\n                \"St. Mark's Maternity Hospital (SMMH)\") \n\nWe then feed these values one-at-a-time into the render() command using a loop, which runs the command once for each value in the hospitals vector. The letter i represents the index position (1 through 4) of the hospital currently being used in that iteration, such that hospital_list[1] would be “Central Hospital”. This information is supplied in two places in the render() command:\n\nTo the file name, such that the file name of the first iteration if produced on 10th April 2021 would be “Report_Central Hospital_2021-04-10.docx”, saved in the ‘output’ subfolder of the working directory.\n\nTo params = such that the Rmd uses the hospital name internally whenever the params$hospital value is called (e.g. to filter the dataset to the particular hospital only). In this example, four files would be created - one for each hospital.\n\n\nfor(i in 1:length(hospitals)){\n  rmarkdown::render(\n    input = \"surveillance_report.Rmd\",\n    output_file = str_glue(\"output/Report_{hospitals[i]}_{Sys.Date()}.docx\"),\n    params = list(hospital  = hospitals[i]))\n}",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#templates",
    "href": "new_pages/rmarkdown.html#templates",
    "title": "40  Reports with R Markdown",
    "section": "40.8 Templates",
    "text": "40.8 Templates\nBy using a template document that contains any desired formatting, you can adjust the aesthetics of how the Rmd output will look. You can create for instance an MS Word or Powerpoint file that contains pages/slides with the desired dimensions, watermarks, backgrounds, and fonts.\n\nWord documents\nTo create a template, start a new word document (or use an existing output with formatting the suits you), and edit fonts by defining the Styles. In Style,Headings 1, 2, and 3 refer to the various markdown header levels (# Header 1, ## Header 2 and ### Header 3 respectively). Right click on the style and click ‘modify’ to change the font formatting as well as the paragraph (e.g. you can introduce page breaks before certain styles which can help with spacing). Other aspects of the word document such as margins, page size, headers etc, can be changed like a usual word document you are working directly within.\n\n\n\n\n\n\n\n\n\n\n\nPowerpoint documents\nAs above, create a new slideset or use an existing powerpoint file with the desired formatting. For further editing, click on ‘View’ and ‘Slide Master’. From here you can change the ‘master’ slide appearance by editing the text formatting in the text boxes, as well as the background/page dimensions for the overall page.\n\n\n\n\n\n\n\n\n\nUnfortunately, editing powerpoint files is slightly less flexible:\n\nA first level header (# Header 1) will automatically become the title of a new slide,\nA ## Header 2 text will not come up as a subtitle but text within the slide’s main textbox (unless you find a way to maniuplate the Master view).\nOutputted plots and tables will automatically go into new slides. You will need to combine them, for instance the the patchwork function to combine ggplots, so that they show up on the same page. See this blog post about using the patchwork package to put multiple images on one slide.\n\nSee the officer package for a tool to work more in-depth with powerpoint presentations.\n\n\nIntegrating templates into the YAML\nOnce a template is prepared, the detail of this can be added in the YAML of the Rmd underneath the ‘output’ line and underneath where the document type is specified (which goes to a separate line itself). Note reference_doc can be used for powerpoint slide templates.\nIt is easiest to save the template in the same folder as where the Rmd file is (as in the example below), or in a subfolder within.\n---\ntitle: Surveillance report\noutput: \n word_document:\n  reference_docx: \"template.docx\"\nparams:\n date: 2021-04-10\n hospital: Central Hospital\ntemplate:\n \n---\n\n\nFormatting HTML files\nHTML files do not use templates, but can have the styles configured within the YAML. HTMLs are interactive documents, and are particularly flexible. We cover some basic options here.\n\nTable of contents: We can add a table of contents with toc: true below, and also specify that it remains viewable (“floats”) as you scroll, with toc_float: true.\nThemes: We can refer to some pre-made themes, which come from a Bootswatch theme library. In the below example we use cerulean. Other options include: journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, and yeti.\nHighlight: Configuring this changes the look of highlighted text (e.g. code within chunks that are shown). Supported styles include default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark, and textmate.\n\nHere is an example of how to integrate the above options into the YAML.\n---\ntitle: \"HTML example\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n    theme: cerulean\n    highlight: kate\n    \n---\nBelow are two examples of HTML outputs which both have floating tables of contents, but different theme and highlight styles selected:",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#dynamic-content",
    "href": "new_pages/rmarkdown.html#dynamic-content",
    "title": "40  Reports with R Markdown",
    "section": "40.9 Dynamic content",
    "text": "40.9 Dynamic content\nIn an HTML output, your report content can be dynamic. Below are some examples:\n\nTables\nIn an HTML report, you can print data frame / tibbles such that the content is dynamic, with filters and scroll bars. There are several packages that offer this capability.\nTo do this with the DT package, as is used throughout this handbook, you can insert a code chunk like this:\n\n\n\n\n\n\n\n\n\nThe function datatable() will print the provided data frame as a dynamic table for the reader. You can set rownames = FALSE to simplify the far left-side of the table. filter = \"top\" provides a filter over each column. In the option() argument provide a list of other specifications. Below we include two: pageLength = 5 set the number of rows that appear as 5 (the remaining rows can be viewed by paging through arrows), and scrollX=TRUE enables a scrollbar on the bottom of the table (for columns that extend too far to the right).\nIf your dataset is very large, consider only showing the top X rows by wrapping the data frame in head().\n\n\nHTML widgets\nHTML widgets for R are a special class of R packages that enable increased interactivity by utilizing JavaScript libraries. You can embed them in HTML R Markdown outputs.\nSome common examples of these widgets include:\n\nPlotly (used in this handbook page and in the Interative plots page)\nvisNetwork (used in the Transmission Chains page of this handbook)\n\nLeaflet (used in the GIS Basics page of this handbook)\n\ndygraphs (useful for interactively showing time series data)\n\nDT (datatable()) (used to show dynamic tables with filter, sort, etc.)\n\nThe ggplotly() function from plotly is particularly easy to use. See the Interactive plots page.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/rmarkdown.html#resources",
    "href": "new_pages/rmarkdown.html#resources",
    "title": "40  Reports with R Markdown",
    "section": "40.10 Resources",
    "text": "40.10 Resources\nFurther information can be found via:\n\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://rmarkdown.rstudio.com/articles_intro.html\n\nA good explainer of markdown vs knitr vs Rmarkdown is here: https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Reports with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html",
    "href": "new_pages/reportfactory.html",
    "title": "41  Organizing routine reports",
    "section": "",
    "text": "41.1 Preparation",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html#preparation",
    "href": "new_pages/reportfactory.html#preparation",
    "title": "41  Organizing routine reports",
    "section": "",
    "text": "Load packages\nFrom within RStudio, install the latest version of the reportfactory package from Github.\nYou can do this via the pacman package with p_load_current_gh() which will force intall of the latest version from Github. Provide the character string “reconverse/reportfactory”, which specifies the Github organization (reconverse) and repository (reportfactory). You can also use install_github() from the remotes package, as an alternative.\n\n# Install and load the latest version of the package from Github\npacman::p_load_current_gh(\"reconverse/reportfactory\")\n#remotes::install_github(\"reconverse/reportfactory\") # alternative",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html#new-factory",
    "href": "new_pages/reportfactory.html#new-factory",
    "title": "41  Organizing routine reports",
    "section": "41.2 New factory",
    "text": "41.2 New factory\nTo create a new factory, run the function new_factory(). This will create a new self-contained R project folder. By default:\n\nThe factory will be added to your working directory\nThe name of the factory R project will be called “new_factory.Rproj”\n\nYour RStudio session will “move in” to this R project\n\n\n# This will create the factory in the working directory\nnew_factory()\n\nLooking inside the factory, you can see that sub-folders and some files were created automatically.\n\n\n\n\n\n\n\n\n\n\nThe report_sources folder will hold your R Markdown scripts, which generate your reports\n\nThe outputs folder will hold the report outputs (e.g. HTML, Word, PDF, etc.)\n\nThe scripts folder can be used to store other R scripts (e.g. that are sourced by your Rmd scripts)\n\nThe data folder can be used to hold your data (“raw” and “clean” subfolders are included)\n\nA .here file, so you can use the here package to call files in sub-folders by their relation to this root folder (see R projects page for details)\n\nA gitignore file was created in case you link this R project to a Github repository (see Version control and collaboration with Github)\n\nAn empty README file, for if you use a Github repository\n\nCAUTION: depending on your computer’s setting, files such as “.here” may exist but be invisible.\nOf the default settings, below are several that you might want to adjust within the new_factory() command:\n\nfactory = - Provide a name for the factory folder (default is “new_factory”)\n\npath = - Designate a file path for the new factory (default is the working directory)\n\nreport_sources = Provide an alternate name for the subfolder which holds the R Markdown scripts (default is “report_sources”)\n\noutputs = Provide an alternate name for the folder which holds the report outputs (default is “outputs”)\n\nSee ?new_factory for a complete list of the arguments.\nWhen you create the new factory, your R session is transferred to the new R project, so you should again load the reportfactory package.\n\npacman::p_load(reportfactory)\n\nNow you can run a the factory_overview() command to see the internal structure (all folders and files) in the factory.\n\nfactory_overview()            # print overview of the factory to console\n\nThe following “tree” of the factory’s folders and files is printed to the R console. Note that in the “data” folder there are sub-folders for “raw” and “clean” data, and example CSV data. There is also “example_report.Rmd” in the “report_sources” folder.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html#create-a-report",
    "href": "new_pages/reportfactory.html#create-a-report",
    "title": "41  Organizing routine reports",
    "section": "41.3 Create a report",
    "text": "41.3 Create a report\nFrom within the factory R project, create a R Markdown report just as you would normally, and save it into the “report_sources” folder. See the R Markdown page for instructions. For purposes of example, we have added the following to the factory:\n\nA new R markdown script entitled “daily_sitrep.Rmd”, saved within the “report_sources” folder\n\nData for the report (“linelist_cleaned.rds”), saved to the “clean” sub-folder within the “data” folder\n\nWe can see using factory_overview() our R Markdown in the “report_sources” folder and the data file in the “clean” data folder (highlighted):\n\n\n\n\n\n\n\n\n\nBelow is a screenshot of the beginning of the R Markdown “daily_sitrep.Rmd”. You can see that the output format is set to be HTML, via the YAML header output: html_document.\n\n\n\n\n\n\n\n\n\nIn this simple script, there are commands to:\n\nLoad necessary packages\n\nImport the linelist data using a filepath from the here package (read more in the page on Import and export)\n\n\nlinelist &lt;- import(here(\"data\", \"clean\", \"linelist_cleaned.rds\"))\n\n\nPrint a summary table of cases, and export it with export() as a .csv file\n\nPrint an epicurve, and export it with ggsave() as a .png file\n\nYou can review just the list of R Markdown reports in the “report_sources” folder with this command:\n\nlist_reports()",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html#compile",
    "href": "new_pages/reportfactory.html#compile",
    "title": "41  Organizing routine reports",
    "section": "41.4 Compile",
    "text": "41.4 Compile\nIn a report factory, to “compile” a R Markdown report means that the .Rmd script will be run and the output will be produced (as specified in the script YAML e.g. as HTML, Word, PDF, etc).\nThe factory will automatically create a date- and time-stamped folder for the outputs in the “outputs” folder.\nThe report itself and any exported files produced by the script (e.g. csv, png, xlsx) will be saved into this folder. In addition, the Rmd script itself will be saved in this folder, so you have a record of that version of the script.\nThis contrasts with the normal behavior of a “knitted” R Markdown, which saves outputs to the location of the Rmd script. This default behavior can result in crowded, messy folders. The factory aims to improve organization when one needs to run reports frequently.\n\nCompile by name\nYou can compile a specific report by running compile_reports() and providing the Rmd script name (without .Rmd extension) to reports =. For simplicity, you can skip the reports = and just write the R Markdown name in quotes, as below.\n\n\n\n\n\n\n\n\n\nThis command would compile only the “daily_sitrep.Rmd” report, saving the HTML report, and the .csv table and .png epicurve exports into a date- and time-stamped sub-folder specific to the report, within the “outputs” folder.\nNote that if you choose to provide the .Rmd extension, you must correctly type the extension as it is saved in the file name (.rmd vs. .Rmd).\nAlso note that when you compile, you may see several files temporarily appear in the “report_sources” folder - but they will soon disappear as they are transferred to the correct “outputs” folder.\n\n\nCompile by number\nYou can also specify the Rmd script to compile by providing a number or vector of numbers to reports =. The numbers must align with the order the reports appear when you run list_reports().\n\n# Compile the second and fourth Rmds in the \"report_sources\" folder\ncompile_reports(reports = c(2, 4))\n\n\n\nCompile all\nYou can compile all the R Markdown reports in the “report_sources” folder by setting the reports = argument to TRUE.\n\n\n\n\n\n\n\n\n\n\n\nCompile from sub-folder\nYou can add sub-folders to the “report_sources” folder. To run an R Markdown report from a subfolder, simply provide the name of the folder to subfolder =. Below is an example of code to compile a Rmd report that lives in a sub_folder of “report_sources”.\n\ncompile_reports(\n     reports = \"summary_for_partners.Rmd\",\n     subfolder = \"for_partners\")\n\nYou can compile all Rmd reports within a subfolder by providing the subfolder name to reports =, with a slash on the end, as below.\n\ncompile_reports(reports = \"for_partners/\")\n\n\n\nParameterization\nAs noted in the page on Reports with R Markdown, you can run reports with specified parameters. You can pass these parameters as a list to compile_reports() via the params = argument. For example, in this fictional report there are three parameters provided to the R Markdown reports.\n\ncompile_reports(\n  reports = \"daily_sitrep.Rmd\",\n  params = list(most_recent_data = TRUE,\n                region = \"NORTHERN\",\n                rates_denominator = 10000),\n  subfolder = \"regional\"\n)\n\n\n\nUsing a “run-file”\nIf you have multiple reports to run, consider creating a R script that contains all the compile_reports() commands. A user can simply run all the commands in this R script and all the reports will compile. You can save this “run-file” to the “scripts” folder.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html#outputs",
    "href": "new_pages/reportfactory.html#outputs",
    "title": "41  Organizing routine reports",
    "section": "41.5 Outputs",
    "text": "41.5 Outputs\nAfter we have compiled the reports a few times, the “outputs” folder might look like this (highlights added for clarity):\n\n\n\n\n\n\n\n\n\n\nWithin “outputs”, sub-folders have been created for each Rmd report\n\nWithin those, further sub-folders have been created for each unique compiling\n\nThese are date- and time-stamped (“2021-04-23_T11-07-36” means 23rd April 2021 at 11:07:36)\n\nYou can edit the date/time-stamp format. See ?compile_reports\n\nWithin each date/time compiled folder, the report output is stored (e.g. HTML, PDF, Word) along with the Rmd script (version control!) and any other exported files (e.g. table.csv, epidemic_curve.png)\n\nHere is a view inside one of the date/time-stamped folders, for the “daily_sitrep” report. The file path is highlighted in yellow for emphasis.\n\n\n\n\n\n\n\n\n\nFinally, below is a screenshot of the HTML report output.\n\n\n\n\n\n\n\n\n\nYou can use list_outputs() to review a list of the outputs.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html#miscellaneous",
    "href": "new_pages/reportfactory.html#miscellaneous",
    "title": "41  Organizing routine reports",
    "section": "41.6 Miscellaneous",
    "text": "41.6 Miscellaneous\n\nKnit\nYou can still “knit” one of your R Markdown reports by pressing the “Knit” button, if you want. If you do this, as by default, the outputs will appear in the folder where the Rmd is saved - the “report_sources” folder. In prior versions of reportfactory, having any non-Rmd files in “report_sources” would prevent compiling, but this is no longer the case. You can run compile_reports() and no error will occur.\n\n\nScripts\nWe encourage you to utilize the “scripts” folder to store “runfiles” or .R scripts that are sourced by your .Rmd scripts. See the page on R Markdown for tips on how to structure your code across several files.\n\n\nExtras\n\nWith reportfactory, you can use the function list_deps() to list all packages required across all the reports in the entire factory.\nThere is an accompanying package in development called rfextras that offers more helper functions to assist you in building reports, such as:\n\nload_scripts() - sources/loads all .R scripts in a given folder (the “scripts” folder by default)\n\nfind_latest() - finds the latest version of a file (e.g. the latest dataset)",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/reportfactory.html#resources",
    "href": "new_pages/reportfactory.html#resources",
    "title": "41  Organizing routine reports",
    "section": "41.7 Resources",
    "text": "41.7 Resources\nSee the reportfactory package’s Github page\nSee the rfextras package’s Github page",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Organizing routine reports</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html",
    "href": "new_pages/flexdashboard.html",
    "title": "42  Dashboards with R Markdown",
    "section": "",
    "text": "42.1 Preparation",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#preparation",
    "href": "new_pages/flexdashboard.html#preparation",
    "title": "42  Dashboards with R Markdown",
    "section": "",
    "text": "Load packages\nIn this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\npacman::p_load(\n  rio,             # data import/export     \n  here,            # locate files\n  tidyverse,       # data management and visualization\n  flexdashboard,   # dashboard versions of R Markdown reports\n  shiny,           # interactive figures\n  plotly           # interactive figures\n)\n\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file). Import data with the import() function from the rio package (it handles many file types like .xlsx, .csv, .rds - see the Import and export page for details).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#create-new-r-markdown",
    "href": "new_pages/flexdashboard.html#create-new-r-markdown",
    "title": "42  Dashboards with R Markdown",
    "section": "42.2 Create new R Markdown",
    "text": "42.2 Create new R Markdown\nAfter you have installed the package, create a new R Markdown file by clicking through to File &gt; New file &gt; R Markdown.\n\n\n\n\n\n\n\n\n\nIn the window that opens, select “From Template” and select the “Flex Dashboard” template. You will then be prompted to name the document. In this page’s example, we will name our R Markdown as “outbreak_dashboard.Rmd”.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#the-script",
    "href": "new_pages/flexdashboard.html#the-script",
    "title": "42  Dashboards with R Markdown",
    "section": "42.3 The script",
    "text": "42.3 The script\nThe script is an R Markdown script, and so has the same components and organization as described in the page on Reports with R Markdown. We briefly re-visit these and highlight differences from other R Markdown output formats.\n\nYAML\nAt the top of the script is the “YAML” header. This must begin with three dashes --- and must close with three dashes ---. YAML parameters comes in key:value pairs. The indentation and placement of colons in YAML is important - the key:value pairs are separated by colons (not equals signs!).\nThe YAML should begin with metadata for the document. The order of these primary YAML parameters (not indented) does not matter. For example:\n\ntitle: \"My document\"\nauthor: \"Me\"\ndate: \"`r Sys.Date()`\"\n\nYou can use R code in YAML values by putting it like in-line code (preceeded by r within backticks) but also within quotes (see above for Date).\nA required YAML parameter is output:, which specifies the type of file to be produced (e.g. html_document, pdf_document, word_document, or powerpoint_presentation). For flexdashboard this parameter value is a bit confusing - it must be set as output:flexdashboard::flex_dashboard. Note the single and double colons, and the underscore. This YAML output parameter is often followed by an additional colon and indented sub-parameters (see orientation: and vertical_layout: parameters below).\n\ntitle: \"My dashboard\"\nauthor: \"Me\"\ndate: \"`r Sys.Date()`\"\noutput:\n  flexdashboard::flex_dashboard:\n    orientation: rows\n    vertical_layout: scroll\n\nAs shown above, indentations (2 spaces) are used for sub-parameters. In this case, do not forget to put an additional colon after the primary, like key:value:.\nIf appropriate, logical values should be given in YAML in lowercase (true, false, null). If a colon is part of your value (e.g. in the title) put the value in quotes. See the examples in sections below.\n\n\nCode chunks\nAn R Markdown script can contain multiple code “chunks” - these are areas of the script where you can write multiple-line R code and they function just like mini R scripts.\nCode chunks are created with three back-ticks and curly brackets with a lowercase “r” within. The chunk is closed with three backticks. You can create a new chunk by typing it out yourself, by using the keyboard shortcut “Ctrl + Alt + i” (or Cmd + Shift + r in Mac), or by clicking the green ‘insert a new code chunk’ icon at the top of your script editor. Many examples are given below.\n\n\nNarrative text\nOutside of an R code “chunk”, you can write narrative text. As described in the page on Reports with R Markdown, you can italicize text by surrounding it with one asterisk (*), or bold by surrounding it with two asterisks (**). Recall that bullets and numbering schemes are sensitive to newlines, indentation, and finishing a line with two spaces.\nYou can also insert in-line R code into text as described in the Reports with R Markdown page, by surrounding the code with backticks and starting the command with “r”: ` 1+1`(see example with date above).\n\n\nHeadings\nDifferent heading levels are established with different numbers of hash symbols, as described in the Reports with R Markdown page.\nIn flexdashboard, a primary heading (#) creates a “page” of the dashboard. Second-level headings (##) create a column or a row depending on your orientation: parameter (see details below). Third-level headings (###) create panels for plots, charts, tables, text, etc.\n# First-level heading (page)\n\n## Second level heading (row or column)  \n\n### Third-level heading (pane for plot, chart, etc.)",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#section-attributes",
    "href": "new_pages/flexdashboard.html#section-attributes",
    "title": "42  Dashboards with R Markdown",
    "section": "42.4 Section attributes",
    "text": "42.4 Section attributes\nAs in a normal R markdown, you can specify attributes to apply to parts of your dashboard by including key=value options after a heading, within curly brackets { }. For example, in a typical HTML R Markdown report you might organize sub-headings into tabs with ## My heading {.tabset}.\nNote that these attributes are written after a heading in a text portion of the script. These are different than the knitr options inserted within at the top of R code chunks, such as out.height =.\nSection attributes specific to flexdashboard include:\n\n{data-orientation=} Set to either rows or columns. If your dashboard has multiple pages, add this attribute to each page to indicate orientation (further explained in layout section).\n\n{data-width=} and {data-height=} set relative size of charts, columns, rows laid out in the same dimension (horizontal or vertical). Absolute sizes are adjusted to best fill the space on any display device thanks to the flexbox engine.\n\nHeight of charts also depends on whether you set the YAML parameter vertical_layout: fill or vertical_layout: scroll. If set to scroll, figure height will reflect the traditional fig.height = option in the R code chunk.\n\nSee complete size documentation at the flexdashboard website\n\n\n{.hidden} Use this to exclude a specific page from the navigation bar\n\n{data-navbar=} Use this in a page-level heading to nest it within a navigation bar drop-down menu. Provide the name (in quotes) of the drop-down menu. See example below.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#layout",
    "href": "new_pages/flexdashboard.html#layout",
    "title": "42  Dashboards with R Markdown",
    "section": "42.5 Layout",
    "text": "42.5 Layout\nAdjust the layout of your dashboard in the following ways:\n\nAdd pages, columns/rows, and charts with R Markdown headings (e.g. #, ##, or ###)\n\nAdjust the YAML parameter orientation: to either rows or columns\n\nSpecify whether the layout fills the browser or allows scrolling\n\nAdd tabs to a particular section heading\n\n\nPages\nFirst-level headings (#) in the R Markdown will represent “pages” of the dashboard. By default, pages will appear in a navigation bar along the top of the dashboard.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can group pages into a “menu” within the top navigation bar by adding the attribute {data-navmenu=} to the page heading. Be careful - do not include spaces around the equals sign otherwise it will not work!\n\n\n\n\n\n\n\n\n\nHere is what the script produces:\n\n\n\n\n\n\n\n\n\nYou can also convert a page or a column into a “sidebar” on the left side of the dashboard by adding the {.sidebar} attribute. It can hold text (viewable from any page), or if you have integrated shiny interactivity it can be useful to hold user-input controls such as sliders or drop-down menus.\n\n\n\n\n\n\n\n\n\nHere is what the script produces:\n\n\n\n\n\n\n\n\n\n\n\nOrientation\nSet the orientation: yaml parameter to indicate how your second-level (##) R Markdown headings should be interpreted - as either orientation: columns or orientation: rows.\nSecond-level headings (##) will be interpreted as new columns or rows based on this orientation setting.\nIf you set orientation: columns, second-level headers will create new columns in the dashboard. The below dashboard has one page, containing two columns, with a total of three panels. You can adjust the relative width of the columns with {data-width=} as shown below.\n\n\n\n\n\n\n\n\n\nHere is what the script produces:\n\n\n\n\n\n\n\n\n\nIf you set orientation: rows, second-level headers will create new rows instead of columns. Below is the same script as above, but orientation: rows so that second-level headings produce rows instead of columns. You can adjust the relative height of the rows with {data-height=} as shown below.\n\n\n\n\n\n\n\n\n\nHere is what the script produces:\n\n\n\n\n\n\n\n\n\nIf your dashboard has multiple pages, you can designate the orientation for each specific page by adding the {data-orientation=} attribute the header of each page (specify either rows or columns without quotes).\n\n\nTabs\nYou can divide content into tabs with the {.tabset} attribute, as in other HTML R Markdown outputs.\nSimply add this attribute after the desired heading. Sub-headings under that heading will be displayed as tabs. For example, in the example script below column 2 on the right (##) is modified so that the epidemic curve and table panes (###) are displayed in tabs.\nYou can do the same with rows if your orientation is rows.\n\n\n\n\n\n\n\n\n\nHere is what the script produces:",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#adding-content",
    "href": "new_pages/flexdashboard.html#adding-content",
    "title": "42  Dashboards with R Markdown",
    "section": "42.6 Adding content",
    "text": "42.6 Adding content\nLet’s begin to build a dashboard. Our simple dashboard will have 1 page, 2 columns, and 4 panels. We will build the panels piece-by-piece for demonstration.\nYou can easily include standard R outputs such as text, ggplots, and tables (see Tables for presentation page). Simply code them within an R code chunk as you would for any other R Markdown script.\nNote: you can download the finished Rmd script and HTML dashboard output - see the Download handbook and data page.\n\nText\nYou can type in Markdown text and include in-line code as for any other R Markdown output. See the Reports with R Markdown page for details.\nIn this dashboard we include a summary text panel that includes dynamic text showing the latest hospitalisation date and number of cases reported in the outbreak.\n\n\nTables\nYou can include R code chunks that print outputs such as tables. But the output will look best and respond to the window size if you use the kable() function from knitr to display your tables. The flextable functions may produce tables that are shortened / cut-off.\nFor example, below we feed the linelist() through a count() command to produce a summary table of cases by hospital. Ultimately, the table is piped to knitr::kable() and the result has a scroll bar on the right. You can read more about customizing your table with kable() and kableExtra here.\n\n\n\n\n\n\n\n\n\nHere is what the script produces:\n\n\n\n\n\n\n\n\n\nIf you want to show a dynamic table that allows the user to filter, sort, and/or click through “pages” of the data frame, use the package DT and it’s function datatable(), as in the code below.\nThe example code below, the data frame linelist is printed. You can set rownames = FALSE to conserve horizontal space, and filter = \"top\" to have filters on top of every column. A list of other specifications can be provided to options =. Below, we set pageLength = so that 5 rows appear and scrollX = so the user can use a scroll bar on the bottom to scroll horizontally. The argument class = 'white-space: nowrap' ensures that each row is only one line (not multiple lines). You can read about other possible arguments and values here or by entering ?datatable\n\nDT::datatable(linelist, \n              rownames = FALSE, \n              options = list(pageLength = 5, scrollX = TRUE), \n              class = 'white-space: nowrap' )\n\n\n\nPlots\nYou can print plots to a dashboard pane as you would in an R script. In our example, we use the incidence2 package to create an “epicurve” by age group with two simple commands (see Epidemic curves page). However, you could use ggplot() and print a plot in the same manner.\n\n\n\n\n\n\n\n\n\nHere is what the script produces:\n\n\n\n\n\n\n\n\n\n\n\nInteractive plots\nYou can also pass a standard ggplot or other plot object to ggplotly() from the plotly package (see the Interactive plots page). This will make your plot interactive, allow the reader to “zoom in”, and show-on-hover the value of every data point (in this scenario the number of cases per week and age group in the curve).\n\nage_outbreak &lt;- incidence(linelist, date_onset, \"week\", groups = age_cat)\nplot(age_outbreak, fill = age_cat, col_pal = muted, title = \"\") %&gt;% \n  plotly::ggplotly()\n\nHere is what this looks like in the dashboard (gif). This interactive functionality will still work even if you email the dashboard as a static file (not online on a server).\n\n\n\n\n\n\n\n\n\n\n\nHTML widgets\nHTML widgets for R are a special class of R packages that increases interactivity by utilizing JavaScript libraries. You can embed them in R Markdown outputs (such as a flexdashboard) and in Shiny dashboards.\nSome common examples of these widgets include:\n\nPlotly (used in this handbook page and in the Interative plots page)\nvisNetwork (used in the Transmission Chains page of this handbook)\n\nLeaflet (used in the GIS Basics page of this handbook)\n\ndygraphs (useful for interactively showing time series data)\n\nDT (datatable()) (used to show dynamic tables with filter, sort, etc.)\n\nBelow we demonstrate adding an epidemic transmission chain which uses visNetwork to the dashboard. The script shows only the new code added to the “Column 2” section of the R Markdown script. You can find the code in the Transmission chains page of this handbook.\n\n\n\n\n\n\n\n\n\nHere is what the script produces:",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#code-organization",
    "href": "new_pages/flexdashboard.html#code-organization",
    "title": "42  Dashboards with R Markdown",
    "section": "42.7 Code organization",
    "text": "42.7 Code organization\nYou may elect to have all code within the R Markdown flexdashboard script. Alternatively, to have a more clean and concise dashboard script you may choose to call upon code/figures that are hosted or created in external R scripts. This is described in greater detail in the Reports with R Markdown page.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#shiny",
    "href": "new_pages/flexdashboard.html#shiny",
    "title": "42  Dashboards with R Markdown",
    "section": "42.8 Shiny",
    "text": "42.8 Shiny\nIntegrating the R package shiny can make your dashboards even more reactive to user input. For example, you could have the user select a jurisdiction, or a date range, and have panels react to their choice (e.g. filter the data displayed). To embed shiny reactivity into flexdashboard, you need only make a few changes to your flexdashboard R Markdown script.\nYou can use shiny to produce apps/dashboards without flexdashboard too. The handbook page on Dashboards with Shiny gives an overview of this approach, including primers on shiny syntax, app file structure, and options for sharing/publishing (including free server options). These syntax and general tips translate into the flexdashboard context as well.\nEmbedding shiny in flexdashboard is however, a fundamental change to your flexdashboard. It will no longer produce an HTML output that you can send by email and anyone could open and view. Instead, it will be an “app”. The “Knit” button at the top of the script will be replaced by a “Run document” icon, which will open an instance of the interactive the dashboard locally on your computer.\nSharing your dashboard will now require that you either:\n\nSend the Rmd script to the viewer, they open it in R on their computer, and run the app, or\n\nThe app/dashboard is hosted on a server accessible to the viewer\n\nThus, there are benefits to integrating shiny, but also complications. If easy sharing by email is a priority and you don’t need shiny reactive capabilities, consider the reduced interactivity offered by ggplotly() as demonstrated above.\nBelow we give a very simple example using the same “outbreak_dashboard.Rmd” as above. Extensive documentation on integrating Shiny into flexdashboard is available online here.\n\nSettings\nEnable shiny in a flexdashboard by adding the YAML parameter runtime: shiny at the same indentation level as output:, as below:\n---\ntitle: \"Outbreak dashboard (Shiny demo)\"\noutput: \n  flexdashboard::flex_dashboard:\n    orientation: columns\n    vertical_layout: fill\nruntime: shiny\n---\nIt is also convenient to enable a “side bar” to hold the shiny input widgets that will collect information from the user. As explained above, create a column and indicate the {.sidebar} option to create a side bar on the left side. You can add text and R chunks containing the shiny input commands within this column.\nIf your app/dashboard is hosted on a server and may have multiple simultaneous users, name the first R code chunk as global. Include the commands to import/load your data in this chunk. This special named chunk is treated differently, and the data imported within it are only imported once (not continuously) and are available for all users. This improves the start-up speed of the app.\n\n\nWorked example\nHere we adapt the flexdashboard script “outbreak_dashboard.Rmd” to include shiny. We will add the capability for the user to select a hospital from a drop-down menu, and have the epidemic curve reflect only cases from that hospital, with a dynamic plot title. We do the following:\n\nAdd runtime: shiny to the YAML\n\nRe-name the setup chunk as global\n\nCreate a sidebar containing:\n\nCode to create a vector of unique hospital names\n\nA selectInput() command (shiny drop-down menu) with the choice of hospital names. The selection is saved as hospital_choice, which can be referenced in later code as input$hospital_choice\n\n\nThe epidemic curve code (column 2) is wrapped within renderPlot({ }), including:\n\nA filter on the dataset restricting the column hospital to the current value of input$hospital_choice\n\nA dynamic plot title that incorporates input$hospital_choice\n\n\nNote that any code referencing an input$ value must be within a render({}) function (to be reactive).\nHere is the top of the script, including YAML, global chunk, and sidebar:\n\n\n\n\n\n\n\n\n\nHere is the Column 2, with the reactive epicurve plot:\n\n\n\n\n\n\n\n\n\nAnd here is the dashboard:\n\n\n\n\n\n\n\n\n\n\n\nOther examples\nTo read a health-related example of a Shiny-flexdashboard using the shiny interactivity and the leaflet mapping widget, see this chapter of the online book Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#sharing",
    "href": "new_pages/flexdashboard.html#sharing",
    "title": "42  Dashboards with R Markdown",
    "section": "42.9 Sharing",
    "text": "42.9 Sharing\nDashboards that do not contain Shiny elements will output an HTML file (.html), which can be emailed (if size permits). This is useful, as you can send the “dashboard” report and not have to set up a server to host it as a website.\nIf you have embedded shiny, you will not be able to send an output by email, but you can send the script itself to an R user, or host the dashboard on a server as explained above.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/flexdashboard.html#resources",
    "href": "new_pages/flexdashboard.html#resources",
    "title": "42  Dashboards with R Markdown",
    "section": "42.10 Resources",
    "text": "42.10 Resources\nExcellent tutorials that informed this page can be found below. If you review these, most likely within an hour you can have your own dashboard.\nhttps://bookdown.org/yihui/rmarkdown/dashboards.html\nhttps://rmarkdown.rstudio.com/flexdashboard/\nhttps://rmarkdown.rstudio.com/flexdashboard/using.html\nhttps://rmarkdown.rstudio.com/flexdashboard/examples.html",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Dashboards with R Markdown</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html",
    "href": "new_pages/shiny_basics.html",
    "title": "43  Dashboards with Shiny",
    "section": "",
    "text": "43.1 Preparation",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#preparation",
    "href": "new_pages/shiny_basics.html#preparation",
    "title": "43  Dashboards with Shiny",
    "section": "",
    "text": "Load packages\nIn this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\nWe begin by installing the shiny R package:\n\npacman::p_load(shiny)\n\n\n\nImport data\nIf you would like to follow-along with this page, see this section of the Download handbook and data. There are links to download the R scripts and data files that produce the final Shiny app.\nIf you try to re-construct the app using these files, please be aware of the R project folder structure that is created over the course of the demonstration (e.g. folders for “data” and for “funcs”).",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#the-structure-of-a-shiny-app",
    "href": "new_pages/shiny_basics.html#the-structure-of-a-shiny-app",
    "title": "43  Dashboards with Shiny",
    "section": "43.2 The structure of a shiny app",
    "text": "43.2 The structure of a shiny app\n\nBasic file structures\nTo understand shiny, we first need to understand how the file structure of an app works! We should make a brand new directory before we start. This can actually be made easier by choosing New project in Rstudio, and choosing Shiny Web Application. This will create the basic structure of a shiny app for you.\nWhen opening this project, you’ll notice there is a .R file already present called app.R. It is essential that we have one of two basic file structures:\n\nOne file called app.R, or\n\nTwo files, one called ui.R and the other server.R\n\nIn this page, we will use the first approach of having one file called app.R. Here is an example script:\n\n# an example of app.R\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"My app\"),\n\n    # Sidebar with a slider input widget\n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"input_1\")\n        ),\n\n        # Show a plot \n        mainPanel(\n           plotOutput(\"my_plot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n     \n     plot_1 &lt;- reactive({\n          plot_func(param = input_1)\n     })\n     \n    output$my_plot &lt;- renderPlot({\n       plot_1()\n    })\n}\n\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nIf you open this file, you’ll notice that two objects are defined - one called ui and another called server. These objects must be defined in every shiny app and are central to the structure of the app itself! In fact, the only difference between the two file structures described above is that in structure 1, both ui and server are defined in one file, whereas in structure 2 they are defined in separate files. Note: we can also (and we should if we have a larger app) have other .R files in our structure that we can source() into our app.\n\n\nThe server and the ui\nWe next need to understand what the server and ui objects actually do. Put simply, these are two objects that are interacting with each other whenever the user interacts with the shiny app.\nThe UI element of a shiny app is, on a basic level, R code that creates an HTML interface. This means everything that is displayed in the UI of an app. This generally includes:\n\n“Widgets” - dropdown menus, check boxes, sliders, etc that can be interacted with by the user\nPlots, tables, etc - outputs that are generated with R code\nNavigation aspects of an app - tabs, panes, etc.\nGeneric text, hyperlinks, etc\nHTML and CSS elements (addressed later)\n\nThe most important thing to understand about the UI is that it receives inputs from the user and displays outputs from the server. There is no active code running in the ui at any time - all changes seen in the UI are passed through the server (more or less). So we have to make our plots, downloads, etc in the server\nThe server of the shiny app is where all code is being run once the app starts up. The way this works is a little confusing. The server function will effectively react to the user interfacing with the UI, and run chunks of code in response. If things change in the server, these will be passed back up to the ui, where the changes can be seen. Importantly, the code in the server will be executed non-consecutively (or it’s best to think of it this way). Basically, whenever a ui input affects a chunk of code in the server, it will run automatically, and that output will be produced and displayed.\nThis all probably sounds very abstract for now, so we’ll have to dive into some examples to get a clear idea of how this actually works.\n\n\nBefore you start to build an app\nBefore you begin to build an app, its immensely helpful to know what you want to build. Since your UI will be written in code, you can’t really visualise what you’re building unless you are aiming for something specific. For this reason, it is immensely helpful to look at lots of examples of shiny apps to get an idea of what you can make - even better if you can look at the source code behind these apps! Some great resources for this are:\n\nThe Rstudio app gallery\n\nOnce you get an idea for what is possible, it’s also helpful to map out what you want yours to look like - you can do this on paper or in any drawing software (PowerPoint, MS paint, etc.). It’s helpful to start simple for your first app! There’s also no shame in using code you find online of a nice app as a template for your work - its much easier than building something from scratch!",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#building-a-ui",
    "href": "new_pages/shiny_basics.html#building-a-ui",
    "title": "43  Dashboards with Shiny",
    "section": "43.3 Building a UI",
    "text": "43.3 Building a UI\nWhen building our app, its easier to work on the UI first so we can see what we’re making, and not risk the app failing because of any server errors. As mentioned previously, its often good to use a template when working on the UI. There are a number of standard layouts that can be used with shiny that are available from the base shiny package, but it’s worth noting that there are also a number of package extensions such as shinydashboard. We’ll use an example from base shiny to start with.\nA shiny UI is generally defined as a series of nested functions, in the following order\n\nA function defining the general layout (the most basic is fluidPage(), but more are available)\nPanels within the layout such as:\n\na sidebar (sidebarPanel())\na “main” panel (mainPanel())\na tab (tabPanel())\na generic “column” (column())\n\nWidgets and outputs - these can confer inputs to the server (widgets) or outputs from the server (outputs)\n\nWidgets generally are styled as xxxInput() e.g. selectInput()\nOutputs are generally styled as xxxOutput() e.g. plotOutput()\n\n\nIt’s worth stating again that these can’t be visualised easily in an abstract way, so it’s best to look at an example! Lets consider making a basic app that visualises our malaria facility count data by district. This data has a lot of differnet parameters, so it would be great if the end user could apply some filters to see the data by age group/district as they see fit! We can use a very simple shiny layout to start - the sidebar layout. This is a layout where widgets are placed in a sidebar on the left, and the plot is placed on the right.\nLets plan our app - we can start with a selector that lets us choose the district where we want to visualise data, and another to let us visualise the age group we are interested in. We’ll aim to use these filters to show an epicurve that reflects these parameters. So for this we need:\n\nTwo dropdown menus that let us choose the district we want, and the age group we’re interested in.\nAn area where we can show our resulting epicurve.\n\nThis might look something like this:\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n\n  titlePanel(\"Malaria facility visualisation app\"),\n\n  sidebarLayout(\n\n    sidebarPanel(\n         # selector for district\n         selectInput(\n              inputId = \"select_district\",\n              label = \"Select district\",\n              choices = c(\n                   \"All\",\n                   \"Spring\",\n                   \"Bolo\",\n                   \"Dingo\",\n                   \"Barnard\"\n              ),\n              selected = \"All\",\n              multiple = TRUE\n         ),\n         # selector for age group\n         selectInput(\n              inputId = \"select_agegroup\",\n              label = \"Select age group\",\n              choices = c(\n                   \"All ages\" = \"malaria_tot\",\n                   \"0-4 yrs\" = \"malaria_rdt_0-4\",\n                   \"5-14 yrs\" = \"malaria_rdt_5-14\",\n                   \"15+ yrs\" = \"malaria_rdt_15\"\n              ), \n              selected = \"All\",\n              multiple = FALSE\n         )\n\n    ),\n\n    mainPanel(\n      # epicurve goes here\n      plotOutput(\"malaria_epicurve\")\n    )\n    \n  )\n)\n\nWhen app.R is run with the above UI code (with no active code in the server portion of app.R) the layout appears looking like this - note that there will be no plot if there is no server to render it, but our inputs are working!\n\n\n\n\n\n\n\n\n\nThis is a good opportunity to discuss how widgets work - note that each widget is accepting an inputId, a label, and a series of other options that are specific to the widget type. This inputId is extremely important - these are the IDs that are used to pass information from the UI to the server. For this reason, they must be unique. You should make an effort to name them something sensible, and specific to what they are interacting with in cases of larger apps.\nYou should read documentation carefully for full details on what each of these widgets do. Widgets will pass specific types of data to the server depending on the widget type, and this needs to be fully understood. For example, selectInput() will pass a character type to the server:\n\nIf we select Spring for the first widget here, it will pass the character object \"Spring\" to the server.\nIf we select two items from the dropdown menu, they will come through as a character vector (e.g. c(\"Spring\", \"Bolo\")).\n\nOther widgets will pass different types of object to the server! For example:\n\nnumericInput() will pass a numeric type object to the server\ncheckboxInput() will pass a logical type object to the server (TRUE or FALSE)\n\nIt’s also worth noting the named vector we used for the age data here. For many widgets, using a named vector as the choices will display the names of the vector as the display choices, but pass the selected value from the vector to the server. I.e. here someone can select “15+” from the drop-down menu, and the UI will pass \"malaria_rdt_15\" to the server - which happens to be the name of the column we’re interested in!\nThere are loads of widgets that you can use to do lots of things with your app. Widgets also allow you to upload files into your app, and download outputs. There are also some excellent shiny extensions that give you access to more widgets than base shiny - the shinyWidgets package is a great example of this. To look at some examples you can look at the following links:\n\nbase shiny widget gallery\nshinyWidgets gallery",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#loading-data-into-our-app",
    "href": "new_pages/shiny_basics.html#loading-data-into-our-app",
    "title": "43  Dashboards with Shiny",
    "section": "43.4 Loading data into our app",
    "text": "43.4 Loading data into our app\nThe next step in our app development is getting the server up and running. To do this however, we need to get some data into our app, and figure out all the calculations we’re going to do. A shiny app is not straightforward to debug, as it’s often not clear where errors are coming from, so it’s ideal to get all our data processing and visualisation code working before we start making the server itself.\nSo given we want to make an app that shows epi curves that change based on user input, we should think about what code we would need to run this in a normal R script. We’ll need to:\n\nLoad our packages\nLoad our data\nTransform our data\nDevelop a function to visualise our data based on user inputs\n\nThis list is pretty straightforward, and shouldn’t be too hard to do. It’s now important to think about which parts of this process need to be done only once and which parts need to run in response to user inputs. This is because shiny apps generally run some code before running, which is only performed once. It will help our app’s performance if as much of our code can be moved to this section. For this example, we only need to load our data/packages and do basic transformations once, so we can put that code outside the server. This means the only thing we’ll need in the server is the code to visualise our data. Lets develop all of these componenets in a script first. However, since we’re visualising our data with a function, we can also put the code for the function outside the server so our function is in the environment when the app runs!\nFirst lets load our data. Since we’re working with a new project, and we want to make it clean, we can create a new directory called data, and add our malaria data in there. We can run this code below in a testing script we will eventually delete when we clean up the structure of our app.\n\npacman::p_load(\"tidyverse\", \"lubridate\")\n\n# read data\nmalaria_data &lt;- rio::import(here::here(\"data\", \"malaria_facility_count_data.rds\")) %&gt;% \n  as_tibble()\n\nprint(malaria_data)\n\n# A tibble: 3,038 × 10\n   location_name data_date  submitted_date Province District `malaria_rdt_0-4`\n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt;\n 1 Facility 1    2020-08-11 2020-08-12     North    Spring                  11\n 2 Facility 2    2020-08-11 2020-08-12     North    Bolo                    11\n 3 Facility 3    2020-08-11 2020-08-12     North    Dingo                    8\n 4 Facility 4    2020-08-11 2020-08-12     North    Bolo                    16\n 5 Facility 5    2020-08-11 2020-08-12     North    Bolo                     9\n 6 Facility 6    2020-08-11 2020-08-12     North    Dingo                    3\n 7 Facility 6    2020-08-10 2020-08-12     North    Dingo                    4\n 8 Facility 5    2020-08-10 2020-08-12     North    Bolo                    15\n 9 Facility 5    2020-08-09 2020-08-12     North    Bolo                    11\n10 Facility 5    2020-08-08 2020-08-12     North    Bolo                    19\n# ℹ 3,028 more rows\n# ℹ 4 more variables: `malaria_rdt_5-14` &lt;int&gt;, malaria_rdt_15 &lt;int&gt;,\n#   malaria_tot &lt;int&gt;, newid &lt;int&gt;\n\n\nIt will be easier to work with this data if we use tidy data standards, so we should also transform into a longer data format, where age group is a column, and cases is another column. We can do this easily using what we’ve learned in the Pivoting data page.\n\nmalaria_data &lt;- malaria_data %&gt;%\n  select(-newid) %&gt;%\n  pivot_longer(cols = starts_with(\"malaria_\"), names_to = \"age_group\", values_to = \"cases_reported\")\n\nprint(malaria_data)\n\n# A tibble: 12,152 × 7\n   location_name data_date  submitted_date Province District age_group       \n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;           \n 1 Facility 1    2020-08-11 2020-08-12     North    Spring   malaria_rdt_0-4 \n 2 Facility 1    2020-08-11 2020-08-12     North    Spring   malaria_rdt_5-14\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring   malaria_rdt_15  \n 4 Facility 1    2020-08-11 2020-08-12     North    Spring   malaria_tot     \n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo     malaria_rdt_0-4 \n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo     malaria_rdt_5-14\n 7 Facility 2    2020-08-11 2020-08-12     North    Bolo     malaria_rdt_15  \n 8 Facility 2    2020-08-11 2020-08-12     North    Bolo     malaria_tot     \n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo    malaria_rdt_0-4 \n10 Facility 3    2020-08-11 2020-08-12     North    Dingo    malaria_rdt_5-14\n# ℹ 12,142 more rows\n# ℹ 1 more variable: cases_reported &lt;int&gt;\n\n\nAnd with that we’ve finished preparing our data! This crosses items 1, 2, and 3 off our list of things to develop for our “testing R script”. The last, and most difficult task will be building a function to produce an epicurve based on user defined parameters. As mentioned previously, it’s highly recommended that anyone learning shiny first look at the section on functional programming (Writing functions) to understand how this works!\nWhen defining our function, it might be hard to think about what parameters we want to include. For functional programming with shiny, every relevent parameter will generally have a widget associated with it, so thinking about this is usually quite easy! For example in our current app, we want to be able to filter by district, and have a widget for this, so we can add a district parameter to reflect this. We don’t have any app functionality to filter by facility (for now), so we don’t need to add this as a parameter. Lets start by making a function with three parameters:\n\nThe core dataset\nThe district of choice\nThe age group of choice\n\n\nplot_epicurve &lt;- function(data, district = \"All\", agegroup = \"malaria_tot\") {\n  \n  if (!(\"All\" %in% district)) {\n    data &lt;- data %&gt;%\n      filter(District %in% district)\n    \n    plot_title_district &lt;- stringr::str_glue(\"{paste0(district, collapse = ', ')} districts\")\n    \n  } else {\n    \n    plot_title_district &lt;- \"all districts\"\n    \n  }\n  \n  # if no remaining data, return NULL\n  if (nrow(data) == 0) {\n    \n    return(NULL)\n  }\n  \n  data &lt;- data %&gt;%\n    filter(age_group == agegroup)\n  \n  \n  # if no remaining data, return NULL\n  if (nrow(data) == 0) {\n    \n    return(NULL)\n  }\n  \n  if (agegroup == \"malaria_tot\") {\n      agegroup_title &lt;- \"All ages\"\n  } else {\n    agegroup_title &lt;- stringr::str_glue(\"{str_remove(agegroup, 'malaria_rdt')} years\")\n  }\n  \n  \n  ggplot(data, aes(x = data_date, y = cases_reported)) +\n    geom_col(width = 1, fill = \"darkred\") +\n    theme_minimal() +\n    labs(\n      x = \"date\",\n      y = \"number of cases\",\n      title = stringr::str_glue(\"Malaria cases - {plot_title_district}\"),\n      subtitle = agegroup_title\n    )\n  \n  \n  \n}\n\nWe won’t go into great detail about this function, as it’s relatively simple in how it works. One thing to note however, is we handle errors by returning NULL when it would otherwise give an error. This is because when a shiny server produces a NULL object instead of a plot object, nothing will be shown in the ui! This is important, as otherwise errors will often cause your app to stop working.\nAnother thing to note is the use of the %in% operator when evaluating the district input. As mentioned above, this could arrive as a character vector with multiple values, so using %in% is more flexible than say, ==.\nLet’s test our function!\n\nplot_epicurve(malaria_data, district = \"Bolo\", agegroup = \"malaria_rdt_0-4\")\n\n\n\n\n\n\n\n\nWith our function working, we now have to understand how this all is going to fit into our shiny app. We mentioned the concept of startup code before, but lets look at how we can actually incorporate this into the structure of our app. There are two ways we can do this!\n\nPut this code in your app.R file at the start of the script (above the UI), or\n\nCreate a new file in your app’s directory called global.R, and put the startup code in this file.\n\nIt’s worth noting at this point that it’s generally easier, especially with bigger apps, to use the second file structure, as it lets you separate your file structure in a simple way. Lets fully develop a this global.R script now. Here is what it could look like:\n\n# global.R script\n\npacman::p_load(\"tidyverse\", \"lubridate\", \"shiny\")\n\n# read data\nmalaria_data &lt;- rio::import(here::here(\"data\", \"malaria_facility_count_data.rds\")) %&gt;% \n  as_tibble()\n\n# clean data and pivot longer\nmalaria_data &lt;- malaria_data %&gt;%\n  select(-newid) %&gt;%\n  pivot_longer(cols = starts_with(\"malaria_\"), names_to = \"age_group\", values_to = \"cases_reported\")\n\n\n# define plotting function\nplot_epicurve &lt;- function(data, district = \"All\", agegroup = \"malaria_tot\") {\n  \n  # create plot title\n  if (!(\"All\" %in% district)) {            \n    data &lt;- data %&gt;%\n      filter(District %in% district)\n    \n    plot_title_district &lt;- stringr::str_glue(\"{paste0(district, collapse = ', ')} districts\")\n    \n  } else {\n    \n    plot_title_district &lt;- \"all districts\"\n    \n  }\n  \n  # if no remaining data, return NULL\n  if (nrow(data) == 0) {\n    \n    return(NULL)\n  }\n  \n  # filter to age group\n  data &lt;- data %&gt;%\n    filter(age_group == agegroup)\n  \n  \n  # if no remaining data, return NULL\n  if (nrow(data) == 0) {\n    \n    return(NULL)\n  }\n  \n  if (agegroup == \"malaria_tot\") {\n      agegroup_title &lt;- \"All ages\"\n  } else {\n    agegroup_title &lt;- stringr::str_glue(\"{str_remove(agegroup, 'malaria_rdt')} years\")\n  }\n  \n  \n  ggplot(data, aes(x = data_date, y = cases_reported)) +\n    geom_col(width = 1, fill = \"darkred\") +\n    theme_minimal() +\n    labs(\n      x = \"date\",\n      y = \"number of cases\",\n      title = stringr::str_glue(\"Malaria cases - {plot_title_district}\"),\n      subtitle = agegroup_title\n    )\n  \n  \n  \n}\n\nEasy! One great feature of shiny is that it will understand what files named app.R, server.R, ui.R, and global.R are for, so there is no need to connect them to each other via any code. So just by having this code in global.R in the directory it will run before we start our app!.\nWe should also note that it would improve our app’s organisation if we moved the plotting function to its own file - this will be especially helpful as apps become larger. To do this, we could make another directory called funcs, and put this function in as a file called plot_epicurve.R. We could then read this function in via the following command in global.R\n\nsource(here(\"funcs\", \"plot_epicurve.R\"), local = TRUE)\n\nNote that you should always specify local = TRUE in shiny apps, since it will affect sourcing when/if the app is published on a server.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#developing-an-app-server",
    "href": "new_pages/shiny_basics.html#developing-an-app-server",
    "title": "43  Dashboards with Shiny",
    "section": "43.5 Developing an app server",
    "text": "43.5 Developing an app server\nNow that we have most of our code, we just have to develop our server. This is the final piece of our app, and is probably the hardest to understand. The server is a large R function, but its helpful to think of it as a series of smaller functions, or tasks that the app can perform. It’s important to understand that these functions are not executed in a linear order. There is an order to them, but it’s not fully necessary to understand when starting out with shiny. At a very basic level, these tasks or functions will activate when there is a change in user inputs that affects them, unless the developer has set them up so they behave differently. Again, this is all quite abstract, but lets first go through the three basic types of shiny objects\n\nReactive sources - this is another term for user inputs. The shiny server has access to the outputs from the UI through the widgets we’ve programmed. Every time the values for these are changed, this is passed down to the server.\nReactive conductors - these are objects that exist only inside the shiny server. We don’t actually need these for simple apps, but they produce objects that can only be seen inside the server, and used in other operations. They generally depend on reactive sources.\nEndpoints - these are outputs that are passed from the server to the UI. In our example, this would be the epi curve we are producing.\n\nWith this in mind lets construct our server step-by-step. We’ll show our UI code again here just for reference:\n\nui &lt;- fluidPage(\n\n  titlePanel(\"Malaria facility visualisation app\"),\n\n  sidebarLayout(\n\n    sidebarPanel(\n         # selector for district\n         selectInput(\n              inputId = \"select_district\",\n              label = \"Select district\",\n              choices = c(\n                   \"All\",\n                   \"Spring\",\n                   \"Bolo\",\n                   \"Dingo\",\n                   \"Barnard\"\n              ),\n              selected = \"All\",\n              multiple = TRUE\n         ),\n         # selector for age group\n         selectInput(\n              inputId = \"select_agegroup\",\n              label = \"Select age group\",\n              choices = c(\n                   \"All ages\" = \"malaria_tot\",\n                   \"0-4 yrs\" = \"malaria_rdt_0-4\",\n                   \"5-14 yrs\" = \"malaria_rdt_5-14\",\n                   \"15+ yrs\" = \"malaria_rdt_15\"\n              ), \n              selected = \"All\",\n              multiple = FALSE\n         )\n\n    ),\n\n    mainPanel(\n      # epicurve goes here\n      plotOutput(\"malaria_epicurve\")\n    )\n    \n  )\n)\n\nFrom this code UI we have:\n\nTwo inputs:\n\nDistrict selector (with an inputId of select_district)\nAge group selector (with an inputId of select_agegroup)\n\nOne output:\n\nThe epicurve (with an outputId of malaria_epicurve)\n\n\nAs stated previously, these unique names we have assigned to our inputs and outputs are crucial. They must be unique and are used to pass information between the ui and server. In our server, we access our inputs via the syntax input$inputID and outputs and passed to the ui through the syntax output$output_name Lets have a look at an example, because again this is hard to understand otherwise!\n\nserver &lt;- function(input, output, session) {\n  \n  output$malaria_epicurve &lt;- renderPlot(\n    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)\n  )\n  \n}\n\nThe server for a simple app like this is actually quite straightforward! You’ll notice that the server is a function with three parameters - input, output, and session - this isn’t that important to understand for now, but its important to stick to this setup! In our server we only have one task - this renders a plot based on our function we made earlier, and the inputs from the server. Notice how the names of the input and output objects correspond exactly to those in the ui.\nTo understand the basics of how the server reacts to user inputs, you should note that the output will know (through the underlying package) when inputs change, and rerun this function to create a plot every time they change. Note that we also use the renderPlot() function here - this is one of a family of class-specific functions that pass those objects to a ui output. There are a number of functions that behave similarly, but you need to ensure the function used matches the class of object you’re passing to the ui! For example:\n\nrenderText() - send text to the ui\nrenderDataTable - send an interactive table to the ui.\n\nRemember that these also need to match the output function used in the ui - so renderPlot() is paired with plotOutput(), and renderText() is matched with textOutput().\nSo we’ve finally made a functioning app! We can run this by pressing the Run App button on the top right of the script window in Rstudio. You should note that you can choose to run your app in your default browser (rather than Rstudio) which will more accurately reflect what the app will look like for other users.\n\n\n\n\n\n\n\n\n\nIt is fun to note that in the R console, the app is “listening”! Talk about reactivity!",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#adding-more-functionality",
    "href": "new_pages/shiny_basics.html#adding-more-functionality",
    "title": "43  Dashboards with Shiny",
    "section": "43.6 Adding more functionality",
    "text": "43.6 Adding more functionality\nAt this point we’ve finally got a running app, but we have very little functionality. We also haven’t really scratched the surface of what shiny can do, so there’s a lot more to learn about! Lets continue to build our existing app by adding some extra features. Some things that could be nice to add could be:\n\nSome explanatory text\nA download button for our plot - this would provide the user with a high quality version of the image that they’re generating in the app\nA selector for specific facilities\nAnother dashboard page - this could show a table of our data.\n\nThis is a lot to add, but we can use it to learn about a bunch of different shiny featues on the way. There is so much to learn about shiny (it can get very advanced, but its hopefully the case that once users have a better idea of how to use it they can become more comfortable using external learning sources as well).\n\nAdding static text\nLets first discuss adding static text to our shiny app. Adding text to our app is extremely easy, once you have a basic grasp of it. Since static text doesn’t change in the shiny app (If you’d like it to change, you can use text rendering functions in the server!), all of shiny’s static text is generally added in the ui of the app. We wont go through this in great detail, but you can add a number of different elements to your ui (and even custom ones) by interfacing R with HTML and css.\nHTML and css are languages that are explicitly involved in user interface design. We don’t need to understand these too well, but HTML creates objects in UI (like a text box, or a table), and css is generally used to change the style and aesthetics of those objects. Shiny has access to a large array of HTML tags - these are present for objects that behave in a specific way, such as headers, paragraphs of text, line breaks, tables, etc. We can use some of these examples like this:\n\nh1() - this a a header tag, which will make enclosed text automatically larger, and change defaults as they pertain to the font face, colour etc (depending on the overall theme of your app). You can access smaller and smaller sub-heading with h2() down to h6() as well. Usage looks like:\n\nh1(\"my header - section 1\")\n\np() - this is a paragraph tag, which will make enclosed text similar to text in a body of text. This text will automatically wrap, and be of a relatively small size (footers could be smaller for example.) Think of it as the text body of a word document. Usage looks like:\n\np(\"This is a larger body of text where I am explaining the function of my app\")\n\ntags$b() and tags$i() - these are used to create bold tags$b() and italicised tags$i() with whichever text is enclosed!\ntags$ul(), tags$ol() and tags$li() - these are tags used in creating lists. These are all used within the syntax below, and allow the user to create either an ordered list (tags$ol(); i.e. numbered) or unordered list (tags$ul(), i.e. bullet points). tags$li() is used to denote items in the list, regardless of which type of list is used. e.g.:\n\n\ntags$ol(\n  \n  tags$li(\"Item 1\"),\n  \n  tags$li(\"Item 2\"),\n  \n  tags$li(\"Item 3\")\n  \n)\n\n\nbr() and hr() - these tags create linebreaks and horizontal lines (with a linebreak) respectively. Use them to separate out the sections of your app and text! There is no need to pass any items to these tags (parentheses can remain empty).\ndiv() - this is a generic tag that can contain anything, and can be named anything. Once you progress with ui design, you can use these to compartmentalize your ui, give specific sections specific styles, and create interactions between the server and UI elements. We won’t go into these in detail, but they’re worth being aware of!\n\nNote that every one of these objects can be accessed through tags$... or for some, just the function. These are effectively synonymous, but it may help to use the tags$... style if you’d rather be more explicit and not overwrite the functions accidentally. This is also by no means an exhaustive list of tags available. There is a full list of all tags available in shiny here and even more can be used by inserting HTML directly into your ui!\nIf you’re feeling confident, you can also add any css styling elements to your HTML tags with the style argument in any of them. We won’t go into how this works in detail, but one tip for testing aesthetic changes to a UI is using the HTML inspector mode in chrome (of your shiny app you are running in browser), and editing the style of objects yourself!\nLets add some text to our app\n\nui &lt;- fluidPage(\n\n  titlePanel(\"Malaria facility visualisation app\"),\n\n  sidebarLayout(\n\n    sidebarPanel(\n         h4(\"Options\"),\n         # selector for district\n         selectInput(\n              inputId = \"select_district\",\n              label = \"Select district\",\n              choices = c(\n                   \"All\",\n                   \"Spring\",\n                   \"Bolo\",\n                   \"Dingo\",\n                   \"Barnard\"\n              ),\n              selected = \"All\",\n              multiple = TRUE\n         ),\n         # selector for age group\n         selectInput(\n              inputId = \"select_agegroup\",\n              label = \"Select age group\",\n              choices = c(\n                   \"All ages\" = \"malaria_tot\",\n                   \"0-4 yrs\" = \"malaria_rdt_0-4\",\n                   \"5-14 yrs\" = \"malaria_rdt_5-14\",\n                   \"15+ yrs\" = \"malaria_rdt_15\"\n              ), \n              selected = \"All\",\n              multiple = FALSE\n         ),\n    ),\n\n    mainPanel(\n      # epicurve goes here\n      plotOutput(\"malaria_epicurve\"),\n      br(),\n      hr(),\n      p(\"Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:\"),\n    tags$ul(\n      tags$li(tags$b(\"location_name\"), \" - the facility that the data were collected at\"),\n      tags$li(tags$b(\"data_date\"), \" - the date the data were collected at\"),\n      tags$li(tags$b(\"submitted_daate\"), \" - the date the data were submitted at\"),\n      tags$li(tags$b(\"Province\"), \" - the province the data were collected at (all 'North' for this dataset)\"),\n      tags$li(tags$b(\"District\"), \" - the district the data were collected at\"),\n      tags$li(tags$b(\"age_group\"), \" - the age group the data were collected for (0-5, 5-14, 15+, and all ages)\"),\n      tags$li(tags$b(\"cases_reported\"), \" - the number of cases reported for the facility/age group on the given date\")\n    )\n    \n  )\n)\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAdding a link\nTo add a link to a website, use tags$a() with the link and display text as shown below. To have as a standalone paragraph, put it within p(). To have only a few words of a sentence linked, break the sentence into parts and use tags$a() for the hyperlinked part. To ensure the link opens in a new browser window, add target = \"_blank\" as an argument.\n\ntags$a(href = \"www.epiRhandbook.com\", \"Visit our website!\")\n\n\n\nAdding a download button\nLets move on to the second of the three features. A download button is a fairly common thing to add to an app and is fairly easy to make. We need to add another Widget to our ui, and we need to add another output to our server to attach to it. We can also introduce reactive conductors in this example!\nLets update our ui first - this is easy as shiny comes with a widget called downloadButton() - lets give it an inputId and a label.\n\nui &lt;- fluidPage(\n\n  titlePanel(\"Malaria facility visualisation app\"),\n\n  sidebarLayout(\n\n    sidebarPanel(\n         # selector for district\n         selectInput(\n              inputId = \"select_district\",\n              label = \"Select district\",\n              choices = c(\n                   \"All\",\n                   \"Spring\",\n                   \"Bolo\",\n                   \"Dingo\",\n                   \"Barnard\"\n              ),\n              selected = \"All\",\n              multiple = FALSE\n         ),\n         # selector for age group\n         selectInput(\n              inputId = \"select_agegroup\",\n              label = \"Select age group\",\n              choices = c(\n                   \"All ages\" = \"malaria_tot\",\n                   \"0-4 yrs\" = \"malaria_rdt_0-4\",\n                   \"5-14 yrs\" = \"malaria_rdt_5-14\",\n                   \"15+ yrs\" = \"malaria_rdt_15\"\n              ), \n              selected = \"All\",\n              multiple = FALSE\n         ),\n         # horizontal line\n         hr(),\n         downloadButton(\n           outputId = \"download_epicurve\",\n           label = \"Download plot\"\n         )\n\n    ),\n\n    mainPanel(\n      # epicurve goes here\n      plotOutput(\"malaria_epicurve\"),\n      br(),\n      hr(),\n      p(\"Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:\"),\n      tags$ul(\n        tags$li(tags$b(\"location_name\"), \" - the facility that the data were collected at\"),\n        tags$li(tags$b(\"data_date\"), \" - the date the data were collected at\"),\n        tags$li(tags$b(\"submitted_daate\"), \" - the date the data were submitted at\"),\n        tags$li(tags$b(\"Province\"), \" - the province the data were collected at (all 'North' for this dataset)\"),\n        tags$li(tags$b(\"District\"), \" - the district the data were collected at\"),\n        tags$li(tags$b(\"age_group\"), \" - the age group the data were collected for (0-5, 5-14, 15+, and all ages)\"),\n        tags$li(tags$b(\"cases_reported\"), \" - the number of cases reported for the facility/age group on the given date\")\n      )\n      \n    )\n    \n  )\n)\n\nNote that we’ve also added in a hr() tag - this adds a horizontal line separating our control widgets from our download widgets. This is another one of the HTML tags that we discussed previously.\nNow that we have our ui ready, we need to add the server component. Downloads are done in the server with the downloadHandler() function. Similar to our plot, we need to attach it to an output that has the same inputId as the download button. This function takes two arguments - filename and content - these are both functions. As you might be able to guess, filename is used to specify the name of the downloaded file, and content is used to specify what should be downloaded. content contain a function that you would use to save data locally - so if you were downloading a csv file you could use rio::export(). Since we’re downloading a plot, we’ll use ggplot2::ggsave(). Lets look at how we would program this (we won’t add it to the server yet).\n\nserver &lt;- function(input, output, session) {\n  \n  output$malaria_epicurve &lt;- renderPlot(\n    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)\n  )\n  \n  output$download_epicurve &lt;- downloadHandler(\n    filename = function() {\n      stringr::str_glue(\"malaria_epicurve_{input$select_district}.png\")\n    },\n    \n    content = function(file) {\n      ggsave(file, \n             plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup),\n             width = 8, height = 5, dpi = 300)\n    }\n    \n  )\n  \n}\n\nNote that the content function always takes a file argument, which we put where the output file name is specified. You might also notice that we’re repeating code here - we are using our plot_epicurve() function twice in this server, once for the download and once for the image displayed in the app. While this wont massively affect performance, this means that the code to generate this plot will have to be run when the user changes the widgets specifying the district and age group, and again when you want to download the plot. In larger apps, suboptimal decisions like this one will slow things down more and more, so it’s good to learn how to make our app more efficient in this sense. What would make more sense is if we had a way to run the epicurve code when the districts/age groups are changes, and let that be used by the renderPlot() and downloadHandler() functions. This is where reactive conductors come in!\nReactive conductors are objects that are created in the shiny server in a reactive way, but are not outputted - they can just be used by other parts of the server. There are a number of different kinds of reactive conductors, but we’ll go through the basic two.\n1.reactive() - this is the most basic reactive conductor - it will react whenever any inputs used inside of it change (so our district/age group widgets)\n2. eventReactive()- this rective conductor works the same as reactive(), except that the user can specify which inputs cause it to rerun. This is useful if your reactive conductor takes a long time to process, but this will be explained more later.\nLets look at the two examples:\n\nmalaria_plot_r &lt;- reactive({\n  \n  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)\n  \n})\n\n\n# only runs when the district selector changes!\nmalaria_plot_er &lt;- eventReactive(input$select_district, {\n  \n  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)\n  \n})\n\nWhen we use the eventReactive() setup, we can specify which inputs cause this chunk of code to run - this isn’t very useful to us at the moment, so we can leave it for now. Note that you can include multiple inputs with c()\nLets look at how we can integrate this into our server code:\n\nserver &lt;- function(input, output, session) {\n  \n  malaria_plot &lt;- reactive({\n    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)\n  })\n  \n  \n  \n  output$malaria_epicurve &lt;- renderPlot(\n    malaria_plot()\n  )\n  \n  output$download_epicurve &lt;- downloadHandler(\n    \n    filename = function() {\n      stringr::str_glue(\"malaria_epicurve_{input$select_district}.png\")\n    },\n    \n    content = function(file) {\n      ggsave(file, \n             malaria_plot(),\n             width = 8, height = 5, dpi = 300)\n    }\n    \n  )\n  \n}\n\nYou can see we’re just calling on the output of our reactive we’ve defined in both our download and plot rendering functions. One thing to note that often trips people up is you have to use the outputs of reactives as if they were functions - so you must add empty brackets at the end of them (i.e. malaria_plot() is correct, and malaria_plot is not). Now that we’ve added this solution our app is a little tidyer, faster, and easier to change since all our code that runs the epicurve function is in one place.\n\n\n\n\n\n\n\n\n\n\n\nAdding a facility selector\nLets move on to our next feature - a selector for specific facilities. We’ll implement another parameter into our function so we can pass this as an argument from our code. Lets look at doing this first - it just operates off the same principles as the other parameters we’ve set up. Lets update and test our function.\n\nplot_epicurve &lt;- function(data, district = \"All\", agegroup = \"malaria_tot\", facility = \"All\") {\n  \n  if (!(\"All\" %in% district)) {\n    data &lt;- data %&gt;%\n      filter(District %in% district)\n    \n    plot_title_district &lt;- stringr::str_glue(\"{paste0(district, collapse = ', ')} districts\")\n    \n  } else {\n    \n    plot_title_district &lt;- \"all districts\"\n    \n  }\n  \n  # if no remaining data, return NULL\n  if (nrow(data) == 0) {\n    \n    return(NULL)\n  }\n  \n  data &lt;- data %&gt;%\n    filter(age_group == agegroup)\n  \n  \n  # if no remaining data, return NULL\n  if (nrow(data) == 0) {\n    \n    return(NULL)\n  }\n  \n  if (agegroup == \"malaria_tot\") {\n      agegroup_title &lt;- \"All ages\"\n  } else {\n    agegroup_title &lt;- stringr::str_glue(\"{str_remove(agegroup, 'malaria_rdt')} years\")\n  }\n  \n    if (!(\"All\" %in% facility)) {\n    data &lt;- data %&gt;%\n      filter(location_name == facility)\n    \n    plot_title_facility &lt;- facility\n    \n  } else {\n    \n    plot_title_facility &lt;- \"all facilities\"\n    \n  }\n  \n  # if no remaining data, return NULL\n  if (nrow(data) == 0) {\n    \n    return(NULL)\n  }\n\n  \n  \n  ggplot(data, aes(x = data_date, y = cases_reported)) +\n    geom_col(width = 1, fill = \"darkred\") +\n    theme_minimal() +\n    labs(\n      x = \"date\",\n      y = \"number of cases\",\n      title = stringr::str_glue(\"Malaria cases - {plot_title_district}; {plot_title_facility}\"),\n      subtitle = agegroup_title\n    )\n  \n  \n  \n}\n\nLet’s test it:\n\nplot_epicurve(malaria_data, district = \"Spring\", agegroup = \"malaria_rdt_0-4\", facility = \"Facility 1\")\n\n\n\n\n\n\n\n\nWith all the facilites in our data, it isn’t very clear which facilities correspond to which districts - and the end user won’t know either. This might make using the app quite unintuitive. For this reason, we should make the facility options in the UI change dynamically as the user changes the district - so one filters the other! Since we have so many variables that we’re using in the options, we might also want to generate some of our options for the ui in our global.R file from the data. For example, we can add this code chunk to global.R after we’ve read our data in:\n\nall_districts &lt;- c(\"All\", unique(malaria_data$District))\n\n# data frame of location names by district\nfacility_list &lt;- malaria_data %&gt;%\n  group_by(location_name, District) %&gt;%\n  summarise() %&gt;% \n  ungroup()\n\nLet’s look at them:\n\nall_districts\n\n[1] \"All\"     \"Spring\"  \"Bolo\"    \"Dingo\"   \"Barnard\"\n\n\n\nfacility_list\n\n# A tibble: 65 × 2\n   location_name District\n   &lt;chr&gt;         &lt;chr&gt;   \n 1 Facility 1    Spring  \n 2 Facility 10   Bolo    \n 3 Facility 11   Spring  \n 4 Facility 12   Dingo   \n 5 Facility 13   Bolo    \n 6 Facility 14   Dingo   \n 7 Facility 15   Barnard \n 8 Facility 16   Barnard \n 9 Facility 17   Barnard \n10 Facility 18   Bolo    \n# ℹ 55 more rows\n\n\nWe can pass these new variables to the ui without any issue, since they are globally visible by both the server and the ui! Lets update our UI:\n\nui &lt;- fluidPage(\n\n  titlePanel(\"Malaria facility visualisation app\"),\n\n  sidebarLayout(\n\n    sidebarPanel(\n         # selector for district\n         selectInput(\n              inputId = \"select_district\",\n              label = \"Select district\",\n              choices = all_districts,\n              selected = \"All\",\n              multiple = FALSE\n         ),\n         # selector for age group\n         selectInput(\n              inputId = \"select_agegroup\",\n              label = \"Select age group\",\n              choices = c(\n                   \"All ages\" = \"malaria_tot\",\n                   \"0-4 yrs\" = \"malaria_rdt_0-4\",\n                   \"5-14 yrs\" = \"malaria_rdt_5-14\",\n                   \"15+ yrs\" = \"malaria_rdt_15\"\n              ), \n              selected = \"All\",\n              multiple = FALSE\n         ),\n         # selector for facility\n         selectInput(\n           inputId = \"select_facility\",\n           label = \"Select Facility\",\n           choices = c(\"All\", facility_list$location_name),\n           selected = \"All\"\n         ),\n         \n         # horizontal line\n         hr(),\n         downloadButton(\n           outputId = \"download_epicurve\",\n           label = \"Download plot\"\n         )\n\n    ),\n\n    mainPanel(\n      # epicurve goes here\n      plotOutput(\"malaria_epicurve\"),\n      br(),\n      hr(),\n      p(\"Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:\"),\n      tags$ul(\n        tags$li(tags$b(\"location_name\"), \" - the facility that the data were collected at\"),\n        tags$li(tags$b(\"data_date\"), \" - the date the data were collected at\"),\n        tags$li(tags$b(\"submitted_daate\"), \" - the date the data were submitted at\"),\n        tags$li(tags$b(\"Province\"), \" - the province the data were collected at (all 'North' for this dataset)\"),\n        tags$li(tags$b(\"District\"), \" - the district the data were collected at\"),\n        tags$li(tags$b(\"age_group\"), \" - the age group the data were collected for (0-5, 5-14, 15+, and all ages)\"),\n        tags$li(tags$b(\"cases_reported\"), \" - the number of cases reported for the facility/age group on the given date\")\n      )\n      \n    )\n    \n  )\n)\n\nNotice how we’re now passing variables for our choices instead of hard coding them in the ui! This might make our code more compact as well! Lastly, we’ll have to update the server. It will be easy to update our function to incorporate our new input (we just have to pass it as an argument to our new parameter), but we should remember we also want the ui to update dynamically when the user changes the selected district. It is important to understand here that we can change the parameters and behaviour of widgets while the app is running, but this needs to be done in the server. We need to understand a new way to output to the server to learn how to do this.\nThe functions we need to understand how to do this are known as observer functions, and are similar to reactive functions in how they behave. They have one key difference though:\n\nReactive functions do not directly affect outputs, and produce objects that can be seen in other locations in the server\nObserver functions can affect server outputs, but do so via side effects of other functions. (They can also do other things, but this is their main function in practice)\n\nSimilar to reactive functions, there are two flavours of observer functions, and they are divided by the same logic that divides reactive functions:\n\nobserve() - this function runs whenever any inputs used inside of it change\nobserveEvent() - this function runs when a user-specified input changes\n\nWe also need to understand the shiny-provided functions that update widgets. These are fairly straightforward to run - they first take the session object from the server function (this doesn’t need to be understood for now), and then the inputId of the function to be changed. We then pass new versions of all parameters that are already taken by selectInput() - these will be automatically updated in the widget.\nLets look at an isolated example of how we could use this in our server. When the user changes the district, we want to filter our tibble of facilities by district, and update the choices to only reflect those that are available in that district (and an option for all facilities)\n\nobserve({\n  \n  if (input$select_district == \"All\") {\n    new_choices &lt;- facility_list$location_name\n  } else {\n    new_choices &lt;- facility_list %&gt;%\n      filter(District == input$select_district) %&gt;%\n      pull(location_name)\n  }\n  \n  new_choices &lt;- c(\"All\", new_choices)\n  \n  updateSelectInput(session, inputId = \"select_facility\",\n                    choices = new_choices)\n  \n})\n\nAnd that’s it! we can add it into our server, and that behaviour will now work. Here’s what our new server should look like:\n\nserver &lt;- function(input, output, session) {\n  \n  malaria_plot &lt;- reactive({\n    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)\n  })\n  \n  \n  \n  observe({\n    \n    if (input$select_district == \"All\") {\n      new_choices &lt;- facility_list$location_name\n    } else {\n      new_choices &lt;- facility_list %&gt;%\n        filter(District == input$select_district) %&gt;%\n        pull(location_name)\n    }\n    \n    new_choices &lt;- c(\"All\", new_choices)\n    \n    updateSelectInput(session, inputId = \"select_facility\",\n                      choices = new_choices)\n    \n  })\n  \n  \n  output$malaria_epicurve &lt;- renderPlot(\n    malaria_plot()\n  )\n  \n  output$download_epicurve &lt;- downloadHandler(\n    \n    filename = function() {\n      stringr::str_glue(\"malaria_epicurve_{input$select_district}.png\")\n    },\n    \n    content = function(file) {\n      ggsave(file, \n             malaria_plot(),\n             width = 8, height = 5, dpi = 300)\n    }\n    \n  )\n  \n  \n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\nAdding another tab with a table\nNow we’ll move on to the last component we want to add to our app. We’ll want to separate our ui into two tabs, one of which will have an interactive table where the user can see the data they are making the epidemic curve with. To do this, we can use the packaged ui elements that come with shiny relevant to tabs. On a basic level, we can enclose most of our main panel in this general structure:\n\n# ... the rest of ui\n\nmainPanel(\n  \n  tabsetPanel(\n    type = \"tabs\",\n    tabPanel(\n      \"Epidemic Curves\",\n      ...\n    ),\n    tabPanel(\n      \"Data\",\n      ...\n    )\n  )\n)\n\nLets apply this to our ui. We also will want to use the DT package here - this is a great package for making interactive tables from pre-existing data. We can see it being used for DT::datatableOutput() in this example.\n\nui &lt;- fluidPage(\n     \n     titlePanel(\"Malaria facility visualisation app\"),\n     \n     sidebarLayout(\n          \n          sidebarPanel(\n               # selector for district\n               selectInput(\n                    inputId = \"select_district\",\n                    label = \"Select district\",\n                    choices = all_districts,\n                    selected = \"All\",\n                    multiple = FALSE\n               ),\n               # selector for age group\n               selectInput(\n                    inputId = \"select_agegroup\",\n                    label = \"Select age group\",\n                    choices = c(\n                         \"All ages\" = \"malaria_tot\",\n                         \"0-4 yrs\" = \"malaria_rdt_0-4\",\n                         \"5-14 yrs\" = \"malaria_rdt_5-14\",\n                         \"15+ yrs\" = \"malaria_rdt_15\"\n                    ), \n                    selected = \"All\",\n                    multiple = FALSE\n               ),\n               # selector for facility\n               selectInput(\n                    inputId = \"select_facility\",\n                    label = \"Select Facility\",\n                    choices = c(\"All\", facility_list$location_name),\n                    selected = \"All\"\n               ),\n               \n               # horizontal line\n               hr(),\n               downloadButton(\n                    outputId = \"download_epicurve\",\n                    label = \"Download plot\"\n               )\n               \n          ),\n          \n          mainPanel(\n               tabsetPanel(\n                    type = \"tabs\",\n                    tabPanel(\n                         \"Epidemic Curves\",\n                         plotOutput(\"malaria_epicurve\")\n                    ),\n                    tabPanel(\n                         \"Data\",\n                         DT::dataTableOutput(\"raw_data\")\n                    )\n               ),\n               br(),\n               hr(),\n               p(\"Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:\"),\n               tags$ul(\n                    tags$li(tags$b(\"location_name\"), \" - the facility that the data were collected at\"),\n                    tags$li(tags$b(\"data_date\"), \" - the date the data were collected at\"),\n                    tags$li(tags$b(\"submitted_daate\"), \" - the date the data were submitted at\"),\n                    tags$li(tags$b(\"Province\"), \" - the province the data were collected at (all 'North' for this dataset)\"),\n                    tags$li(tags$b(\"District\"), \" - the district the data were collected at\"),\n                    tags$li(tags$b(\"age_group\"), \" - the age group the data were collected for (0-5, 5-14, 15+, and all ages)\"),\n                    tags$li(tags$b(\"cases_reported\"), \" - the number of cases reported for the facility/age group on the given date\")\n               )\n               \n               \n          )\n     )\n)\n\nNow our app is arranged into tabs! Lets make the necessary edits to the server as well. Since we dont need to manipulate our dataset at all before we render it this is actually very simple - we just render the malaria_data dataset via DT::renderDT() to the ui!\n\nserver &lt;- function(input, output, session) {\n  \n  malaria_plot &lt;- reactive({\n    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)\n  })\n  \n  \n  \n  observe({\n    \n    if (input$select_district == \"All\") {\n      new_choices &lt;- facility_list$location_name\n    } else {\n      new_choices &lt;- facility_list %&gt;%\n        filter(District == input$select_district) %&gt;%\n        pull(location_name)\n    }\n    \n    new_choices &lt;- c(\"All\", new_choices)\n    \n    updateSelectInput(session, inputId = \"select_facility\",\n                      choices = new_choices)\n    \n  })\n  \n  \n  output$malaria_epicurve &lt;- renderPlot(\n    malaria_plot()\n  )\n  \n  output$download_epicurve &lt;- downloadHandler(\n    \n    filename = function() {\n      stringr::str_glue(\"malaria_epicurve_{input$select_district}.png\")\n    },\n    \n    content = function(file) {\n      ggsave(file, \n             malaria_plot(),\n             width = 8, height = 5, dpi = 300)\n    }\n    \n  )\n  \n  # render data table to ui\n  output$raw_data &lt;- DT::renderDT(\n    malaria_data\n  )\n  \n  \n}",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#sharing-shiny-apps",
    "href": "new_pages/shiny_basics.html#sharing-shiny-apps",
    "title": "43  Dashboards with Shiny",
    "section": "43.7 Sharing shiny apps",
    "text": "43.7 Sharing shiny apps\nNow that you’ve developed your app, you probably want to share it with others - this is the main advantage of shiny after all! We can do this by sharing the code directly, or we could publish on a server. If we share the code, others will be able to see what you’ve done and build on it, but this will negate one of the main advantages of shiny - it can eliminate the need for end-users to maintain an R installation. For this reason, if you’re sharing your app with users who are not comfortable with R, it is much easier to share an app that has been published on a server.\nIf you’d rather share the code, you could make a .zip file of the app, or better yet, publish your app on github and add collaborators. You can refer to the section on github for further information here.\nHowever, if we’re publishing the app online, we need to do a little more work. Ultimately, we want your app to be able to be accessed via a web URL so others can get quick and easy access to it. Unfortunately, to publish you app on a server, you need to have access to a server to publish it on! There are a number of hosting options when it comes to this:\n\nshinyapps.io: this is the easiest place to publish shiny apps, as it has the smallest amount of configuration work needed, and has some free, but limited licenses.\nRStudio Connect: this is a far more powerful version of an R server, that can perform many operations, including publishing shiny apps. It is however, harder to use, and less recommended for first-time users.\n\nFor the purposes of this document, we will use shinyapps.io, since it is easier for first time users. You can make a free account here to start - there are also different price plans for server licesnses if needed. The more users you expect to have, the more expensive your price plan may have to be, so keep this under consideration. If you’re looking to create something for a small set of individuals to use, a free license may be perfectly suitable, but a public facing app may need more licenses.\nFirst we should make sure our app is suitable for publishing on a server. In your app, you should restart your R session, and ensure that it runs without running any extra code. This is important, as an app that requires package loading, or data reading not defined in your app code won’t run on a server. Also note that you can’t have any explicit file paths in your app - these will be invalid in the server setting - using the here package solves this issue very well. Finally, if you’re reading data from a source that requires user-authentication, such as your organisation’s servers, this will not generally work on a server. You will need to liase with your IT department to figure out how to whitelist the shiny server here.\nsigning up for account\nOnce you have your account, you can navigate to the tokens page under Accounts. Here you will want to add a new token - this will be used to deploy your app.\nFrom here, you should note that the url of your account will reflect the name of your app - so if your app is called my_app, the url will be appended as xxx.io/my_app/. Choose your app name wisely! Now that you are all ready, click deploy - if successful this will run your app on the web url you chose!\nsomething on making apps in documents?",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#further-reading",
    "href": "new_pages/shiny_basics.html#further-reading",
    "title": "43  Dashboards with Shiny",
    "section": "43.8 Further reading",
    "text": "43.8 Further reading\nSo far, we’ve covered a lot of aspects of shiny, and have barely scratched the surface of what is on offer for shiny. While this guide serves as an introduction, there is loads more to learn to fully understand shiny. You should start making apps and gradually add more and more functionality",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#recommended-extension-packages",
    "href": "new_pages/shiny_basics.html#recommended-extension-packages",
    "title": "43  Dashboards with Shiny",
    "section": "43.9 Recommended extension packages",
    "text": "43.9 Recommended extension packages\nThe following represents a selection of high quality shiny extensions that can help you get a lot more out of shiny. In no particular order:\n\nshinyWidgets - this package gives you many many more widgets that can be used in your app. Run shinyWidgets::shinyWidgetsGallery() to see a selection of available widgets with this package. See examples here\nshinyjs - this is an excellent package that gives the user the ability to greatly extend shiny’s utility via a series of javascript. The applications of this package range from very simple to highly advanced, but you might want to first use it to manipulate the ui in simple ways, like hiding/showing elements, or enabling/disabling buttons. Find out more here\nshinydashboard - this package massively expands the available ui that can be used in shiny, specifically letting the user create a complex dashboard with a variety of complex layouts. See more here\nshinydashboardPlus - get even more features out of the shinydashboard framework! See more here\nshinythemes - change the default css theme for your shiny app with a wide range of preset templates! See more here\n\nThere are also a number of packages that can be used to create interactive outputs that are shiny compatible.\n\nDT is semi-incorporated into base-shiny, but provides a great set of functions to create interactive tables.\nplotly is a package for creating interactive plots that the user can manipulate in app. You can also convert your plot to interactive versions via plotly::ggplotly()! As alternatives, dygraphs and highcharter are also excellent.",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/shiny_basics.html#recommended-resources",
    "href": "new_pages/shiny_basics.html#recommended-resources",
    "title": "43  Dashboards with Shiny",
    "section": "43.10 Recommended resources",
    "text": "43.10 Recommended resources",
    "crumbs": [
      "Reports and dashboards",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Dashboards with Shiny</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html",
    "href": "new_pages/writing_functions.html",
    "title": "44  Writing functions",
    "section": "",
    "text": "44.1 Preparation",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#preparation",
    "href": "new_pages/writing_functions.html#preparation",
    "title": "44  Writing functions",
    "section": "",
    "text": "Load packages\nThis code chunk shows the loading of packages required for the analyses. In this handbook we emphasize p_load() from pacman, which installs the package if necessary and loads it for use. You can also load installed packages with library() from base R. See the page on R basics for more information on R packages.\n\n\nImport data\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the Download book and data page. The dataset is imported using the import() function from the rio package. See the page on Import and export for various ways to import data.\nWe will also use in the last part of this page some data on H7N9 flu from 2013.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#functions",
    "href": "new_pages/writing_functions.html#functions",
    "title": "44  Writing functions",
    "section": "44.2 Functions",
    "text": "44.2 Functions\nFunctions are helpful in programming since they allow to make codes easier to understand, somehow shorter and less prone to errors (given there were no errors in the function itself).\nIf you have come so far to this handbook, it means you have came across endless functions since in R, every operation is a function call +, for, if, [, $, { …. For example x + y is the same as'+'(x, y)\nR is one the languages that offers the most possibility to work with functions and give enough tools to the user to easily write them. We should not think about functions as fixed at the top or at the end of the programming chain, R offers the possibility to use them as if they were vectors and even to use them inside other functions, lists…\nLot of very advanced resources on functional programming exist and we will only give here an insight to help you start with functional programming with short practical examples. You are then encouraged to visit the links on references to read more about it.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#why-would-you-use-a-function",
    "href": "new_pages/writing_functions.html#why-would-you-use-a-function",
    "title": "44  Writing functions",
    "section": "44.3 Why would you use a function?",
    "text": "44.3 Why would you use a function?\nBefore answering this question, it is important to note that you have already had tips to get to write your very first R functions in the page on Iteration, loops, and lists of this handbook. In fact, use of “if/else” and loops is often a core part of many of our functions since they easily help to either broaden the application of our code allowing multiple conditions or to iterate codes for repeating tasks.\n\nI am repeating multiple times the same block of code to apply it to a different variable or data?\nGetting rid of it will it substantially shorten my overall code and make it run quicker?\nIs it possible that the code I have written is used again but with a different value at many places of the code?\n\nIf the answer to one of the previous questions is “YES”, then you probably need to write a function",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#how-does-r-build-functions",
    "href": "new_pages/writing_functions.html#how-does-r-build-functions",
    "title": "44  Writing functions",
    "section": "44.4 How does R build functions?",
    "text": "44.4 How does R build functions?\nFunctions in R have three main components:\n\nthe formals() which is the list of arguments which controls how we can call the function\nthe body() that is the code inside the function i.e. within the brackets or following the parenthesis depending on how we write it\n\nand,\n\nthe environment() which will help locate the function’s variables and determines how the function finds value.\n\nOnce you have created your function, you can verify each of these components by calling the function associated.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#basic-syntax-and-structure",
    "href": "new_pages/writing_functions.html#basic-syntax-and-structure",
    "title": "44  Writing functions",
    "section": "44.5 Basic syntax and structure",
    "text": "44.5 Basic syntax and structure\n\nA function will need to be named properly so that its job is easily understandable as soon as we read its name. Actually this is already the case with majority of the base R architecture. Functions like mean(), print(), summary() have names that are very straightforward\nA function will need arguments, such as the data to work on and other objects that can be static values among other options\nAnd finally a function will give an output based on its core task and the arguments it has been given. Usually we will use the built-in functions as print(), return()… to produce the output. The output can be a logical value, a number, a character, a data frame…in short any kind of R object.\n\nBasically this is the composition of a function:\n\nfunction_name &lt;- function(argument_1, argument_2, argument_3){\n  \n           function_task\n  \n           return(output)\n}\n\nWe can create our first function that will be called contain_covid19().\n\ncontain_covid19 &lt;- function(barrier_gest, wear_mask, get_vaccine){\n  \n                            if(barrier_gest == \"yes\" & wear_mask == \"yes\" & get_vaccine == \"yes\" ) \n       \n                            return(\"success\")\n  \n  else(\"please make sure all are yes, this pandemic has to end!\")\n}\n\nWe can then verify the components of our newly created function.\n\nformals(contain_covid19)\n\n$barrier_gest\n\n\n$wear_mask\n\n\n$get_vaccine\n\nbody(contain_covid19)\n\n{\n    if (barrier_gest == \"yes\" & wear_mask == \"yes\" & get_vaccine == \n        \"yes\") \n        return(\"success\")\n    else (\"please make sure all are yes, this pandemic has to end!\")\n}\n\nenvironment(contain_covid19)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nNow we will test our function. To call our written function, you use it as you use all R functions i.e by writing the function name and adding the required arguments.\n\ncontain_covid19(barrier_gest = \"yes\", wear_mask = \"yes\", get_vaccine = \"yes\")\n\n[1] \"success\"\n\n\nWe can write again the name of each argument for precautionary reasons. But without specifying them, the code should work since R has in memory the positioning of each argument. So as long as you put the values of the arguments in the correct order, you can skip writing the arguments names when calling the functions.\n\ncontain_covid19(\"yes\", \"yes\", \"yes\")\n\n[1] \"success\"\n\n\nThen let’s look what happens if one of the values is \"no\" or not \"yes\".\n\ncontain_covid19(barrier_gest = \"yes\", wear_mask = \"yes\", get_vaccine = \"no\")\n\n[1] \"please make sure all are yes, this pandemic has to end!\"\n\n\nIf we provide an argument that is not recognized, we get an error:\n\ncontain_covid19(barrier_gest = \"sometimes\", wear_mask = \"yes\", get_vaccine = \"no\")\n\nError in contain_covid19(barrier_gest = \"sometimes\", wear_mask = \"yes\",  :    could not find function \"contain_covid19\"\nNOTE: Some functions (most of time very short and straightforward) may not need a name and can be used directly on a line of code or inside another function to do quick task. They are called anonymous functions .\nFor instance below is a first anonymous function that keeps only character variables the dataset.\n\nlinelist %&gt;% \n  dplyr::slice_head(n=10) %&gt;%  #equivalent to R base \"head\" function and that return first n observation of the  dataset\n  select(function(x) is.character(x)) \n\n\n\n\n\n\n\nThen another function that selects every second observation of our dataset (may be relevant when we have longitudinal data with many records per patient for instance after having ordered by date or visit). In this case, the proper function writing outside dplyr would be function (x) (x%%2 == 0) to apply to the vector containing all row numbers.\n\nlinelist %&gt;%   \n   slice_head(n=20) %&gt;% \n   tibble::rownames_to_column() %&gt;% # add indices of each obs as rownames to clearly see the final selection\n   filter(row_number() %%2 == 0)\n\n\n\n\n\n\n\nA possible base R code for the same task would be:\n\nlinelist_firstobs &lt;- head(linelist, 20)\n\nlinelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),]\n\n\n\n\n\n\n\nCAUTION: Though it is true that using functions can help us with our code, it can nevertheless be time consuming to write some functions or to fix one if it has not been thought thoroughly, written adequately and is returning errors as a result. For this reason it is often recommended to first write the R code, make sure it does what we intend it to do, and then transform it into a function with its three main components as listed above.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#examples",
    "href": "new_pages/writing_functions.html#examples",
    "title": "44  Writing functions",
    "section": "44.6 Examples",
    "text": "44.6 Examples\n\nReturn proportion tables for several columns\nYes, we already have nice functions in many packages allowing to summarize information in a very easy and nice way. But we will still try to make our own, in our first steps to getting used to writing functions.\nIn this example we want to show how writing a simple function would avoid you copy-pasting the same code multiple times.\n\nproptab_multiple &lt;- function(my_data, var_to_tab){\n  \n  #print the name of each variable of interest before doing the tabulation\n  print(var_to_tab)\n\n  with(my_data,\n       rbind( #bind the results of the two following function by row\n        #tabulate the variable of interest: gives only numbers\n          table(my_data[[var_to_tab]], useNA = \"no\"),\n          #calculate the proportions for each variable of interest and round the value to 2 decimals\n         round(prop.table(table(my_data[[var_to_tab]]))*100,2)\n         )\n       )\n}\n\n\nproptab_multiple(linelist, \"gender\")\n\n[1] \"gender\"\n\n\n           f       m\n[1,] 2807.00 2803.00\n[2,]   50.04   49.96\n\nproptab_multiple(linelist, \"age_cat\")\n\n[1] \"age_cat\"\n\n\n         0-4     5-9  10-14  15-19   20-29 30-49 50-69 70+\n[1,] 1095.00 1095.00 941.00 743.00 1073.00   754 95.00 6.0\n[2,]   18.87   18.87  16.22  12.81   18.49    13  1.64 0.1\n\nproptab_multiple(linelist, \"outcome\")\n\n[1] \"outcome\"\n\n\n       Death Recover\n[1,] 2582.00 1983.00\n[2,]   56.56   43.44\n\n\nTIP: As shown above, it is very important to comment your functions as you would do for the general programming. Bear in mind that a function’s aim is to make a code ready to read, shorter and more efficient. Then one should be able to understand what the function does just by reading its name and should have more details reading the comments.\nA second option is to use this function in another one via a loop to make the process at once:\n\nfor(var_to_tab in c(\"gender\",\"age_cat\",  \"outcome\")){\n  \n  print(proptab_multiple(linelist, var_to_tab))\n  \n}\n\n[1] \"gender\"\n           f       m\n[1,] 2807.00 2803.00\n[2,]   50.04   49.96\n[1] \"age_cat\"\n         0-4     5-9  10-14  15-19   20-29 30-49 50-69 70+\n[1,] 1095.00 1095.00 941.00 743.00 1073.00   754 95.00 6.0\n[2,]   18.87   18.87  16.22  12.81   18.49    13  1.64 0.1\n[1] \"outcome\"\n       Death Recover\n[1,] 2582.00 1983.00\n[2,]   56.56   43.44\n\n\nA simpler way could be using the base R “apply” instead of a “for loop” as expressed below:\nTIP: R is often defined as a functional programming language and almost anytime you run a line of code you are using some built-in functions. A good habit to be more comfortable with writing functions is to often have an internal look at how the basic functions you are using daily are built. The shortcut to do so is selecting the function name and then clicking onCtrl+F2 or fn+F2 or Cmd+F2 (depending on your computer) .",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#using-purrr-writing-functions-that-can-be-iteratively-applied",
    "href": "new_pages/writing_functions.html#using-purrr-writing-functions-that-can-be-iteratively-applied",
    "title": "44  Writing functions",
    "section": "44.7 Using purrr: writing functions that can be iteratively applied",
    "text": "44.7 Using purrr: writing functions that can be iteratively applied\n\nModify class of multiple columns in a dataset\nLet’s say many character variables in the original linelist data need to be changes to “factor” for analysis and plotting purposes. Instead of repeating the step several times, we can just use lapply() to do the transformation of all variables concerned on a single line of code.\nCAUTION: lapply() returns a list, thus its use may require an additional modification as a last step.\nThe same step can be done using map_if() function from the purrr package\n\nlinelist_factor2 &lt;- linelist %&gt;%\n  purrr::map_if(is.character, as.factor)\n\n\nlinelist_factor2 %&gt;%\n        glimpse()\n\nList of 30\n $ case_id             : Factor w/ 5888 levels \"00031d\",\"00086d\",..: 2134 3022 396 4203 3084 4347 179 1241 5594 430 ...\n $ generation          : num [1:5888] 4 4 2 3 3 3 4 4 4 4 ...\n $ date_infection      : Date[1:5888], format: \"2014-05-08\" NA ...\n $ date_onset          : Date[1:5888], format: \"2014-05-13\" \"2014-05-13\" ...\n $ date_hospitalisation: Date[1:5888], format: \"2014-05-15\" \"2014-05-14\" ...\n $ date_outcome        : Date[1:5888], format: NA \"2014-05-18\" ...\n $ outcome             : Factor w/ 2 levels \"Death\",\"Recover\": NA 2 2 NA 2 2 2 1 2 1 ...\n $ gender              : Factor w/ 2 levels \"f\",\"m\": 2 1 2 1 2 1 1 1 2 1 ...\n $ age                 : num [1:5888] 2 3 56 18 3 16 16 0 61 27 ...\n $ age_unit            : Factor w/ 2 levels \"months\",\"years\": 2 2 2 2 2 2 2 2 2 2 ...\n $ age_years           : num [1:5888] 2 3 56 18 3 16 16 0 61 27 ...\n $ age_cat             : Factor w/ 8 levels \"0-4\",\"5-9\",\"10-14\",..: 1 1 7 4 1 4 4 1 7 5 ...\n $ age_cat5            : Factor w/ 18 levels \"0-4\",\"5-9\",\"10-14\",..: 1 1 12 4 1 4 4 1 13 6 ...\n $ hospital            : Factor w/ 6 levels \"Central Hospital\",..: 4 3 6 5 2 5 3 3 3 3 ...\n $ lon                 : num [1:5888] -13.2 -13.2 -13.2 -13.2 -13.2 ...\n $ lat                 : num [1:5888] 8.47 8.45 8.46 8.48 8.46 ...\n $ infector            : Factor w/ 2697 levels \"00031d\",\"002e6c\",..: 2594 NA NA 2635 180 1799 1407 195 NA NA ...\n $ source              : Factor w/ 2 levels \"funeral\",\"other\": 2 NA NA 2 2 2 2 2 NA NA ...\n $ wt_kg               : num [1:5888] 27 25 91 41 36 56 47 0 86 69 ...\n $ ht_cm               : num [1:5888] 48 59 238 135 71 116 87 11 226 174 ...\n $ ct_blood            : num [1:5888] 22 22 21 23 23 21 21 22 22 22 ...\n $ fever               : Factor w/ 2 levels \"no\",\"yes\": 1 NA NA 1 1 1 NA 1 1 1 ...\n $ chills              : Factor w/ 2 levels \"no\",\"yes\": 1 NA NA 1 1 1 NA 1 1 1 ...\n $ cough               : Factor w/ 2 levels \"no\",\"yes\": 2 NA NA 1 2 2 NA 2 2 2 ...\n $ aches               : Factor w/ 2 levels \"no\",\"yes\": 1 NA NA 1 1 1 NA 1 1 1 ...\n $ vomit               : Factor w/ 2 levels \"no\",\"yes\": 2 NA NA 1 2 2 NA 2 2 1 ...\n $ temp                : num [1:5888] 36.8 36.9 36.9 36.8 36.9 37.6 37.3 37 36.4 35.9 ...\n $ time_admission      : Factor w/ 1072 levels \"00:10\",\"00:29\",..: NA 308 746 415 514 589 609 297 409 387 ...\n $ bmi                 : num [1:5888] 117.2 71.8 16.1 22.5 71.4 ...\n $ days_onset_hosp     : num [1:5888] 2 1 2 2 1 1 2 1 1 2 ...\n\n\n\n\nIteratively produce graphs for different levels of a variable\nWe will produce here pie chart to look at the distribution of patient’s outcome in China during the H7N9 outbreak for each province. Instead of repeating the code for each of them, we will just apply a function that we will create.\n\n#precising options for the use of highchart\noptions(highcharter.theme =   highcharter::hc_theme_smpl(tooltip = list(valueDecimals = 2)))\n\n\n#create a function called \"chart_outcome_province\" that takes as argument the dataset and the name of the province for which to plot the distribution of the outcome.\n\nchart_outcome_province &lt;- function(data_used, prov){\n  \n  tab_prov &lt;- data_used %&gt;% \n    filter(province == prov,\n           !is.na(outcome))%&gt;% \n    group_by(outcome) %&gt;% \n    count() %&gt;%\n    adorn_totals(where = \"row\") %&gt;% \n    adorn_percentages(denominator = \"col\", )%&gt;%\n    mutate(\n        perc_outcome= round(n*100,2))\n  \n  \n  tab_prov %&gt;%\n    filter(outcome != \"Total\") %&gt;% \n  highcharter::hchart(\n    \"pie\", hcaes(x = outcome, y = perc_outcome),\n    name = paste0(\"Distibution of the outcome in:\", prov)\n    )\n  \n}\n\nchart_outcome_province(flu_china, \"Shanghai\")\n\n\n\n\nchart_outcome_province(flu_china,\"Zhejiang\")\n\n\n\n\nchart_outcome_province(flu_china,\"Jiangsu\")\n\n\n\n\n\n\n\nIteratively produce tables for different levels of a variable\nHere we will create three indicators to summarize in a table and we would like to produce this table for each of the provinces. Our indicators are the delay between onset and hospitalization, the percentage of recovery and the median age of cases.\n\nindic_1 &lt;- flu_china %&gt;% \n  group_by(province) %&gt;% \n  mutate(\n    date_hosp= strptime(date_of_hospitalisation, format = \"%m/%d/%Y\"),\n    date_ons= strptime(date_of_onset, format = \"%m/%d/%Y\"), \n    delay_onset_hosp= as.numeric(date_hosp - date_ons)/86400,\n    mean_delay_onset_hosp = round(mean(delay_onset_hosp, na.rm=TRUE ), 0)) %&gt;%\n  select(province, mean_delay_onset_hosp)  %&gt;% \n  distinct()\n     \n\nindic_2 &lt;-  flu_china %&gt;% \n            filter(!is.na(outcome)) %&gt;% \n            group_by(province, outcome) %&gt;% \n            count() %&gt;%\n            pivot_wider(names_from = outcome, values_from = n) %&gt;% \n    adorn_totals(where = \"col\") %&gt;% \n    mutate(\n        perc_recovery= round((Recover/Total)*100,2))%&gt;% \n  select(province, perc_recovery)\n    \n    \n    \nindic_3 &lt;-  flu_china %&gt;% \n            group_by(province) %&gt;% \n            mutate(\n                    median_age_cases = median(as.numeric(age), na.rm = TRUE)\n            ) %&gt;% \n  select(province, median_age_cases)  %&gt;% \n  distinct()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `median_age_cases = median(as.numeric(age), na.rm = TRUE)`.\nℹ In group 11: `province = \"Shanghai\"`.\nCaused by warning in `median()`:\n! NAs introduced by coercion\n\n#join the three indicator datasets\n\ntable_indic_all &lt;- indic_1 %&gt;% \n  dplyr::left_join(indic_2, by = \"province\") %&gt;% \n        left_join(indic_3, by = \"province\")\n\n\n#print the indicators in a flextable\n\n\nprint_indic_prov &lt;-  function(table_used, prov){\n  \n  #first transform a bit the dataframe for printing ease\n  indic_prov &lt;- table_used %&gt;%\n    filter(province==prov) %&gt;%\n    pivot_longer(names_to = \"Indicateurs\", cols = 2:4) %&gt;% \n   mutate( indic_label = factor(Indicateurs,\n   levels= c(\"mean_delay_onset_hosp\",\"perc_recovery\",\"median_age_cases\"),\n   labels=c(\"Mean delay onset-hosp\",\"Percentage of recovery\", \"Median age of the cases\"))\n   ) %&gt;% \n    ungroup(province) %&gt;% \n    select(indic_label, value)\n  \n\n    tab_print &lt;- flextable(indic_prov)  %&gt;%\n    theme_vanilla() %&gt;% \n    flextable::fontsize(part = \"body\", size = 10) \n    \n    \n     tab_print &lt;- tab_print %&gt;% \n                  autofit()   %&gt;%\n                  set_header_labels( \n                indic_label= \"Indicateurs\", value= \"Estimation\") %&gt;%\n    flextable::bg( bg = \"darkblue\", part = \"header\") %&gt;%\n    flextable::bold(part = \"header\") %&gt;%\n    flextable::color(color = \"white\", part = \"header\") %&gt;% \n    add_header_lines(values = paste0(\"Indicateurs pour la province de: \", prov)) %&gt;% \nbold(part = \"header\")\n \n tab_print &lt;- set_formatter_type(tab_print,\n   fmt_double = \"%.2f\",\n   na_str = \"-\")\n\ntab_print \n    \n}\n\n\n\n\nprint_indic_prov(table_indic_all, \"Shanghai\")\n\nIndicateurs pour la province de: ShanghaiIndicateursEstimationMean delay onset-hosp4.0Percentage of recovery46.7Median age of the cases67.0\n\nprint_indic_prov(table_indic_all, \"Jiangsu\")\n\nIndicateurs pour la province de: JiangsuIndicateursEstimationMean delay onset-hosp6.0Percentage of recovery71.4Median age of the cases55.0",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#tips-and-best-practices-for-well-functioning-functions",
    "href": "new_pages/writing_functions.html#tips-and-best-practices-for-well-functioning-functions",
    "title": "44  Writing functions",
    "section": "44.8 Tips and best Practices for well functioning functions",
    "text": "44.8 Tips and best Practices for well functioning functions\nFunctional programming is meant to ease code and facilitates its reading. It should produce the contrary. The tips below will help you having a clean code and easy to read code.\n\nNaming and syntax\n\nAvoid using character that could have been easily already taken by other functions already existing in your environment\nIt is recommended for the function name to be short and straightforward to understand for another reader\nIt is preferred to use verbs as the function name and nouns for the argument names.\n\n\n\nColumn names and tidy evaluation\nIf you want to know how to reference column names that are provided to your code as arguments, read this tidyverse programming guidance. Among the topics covered are tidy evaluation and use of the embrace { } “double braces”\nFor example, here is a quick skeleton template code from page tutorial mentioned just above:\n\nvar_summary &lt;- function(data, var) {\n  data %&gt;%\n    summarise(n = n(), min = min({{ var }}), max = max({{ var }}))\n}\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  var_summary(mpg)\n\n\n\nTesting and Error handling\nThe more complicated a function’s task the higher the possibility of errors. Thus it is sometimes necessary to add some verification within the funtion to help quickly understand where the error is from and find a way t fix it.\n\nIt can be more than recommended to introduce a check on the missingness of one argument using missing(argument). This simple check can return “TRUE” or “FALSE” value.\n\n\ncontain_covid19_missing &lt;- function(barrier_gest, wear_mask, get_vaccine){\n  \n  if (missing(barrier_gest)) (print(\"please provide arg1\"))\n  if (missing(wear_mask)) print(\"please provide arg2\")\n  if (missing(get_vaccine)) print(\"please provide arg3\")\n\n\n  if (!barrier_gest == \"yes\" | wear_mask ==\"yes\" | get_vaccine == \"yes\" ) \n       \n       return (\"you can do better\")\n  \n  else(\"please make sure all are yes, this pandemic has to end!\")\n}\n\n\ncontain_covid19_missing(get_vaccine = \"yes\")\n\n[1] \"please provide arg1\"\n[1] \"please provide arg2\"\n\n\nError in contain_covid19_missing(get_vaccine = \"yes\"): argument \"barrier_gest\" is missing, with no default\n\n\n\nUse stop() for more detectable errors.\n\n\ncontain_covid19_stop &lt;- function(barrier_gest, wear_mask, get_vaccine){\n  \n  if(!is.character(barrier_gest)) (stop(\"arg1 should be a character, please enter the value with `yes`, `no` or `sometimes\"))\n  \n  if (barrier_gest == \"yes\" & wear_mask ==\"yes\" & get_vaccine == \"yes\" ) \n       \n       return (\"success\")\n  \n  else(\"please make sure all are yes, this pandemic has to end!\")\n}\n\n\ncontain_covid19_stop(barrier_gest=1, wear_mask=\"yes\", get_vaccine = \"no\")\n\nError in contain_covid19_stop(barrier_gest = 1, wear_mask = \"yes\", get_vaccine = \"no\"): arg1 should be a character, please enter the value with `yes`, `no` or `sometimes\n\n\n\nAs we see when we run most of the built-in functions, there are messages and warnings that can pop-up in certain conditions. We can integrate those in our written functions by using the functions message() and warning().\nWe can handle errors also by using safely() which takes one function as an argument and executes it in a safe way. In fact the function will execute without stopping if it encounters an error. safely() returns as output a list with two objects which are the results and the error it “skipped”.\n\nWe can verify by first running the mean() as function, then run it with safely().\n\nmap(linelist, mean)\n\n$case_id\n[1] NA\n\n$generation\n[1] 16.56165\n\n$date_infection\n[1] NA\n\n$date_onset\n[1] NA\n\n$date_hospitalisation\n[1] \"2014-11-03\"\n\n$date_outcome\n[1] NA\n\n$outcome\n[1] NA\n\n$gender\n[1] NA\n\n$age\n[1] NA\n\n$age_unit\n[1] NA\n\n$age_years\n[1] NA\n\n$age_cat\n[1] NA\n\n$age_cat5\n[1] NA\n\n$hospital\n[1] NA\n\n$lon\n[1] -13.23381\n\n$lat\n[1] 8.469638\n\n$infector\n[1] NA\n\n$source\n[1] NA\n\n$wt_kg\n[1] 52.64487\n\n$ht_cm\n[1] 124.9633\n\n$ct_blood\n[1] 21.20686\n\n$fever\n[1] NA\n\n$chills\n[1] NA\n\n$cough\n[1] NA\n\n$aches\n[1] NA\n\n$vomit\n[1] NA\n\n$temp\n[1] NA\n\n$time_admission\n[1] NA\n\n$bmi\n[1] 46.89023\n\n$days_onset_hosp\n[1] NA\n\n\n\nsafe_mean &lt;- safely(mean)\nlinelist %&gt;% \n  map(safe_mean)\n\n$case_id\n$case_id$result\n[1] NA\n\n$case_id$error\nNULL\n\n\n$generation\n$generation$result\n[1] 16.56165\n\n$generation$error\nNULL\n\n\n$date_infection\n$date_infection$result\n[1] NA\n\n$date_infection$error\nNULL\n\n\n$date_onset\n$date_onset$result\n[1] NA\n\n$date_onset$error\nNULL\n\n\n$date_hospitalisation\n$date_hospitalisation$result\n[1] \"2014-11-03\"\n\n$date_hospitalisation$error\nNULL\n\n\n$date_outcome\n$date_outcome$result\n[1] NA\n\n$date_outcome$error\nNULL\n\n\n$outcome\n$outcome$result\n[1] NA\n\n$outcome$error\nNULL\n\n\n$gender\n$gender$result\n[1] NA\n\n$gender$error\nNULL\n\n\n$age\n$age$result\n[1] NA\n\n$age$error\nNULL\n\n\n$age_unit\n$age_unit$result\n[1] NA\n\n$age_unit$error\nNULL\n\n\n$age_years\n$age_years$result\n[1] NA\n\n$age_years$error\nNULL\n\n\n$age_cat\n$age_cat$result\n[1] NA\n\n$age_cat$error\nNULL\n\n\n$age_cat5\n$age_cat5$result\n[1] NA\n\n$age_cat5$error\nNULL\n\n\n$hospital\n$hospital$result\n[1] NA\n\n$hospital$error\nNULL\n\n\n$lon\n$lon$result\n[1] -13.23381\n\n$lon$error\nNULL\n\n\n$lat\n$lat$result\n[1] 8.469638\n\n$lat$error\nNULL\n\n\n$infector\n$infector$result\n[1] NA\n\n$infector$error\nNULL\n\n\n$source\n$source$result\n[1] NA\n\n$source$error\nNULL\n\n\n$wt_kg\n$wt_kg$result\n[1] 52.64487\n\n$wt_kg$error\nNULL\n\n\n$ht_cm\n$ht_cm$result\n[1] 124.9633\n\n$ht_cm$error\nNULL\n\n\n$ct_blood\n$ct_blood$result\n[1] 21.20686\n\n$ct_blood$error\nNULL\n\n\n$fever\n$fever$result\n[1] NA\n\n$fever$error\nNULL\n\n\n$chills\n$chills$result\n[1] NA\n\n$chills$error\nNULL\n\n\n$cough\n$cough$result\n[1] NA\n\n$cough$error\nNULL\n\n\n$aches\n$aches$result\n[1] NA\n\n$aches$error\nNULL\n\n\n$vomit\n$vomit$result\n[1] NA\n\n$vomit$error\nNULL\n\n\n$temp\n$temp$result\n[1] NA\n\n$temp$error\nNULL\n\n\n$time_admission\n$time_admission$result\n[1] NA\n\n$time_admission$error\nNULL\n\n\n$bmi\n$bmi$result\n[1] 46.89023\n\n$bmi$error\nNULL\n\n\n$days_onset_hosp\n$days_onset_hosp$result\n[1] NA\n\n$days_onset_hosp$error\nNULL\n\n\nAs said previously, well commenting our codes is already a good way for having documentation in our work.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/writing_functions.html#resources",
    "href": "new_pages/writing_functions.html#resources",
    "title": "44  Writing functions",
    "section": "44.9 Resources",
    "text": "44.9 Resources\nR for Data Science link\nCheatsheet advance R programming\nCheatsheet purr Package\nVideo-ACM talk by Hadley Wickham: The joy of functional programming (how does map_dbl work)",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Writing functions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html",
    "href": "new_pages/directories.html",
    "title": "45  Directory interactions",
    "section": "",
    "text": "45.1 Preparation",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#preparation",
    "href": "new_pages/directories.html#preparation",
    "title": "45  Directory interactions",
    "section": "",
    "text": "fs package\nThe fs package is a tidyverse package that facilitate directory interactions, improving on some of the base R functions. In the sections below we will often use functions from fs.\n\npacman::p_load(\n  fs,             # file/directory interactions\n  rio,            # import/export\n  here,           # relative file pathways\n  tidyverse)      # data management and visualization\n\n\n\nPrint directory as a dendrogram tree\nUse the function dir_tree() from fs.\nProvide the folder filepath to path = and decide whether you want to show only one level (recurse = FALSE) or all files in all sub-levels (recurse = TRUE). Below we use here() as shorthand for the R project and specify its sub-folder “data”, which contains all the data used for this R handbook. We set it to display all files within “data” and its sub-folders (e.g. “cache”, “epidemic models”, “population”, “shp”, and “weather”).\n\nfs::dir_tree(path = here(\"data\"), recurse = TRUE)\n\nC:/Users/ngulu864/AppData/Local/Temp/RtmpyIElas/file7c74679b5e85/data\n├── africa_countries.geo.json\n├── cache\n│   └── epidemic_models\n│       ├── 2015-04-30\n│       │   ├── estimated_reported_cases_samples.rds\n│       │   ├── estimate_samples.rds\n│       │   ├── latest_date.rds\n│       │   ├── reported_cases.rds\n│       │   ├── summarised_estimated_reported_cases.rds\n│       │   ├── summarised_estimates.rds\n│       │   └── summary.rds\n│       ├── epinow_res.rds\n│       ├── epinow_res_small.rds\n│       ├── generation_time.rds\n│       └── incubation_period.rds\n├── case_linelists\n│   ├── cleaning_dict.csv\n│   ├── fluH7N9_China_2013.csv\n│   ├── linelist_cleaned.rds\n│   ├── linelist_cleaned.xlsx\n│   └── linelist_raw.xlsx\n├── country_demographics.csv\n├── covid_example_data\n│   ├── covid_example_data.xlsx\n│   └── covid_shapefile\n│       ├── FultonCountyZipCodes.cpg\n│       ├── FultonCountyZipCodes.dbf\n│       ├── FultonCountyZipCodes.prj\n│       ├── FultonCountyZipCodes.sbn\n│       ├── FultonCountyZipCodes.sbx\n│       ├── FultonCountyZipCodes.shp\n│       ├── FultonCountyZipCodes.shp.xml\n│       └── FultonCountyZipCodes.shx\n├── covid_incidence.csv\n├── covid_incidence_map.R\n├── district_count_data.xlsx\n├── example\n│   ├── Central Hospital.csv\n│   ├── district_weekly_count_data.xlsx\n│   ├── fluH7N9_China_2013.csv\n│   ├── hospital_linelists.xlsx\n│   ├── linelists\n│   │   ├── 20201007linelist.csv\n│   │   ├── case_linelist20201006.csv\n│   │   ├── case_linelist_2020-10-02.csv\n│   │   ├── case_linelist_2020-10-03.csv\n│   │   ├── case_linelist_2020-10-04.csv\n│   │   ├── case_linelist_2020-10-05.csv\n│   │   └── case_linelist_2020-10-08.xlsx\n│   ├── Military Hospital.csv\n│   ├── Missing.csv\n│   ├── Other.csv\n│   ├── Port Hospital.csv\n│   └── St. Mark's Maternity Hospital (SMMH).csv\n├── facility_count_data.rds\n├── flexdashboard\n│   ├── outbreak_dashboard.html\n│   ├── outbreak_dashboard.Rmd\n│   ├── outbreak_dashboard_shiny.Rmd\n│   ├── outbreak_dashboard_test.html\n│   └── outbreak_dashboard_test.Rmd\n├── fluH7N9_China_2013.csv\n├── gis\n│   ├── africa_countries.geo.json\n│   ├── covid_incidence.csv\n│   ├── covid_incidence_map.R\n│   ├── linelist_cleaned_with_adm3.rds\n│   ├── population\n│   │   ├── sle_admpop_adm3_2020.csv\n│   │   └── sle_population_statistics_sierraleone_2020.xlsx\n│   └── shp\n│       ├── README.txt\n│       ├── sle_adm3.CPG\n│       ├── sle_adm3.dbf\n│       ├── sle_adm3.prj\n│       ├── sle_adm3.sbn\n│       ├── sle_adm3.sbx\n│       ├── sle_adm3.shp\n│       ├── sle_adm3.shp.xml\n│       ├── sle_adm3.shx\n│       ├── sle_hf.CPG\n│       ├── sle_hf.dbf\n│       ├── sle_hf.prj\n│       ├── sle_hf.sbn\n│       ├── sle_hf.sbx\n│       ├── sle_hf.shp\n│       └── sle_hf.shx\n├── godata\n│   ├── cases_clean.rds\n│   ├── contacts_clean.rds\n│   ├── followups_clean.rds\n│   └── relationships_clean.rds\n├── likert_data.csv\n├── linelist_cleaned.rds\n├── linelist_cleaned.xlsx\n├── linelist_raw.xlsx\n├── make_evd_dataset-DESKTOP-JIEUMMI.R\n├── make_evd_dataset.R\n├── malaria_app\n│   ├── app.R\n│   ├── data\n│   │   └── facility_count_data.rds\n│   ├── funcs\n│   │   └── plot_epicurve.R\n│   ├── global.R\n│   ├── malaria_app.Rproj\n│   ├── server.R\n│   └── ui.R\n├── malaria_facility_count_data.rds\n├── phylo\n│   ├── sample_data_Shigella_tree.csv\n│   ├── Shigella_subtree_2.nwk\n│   ├── Shigella_subtree_2.txt\n│   └── Shigella_tree.txt\n├── rmarkdown\n│   ├── outbreak_report.docx\n│   ├── outbreak_report.html\n│   ├── outbreak_report.pdf\n│   ├── outbreak_report.pptx\n│   ├── outbreak_report.Rmd\n│   ├── report_tabbed_example.html\n│   └── report_tabbed_example.Rmd\n├── standardization\n│   ├── country_demographics.csv\n│   ├── country_demographics_2.csv\n│   ├── deaths_countryA.csv\n│   ├── deaths_countryB.csv\n│   └── world_standard_population_by_sex.csv\n├── surveys\n│   ├── population.xlsx\n│   ├── survey_data.xlsx\n│   └── survey_dict.xlsx\n└── time_series\n    ├── campylobacter_germany.xlsx\n    └── weather\n        ├── germany_weather2002.nc\n        ├── germany_weather2003.nc\n        ├── germany_weather2004.nc\n        ├── germany_weather2005.nc\n        ├── germany_weather2006.nc\n        ├── germany_weather2007.nc\n        ├── germany_weather2008.nc\n        ├── germany_weather2009.nc\n        ├── germany_weather2010.nc\n        └── germany_weather2011.nc",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#list-files-in-a-directory",
    "href": "new_pages/directories.html#list-files-in-a-directory",
    "title": "45  Directory interactions",
    "section": "45.2 List files in a directory",
    "text": "45.2 List files in a directory\nTo list just the file names in a directory you can use dir() from base R. For example, this command lists the file names of the files in the “population” subfolder of the “data” folder in an R project. The relative filepath is provided using here() (which you can read about more in the Import and export page).\n\n# file names\ndir(here(\"data\", \"gis\", \"population\"))\n\n[1] \"sle_admpop_adm3_2020.csv\"                       \n[2] \"sle_population_statistics_sierraleone_2020.xlsx\"\n\n\nTo list the full file paths of the directory’s files, you can use you can use dir_ls() from fs. A base R alternative is list.files().\n\n# file paths\ndir_ls(here(\"data\", \"gis\", \"population\"))\n\nC:/Users/ngulu864/AppData/Local/Temp/RtmpyIElas/file7c74679b5e85/data/gis/population/sle_admpop_adm3_2020.csv\nC:/Users/ngulu864/AppData/Local/Temp/RtmpyIElas/file7c74679b5e85/data/gis/population/sle_population_statistics_sierraleone_2020.xlsx\n\n\nTo get all the metadata information about each file in a directory, (e.g. path, modification date, etc.) you can use dir_info() from fs.\nThis can be particularly useful if you want to extract the last modification time of the file, for example if you want to import the most recent version of a file. For an example of this, see the Import and export page.\n\n# file info\ndir_info(here(\"data\", \"gis\", \"population\"))\n\nHere is the data frame returned. Scroll to the right to see all the columns.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#file-information",
    "href": "new_pages/directories.html#file-information",
    "title": "45  Directory interactions",
    "section": "45.3 File information",
    "text": "45.3 File information\nTo extract metadata information about a specific file, you can use file_info() from fs (or file.info() from base R).\n\nfile_info(here(\"data\", \"case_linelists\", \"linelist_cleaned.rds\"))\n\n\n\n\n\n\n\nHere we use the $ to index the result and return only the modification_time value.\n\nfile_info(here(\"data\", \"case_linelists\", \"linelist_cleaned.rds\"))$modification_time\n\n[1] \"2024-02-18 14:56:16 CET\"",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#check-if-exists",
    "href": "new_pages/directories.html#check-if-exists",
    "title": "45  Directory interactions",
    "section": "45.4 Check if exists",
    "text": "45.4 Check if exists\n\nR objects\nYou can use exists() from base R to check whether an R object exists within R (supply the object name in quotes).\n\nexists(\"linelist\")\n\n[1] FALSE\n\n\nNote that some base R packages use generic object names like “data” behind the scenes, that will appear as TRUE unless inherit = FALSE is specified. This is one reason to not name your dataset “data”.\n\nexists(\"data\")\n\n[1] TRUE\n\nexists(\"data\", inherit = FALSE)\n\n[1] FALSE\n\n\nIf you are writing a function, you should use missing() from base R to check if an argument is present or not, instead of exists().\n\n\nDirectories\nTo check whether a directory exists, provide the file path (and file name) to is_dir() from fs. Scroll to the right to see that TRUE is printed.\n\nis_dir(here(\"data\"))\n\nC:/Users/ngulu864/AppData/Local/Temp/RtmpyIElas/file7c74679b5e85/data \n                                                                 TRUE \n\n\nAn alternative is file.exists() from base R.\n\n\nFiles\nTo check if a specific file exists, use is_file() from fs. Scroll to the right to see that TRUE is printed.\n\nis_file(here(\"data\", \"case_linelists\", \"linelist_cleaned.rds\"))\n\nC:/Users/ngulu864/AppData/Local/Temp/RtmpyIElas/file7c74679b5e85/data/case_linelists/linelist_cleaned.rds \n                                                                                                     TRUE \n\n\nA base R alternative is file.exists().",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#create",
    "href": "new_pages/directories.html#create",
    "title": "45  Directory interactions",
    "section": "45.5 Create",
    "text": "45.5 Create\n\nDirectories\nTo create a new directory (folder) you can use dir_create() from fs. If the directory already exists, it will not be overwritten and no error will be returned.\n\ndir_create(here(\"data\", \"test\"))\n\nAn alternative is dir.create() from base R, which will show an error if the directory already exists. In contrast, dir_create() in this scenario will be silent.\n\n\nFiles\nYou can create an (empty) file with file_create() from fs. If the file already exists, it will not be over-written or changed.\n\nfile_create(here(\"data\", \"test.rds\"))\n\nA base R alternative is file.create(). But if the file already exists, this option will truncate it. If you use file_create() the file will be left unchanged.\n\n\nCreate if does not exists\nUNDER CONSTRUCTION",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#delete",
    "href": "new_pages/directories.html#delete",
    "title": "45  Directory interactions",
    "section": "45.6 Delete",
    "text": "45.6 Delete\n\nR objects\nUse rm() from base R to remove an R object.\n\n\nDirectories\nUse dir_delete() from fs.\n\n\nFiles\nYou can delete files with file_delete() from fs.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#running-other-files",
    "href": "new_pages/directories.html#running-other-files",
    "title": "45  Directory interactions",
    "section": "45.7 Running other files",
    "text": "45.7 Running other files\n\nsource()\nTo run one R script from another R script, you can use the source() command (from base R).\n\nsource(here(\"scripts\", \"cleaning_scripts\", \"clean_testing_data.R\"))\n\nThis is equivalent to viewing the above R script and clicking the “Source” button in the upper-right of the script. This will execute the script but will do it silently (no output to the R console) unless specifically intended. See the page on [Interactive console] for examples of using source() to interact with a user via the R console in question-and-answer mode.\n\n\n\n\n\n\n\n\n\n\n\nrender()\nrender() is a variation on source() most often used for R markdown scripts. You provide the input = which is the R markdown file, and also the output_format = (typically either “html_document”, “pdf_document”, “word_document”, ““)\nSee the page on Reports with R Markdown for more details. Also see the documentation for render() here or by entering ?render.\n\n\nRun files in a directory\nYou can create a for loop and use it to source() every file in a directory, as identified with dir().\n\nfor(script in dir(here(\"scripts\"), pattern = \".R$\")) {   # for each script name in the R Project's \"scripts\" folder (with .R extension)\n  source(here(\"scripts\", script))                        # source the file with the matching name that exists in the scripts folder\n}\n\nIf you only want to run certain scripts, you can identify them by name like this:\n\nscripts_to_run &lt;- c(\n     \"epicurves.R\",\n     \"demographic_tables.R\",\n     \"survival_curves.R\"\n)\n\nfor(script in scripts_to_run) {\n  source(here(\"scripts\", script))\n}\n\nHere is a comparison of the fs and base R functions.\n\n\nImport files in a directory\nSee the page on [Import and export] for importing and exporting individual files.\nAlso see the Import and export page for methods to automatically import the most recent file, based on a date in the file name or by looking at the file meta-data.\nSee the page on [Iteration, loops, and lists] for an example with the package purrr demonstrating:\n\nSplitting a data frame and saving it out as multiple CSV files\n\nSplitting a data frame and saving each part as a separate sheet within one Excel workbook\n\nImporting multiple CSV files and combining them into one dataframe\n\nImporting an Excel workbook with multiple sheets and combining them into one dataframe",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#base-r",
    "href": "new_pages/directories.html#base-r",
    "title": "45  Directory interactions",
    "section": "45.8 base R",
    "text": "45.8 base R\nSee below the functions list.files() and dir(), which perform the same operation of listing files within a specified directory. You can specify ignore.case = or a specific pattern to look for.\n\nlist.files(path = here(\"data\"))\n\nlist.files(path = here(\"data\"), pattern = \".csv\")\n# dir(path = here(\"data\"), pattern = \".csv\")\n\nlist.files(path = here(\"data\"), pattern = \"evd\", ignore.case = TRUE)\n\nIf a file is currently “open”, it will display in your folder with a tilde in front, like “~$hospital_linelists.xlsx”.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/directories.html#resources",
    "href": "new_pages/directories.html#resources",
    "title": "45  Directory interactions",
    "section": "45.9 Resources",
    "text": "45.9 Resources\nhttps://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Directory interactions</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html",
    "href": "new_pages/collaboration.html",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "",
    "text": "46.1 What is Git?\nGit is a version control software that allows tracking changes in a folder. It can be used like the “track change” option in Word, LibreOffice or Google docs, but for all types of files. It is one of the most powerful and most used options for version control.\nWhy have I never heard of it? - While people with a developer background routinely learn to use version control software (Git, Mercurial, Subversion or others), few of us from quantitative disciplines are taught these skills. Consequently, most epidemiologists never hear of it during their studies, and have to learn it on the fly.\nWait, I heard of Github, is it the same? - Not exactly, but you often use them together, and we will show you how to. In short:\nSo you could use the client/interface Github Desktop, which uses Git in the background to manage your files, both locally on your computer, and remotely on a Github server.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#what-is-git",
    "href": "new_pages/collaboration.html#what-is-git",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "",
    "text": "Git is the version control system, a piece of software. You can use it locally on your computer or to synchronize a folder with a host website. By default, one uses a terminal to give Git instructions in command-line.\nYou can use a Git client/interface to avoid the command-line and perform the same actions (at least for the simple, super common ones).\nIf you want to store your folder in a host website to collaborate with others, you may create an account at Github, Gitlab, Bitbucket or others.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#why-use-the-combo-git-and-github",
    "href": "new_pages/collaboration.html#why-use-the-combo-git-and-github",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.2 Why use the combo Git and Github?",
    "text": "46.2 Why use the combo Git and Github?\nUsing Git facilitates:\n\nArchiving documented versions with incremental changes so that you can easily revert backwards to any previous state\nHaving parallel branches, i.e. developing/“working” versions with structured ways to integrate the changes after review\n\nThis can be done locally on your computer, even if you don’t collaborate with other people. Have you ever:\n\nregretted having deleted a section of code, only to realize two months later that you actually needed it?\ncome back on a project that had been on pause and attempted to remember whether you had made that tricky modification in one of the models?\nhad a file model_1.R and another file model_1_test.R and a file model_1_not_working.R to try things out?\nhad a file report.Rmd, a file report_full.Rmd, a file report_true_final.Rmd, a file report_final_20210304.Rmd, a file report_final_20210402.Rmd and cursed your archiving skills?\n\nGit will help with all that, and is worth to learn for that alone.\nHowever, it becomes even more powerful when used with a online repository such as Github to support collaborative projects. This facilitates:\n\nCollaboration: others can review, comment on, and accept/decline changes\nSharing your code, data, and outputs, and invite feedback from the public (or privately, with your team)\n\nand avoids:\n\n“Oops, I forgot to send the last version and now you need to redo two days worth of work on this new file”\nMina, Henry and Oumar all worked at the same time on one script and need to manually merge their changes\nTwo people try to modify the same file on Dropbox and Sharepoint and this creates a synchronization error.\n\n\nThis sounds complicated, I am not a programmer\nIt can be. Examples of advanced uses can be quite scary. However, much like R, or even Excel, you don’t need to become an expert to reap the benefits of the tool. Learning a small number of functions and notions lets you track your changes, synchronize your files on a online repository and collaborate with your colleagues in a very short amount of time.\nDue to the learning curve, emergency context may not be the best of time to learn these tools. But learning can be achieved by steps. Once you acquire a couple of notions, your workflow can be quite efficient and fast. If you are not working on a project where collaborating with people through Git is a necessity, it is actually a good time to get confident using it in solo before diving in collaboration.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#setup",
    "href": "new_pages/collaboration.html#setup",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.3 Setup",
    "text": "46.3 Setup\n\nInstall Git\nGit is the engine behind the scenes on your computer, which tracks changes, branches (versions), merges, and reverting. You must first install Git from https://git-scm.com/downloads.\n\n\nInstall an interface (optional but recommended)\nGit has its own language of commands, which can be typed into a command line terminal. However, there are many clients/interfaces and as non-developpers, in your day-to-day use, you will rarely need to interact with Git directly and interface usually provide nice visualisation tools for file modifications or branches.\nMany options exist, on all OS, from beginner friendly to more complex ones. Good options for beginners include the RStudio Git pane and Github Desktop, which we will showcase in this chapter. Intermediate (more powerfull, but more complex) options include Source Tree, Gitkracken, Smart Git and others.\nQuick explanation on Git clients.\nNote: since interfaces actually all use Git internally, you can try several of them, switch from one to another on a given project, use the console punctually for an action your interface does not support, or even perform any number of actions online on Github.\nAs noted below, you may occasionally have to write Git commands into a terminal such as the RStudio terminal pane (a tab adjacent to the R Console) or the Git Bash terminal.\n\n\nGithub account\nSign-up for a free account at github.com.\nYou may be offered to set-up two-factor authentication with an app on your phone. Read more in the Github help documents.\nIf you use Github Desktop, you can enter your Gitub credentials after installation following these steps. If you don’t do it know, credentials will be asked later when you try to clone a project from Github.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#vocabulary-concepts-and-basic-functions",
    "href": "new_pages/collaboration.html#vocabulary-concepts-and-basic-functions",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.4 Vocabulary, concepts and basic functions",
    "text": "46.4 Vocabulary, concepts and basic functions\nAs when learning R, there is a bit of vocabulary to remember to understand Git. Here are the basics to get you going / interactive tutorial. In the next sections, we will show how to use interfaces, but it is good to have the vocabulary and concepts in mind, to build your mental model, and as you’ll need them when using interfaces anyway.\n\nRepository\nA Git repository (“repo”) is a folder that contains all the sub-folders and files for your project (data, code, images, etc.) and their revision histories. When you begin tracking changes in the repository with it, Git will create a hidden folder that contains all tracking information. A typical Git repository is your R Project folder (see handbook page on R projects).\nWe will show how to create (initialize) a Git repository from Github, Github Desktop or Rstudio in the next sections.\n\n\nCommits\nA commit is a snapshot of the project at a given time. When you make a change to the project, you will make a new commit to track the changes (the delta) made to your files. For example, perhaps you edited some lines of code and updated a related dataset. Once your changes are saved, you can bundle these changes together into one “commit”.\nEach commit has a unique ID (a hash). For version control purposes, you can revert your project back in time based on commits, so it is best to keep them relatively small and coherent. You will also attach a brief description of the changes called the “commit message”.\nStaged changes? To stage changes is to add them to the staging area in preparation for the next commit. The idea is that you can finely decide which changes to include in a given commit. For example, if you worked on model specification in one script, and later on a figure in another script, it would make sense to have two different commits (it would be easier in case you wanted to revert the changes on the figure but not the model).\n\n\nBranches\nA branch represents an independent line of changes in your repo, a parallel, alternate version of your project files.\nBranches are useful to test changes before they are incorporated into the main branch, which is usually the primary/final/“live” version of your project. When you are done experimenting on a branch, you can bring the changes into your main branch, by merging it, or delete it, if the changes were not so successful.\nNote: you do not have to collaborate with other people to use branches, nor need to have a remote online repository.\n\n\nLocal and remote repositories\nTo clone is to create a copy of a Git repository in another place.\nFor example, you can clone a online repository from Github locally on your computer, or begin with a local repository and clone it online to Github.\nWhen you have cloned a repository, the project files exist in two places:\n\nthe LOCAL repository on your physical computer. This is where you make the actual changes to the files/code.\nthe REMOTE, online repository: the versions of your project files in the Github repository (or on any other web host).\n\nTo synchronize these repositories, we will use more functions. Indeed, unlike Sharepoint, Dropbox or other synchronizing software, Git does not automatically update your local repository based or what’s online, or vice-versa. You get to choose when and how to synchronize.\n\ngit fetch downloads the new changes from the remote repository but does not change your local repository. Think of it as checking the state of the remote repository.\ngit pull downloads the new changes from the remote repositories and update your local repository.\nWhen you have made one or several commits locally, you can git push the commits to the remote repository. This sends your changes on Github so that other people can see and pull them if they want to.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#get-started-create-a-new-repository",
    "href": "new_pages/collaboration.html#get-started-create-a-new-repository",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.5 Get started: create a new repository",
    "text": "46.5 Get started: create a new repository\nThere are many ways to create new repositories. You can do it from the console, from Github, from an interface.\nTwo general approaches to set-up are:\n\nCreate a new R Project from an existing or new Github repository (preferred for beginners), or\nCreate a Github repository for an existing R project\n\n\nStart-up files\nWhen you create a new repository, you can optionally create all of the below files, or you can add them to your repository at a later stage. They would typically live in the “root” folder of the repository.\n\nA README file is a file that someone can read to understand why your project exists and what else they should know to use it. It will be empty at first, but you should complete it later.\nA .gitignore file is a text file where each line would contain folders or files that Git should ignore (not track changes). Read more about it and see examples here.\nYou can choose a license for your work, so that other people know under which conditions they can use or reproduce your work. For more information, see the Creative Commons licenses.\n\n\n\nCreate a new repository in Github\nTo create a new repository, log into Github and look for the green button to create a new repository. This now empty repository can be cloned locally to your computer (see next section).\n\n\n\n\n\n\n\n\n\nYou must choose if you want your repository to be public (visible to everyone on the internet) or private (only visible to those with permission). This has important implications if your data are sensitive. If your repository is private you will encounter some quotas in advanced special circumstances, such as if you are using Github actions to automatically run your code in the cloud.\n\n\nClone from a Github repository\nYou can clone an existing Github repository to create a new local R project on your computer.\nThe Github repository could be one that already exists and contains content, or could be an empty repository that you just created. In this latter case you are essentially creating the Github repo and local R project at the same time (see instructions above).\nNote: if you do not have contributing rights on a Github repository, it is possible to first fork the repository to your profile, and then proceed with the other actions. Forking is explained at the end of this chapter, but we recommend that you read the other sections first.\nStep 1: Navigate in Github to the repository, click on the green “Code” button and copy the HTTPS clone URL (see image below)\n\n\n\n\n\n\n\n\n\nThe next step can be performed in any interface. We will illustrate with Rstudio and Github desktop.\n\nIn Rstudio\nIn RStudio, start a new R project by clicking File &gt; New Project &gt; Version Control &gt; Git\n\nWhen prompted for the “Repository URL”, paste the HTTPS URL from Github\n\nAssign the R project a short, informative name\n\nChoose where the new R Project will be saved locally\n\nCheck “Open in new session” and click “Create project”\n\nYou are now in a new, local, RStudio project that is a clone of the Github repository. This local project and the Github repository are now linked.\n\n\nIn Github Desktop\n\nClick on File &gt; Clone a repository\nSelect the URL tab\nPaste the HTTPS URL from Github in the first box\nSelect the folder in which you want to have your local repository\nClick “CLONE”\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Github repo from existing R project\nAn alternative setup scenario is that you have an existing R project with content, and you want to create a Github repository for it.\n\nCreate a new, empty Github repository for the project (see instructions above)\n\nClone this repository locally (see HTTPS instructions above)\n\nCopy all the content from your pre-existing R project (codes, data, etc.) into this new empty, local, repository (e.g. use copy and paste).\n\nOpen your new project in RStudio, and go to the Git pane. The new files should register as file changes, now tracked by Git. Therefore, you can bundle these changes as a commit and push them up to Github. Once pushed, the repository on Github will reflect all the files.\n\nSee the Github workflow section below for details on this process.\n\n\nWhat does it look like now?\n\nIn RStudio\nOnce you have cloned a Github repository to a new R project, you now see in RStudio a “Git” tab. This tab appears in the same RStudio pane as your R Environment:\n\n\n\n\n\n\n\n\n\nPlease note the buttons circled in the image above, as they will be referenced later (from left to right):\n\nButton to commit the saved file changes to the local branch (this will open a new window)\nBlue arrow to pull (update your local version of the branch with any changes made on the remote/Github version of that branch)\nGreen arrow to push (send any commits/changes for your local version of the branch to the remote/Github version of that branch)\nThe Git tab in RStudio\nButton to create a NEW branch using whichever local branch is shown to the right as the base. You almost always want to branch off from the main branch (after you first pull to update the main branch)\nThe branch you are currently working in\nChanges you made to code or other files will appear below\n\n\n\nIn Github Desktop\nGithub Desktop is an independent application that allows you to manage all your repositories. When you open it, the interface allows you to choose the repository you want to work on, and then to perform basic Git actions from there.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#git-github-workflow",
    "href": "new_pages/collaboration.html#git-github-workflow",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.6 Git + Github workflow",
    "text": "46.6 Git + Github workflow\n\nProcess overview\nOnce you have completed the setup (described above), you will have a Github repo that is connected (cloned) to a local R project. The main branch (created by default) is the so-called “live” version of all the files. When you want to make modifications, it is a good practice to create a new branch from the main branch (like “Make a Copy”). This is a typical workflow in Git because creating a branch is easy and fast.\nA typical workflow is as follow:\n\nMake sure that your local repository is up-to-date, update it if not\nGo to the branch you were working on previously, or create a new branch to try out some things\nWork on the files locally on your computer, make one or several commits to this branch\nUpdate the remote version of the branch with your changes (push)\nWhen you are satisfied with your branch, you can merge the online version of the working branch into the online “main” branch to transfer the changes\n\nOther team members may be doing the same thing with their own branches, or perhaps contributing commits into your working branch as well.\nWe go through the above process step-by-step in more detail below. Here is a schematic we’ve developed - it’s in the format of a two-way table so it should help epidemiologists understand.\n\n\n\n\n\n\n\n\n\nHere’s another diagram.\nNote: until recently, the term “master” branch was used, but it is now referred to as “main” branch.\n\n\n\n\n\n\n\n\n\nImage source",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#create-a-new-branch",
    "href": "new_pages/collaboration.html#create-a-new-branch",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.7 Create a new branch",
    "text": "46.7 Create a new branch\nWhen you select a branch to work on, Git resets your working directory the way it was the last time you were on this branch.\n\nIn Rstudio Git pane\nEnsure you are in the “main” branch, and then click on the purple icon to create a new branch (see image above).\n\nYou will be prompted to name your branch with a one-word descriptive name (can use underscores if needed).\nYou will see that locally, you are still in the same R project, but you are no longer working on the “main” branch.\nOnce created, the new branch will also appear in the Github website as a branch.\n\nYou can visualize branches in the Git Pane in Rstudio after clicking on “History”\n\n\n\n\n\n\n\n\n\n\n\nIn Github Desktop\nThe process is very much similar, you are prompted to give your branch a name. After, you will be prompted to “Publish you branch to Github” to make the new branch appear in the remote repo as well.\n\n\n\n\n\n\n\n\n\n\n\nIn console\nWhat is actually happening behind the scenes is that you create a new branch with git branch, then go to the branch with git checkout (i.e. tell Git that your next commits will occur there). From your git repository:\n\ngit branch my-new-branch  # Create the new branch branch\ngit checkout my-new-branch # Go to the branch\ngit checkout -b my-new-branch # Both at once (shortcut)\n\nFor more information about using the console, see the section on Git commands at the end.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#commit-changes",
    "href": "new_pages/collaboration.html#commit-changes",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.8 Commit changes",
    "text": "46.8 Commit changes\nNow you can edit code, add new files, update datasets, etc.\nEvery one of your changes is tracked, once the respective file is saved. Changed files will appear in the RStudio Git tab, in Github Desktop, or using the command git status in the terminal (see below).\nWhenever you make substantial changes (e.g. adding or updating a section of code), pause and commit those changes. Think of a commit as a “batch” of changes related to a common purpose. You can always continue to revise a file after having committed changes on it.\nAdvice on commits: generally, it is better to make small commits, that can be easily reverted if a problem arises, to commit together modifications related to a common purpose. To achieve this, you will find that you should commit often. At the beginning, you’ll probably forget to commit often, but then the habit kicks in.\n\nIn Rstudio\nThe example below shows that, since the last commit, the R Markdown script “collaboration.Rmd” has changed, and several PNG images were added.\n\n\n\n\n\n\n\n\n\nYou might be wondering what the yellow, blue, green, and red squares next to the file names represent. Here is a snapshot from the RStudio cheatsheet that explains their meaning. Note that changes with yellow “?” can still be staged, committed, and pushed.\n\n\n\n\n\n\n\n\n\n\nPress the “Commit” button in the Git tab, which will open a new window (shown below)\nClick on a file name in the upper-left box\nReview the changes you made to that file (highlighted below in green or red)\n“Stage” the file, which will include those changes in the commit. Do this by checking the box next to the file name. Alternatively, you can highlight multiple file names and then click “Stage”\nWrite a commit message that is short but descriptive (required)\nPress the “Commit” button. A pop-up box will appear showing success or an error message.\n\nNow you can make more changes and more commits, as many times as you would like\n\n\n\n\n\n\n\n\n\n\n\nIn Github Desktop\nYou can see the list of the files that were changed on the left. If you select a text file, you will see a summary of the modifications that were made in the right pane (the view will not work on more complex files like .docs or .xlsx).\nTo stage the changes, just tick the little box near file names. When you have selected the files you want to add to this commit, give the commit a name, optionally a description and then click on the commit button.\n\n\n\n\n\n\n\n\n\n\n\nIn console\nThe two functions used behind the scenes are git add to select/stage files and git commit to actually do the commit.\n\ngit status # see the changes \n\ngit add new_pages/collaboration.Rmd  # select files to commit (= stage the changes)\n\ngit commit -m \"Describe commit from Github Desktop\" # commit the changes with a message\n\ngit log  # view information on past commits\n\n\n\nAmend a previous commit\nWhat happens if you commit some changes, carry on working, and realize that you made changes that should “belong” to the past commit (in your opinion). Fear not! You can append these changes to your previous commit.\nIn Rstudio, it should be pretty obvious as there is a “Amend previous commit” box on the same line as the COMMIT button.\nFor some unclear reason, the functionality has not been implemented as such in Github Desktop, but there is a (conceptually awkward but easy) way around. If you have committed but not pushed your changes yet, an “UNDO” button appears just under the COMMIT button. Click on it and it will revert your commit (but keep your staged files and your commit message). Save your changes, add new files to the commit if necessary and commit again.\nIn the console:\n\ngit add [YOUR FILES] # Stage your new changes\n\ngit commit --amend  # Amend the previous commit\n\ngit commit --amend -m \"An updated commit message\"  # Amend the previous commit AND update the commit message\n\nNote: think before modifying commits that are already public and shared with your collaborators.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#pull-and-push-changes-up-to-github",
    "href": "new_pages/collaboration.html#pull-and-push-changes-up-to-github",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.9 Pull and push changes up to Github",
    "text": "46.9 Pull and push changes up to Github\n“First PULL, then PUSH”\nIt is good practice to fetch and pull before you begin working on your project, to update the branch version on your local computer with any changes that have been made to it in the remote/Github version.\nPULL often. Don’t hesitate. Always pull before pushing.\nWhen your changes are made and committed and you are happy with the state of your project, you can push your commits up to the remote/Github version of your branch.\nRince and repeat while you are working on the repository.\nNote: it is much easier to revert changes that were committed but not pushed (i.e. are still local) than to revert changes that were pushed to the remote repository (and perhaps already pulled by someone else), so it is better to push when you are done with introducing changes on the task that you were working on.\n\nIn Rstudio\nPULL - First, click the “Pull” icon (downward arrow) which fetches and pulls at the same time.\nPUSH - Clicking the green “Pull” icon (upward arrow). You may be asked to enter your Github username and password. The first time you are asked, you may need to enter two Git command lines into the Terminal:\n\ngit config –global user.email “you@example.com” (your Github email address), and\n\ngit config –global user.name “Your Github username”\n\nTo learn more about how to enter these commands, see the section below on Git commands.\nTIP: Asked to provide your password too often? See these chapters 10 & 11 of this tutorial to connect to a repository using a SSH key (more complicated)\n\n\nIn Github Desktop\nClick on the “Fetch origin” button to check if there are new commits on the remote repository.\n\n\n\n\n\n\n\n\n\nIf Git finds new commits on the remote repository, the button will change into a “Pull” button. Because the same button is used to push and pull, you cannot push your changes if you don’t pull before.\n\n\n\n\n\n\n\n\n\nYou can go to the “History” tab (near the “Changes” tab) to see all commits (yours and others). This is a nice way of acquainting yourself with what your collaborators did. You can read the commit message, the description if there is one, and compare the code of the two files using the diff pane.\n\n\n\n\n\n\n\n\n\nOnce all remote changes have been pulled, and at least one local change has been committed, you can push by clicking on the same button.\n\n\n\n\n\n\n\n\n\n\n\nConsole\nWithout surprise, the commands are fetch, pull and push.\n\ngit fetch  # are there new commits in the remote directory?\ngit pull   # Bring remote commits into your local branch\ngit push   # Puch local commits of this branch to the remote branch\n\n\n\nI want to pull but I have local work\nThis can happen sometimes: you made some changes on your local repository, but the remote repository has commits that you didn’t pull.\nGit will refuse to pull because it might overwrite your changes. There are several strategies to keep your changes, well described in Happy Git with R, among which the two main ones are: - commit your changes, fetch remote changes, pull them in, resolve conflicts if needed (see section below), and push everything online - stash your changes, which sort of stores them aside, pull, unstash (restore), and then commit, solve any conflicts, and push.\nIf the files concerned by the remote changes and the files concerned by your local changes do not overlap, Git may solve conflicts automatically.\nIn Github Desktop, this can be done with buttons. To stash, go to Branch &gt; Stash all changes.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#merge-branch-into-main",
    "href": "new_pages/collaboration.html#merge-branch-into-main",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.10 Merge branch into Main",
    "text": "46.10 Merge branch into Main\nIf you have finished making changes, you can begin the process of merging those changes into the main branch. Depending on your situation, this may be fast, or you may have deliberate review and approval steps involving teammates.\n\nLocally in Github Desktop\nOne can merge branches locally using Github Desktop. First, go to (checkout) the branch that will be the recipient of the commits, in other words, the branch you want to update. Then go to the menu Branch &gt; Merge into current branch and click. A box will allow you to select the branch you want to import from.\n\n\n\n\n\n\n\n\n\n\n\nIn console\nFirst move back to the branch that will be the recipient of the changes. This is usually master, but it could be another branch. Then merge your working branch into master.\n\ngit checkout master  # Go back to master (or to the branch you want to move your )\ngit merge this_fancy_new_branch\n\nThis page shows a more advanced example of branching and explains a bit what is happening behind the scenes.\n\n\nIn Github: submitting pull requests\nWhile it is totally possible to merge two branches locally, or without informing anybody, a merge may be discussed or investigated by several people before being integrated to the master branch. To help with the process, Github offers some discussion features around the merge: the pull request.\nA pull request (a “PR”) is a request to merge one branch into another (in other words, a request that your working branch be pulled into the “main” branch). A pull request typically involves multiple commits. A pull request usually begins a conversation and review process before it is accepted and the branch is merged. For example, you can read pull request discussions on dplyr’s github.\nYou can submit a pull request (PR) directly form the website (as illustrated bellow) or from Github Desktop.\n\nGo to Github repository (online)\nView the tab “Pull Requests” and click the “New pull request” button\nSelect from the drop-down menu to merge your branch into main\nWrite a detailed Pull Request comment and click “Create Pull Request”.\n\nIn the image below, the branch “forests” has been selected to be merged into “main”:\n\n\n\n\n\n\n\n\n\nNow you should be able to see the pull request (example image below):\n\nReview the tab “Files changed” to see how the “main” branch would change if the branch were merged.\n\nOn the right, you can request a review from members of your team by tagging their Github ID. If you like, you can set the repository settings to require one approving review in order to merge into main.\n\nOnce the pull request is approved, a button to “Merge pull request” will become active. Click this.\n\nOnce completed, delete your branch as explained below.\n\n\n\n\n\n\n\n\n\n\n\n\nResolving conflicts\nWhen two people modified the same line(s) at the same time, a merge conflict arises. Indeed, Git refuses to make a decision about which version to keep, but it helps you find where the conflict is. DO NOT PANIC. Most of the time, it is pretty straightforward to resolve.\nFor example, on Github:\n\n\n\n\n\n\n\n\n\nAfter the merge raised a conflict, open the file in your favorite editor. The conflict will be indicated by series of characters:\n\n\n\n\n\n\n\n\n\nThe text between &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD and ======= comes from your local repository, and the one between ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt; from the the other branch (which may be origin, master or any branch of your choice).\nYou need to decide which version of the code you prefer (or even write a third, including changes from both sides if pertinent), delete the rest and remove all the marks that Git added (&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt; origin/master/your_branch_name).\nThen, save the file, stage it and commit it : this is the commit that makes the merged version “official”. Do not forget to push afterwards.\nThe more often you and your collaborators pull and push, the smaller the conflicts will be.\nNote: If you feel at ease with the console, there are more advanced merging options (e.g. ignoring whitespace, giving a collaborator priority etc.).\n\n\nDelete your branch\nOnce a branch was merged into master and is no longer needed, you can delete it.\n\n46.10.0.1 Github + Rstudio\nGo to the repository on Github and click the button to view all the branches (next to the drop-down to select branches). Now find your branch and click the trash icon next to it. Read more detail on deleting a branch here.\nBe sure to also delete the branch locally on your computer. This will not happen automatically.\n\nFrom RStudio, make sure you are in the Main branch\nSwitch to typing Git commands in the RStudio “Terminal” (the tab adjacent to the R console), and type: git branch -d branch_name, where “branch_name” is the name of your branch to be deleted\nRefresh your Git tab and the branch should be gone\n\n\n\n46.10.0.2 In Github Desktop\nJust checkout the branch you want to delete, and go to the menu Branch &gt; Delete.\n\n\n\nForking\nYou can fork a project if you would like to contribute to it but do not have the rights to do so, or if you just want to modify it for your personal use. A short description of forking can be found here.\nOn Github, click on the “Fork” button:\n\n\n\n\n\n\n\n\n\nThis will clone the original repository, but in your own profile. So now, there are two versions of the repository on Github: the original one, that you cannot modify, and the cloned version in your profile.\nThen, you can proceed to clone your version of the online repository locally on your computer, using any of the methods described in previous sections. Then, you can create a new branch, make changes, commit and push them to your remote repository.\nOnce you are happy with the result you can create a Pull Request from Github or Github Desktop to begin the conversation with the owners/maintainers of the original repository.\nWhat if you need some newer commits from the official repository?\nImagine that someone makes a critical modification to the official repository, which you want to include to your cloned version. It is possible to synchronize your fork with the official repository. It involves using the terminal, but it is not too complicated. You mostly need to remember that: - upstream = the official repository, the one that you could not modify - origin = your version of the repository on your Github profile\nYou can read this tutorial or follow along below:\nFirst, type in your Git terminal (inside your repo):\n\ngit remote -v\n\nIf you have not yet configured the upstream repository you should see two lines, beginning by origin. They show the remote repo that fetch and push point to. Remember, origin is the conventional nickname for your own version of the repository on Github. For example:\n\n\n\n\n\n\n\n\n\nNow, add a new remote repository:\n\ngit remote add upstream https://github.com/appliedepi/epirhandbook_eng.git\n\nHere the address is the address that Github generates when you clone a repository (see section on cloning). Now you will have four remote pointers:\n\n\n\n\n\n\n\n\n\nNow that the setup is done, whenever you want to get the changes from the original (upstream) repository, you just have to go (checkout) to the branch you want to update and type:\n\ngit fetch upstream # Get the new commits from the remote repository\ngit checkout the_branch_you_want_to_update\ngit merge upstream/the_branch_you_want_to_update  # Merge the upstream branch into your branch.\ngit push # Update your own version of the remote repo\n\nIf there are conflicts, you will have to solve them, as explained in the Resolving conflicts section.\nSummary: forking is cloning, but on the Github server side. The rest of the actions are typical collaboration workflow actions (clone, push, pull, commit, merge, submit pull requests…).\nNote: while forking is a concept, not a Git command, it also exist on other Web hosts, like Bitbucket.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#what-we-learned",
    "href": "new_pages/collaboration.html#what-we-learned",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.11 What we learned",
    "text": "46.11 What we learned\nYou have learned how to:\n\nsetup Git to track modifications in your folders,\n\nconnect your local repository to a remote online repository,\n\ncommit changes,\n\nsynchronize your local and remote repositories.\n\nAll this should get you going and be enough for most of your needs as epidemiologists. We usually do not have as advanced usage as developers.\nHowever, know that should you want (or need) to go further, Git offers more power to simplify commit histories, revert one or several commits, cherry-pick commits, etc. Some of it may sound like pure wizardry, but now that you have the basics, it is easier to build on it.\nNote that while the Git pane in Rstudio and Github Desktop are good for beginners / day-to-day usage in our line of work, they do not offer an interface to some of the intermediate / advanced Git functions. Some more complete interfaces allows you to do more with point-and-click (usually at the cost of a more complex layout).\nRemember that since you can use any tool at any point to track your repository, you can very easily install an interface to try it out sometimes, or to perform some less common complex task occasionally, while preferring a simplified interface for the rest of time (e.g. using Github Desktop most of the time, and switching to SourceTree or Gitbash for some specific tasks).",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#git",
    "href": "new_pages/collaboration.html#git",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.12 Git commands",
    "text": "46.12 Git commands\n\nRecommended learning\nTo learn Git commands in an interactive tutorial, see this website.\n\n\nWhere to enter commands\nYou enter commands in a Git shell.\nOption 1 You can open a new Terminal in RStudio. This tab is next to the R Console. If you cannot type any text in it, click on the drop-down menu below “Terminal” and select “New terminal”. Type the commands at the blinking space in front of the dollar sign “$”.\n\n\n\n\n\n\n\n\n\nOption 2 You can also open a shell (a terminal to enter commands) by clicking the blue “gears” icon in the Git tab (near the RStudio Environment). Select “Shell” from the drop-down menu. A new window will open where you can type the commands after the dollar sign “$”.\nOption 3 Right click to open “Git Bash here” which will open the same sort of terminal, or open Git Bash form your application list. More beginner-friendly informations on Git Bash, how to find it and some bash commands you will need.\n\n\nSample commands\nBelow we present a few common git commands. When you use them, keep in mind which branch is active (checked-out), as that will change the action!\nIn the commands below,  represents a branch name.  represents the hash ID of a specific commit.  represents a number. Do not type the &lt; or &gt; symbols.\n\n\n\n\n\n\n\nGit command\nAction\n\n\n\n\ngit branch &lt;name&gt;\nCreate a new branch with the name \n\n\ngit checkout &lt;name&gt;\nSwitch current branch to \n\n\ngit checkout -b &lt;name&gt;\nShortcut to create new branch and switch to it\n\n\ngit status\nSee untracked changes\n\n\ngit add &lt;file&gt;\nStage a file\n\n\ngit commit -m &lt;message&gt;\nCommit currently staged changes to current branch with message\n\n\ngit fetch\nFetch commits from remote repository\n\n\ngit pull\nPull commits from remote repository in current branch\n\n\ngit push\nPush local commits to remote directory\n\n\ngit switch\nAn alternative to git checkout that is being phased in to Git\n\n\ngit merge &lt;name&gt;\nMerge  branch into current branch\n\n\ngit rebase &lt;name&gt;\nAppend commits from current branch on to  branch",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/collaboration.html#resources",
    "href": "new_pages/collaboration.html#resources",
    "title": "46  Version control and collaboration with Git and Github",
    "section": "46.13 Resources",
    "text": "46.13 Resources\nMuch of this page was informed by this “Happy Git with R” website by Jenny Bryan. There is a very helpful section of this website that helps you troubleshoot common Git and R-related errors.\nThe Github.com documentation and start guide.\nThe RStudio “IDE” cheatsheet which includes tips on Git with RStudio.\nhttps://ohi-science.org/news/github-going-back-in-time\nGit commands for beginners\nAn interactive tutorial to learn Git commands.\nhttps://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/: good for learning the absolute basics to track changes in one folder on you own computer.\nNice schematics to understand branches: https://speakerdeck.com/alicebartlett/git-for-humans\nTutorials covering both basic and more advanced subjects\nhttps://tutorialzine.com/2016/06/learn-git-in-30-minutes\nhttps://dzone.com/articles/git-tutorial-commands-and-operations-in-git https://swcarpentry.github.io/git-novice/ (short course) https://rsjakob.gitbooks.io/git/content/chapter1.html\nThe Pro Git book is considered an official reference. While some chapters are ok, it is usually a bit technical. It is probably a good resource once you have used Git a bit and want to learn a bit more precisely what happens and how to go further.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Version control and collaboration with Git and Github</span>"
    ]
  },
  {
    "objectID": "new_pages/errors.html",
    "href": "new_pages/errors.html",
    "title": "47  Common errors",
    "section": "",
    "text": "47.1 Interpreting error messages\nR errors can be cryptic at times, so Google is your friend. Search the error message with “R” and look for recent posts in StackExchange.com, stackoverflow.com, community.rstudio.com, twitter (#rstats), and other forums used by programmers to filed questions and answers. Try to find recent posts that have solved similar problems.\nIf after much searching you cannot find an answer to your problem, consider creating a reproducible example (“reprex”) and posting the question yourself. See the page on Getting help for tips on how to create and post a reproducible example to forums.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Common errors</span>"
    ]
  },
  {
    "objectID": "new_pages/errors.html#common-errors",
    "href": "new_pages/errors.html#common-errors",
    "title": "47  Common errors",
    "section": "47.2 Common errors",
    "text": "47.2 Common errors\nBelow, we list some common errors and potential explanations/solutions. Some of these are borrowed from Noam Ross who analyzed the most common forum posts on Stack Overflow about R error messages (see analysis here)\n\nTypo errors\nError: unexpected symbol in:\n\"  geom_histogram(stat = \"identity\")+\n  tidyquant::geom_ma(n=7, size = 2, color = \"red\" lty\"\nIf you see “unexpected symbol”, check for missing commas\n\n\nPackage errors\ncould not find function \"x\"...\nThis likely means that you typed the function name incorrectly, or forgot to install or load a package.\nError in select(data, var) : unused argument (var)\nYou think you are using dplyr::select() but the select() function has been masked by MASS::select() - specify dplyr:: or re-order your package loading so that dplyr is after all the others.\nOther common masking errors stem from: plyr::summarise() and stats::filter(). Consider using the conflicted package.\nError in install.packages : ERROR: failed to lock directory ‘C:\\Users\\Name\\Documents\\R\\win-library\\4.0’ for modifying\nTry removing ‘C:\\Users\\Name\\Documents\\R\\win-library\\4.0/00LOCK’\nIf you get an error saying you need to remove an “00LOCK” file, go to your “R” library in your computer directory (e.g. R/win-library/) and look for a folder called “00LOCK”. Delete this manually, and try installing the package again. A previous install process was probably interrupted, which led to this.\n\n\nObject errors\nNo such file or directory:\nIf you see an error like this when you try to export or import: Check the spelling of the file and filepath, and if the path contains slashes make sure they are forward / and not backward \\. Also make sure you used the correct file extension (e.g. .csv, .xlsx).\nobject 'x' not found \nThis means that an object you are referencing does not exist. Perhaps code above did not run properly?\nError in 'x': subscript out of bounds\nThis means you tried to access something (an element of a vector or a list) that was not there.\n\n\nFunction syntax errors\n# ran recode without re-stating the x variable in mutate(x = recode(x, OLD = NEW)\nError: Problem with `mutate()` input `hospital`.\nx argument \".x\" is missing, with no default\ni Input `hospital` is `recode(...)`.\nThis error above (argument .x is missing, with no default) is common in mutate() if you are supplying a function like recode() or replace_na() where it expects you to provide the column name as the first argument. This is easy to forget.\n\n\nLogic errors\nError in if\nThis likely means an if statement was applied to something that was not TRUE or FALSE.\n\n\nFactor errors\n#Tried to add a value (\"Missing\") to a factor (with replace_na operating on a factor)\nProblem with `mutate()` input `age_cat`.\ni invalid factor level, NA generated\ni Input `age_cat` is `replace_na(age_cat, \"Missing\")`.invalid factor level, NA generated\nIf you see this error about invalid factor levels, you likely have a column of class Factor (which contains pre-defined levels) and tried to add a new value to it. Convert it to class Character before adding a new value.\n\n\nPlotting errors\nError: Insufficient values in manual scale. 3 needed but only 2 provided. ggplot() scale_fill_manual() values = c(“orange”, “purple”) … insufficient for number of factor levels … consider whether NA is now a factor level…\nCan't add x object\nYou probably have an extra + at the end of a ggplot command that you need to delete.\n\n\nR Markdown errors\nIf the error message contains something like Error in options[[sprintf(\"fig.%s\", i)]], check that your knitr options at the top of each chunk correctly use the out.width = or out.height = and not fig.width= and fig.height=.\n\n\nMiscellaneous\nConsider whether you re-arranged piped dplyr verbs and didn’t replace a pipe in the middle, or didn’t remove a pipe from the end after re-arranging.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Common errors</span>"
    ]
  },
  {
    "objectID": "new_pages/errors.html#resources",
    "href": "new_pages/errors.html#resources",
    "title": "47  Common errors",
    "section": "47.3 Resources",
    "text": "47.3 Resources\nThis is another blog post that lists common R programming errors faced by beginners",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Common errors</span>"
    ]
  },
  {
    "objectID": "new_pages/help.html",
    "href": "new_pages/help.html",
    "title": "48  Getting help",
    "section": "",
    "text": "48.1 Github issues\nMany R packages and projects have their code hosted on the website Github.com. You can communicate directly with authors via this website by posting an “Issue”.\nRead more about how to store your work on Github in the page Collaboration and Github.\nOn Github, each project is contained within a repository. Each repository contains code, data, outputs, help documentation, etc. There is also a vehicle to communicate with the authors called “Issues”.\nSee below the Github page for the incidence2 package (used to make epidemic curves). You can see the “Issues” tab highlighted in yellow. You can see that there are 5 open issues.\nOnce in the Issues tab, you can see the open issues. Review them to ensure your problem is not already being addressed. You can open a new issue by clicking the green button on the right. You will need a Github account to do this.\nIn your issue, follow the instructions below to provide a minimal, reproducible example. And please be courteous! Most people developing R packages and projects are doing so in their spare time (like this handbook!).\nTo read more advanced materials about handling issues in your own Github repository, check out the Github documentation on Issues.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Getting help</span>"
    ]
  },
  {
    "objectID": "new_pages/help.html#reproducible-example",
    "href": "new_pages/help.html#reproducible-example",
    "title": "48  Getting help",
    "section": "48.2 Reproducible example",
    "text": "48.2 Reproducible example\nProviding a reproducible example (“reprex”) is key to getting help when posting in a forum or in a Github issue. People want to help you, but you have to give them an example that they can work with on their own computer. The example should:\n\nDemonstrate the problem you encountered\n\nBe minimal, in that it includes only the data and code required to reproduce your problem\n\nBe reproducible, such that all objects (e.g. data), package calls (e.g. library() or p_load()) are included\n\nAlso, be sure you do not post any sensitive data with the reprex! You can create example data frames, or use one of the data frames built into R (enter data() to open a list of these datasets).\n\nThe reprex package\nThe reprex package can assist you with making a reproducible example:\n\nreprex is installed with tidyverse, so load either package\n\n\n# install/load tidyverse (which includes reprex)\npacman::p_load(tidyverse)\n\n\nBegin an R script that creates your problem, step-by-step, starting from loading packages and data.\n\n\n# load packages\npacman::p_load(\n     tidyverse,  # data mgmt and vizualization\n     outbreaks)  # example outbreak datasets\n\n# flu epidemic case linelist\noutbreak_raw &lt;- outbreaks::fluH7N9_china_2013  # retrieve dataset from outbreaks package\n\n# Clean dataset\noutbreak &lt;- outbreak_raw %&gt;% \n     mutate(across(contains(\"date\"), as.Date))\n\n# Plot epidemic\n\nggplot(data = outbreak)+\n     geom_histogram(\n          mapping = aes(x = date_of_onset),\n          binwidth = 7\n     )+\n  scale_x_date(\n    date_format = \"%d %m\"\n  )\n\nCopy all the code to your clipboard, and run the following command:\n\nreprex::reprex()\n\nYou will see an HTML output appear in the RStudio Viewer pane. It will contain all your code and any warnings, errors, or plot outputs. This output is also copied to your clipboard, so you can post it directly into a Github issue or a forum post.\n\n\n\n\n\n\n\n\n\n\nIf you set session_info = TRUE the output of sessioninfo::session_info() with your R and R package versions will be included\n\nYou can provide a working directory to wd =\n\nYou can read more about the arguments and possible variations at the documentation or by entering ?reprex\n\nIn the example above, the ggplot() command did not run because the arguemnt date_format = is not correct - it should be date_labels =.\n\n\nMinimal data\nThe helpers need to be able to use your data - ideally they need to be able to create it with code.\nTo create a minumal dataset, consider anonymising and using only a subset of the observations.\nUNDER CONSTRUCTION - you can also use the function dput() to create minimal dataset.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Getting help</span>"
    ]
  },
  {
    "objectID": "new_pages/help.html#posting-to-a-forum",
    "href": "new_pages/help.html#posting-to-a-forum",
    "title": "48  Getting help",
    "section": "48.3 Posting to a forum",
    "text": "48.3 Posting to a forum\nRead lots of forum posts. Get an understanding for which posts are well-written, and which ones are not.\n\nFirst, decide whether to ask the question at all. Have you thoroughly reviewed the forum website, trying various search terms, to see if your question has already been asked?\nGive your question an informative title (not “Help! this isn’t working”).\nWrite your question:\n\n\nIntroduce your situation and problem\n\nLink to posts of similar issues and explain how they do not answer your question\n\nInclude any relevant information to help someone who does not know the context of your work\n\nGive a minimal reproducible example with your R session information\n\nUse proper spelling, grammar, punctuation, and break your question into paragraphs so that it is easier to read\n\n\nMonitor your question once posted to respond to any requests for clarification. Be courteous and gracious - often the people answering are volunteering their time to help you. If you have a follow-up question consider whether it should be a separate posted question.\nMark the question as answered, if you get an answer that meets the original request. This helps others later quickly recognize the solution.\n\nRead these posts about how to ask a good question the Stack overflow code of conduct.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Getting help</span>"
    ]
  },
  {
    "objectID": "new_pages/help.html#resources",
    "href": "new_pages/help.html#resources",
    "title": "48  Getting help",
    "section": "48.4 Resources",
    "text": "48.4 Resources\nTidyverse page on how to get help!\nTips on producing a minimal dataset\nDocumentation for the dput function",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Getting help</span>"
    ]
  },
  {
    "objectID": "new_pages/network_drives.html",
    "href": "new_pages/network_drives.html",
    "title": "49  R on network drives",
    "section": "",
    "text": "49.1 Overview\nUsing R on network or “company” shared drives can present additional challenges. This page contains approaches, common errors, and suggestions on troubleshooting gained from our experience working through these issues. These include tips for the particularly delicate situations involving R Markdown.\nUsing R on Network Drives: Overarching principles",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>R on network drives</span>"
    ]
  },
  {
    "objectID": "new_pages/network_drives.html#overview",
    "href": "new_pages/network_drives.html#overview",
    "title": "49  R on network drives",
    "section": "",
    "text": "You must get administrator access for your computer. Setup RStudio specifically to run as administrator.\n\nSave packages to a library on a lettered drive (e.g. “C:”) when possible. Use a package library whose path begins with “\\\" as little as possible.\n\nthe rmarkdown package must not be in a “\\\" package library, as then it can’t connect to TinyTex or Pandoc.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>R on network drives</span>"
    ]
  },
  {
    "objectID": "new_pages/network_drives.html#rstudio-as-administrator",
    "href": "new_pages/network_drives.html#rstudio-as-administrator",
    "title": "49  R on network drives",
    "section": "49.2 RStudio as administrator",
    "text": "49.2 RStudio as administrator\nWhen you click the RStudio icon to open RStudio, do so with a right-click. Depending on your machine, you may see an option to “Run as Administrator”. Otherwise, you may see an option to select Properties (then there should appear a window with the option “Compatibility”, and you can select a checkbox “Run as Administrator”).",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>R on network drives</span>"
    ]
  },
  {
    "objectID": "new_pages/network_drives.html#useful-commands",
    "href": "new_pages/network_drives.html#useful-commands",
    "title": "49  R on network drives",
    "section": "49.3 Useful commands",
    "text": "49.3 Useful commands\nBelow are some useful commands when trying to troubleshoot issues using R on network drives.\nYou can return the path(s) to package libraries that R is using. They will be listed in the order that R is using to install/load/search for packages. Thus, if you want R to use a different default library, you can switch the order of these paths (see below).\n\n# Find libraries\n.libPaths()                   # Your library paths, listed in order that R installs/searches. \n                              # Note: all libraries will be listed, but to install to some (e.g. C:) you \n                              # may need to be running RStudio as an administrator (it won't appear in the \n                              # install packages library drop-down menu) \n\nYou may want to switch the order of the package libraries used by R. For example if R is picking up a library location that begins with “\\\" and one that begins with a letter e.g. ”D:“. You can adjust the order of .libPaths() with the following code.\n\n# Switch order of libraries\n# this can effect the priority of R finding a package. E.g. you may want your C: library to be listed first\nmyPaths &lt;- .libPaths() # get the paths\nmyPaths &lt;- c(myPaths[2], myPaths[1]) # switch them\n.libPaths(myPaths) # reassign them\n\nIf you are having difficulties with R Markdown connecting to Pandoc, begin with this code to find out where RStudio thinks your Pandoc installation is.\n\n# Find Pandoc\nSys.getenv(\"RSTUDIO_PANDOC\")  # Find where RStudio thinks your Pandoc installation is\n\nIf you want to see which library a package is loading from, try the below code:\n\n# Find a package\n# gives first location of package (note order of your libraries)\nfind.package(\"rmarkdown\", lib.loc = NULL, quiet = FALSE, verbose = getOption(\"verbose\"))",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>R on network drives</span>"
    ]
  },
  {
    "objectID": "new_pages/network_drives.html#troubleshooting-common-errors",
    "href": "new_pages/network_drives.html#troubleshooting-common-errors",
    "title": "49  R on network drives",
    "section": "49.4 Troubleshooting common errors",
    "text": "49.4 Troubleshooting common errors\n“Failed to compile…tex in rmarkdown”\n\nCheck the installation of TinyTex, or install TinyTex to C: location. See the R basics page on how to install TinyTex.\n\n\n# check/install tinytex, to C: location\ntinytex::install_tinytex()\ntinytex:::is_tinytex() # should return TRUE (note three colons)\n\nInternet routines cannot be loaded\nFor example, Error in tools::startDynamicHelp() : internet routines cannot be loaded\n\nTry selecting 32-bit version from RStudio via Tools/Global Options.\n\nnote: if 32-bit version does not appear in menu, make sure you are not using RStudio v1.2.\n\n\nAlternatively, try uninstalling R and re-installing with different bit version (32 instead of 64)\n\nC: library does not appear as an option when I try to install packages manually\n\nRun RStudio as an administrator, then this option will appear.\n\nTo set-up RStudio to always run as administrator (advantageous when using an Rproject where you don’t click RStudio icon to open)… right-click the Rstudio icon\n\nThe image below shows how you can manually select the library to install a package to. This window appears when you open the Packages RStudio pane and click “Install”.\n\n\n\n\n\n\n\n\n\nPandoc 1 error\nIf you are getting “pandoc error 1” when knitting R Markdowns scripts on network drives:\n\nOf multiple library locations, have the one with a lettered drive listed first (see codes above)\n\nThe above solution worked when knitting on local drive but while on a networked internet connection\n\nSee more tips here: https://ciser.cornell.edu/rmarkdown-knit-to-html-word-pdf/\n\nPandoc Error 83\nThe error will look something like this: can't find file...rmarkdown...lua.... This means that it was unable to find this file.\nSee https://stackoverflow.com/questions/58830927/rmarkdown-unable-to-locate-lua-filter-when-knitting-to-word\nPossibilities:\n\nRmarkdown package is not installed\n\nRmarkdown package is not findable\n\nAn admin rights issue.\n\nIt is possible that R is not able to find the rmarkdown package file, so check which library the rmarkdown package lives (see code above). If the package is installed to a library that in inaccessible (e.g. starts with “\\\") consider manually moving it to C: or other named drive library. Be aware that the rmarkdown package has to be able to connect to TinyTex installation, so can not live in a library on a network drive.\nPandoc Error 61\nFor example: Error: pandoc document conversion failed with error 61 or Could not fetch...\n\nTry running RStudio as administrator (right click icon, select run as admin, see above instructions)\n\nAlso see if the specific package that was unable to be reached can be moved to C: library.\n\nLaTex error (see below)\nAn error like: ! Package pdftex.def Error: File 'cict_qm2_2020-06-29_files/figure-latex/unnamed-chunk-5-1.png' not found: using draft setting. or Error: LaTeX failed to compile file_name.tex.\n\nSee https://yihui.org/tinytex/r/#debugging for debugging tips.\n\nSee file_name.log for more info.\n\nPandoc Error 127\nThis could be a RAM (space) issue. Re-start your R session and try again.\nMapping network drives\nMapping a network drive can be risky. Consult with your IT department before attempting this.\nA tip borrowed from this forum discussion:\nHow does one open a file “through a mapped network drive”?\n\nFirst, you’ll need to know the network location you’re trying to access.\n\nNext, in the Windows file manager, you will need to right click on “This PC” on the right hand pane, and select “Map a network drive”.\n\nGo through the dialogue to define the network location from earlier as a lettered drive.\n\nNow you have two ways to get to the file you’re opening. Using the drive-letter path should work.\n\nError in install.packages()\nIf you get an error that includes mention of a “lock” directory, for example: Error in install.packages : ERROR: failed to lock directory...\nLook in your package library and you will see a folder whose name begins with “00LOCK”. Try the following tips:\n\nManually delete the “00LOCK” folder directory from your package library. Try installing the package again.\n\nYou can also try the command pacman::p_unlock() (you can also put this command in the Rprofile so it runs every time project opens.). Then try installing the package again. It may take several tries.\n\nTry running RStudio in Administrator mode, and try installing the packages one-by-one.\n\nIf all else fails, install the package to another library or folder (e.g. Temp) and then manually copy the package’s folder over to the desired library.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>R on network drives</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html",
    "href": "new_pages/data_table.html",
    "title": "50  Data Table",
    "section": "",
    "text": "50.1 Intro to data tables\nA data table is a 2-dimensional data structure like a data frame that allows complex grouping operations to be performed. The data.table syntax is structured so that operations can be performed on rows, columns and groups.\nThe structure is DT[i, j, by], separated by 3 parts; the i, j and by arguments. The i argument allows for subsetting of required rows, the j argument allows you to operate on columns and the by argument allows you operate on columns by groups.\nThis page will address the following topics:",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html#intro-to-data-tables",
    "href": "new_pages/data_table.html#intro-to-data-tables",
    "title": "50  Data Table",
    "section": "",
    "text": "Importing data and use of fread() and fwrite()\nSelecting and filtering rows using the i argument\nUsing helper functions %like%, %chin%, %between%\nSelecting and computing on columns using the j argument\nComputing by groups using the by argument\nAdding and updating data to data tables using :=",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html#load-packages-and-import-data",
    "href": "new_pages/data_table.html#load-packages-and-import-data",
    "title": "50  Data Table",
    "section": "50.2 Load packages and import data",
    "text": "50.2 Load packages and import data\n\nLoad packages\nUsing the p_load() function from pacman, we load (and install if necessary) packages required for this analysis.\n\npacman::p_load(\n  rio,        # to import data\n  data.table, # to group and clean data\n  tidyverse,  # allows use of pipe (%&gt;%) function in this chapter\n  here \n  ) \n\n\n\nImport data\nThis page will explore some of the core functions of data.table using the case linelist referenced throughout the handbook.\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to download the data to follow step-by-step, see instructions in the Download book and data page. The dataset is imported using the import() function from the rio package. See the page on Import and export for various ways to import data. From here we use data.table() to convert the data frame to a data table.\n\nlinelist &lt;- rio::import(here(\"data\", \"linelist_cleaned.xlsx\")) %&gt;% data.table()\n\nThe fread() function is used to directly import regular delimited files, such as .csv files, directly to a data table format. This function, and its counterpart, fwrite(), used for writing data.tables as regular delimited files are very fast and computationally efficient options for large databases.\nThe first 20 rows of linelist:\nBase R commands such as dim() that are used for data frames can also be used for data tables\n\ndim(linelist) #gives the number of rows and columns in the data table\n\n[1] 5888   30",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html#the-i-argument-selecting-and-filtering-rows",
    "href": "new_pages/data_table.html#the-i-argument-selecting-and-filtering-rows",
    "title": "50  Data Table",
    "section": "50.3 The i argument: selecting and filtering rows",
    "text": "50.3 The i argument: selecting and filtering rows\nRecalling the DT[i, j, by] structure, we can filter rows using either row numbers or logical expressions. The i argument is first; therefore, the syntax DT[i] or DT[i,] can be used.\nThe first example retrieves the first 5 rows of the data table, the second example subsets cases are 18 years or over, and the third example subsets cases 18 years old or over but not diagnosed at the Central Hospital:\n\nlinelist[1:5] #returns the 1st to 5th row\nlinelist[age &gt;= 18] #subsets cases are equal to or over 18 years\nlinelist[age &gt;= 18 & hospital != \"Central Hospital\"] #subsets cases equal to or over 18 years old but not diagnosed at the Central Hospital\n\nUsing .N in the i argument represents the total number of rows in the data table. This can be used to subset on the row numbers:\n\nlinelist[.N] #returns the last row\nlinelist[15:.N] #returns the 15th to the last row\n\n\nUsing helper functions for filtering\nData table uses helper functions that make subsetting rows easy. The %like% function is used to match a pattern in a column, %chin% is used to match a specific character, and the %between% helper function is used to match numeric columns within a prespecified range.\nIn the following examples we: * filter rows where the hospital variable contains “Hospital” * filter rows where the outcome is “Recover” or “Death” * filter rows in the age range 40-60\n\nlinelist[hospital %like% \"Hospital\"] #filter rows where the hospital variable contains “Hospital”\nlinelist[outcome %chin% c(\"Recover\", \"Death\")] #filter rows where the outcome is “Recover” or “Death”\nlinelist[age %between% c(40, 60)] #filter rows in the age range 40-60\n\n#%between% must take a vector of length 2, whereas %chin% can take vectors of length &gt;= 1",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html#the-j-argument-selecting-and-computing-on-columns",
    "href": "new_pages/data_table.html#the-j-argument-selecting-and-computing-on-columns",
    "title": "50  Data Table",
    "section": "50.4 The j argument: selecting and computing on columns",
    "text": "50.4 The j argument: selecting and computing on columns\nUsing the DT[i, j, by] structure, we can select columns using numbers or names. The j argument is second; therefore, the syntax DT[, j] is used. To facilitate computations on the j argument, the column is wrapped using either list() or .().\n\nSelecting columns\nThe first example retrieves the first, third and fifth columns of the data table, the second example selects all columns except the height, weight and gender columns. The third example uses the .() wrap to select the case_id and outcome columns.\n\nlinelist[ , c(1,3,5)]\nlinelist[ , -c(\"gender\", \"age\", \"wt_kg\", \"ht_cm\")]\nlinelist[ , list(case_id, outcome)] #linelist[ , .(case_id, outcome)] works just as well\n\n\n\nComputing on columns\nBy combining the i and j arguments it is possible to filter rows and compute on the columns. Using .N in the j argument also represents the total number of rows in the data table and can be useful to return the number of rows after row filtering.\nIn the following examples we: * Count the number of cases that stayed over 7 days in hospital * Calculate the mean age of the cases that died at the military hospital * Calculate the standard deviation, median, mean age of the cases that recovered at the central hospital\n\nlinelist[days_onset_hosp &gt; 7 , .N]\n\n[1] 189\n\nlinelist[hospital %like% \"Military\" & outcome %chin% \"Death\", .(mean(age, na.rm = T))] #na.rm = T removes N/A values\n\n        V1\n     &lt;num&gt;\n1: 15.9084\n\nlinelist[hospital == \"Central Hospital\" & outcome == \"Recover\", \n                 .(mean_age = mean(age, na.rm = T),\n                   median_age = median(age, na.rm = T),\n                   sd_age = sd(age, na.rm = T))] #this syntax does not use the helper functions but works just as well\n\n   mean_age median_age   sd_age\n      &lt;num&gt;      &lt;num&gt;    &lt;num&gt;\n1: 16.85185         14 12.93857\n\n\nRemember using the .() wrap in the j argument facilitates computation, returns a data table and allows for column naming.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html#the-by-argument-computing-by-groups",
    "href": "new_pages/data_table.html#the-by-argument-computing-by-groups",
    "title": "50  Data Table",
    "section": "50.5 The by argument: computing by groups",
    "text": "50.5 The by argument: computing by groups\nThe by argument is the third argument in the DT[i, j, by] structure. The by argument accepts both a character vector and the list() or .() syntax. Using the .() syntax in the by argument allows column renaming on the fly.\nIn the following examples we:\n* group the number of cases by hospital * in cases 18 years old or over, calculate the mean height and weight of cases according to gender and whether they recovered or died * in admissions that lasted over 7 days, count the number of cases according to the month they were admitted and the hospital they were admitted to\n\nlinelist[, .N, .(hospital)] #the number of cases by hospital\n\n                               hospital     N\n                                 &lt;char&gt; &lt;int&gt;\n1:                                Other   885\n2:                              Missing  1469\n3: St. Mark's Maternity Hospital (SMMH)   422\n4:                        Port Hospital  1762\n5:                    Military Hospital   896\n6:                     Central Hospital   454\n\nlinelist[age &gt; 18, .(mean_wt = mean(wt_kg, na.rm = T),\n                             mean_ht = mean(ht_cm, na.rm = T)), .(gender, outcome)] #NAs represent the categories where the data is missing\n\n   gender outcome  mean_wt  mean_ht\n   &lt;char&gt;  &lt;char&gt;    &lt;num&gt;    &lt;num&gt;\n1:      m Recover 71.90227 178.1977\n2:      f   Death 63.27273 159.9448\n3:      m   Death 71.61770 175.4726\n4:      f    &lt;NA&gt; 64.49375 162.7875\n5:      m    &lt;NA&gt; 72.65505 176.9686\n6:      f Recover 62.86498 159.2996\n7:   &lt;NA&gt; Recover 67.21429 175.2143\n8:   &lt;NA&gt;   Death 69.16667 170.7917\n9:   &lt;NA&gt;    &lt;NA&gt; 70.25000 175.5000\n\nlinelist[days_onset_hosp &gt; 7, .N, .(month = month(date_hospitalisation), hospital)]\n\n    month                             hospital     N\n    &lt;num&gt;                               &lt;char&gt; &lt;int&gt;\n 1:     5                    Military Hospital     3\n 2:     6                        Port Hospital     4\n 3:     7                        Port Hospital     8\n 4:     8 St. Mark's Maternity Hospital (SMMH)     5\n 5:     8                    Military Hospital     9\n 6:     8                                Other    10\n 7:     8                        Port Hospital    10\n 8:     9                        Port Hospital    28\n 9:     9                              Missing    27\n10:     9                     Central Hospital    10\n11:     9 St. Mark's Maternity Hospital (SMMH)     6\n12:    10                              Missing     2\n13:    10                    Military Hospital     3\n14:     3                        Port Hospital     1\n15:     4                    Military Hospital     1\n16:     5                                Other     2\n17:     5                     Central Hospital     1\n18:     5                              Missing     1\n19:     6                              Missing     7\n20:     6 St. Mark's Maternity Hospital (SMMH)     2\n21:     6                    Military Hospital     1\n22:     7                    Military Hospital     3\n23:     7                                Other     1\n24:     7                              Missing     2\n25:     7 St. Mark's Maternity Hospital (SMMH)     1\n26:     8                     Central Hospital     2\n27:     8                              Missing     6\n28:     9                                Other     9\n29:     9                    Military Hospital    11\n30:    10                        Port Hospital     3\n31:    10                                Other     4\n32:    10 St. Mark's Maternity Hospital (SMMH)     1\n33:    10                     Central Hospital     1\n34:    11                              Missing     2\n35:    11                        Port Hospital     1\n36:    12                        Port Hospital     1\n    month                             hospital     N\n\n\nData.table also allows the chaining expressions as follows:\n\nlinelist[, .N, .(hospital)][order(-N)][1:3] #1st selects all cases by hospital, 2nd orders the cases in descending order, 3rd subsets the 3 hospitals with the largest caseload\n\n            hospital     N\n              &lt;char&gt; &lt;int&gt;\n1:     Port Hospital  1762\n2:           Missing  1469\n3: Military Hospital   896\n\n\nIn these examples we are following the assumption that a row in the data table is equal to a new case, and so we can use the .N to represent the number of rows in the data table. Another useful function to represent the number of unique cases is uniqueN(), which returns the number of unique values in a given input. This is illustrated here:\n\nlinelist[, .(uniqueN(gender))] #remember .() in the j argument returns a data table\n\n      V1\n   &lt;int&gt;\n1:     3\n\n\nThe answer is 3, as the unique values in the gender column are m, f and N/A. Compare with the base R function unique(), which returns all the unique values in a given input:\n\nlinelist[, .(unique(gender))]\n\n       V1\n   &lt;char&gt;\n1:      m\n2:      f\n3:   &lt;NA&gt;\n\n\nTo find the number of unique cases in a given month we would write the following:\n\nlinelist[, .(uniqueN(case_id)), .(month = month(date_hospitalisation))]\n\n    month    V1\n    &lt;num&gt; &lt;int&gt;\n 1:     5    62\n 2:     6   100\n 3:     7   198\n 4:     8   509\n 5:     9  1170\n 6:    10  1228\n 7:    11   813\n 8:    12   576\n 9:     1   434\n10:     2   310\n11:     3   290\n12:     4   198",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html#adding-and-updating-to-data-tables",
    "href": "new_pages/data_table.html#adding-and-updating-to-data-tables",
    "title": "50  Data Table",
    "section": "50.6 Adding and updating to data tables",
    "text": "50.6 Adding and updating to data tables\nThe := operator is used to add or update data in a data table. Adding columns to your data table can be done in the following ways:\n\nlinelist[, adult := age &gt;= 18] #adds one column\nlinelist[, c(\"child\", \"wt_lbs\") := .(age &lt; 18, wt_kg*2.204)] #to add multiple columns requires c(\"\") and list() or .() syntax\nlinelist[, `:=` (bmi_in_range = (bmi &gt; 16 & bmi &lt; 40),\n                         no_infector_source_data = is.na(infector) | is.na(source))] #this method uses := as a functional operator `:=`\nlinelist[, adult := NULL] #deletes the column\n\nFurther complex aggregations are beyond the scope of this introductory chapter, but the idea is to provide a popular and viable alternative to dplyr for grouping and cleaning data. The data.table package is a great package that allows for neat and readable code.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  },
  {
    "objectID": "new_pages/data_table.html#resources",
    "href": "new_pages/data_table.html#resources",
    "title": "50  Data Table",
    "section": "50.7 Resources",
    "text": "50.7 Resources\nHere are some useful resources for more information: * https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html * https://github.com/Rdatatable/data.table * https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf * https://www.machinelearningplus.com/data-manipulation/datatable-in-r-complete-guide/ * https://www.datacamp.com/community/tutorials/data-table-r-tutorial\nYou can perform any summary function on grouped data; see the Cheat Sheet here for more info: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Data Table</span>"
    ]
  }
]